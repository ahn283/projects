{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>bond_rate</th>\n",
       "      <th>Dow</th>\n",
       "      <th>tech_index</th>\n",
       "      <th>health_index</th>\n",
       "      <th>consumer_dis_index</th>\n",
       "      <th>consumer_staples_index</th>\n",
       "      <th>energy_index</th>\n",
       "      <th>financial_index</th>\n",
       "      <th>...</th>\n",
       "      <th>real_estate_index</th>\n",
       "      <th>utilities_index</th>\n",
       "      <th>JPYUSD</th>\n",
       "      <th>CNYUSD</th>\n",
       "      <th>EURUSD</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>month</th>\n",
       "      <th>log_apple</th>\n",
       "      <th>log_tesla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>24.848475</td>\n",
       "      <td>15.114667</td>\n",
       "      <td>1.398</td>\n",
       "      <td>17050.750000</td>\n",
       "      <td>37.280865</td>\n",
       "      <td>59.642284</td>\n",
       "      <td>70.730659</td>\n",
       "      <td>39.916458</td>\n",
       "      <td>49.383343</td>\n",
       "      <td>16.221687</td>\n",
       "      <td>...</td>\n",
       "      <td>22.927450</td>\n",
       "      <td>33.919144</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.157577</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>2015-10-08</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.212796</td>\n",
       "      <td>2.715666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>25.443027</td>\n",
       "      <td>14.712667</td>\n",
       "      <td>1.406</td>\n",
       "      <td>17084.490234</td>\n",
       "      <td>37.443348</td>\n",
       "      <td>59.913666</td>\n",
       "      <td>70.812683</td>\n",
       "      <td>40.005421</td>\n",
       "      <td>49.063103</td>\n",
       "      <td>16.117661</td>\n",
       "      <td>...</td>\n",
       "      <td>22.889507</td>\n",
       "      <td>33.757278</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>1.128694</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.236442</td>\n",
       "      <td>2.688709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8784</th>\n",
       "      <td>25.325024</td>\n",
       "      <td>14.372000</td>\n",
       "      <td>1.400</td>\n",
       "      <td>17131.859375</td>\n",
       "      <td>37.488480</td>\n",
       "      <td>60.071239</td>\n",
       "      <td>71.149803</td>\n",
       "      <td>40.110588</td>\n",
       "      <td>48.429699</td>\n",
       "      <td>16.131527</td>\n",
       "      <td>...</td>\n",
       "      <td>23.033701</td>\n",
       "      <td>34.057907</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.157846</td>\n",
       "      <td>1.136997</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.231793</td>\n",
       "      <td>2.665282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8785</th>\n",
       "      <td>25.368139</td>\n",
       "      <td>14.616667</td>\n",
       "      <td>1.362</td>\n",
       "      <td>17081.890625</td>\n",
       "      <td>37.407238</td>\n",
       "      <td>59.318375</td>\n",
       "      <td>70.767113</td>\n",
       "      <td>39.867928</td>\n",
       "      <td>47.924412</td>\n",
       "      <td>16.006695</td>\n",
       "      <td>...</td>\n",
       "      <td>22.889507</td>\n",
       "      <td>33.980831</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.158403</td>\n",
       "      <td>1.135551</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.233494</td>\n",
       "      <td>2.682162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>25.009598</td>\n",
       "      <td>14.458667</td>\n",
       "      <td>1.280</td>\n",
       "      <td>16924.750000</td>\n",
       "      <td>37.335026</td>\n",
       "      <td>59.204567</td>\n",
       "      <td>70.047333</td>\n",
       "      <td>39.406887</td>\n",
       "      <td>48.337158</td>\n",
       "      <td>15.874916</td>\n",
       "      <td>...</td>\n",
       "      <td>22.889507</td>\n",
       "      <td>33.973110</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.157920</td>\n",
       "      <td>1.138498</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.219260</td>\n",
       "      <td>2.671294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Apple      Tesla  bond_rate           Dow  tech_index  health_index  \\\n",
       "8782  24.848475  15.114667      1.398  17050.750000   37.280865     59.642284   \n",
       "8783  25.443027  14.712667      1.406  17084.490234   37.443348     59.913666   \n",
       "8784  25.325024  14.372000      1.400  17131.859375   37.488480     60.071239   \n",
       "8785  25.368139  14.616667      1.362  17081.890625   37.407238     59.318375   \n",
       "8786  25.009598  14.458667      1.280  16924.750000   37.335026     59.204567   \n",
       "\n",
       "      consumer_dis_index  consumer_staples_index  energy_index  \\\n",
       "8782           70.730659               39.916458     49.383343   \n",
       "8783           70.812683               40.005421     49.063103   \n",
       "8784           71.149803               40.110588     48.429699   \n",
       "8785           70.767113               39.867928     47.924412   \n",
       "8786           70.047333               39.406887     48.337158   \n",
       "\n",
       "      financial_index  ...  real_estate_index  utilities_index    JPYUSD  \\\n",
       "8782        16.221687  ...          22.927450        33.919144  0.008339   \n",
       "8783        16.117661  ...          22.889507        33.757278  0.008338   \n",
       "8784        16.131527  ...          23.033701        34.057907  0.008321   \n",
       "8785        16.006695  ...          22.889507        33.980831  0.008333   \n",
       "8786        15.874916  ...          22.889507        33.973110  0.008353   \n",
       "\n",
       "        CNYUSD    EURUSD        date  country month log_apple  log_tesla  \n",
       "8782  0.157577  1.124000  2015-10-08       US    10  3.212796   2.715666  \n",
       "8783  0.157642  1.128694  2015-10-09       US    10  3.236442   2.688709  \n",
       "8784  0.157846  1.136997  2015-10-12       US    10  3.231793   2.665282  \n",
       "8785  0.158403  1.135551  2015-10-13       US    10  3.233494   2.682162  \n",
       "8786  0.157920  1.138498  2015-10-14       US    10  3.219260   2.671294  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocessing/datasets/tesla_apple_stock_price.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with MAE, MAPE\n",
    "\n",
    "def evaluation_mae_mape(y_real, y_pred):\n",
    "    mae = mean_absolute_error(y_real, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_real, y_pred)\n",
    "    metrics = pd.DataFrame({\"MAE\": [mae], \"MAPE\": [mape]})\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe for plotting\n",
    "def eval_df(y_real, y_pred):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"Target\"] = y_real\n",
    "    df[\"Pred\"] = y_pred\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 Y와 예측치 시각화\n",
    "def plot_prediction(Y_true_pred):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(Y_true_pred, linewidth=5, label=Y_true_pred.columns)\n",
    "    plt.xticks(fontsize=25, rotation=0)\n",
    "    plt.yticks(fontsize=25)\n",
    "    plt.xlabel('Index', fontname='serif', fontsize=28)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수화\n",
    "def evaluation_reg(Y_real, Y_pred):\n",
    "    MAE = mean_absolute_error(Y_real, Y_pred)\n",
    "    MSE = mean_squared_error(Y_real, Y_pred)\n",
    "    MAPE = mean_absolute_percentage_error(Y_real, Y_pred)\n",
    "    Score = pd.DataFrame([MAE, MSE, MAPE], index=['MAE', 'MSE', 'MAPE'], columns=['Score']).T\n",
    "    \n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test 모두의 검증 함수화\n",
    "def evaluation_reg_trte(Y_real_tr, Y_pred_tr, Y_real_te, Y_pred_te):\n",
    "    Score_tr = evaluation_reg(Y_real_tr, Y_pred_tr)\n",
    "    Score_te = evaluation_reg(Y_real_te, Y_pred_te)\n",
    "    Score_trte = pd.concat([Score_tr, Score_te], axis=0)\n",
    "    Score_trte.index = ['Train', 'Test']\n",
    "\n",
    "    return Score_trte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use simple machine learinng regression with rolling window - 20 days (1 month), 60 days (3 month), 252 days (1 year)\n",
    "- Extract rolling window datasets\n",
    "- Fit regression model and get co-efficient\n",
    "- Predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"stock prediction\")\n",
    "\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60 days datasets\n",
    "\n",
    "- target : Apple price\n",
    "\n",
    "- variables : 60 days variabels (60 x 16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1995 entries, 2015-10-08 to 2023-09-20\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Apple                   1995 non-null   float64\n",
      " 1   Tesla                   1995 non-null   float64\n",
      " 2   bond_rate               1995 non-null   float64\n",
      " 3   Dow                     1995 non-null   float64\n",
      " 4   tech_index              1995 non-null   float64\n",
      " 5   health_index            1995 non-null   float64\n",
      " 6   consumer_dis_index      1995 non-null   float64\n",
      " 7   consumer_staples_index  1995 non-null   float64\n",
      " 8   energy_index            1995 non-null   float64\n",
      " 9   financial_index         1995 non-null   float64\n",
      " 10  industrial_index        1995 non-null   float64\n",
      " 11  material_index          1995 non-null   float64\n",
      " 12  real_estate_index       1995 non-null   float64\n",
      " 13  utilities_index         1995 non-null   float64\n",
      " 14  JPYUSD                  1995 non-null   float64\n",
      " 15  CNYUSD                  1995 non-null   float64\n",
      " 16  EURUSD                  1995 non-null   float64\n",
      " 17  country                 1995 non-null   object \n",
      " 18  month                   1995 non-null   int64  \n",
      " 19  log_apple               1995 non-null   float64\n",
      " 20  log_tesla               1995 non-null   float64\n",
      "dtypes: float64(19), int64(1), object(1)\n",
      "memory usage: 342.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = data.set_index('date')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-10-08</th>\n",
       "      <td>24.848475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-09</th>\n",
       "      <td>25.443027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-12</th>\n",
       "      <td>25.325024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-13</th>\n",
       "      <td>25.368139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-14</th>\n",
       "      <td>25.009598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>175.740005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>175.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>177.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>179.070007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>175.490005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1995 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Apple\n",
       "date                  \n",
       "2015-10-08   24.848475\n",
       "2015-10-09   25.443027\n",
       "2015-10-12   25.325024\n",
       "2015-10-13   25.368139\n",
       "2015-10-14   25.009598\n",
       "...                ...\n",
       "2023-09-14  175.740005\n",
       "2023-09-15  175.009995\n",
       "2023-09-18  177.970001\n",
       "2023-09-19  179.070007\n",
       "2023-09-20  175.490005\n",
       "\n",
       "[1995 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df[['Apple']]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Apple', 'Tesla', 'country', 'log_apple', 'log_tesla'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bond_rate</th>\n",
       "      <th>Dow</th>\n",
       "      <th>tech_index</th>\n",
       "      <th>health_index</th>\n",
       "      <th>consumer_dis_index</th>\n",
       "      <th>consumer_staples_index</th>\n",
       "      <th>energy_index</th>\n",
       "      <th>financial_index</th>\n",
       "      <th>industrial_index</th>\n",
       "      <th>material_index</th>\n",
       "      <th>real_estate_index</th>\n",
       "      <th>utilities_index</th>\n",
       "      <th>JPYUSD</th>\n",
       "      <th>CNYUSD</th>\n",
       "      <th>EURUSD</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-10-08</th>\n",
       "      <td>1.398</td>\n",
       "      <td>17050.750000</td>\n",
       "      <td>37.280865</td>\n",
       "      <td>59.642284</td>\n",
       "      <td>70.730659</td>\n",
       "      <td>39.916458</td>\n",
       "      <td>49.383343</td>\n",
       "      <td>16.221687</td>\n",
       "      <td>45.808945</td>\n",
       "      <td>37.534855</td>\n",
       "      <td>22.927450</td>\n",
       "      <td>33.919144</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.157577</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-09</th>\n",
       "      <td>1.406</td>\n",
       "      <td>17084.490234</td>\n",
       "      <td>37.443348</td>\n",
       "      <td>59.913666</td>\n",
       "      <td>70.812683</td>\n",
       "      <td>40.005421</td>\n",
       "      <td>49.063103</td>\n",
       "      <td>16.117661</td>\n",
       "      <td>45.963940</td>\n",
       "      <td>37.517872</td>\n",
       "      <td>22.889507</td>\n",
       "      <td>33.757278</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>1.128694</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-12</th>\n",
       "      <td>1.400</td>\n",
       "      <td>17131.859375</td>\n",
       "      <td>37.488480</td>\n",
       "      <td>60.071239</td>\n",
       "      <td>71.149803</td>\n",
       "      <td>40.110588</td>\n",
       "      <td>48.429699</td>\n",
       "      <td>16.131527</td>\n",
       "      <td>45.972557</td>\n",
       "      <td>37.194946</td>\n",
       "      <td>23.033701</td>\n",
       "      <td>34.057907</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.157846</td>\n",
       "      <td>1.136997</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-13</th>\n",
       "      <td>1.362</td>\n",
       "      <td>17081.890625</td>\n",
       "      <td>37.407238</td>\n",
       "      <td>59.318375</td>\n",
       "      <td>70.767113</td>\n",
       "      <td>39.867928</td>\n",
       "      <td>47.924412</td>\n",
       "      <td>16.006695</td>\n",
       "      <td>45.481747</td>\n",
       "      <td>37.050476</td>\n",
       "      <td>22.889507</td>\n",
       "      <td>33.980831</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.158403</td>\n",
       "      <td>1.135551</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-14</th>\n",
       "      <td>1.280</td>\n",
       "      <td>16924.750000</td>\n",
       "      <td>37.335026</td>\n",
       "      <td>59.204567</td>\n",
       "      <td>70.047333</td>\n",
       "      <td>39.406887</td>\n",
       "      <td>48.337158</td>\n",
       "      <td>15.874916</td>\n",
       "      <td>44.990925</td>\n",
       "      <td>37.347916</td>\n",
       "      <td>22.889507</td>\n",
       "      <td>33.973110</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.157920</td>\n",
       "      <td>1.138498</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bond_rate           Dow  tech_index  health_index  \\\n",
       "date                                                            \n",
       "2015-10-08      1.398  17050.750000   37.280865     59.642284   \n",
       "2015-10-09      1.406  17084.490234   37.443348     59.913666   \n",
       "2015-10-12      1.400  17131.859375   37.488480     60.071239   \n",
       "2015-10-13      1.362  17081.890625   37.407238     59.318375   \n",
       "2015-10-14      1.280  16924.750000   37.335026     59.204567   \n",
       "\n",
       "            consumer_dis_index  consumer_staples_index  energy_index  \\\n",
       "date                                                                   \n",
       "2015-10-08           70.730659               39.916458     49.383343   \n",
       "2015-10-09           70.812683               40.005421     49.063103   \n",
       "2015-10-12           71.149803               40.110588     48.429699   \n",
       "2015-10-13           70.767113               39.867928     47.924412   \n",
       "2015-10-14           70.047333               39.406887     48.337158   \n",
       "\n",
       "            financial_index  industrial_index  material_index  \\\n",
       "date                                                            \n",
       "2015-10-08        16.221687         45.808945       37.534855   \n",
       "2015-10-09        16.117661         45.963940       37.517872   \n",
       "2015-10-12        16.131527         45.972557       37.194946   \n",
       "2015-10-13        16.006695         45.481747       37.050476   \n",
       "2015-10-14        15.874916         44.990925       37.347916   \n",
       "\n",
       "            real_estate_index  utilities_index    JPYUSD    CNYUSD    EURUSD  \\\n",
       "date                                                                           \n",
       "2015-10-08          22.927450        33.919144  0.008339  0.157577  1.124000   \n",
       "2015-10-09          22.889507        33.757278  0.008338  0.157642  1.128694   \n",
       "2015-10-12          23.033701        34.057907  0.008321  0.157846  1.136997   \n",
       "2015-10-13          22.889507        33.980831  0.008333  0.158403  1.135551   \n",
       "2015-10-14          22.889507        33.973110  0.008353  0.157920  1.138498   \n",
       "\n",
       "            month  \n",
       "date               \n",
       "2015-10-08     10  \n",
       "2015-10-09     10  \n",
       "2015-10-12     10  \n",
       "2015-10-13     10  \n",
       "2015-10-14     10  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>22.949333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>21.980772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>22.097004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11</th>\n",
       "      <td>22.454798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-12</th>\n",
       "      <td>22.780691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>175.740005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>175.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>177.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>179.070007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>175.490005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Apple\n",
       "date                  \n",
       "2016-01-06   22.949333\n",
       "2016-01-07   21.980772\n",
       "2016-01-08   22.097004\n",
       "2016-01-11   22.454798\n",
       "2016-01-12   22.780691\n",
       "...                ...\n",
       "2023-09-14  175.740005\n",
       "2023-09-15  175.009995\n",
       "2023-09-18  177.970001\n",
       "2023-09-19  179.070007\n",
       "2023-09-20  175.490005\n",
       "\n",
       "[1934 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = target[61:]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.39800000e+00, 1.70507500e+04, 3.72808647e+01, ...,\n",
       "         1.57577097e-01, 1.12399960e+00, 1.00000000e+01],\n",
       "        [1.40600002e+00, 1.70844902e+04, 3.74433479e+01, ...,\n",
       "         1.57641679e-01, 1.12869358e+00, 1.00000000e+01],\n",
       "        [1.39999998e+00, 1.71318594e+04, 3.74884796e+01, ...,\n",
       "         1.57845721e-01, 1.13699675e+00, 1.00000000e+01],\n",
       "        ...,\n",
       "        [1.79900002e+00, 1.76038691e+04, 3.94095612e+01, ...,\n",
       "         1.54444918e-01, 1.09280062e+00, 1.20000000e+01],\n",
       "        [1.75800002e+00, 1.74250293e+04, 3.88561668e+01, ...,\n",
       "         1.54292405e-01, 1.09339809e+00, 1.20000000e+01],\n",
       "        [1.73500001e+00, 1.71489395e+04, 3.83481293e+01, ...,\n",
       "         1.54232934e-01, 1.08539915e+00, 1.00000000e+00]],\n",
       "\n",
       "       [[1.40600002e+00, 1.70844902e+04, 3.74433479e+01, ...,\n",
       "         1.57641679e-01, 1.12869358e+00, 1.00000000e+01],\n",
       "        [1.39999998e+00, 1.71318594e+04, 3.74884796e+01, ...,\n",
       "         1.57845721e-01, 1.13699675e+00, 1.00000000e+01],\n",
       "        [1.36199999e+00, 1.70818906e+04, 3.74072380e+01, ...,\n",
       "         1.58403292e-01, 1.13555062e+00, 1.00000000e+01],\n",
       "        ...,\n",
       "        [1.75800002e+00, 1.74250293e+04, 3.88561668e+01, ...,\n",
       "         1.54292405e-01, 1.09339809e+00, 1.20000000e+01],\n",
       "        [1.73500001e+00, 1.71489395e+04, 3.83481293e+01, ...,\n",
       "         1.54232934e-01, 1.08539915e+00, 1.00000000e+00],\n",
       "        [1.72399998e+00, 1.71586602e+04, 3.82483292e+01, ...,\n",
       "         1.53247312e-01, 1.08275497e+00, 1.00000000e+00]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 60\n",
    "test = np.array(df.iloc[0:window, :])\n",
    "test = np.stack([test, df.iloc[0+1:window+1, :]])\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 60, 16)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 16)\n",
      "(2, 60, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1934, 60, 16)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 60\n",
    "\n",
    "for i in range(1934):\n",
    "    if i==0:\n",
    "        x0 = np.array(df.iloc[0+i:window+i, :])\n",
    "        print(x0.shape)\n",
    "    elif i==1:\n",
    "        x1 = np.array(df.iloc[0+i:window+i, :])\n",
    "        X = np.stack([x0, x1])\n",
    "        print(X.shape)\n",
    "    else:\n",
    "        xn = np.array([df.iloc[0+i:window+i, :]])\n",
    "        X = np.append(X, xn, axis=0)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 60, 16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-252:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2016-01-06    22.949333\n",
       "2016-01-07    21.980772\n",
       "2016-01-08    22.097004\n",
       "2016-01-11    22.454798\n",
       "2016-01-12    22.780691\n",
       "                ...    \n",
       "2016-12-30    26.982430\n",
       "2017-01-03    27.059311\n",
       "2017-01-04    27.029016\n",
       "2017-01-05    27.166471\n",
       "2017-01-06    27.469328\n",
       "Name: Apple, Length: 252, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:252, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:-252], X[-252:]\n",
    "y_train, y_test = y.iloc[:-252], y.iloc[-252:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 60, 16)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into 2 dimensional matrix\n",
    "X_train_re = X_train.reshape(1682,-1)\n",
    "X_test_re = X_test.reshape(252,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 960) (1682, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_re.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/23 10:13:44 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '89e7bac6f67742d3877df6e964bd0821', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2023/09/23 10:13:44 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'DataFrame' object has no attribute 'flatten'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_re, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-09-20</th>\n",
       "      <td>155.974686</td>\n",
       "      <td>151.446542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-21</th>\n",
       "      <td>152.813461</td>\n",
       "      <td>157.580399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-22</th>\n",
       "      <td>151.839249</td>\n",
       "      <td>153.768445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23</th>\n",
       "      <td>149.542847</td>\n",
       "      <td>142.942339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-26</th>\n",
       "      <td>149.880859</td>\n",
       "      <td>138.097029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>175.740005</td>\n",
       "      <td>208.778020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>175.009995</td>\n",
       "      <td>211.031291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>177.970001</td>\n",
       "      <td>212.261071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>179.070007</td>\n",
       "      <td>213.628473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>175.490005</td>\n",
       "      <td>205.525454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Target        Pred\n",
       "date                              \n",
       "2022-09-20  155.974686  151.446542\n",
       "2022-09-21  152.813461  157.580399\n",
       "2022-09-22  151.839249  153.768445\n",
       "2022-09-23  149.542847  142.942339\n",
       "2022-09-26  149.880859  138.097029\n",
       "...                ...         ...\n",
       "2023-09-14  175.740005  208.778020\n",
       "2023-09-15  175.009995  211.031291\n",
       "2023-09-18  177.970001  212.261071\n",
       "2023-09-19  179.070007  213.628473\n",
       "2023-09-20  175.490005  205.525454\n",
       "\n",
       "[252 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = pd.DataFrame()\n",
    "graph[\"Target\"] = y_test\n",
    "graph[\"Pred\"] = pred_lr\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='date'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df(y_test, pred_lr).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.464643</td>\n",
       "      <td>0.074195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE      MAPE\n",
       "0  12.464643  0.074195"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mae = mean_absolute_error(y_test, pred)\n",
    "# mape = mean_absolute_percentage_error(y_test, pred)\n",
    "# metrics = pd.DataFrame({\"MAE\": [mae], \"MAPE\": [mape]})\n",
    "# metrics\n",
    "\n",
    "eval_lr = evaluation_mae_mape(y_test, pred_lr)\n",
    "eval_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/23 10:15:17 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '03c1089a2fbf420095e8ce8d65a2d02e', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2023/09/23 10:15:17 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'DataFrame' object has no attribute 'flatten'\n",
      "/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf.fit(X_train_re, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = rf.predict(X_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='date'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df(y_test, pred_rf).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1682 entries, 2016-01-06 to 2022-09-19\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Apple   1682 non-null   float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 26.3 KB\n"
     ]
    }
   ],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.int64)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor To TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Linear(960, 480)\n",
    "        self.hidden_layer1 = nn.Linear(480, 128)\n",
    "        self.dropout_layer1 = nn.Dropout(0.2)\n",
    "        self.hidden_layer2 = nn.Linear(128, 64)\n",
    "        self.dropout_layer2 = nn.Dropout(0.2)\n",
    "        self.hidden_layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self._stacked_model = nn.Sequential(\n",
    "        #     nn.Linear(960, 480),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(480, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(64, 16),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(16, 1)\n",
    "        # )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.flatten(x)\n",
    "        out = self.relu(self.input_layer(x))\n",
    "        out = self.relu(self.hidden_layer1(out))\n",
    "        out = self.dropout_layer1(out)\n",
    "        out = self.relu(self.hidden_layer2(out))\n",
    "        out = self.dropout_layer2(out)\n",
    "        out = self.relu(self.hidden_layer3(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "        # pred = self._stacked_model(x)\n",
    "        # return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compile\n",
    "learing_rate = 0.01\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train loop\n",
    "def train_loop(train_loader, model, loss_fn, optimizer):\n",
    "    size = len(train_loader.dataset)\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred.to(torch.float32), y.to(torch.float32))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # reset gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        print(f'loss: {loss:>7f} [{current:>5d}/{size:5d}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test loop\n",
    "def test_loop(test_loader, model, loss_fn):\n",
    "    size = len(test_loader.dataset)\n",
    "    num_batches = len(test_loader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():   # no gradient calculation\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 9848.121094 [    0/ 1682]\n",
      "loss: 8546408.000000 [   10/ 1682]\n",
      "loss: 18513.238281 [   20/ 1682]\n",
      "loss: 1809.257080 [   30/ 1682]\n",
      "loss: 20137.326172 [   40/ 1682]\n",
      "loss: 8146.293945 [   50/ 1682]\n",
      "loss: 16365.025391 [   60/ 1682]\n",
      "loss: 7323.511719 [   70/ 1682]\n",
      "loss: 10940.121094 [   80/ 1682]\n",
      "loss: 3484.803955 [   90/ 1682]\n",
      "loss: 4744.263184 [  100/ 1682]\n",
      "loss: 10258.448242 [  110/ 1682]\n",
      "loss: 6227.796875 [  120/ 1682]\n",
      "loss: 3369.371094 [  130/ 1682]\n",
      "loss: 3295.960938 [  140/ 1682]\n",
      "loss: 1655.609619 [  150/ 1682]\n",
      "loss: 1764.120361 [  160/ 1682]\n",
      "loss: 170.157257 [  170/ 1682]\n",
      "loss: 557.225952 [  180/ 1682]\n",
      "loss: 694.328735 [  190/ 1682]\n",
      "loss: 281.516418 [  200/ 1682]\n",
      "loss: 2516.312988 [  210/ 1682]\n",
      "loss: 387.305115 [  220/ 1682]\n",
      "loss: 1105.297607 [  230/ 1682]\n",
      "loss: 989.682983 [  240/ 1682]\n",
      "loss: 1151.965454 [  250/ 1682]\n",
      "loss: 1166.311523 [  260/ 1682]\n",
      "loss: 474.161438 [  270/ 1682]\n",
      "loss: 1065.322021 [  280/ 1682]\n",
      "loss: 568.987915 [  290/ 1682]\n",
      "loss: 940.696411 [  300/ 1682]\n",
      "loss: 513.827698 [  310/ 1682]\n",
      "loss: 570.304993 [  320/ 1682]\n",
      "loss: 96.888298 [  330/ 1682]\n",
      "loss: 107.912682 [  340/ 1682]\n",
      "loss: 276.159729 [  350/ 1682]\n",
      "loss: 315.978577 [  360/ 1682]\n",
      "loss: 431.220398 [  370/ 1682]\n",
      "loss: 320.319763 [  380/ 1682]\n",
      "loss: 581.088318 [  390/ 1682]\n",
      "loss: 140.010727 [  400/ 1682]\n",
      "loss: 267.410095 [  410/ 1682]\n",
      "loss: 262.251770 [  420/ 1682]\n",
      "loss: 378.398590 [  430/ 1682]\n",
      "loss: 556.644409 [  440/ 1682]\n",
      "loss: 459.262604 [  450/ 1682]\n",
      "loss: 173.236389 [  460/ 1682]\n",
      "loss: 465.698334 [  470/ 1682]\n",
      "loss: 582.602722 [  480/ 1682]\n",
      "loss: 305.827118 [  490/ 1682]\n",
      "loss: 483.590027 [  500/ 1682]\n",
      "loss: 399.060486 [  510/ 1682]\n",
      "loss: 92.935898 [  520/ 1682]\n",
      "loss: 380.079468 [  530/ 1682]\n",
      "loss: 245.208893 [  540/ 1682]\n",
      "loss: 614.159668 [  550/ 1682]\n",
      "loss: 478.559631 [  560/ 1682]\n",
      "loss: 165.176483 [  570/ 1682]\n",
      "loss: 529.419556 [  580/ 1682]\n",
      "loss: 761.499573 [  590/ 1682]\n",
      "loss: 426.244934 [  600/ 1682]\n",
      "loss: 216.986481 [  610/ 1682]\n",
      "loss: 521.115906 [  620/ 1682]\n",
      "loss: 345.842377 [  630/ 1682]\n",
      "loss: 174.125381 [  640/ 1682]\n",
      "loss: 222.279633 [  650/ 1682]\n",
      "loss: 439.596283 [  660/ 1682]\n",
      "loss: 267.910950 [  670/ 1682]\n",
      "loss: 209.946808 [  680/ 1682]\n",
      "loss: 1088.124390 [  690/ 1682]\n",
      "loss: 355.778625 [  700/ 1682]\n",
      "loss: 85.494614 [  710/ 1682]\n",
      "loss: 104.166237 [  720/ 1682]\n",
      "loss: 328.991241 [  730/ 1682]\n",
      "loss: 69.409348 [  740/ 1682]\n",
      "loss: 179.680893 [  750/ 1682]\n",
      "loss: 54.964668 [  760/ 1682]\n",
      "loss: 272.490662 [  770/ 1682]\n",
      "loss: 467.953033 [  780/ 1682]\n",
      "loss: 97.027267 [  790/ 1682]\n",
      "loss: 203.753098 [  800/ 1682]\n",
      "loss: 359.852020 [  810/ 1682]\n",
      "loss: 133.537857 [  820/ 1682]\n",
      "loss: 365.082764 [  830/ 1682]\n",
      "loss: 585.103149 [  840/ 1682]\n",
      "loss: 109.772202 [  850/ 1682]\n",
      "loss: 159.357346 [  860/ 1682]\n",
      "loss: 445.317139 [  870/ 1682]\n",
      "loss: 414.211487 [  880/ 1682]\n",
      "loss: 108.067322 [  890/ 1682]\n",
      "loss: 456.771179 [  900/ 1682]\n",
      "loss: 317.303864 [  910/ 1682]\n",
      "loss: 156.621689 [  920/ 1682]\n",
      "loss: 465.805023 [  930/ 1682]\n",
      "loss: 822.955566 [  940/ 1682]\n",
      "loss: 298.142181 [  950/ 1682]\n",
      "loss: 393.181458 [  960/ 1682]\n",
      "loss: 207.364944 [  970/ 1682]\n",
      "loss: 532.794922 [  980/ 1682]\n",
      "loss: 364.477631 [  990/ 1682]\n",
      "loss: 101.889145 [ 1000/ 1682]\n",
      "loss: 789.903442 [ 1010/ 1682]\n",
      "loss: 246.814667 [ 1020/ 1682]\n",
      "loss: 1050.038452 [ 1030/ 1682]\n",
      "loss: 454.071472 [ 1040/ 1682]\n",
      "loss: 465.853424 [ 1050/ 1682]\n",
      "loss: 403.107086 [ 1060/ 1682]\n",
      "loss: 1092.832275 [ 1070/ 1682]\n",
      "loss: 479.400208 [ 1080/ 1682]\n",
      "loss: 760.350708 [ 1090/ 1682]\n",
      "loss: 226.651581 [ 1100/ 1682]\n",
      "loss: 315.229309 [ 1110/ 1682]\n",
      "loss: 794.617676 [ 1120/ 1682]\n",
      "loss: 480.623444 [ 1130/ 1682]\n",
      "loss: 2121.728027 [ 1140/ 1682]\n",
      "loss: 4434.540039 [ 1150/ 1682]\n",
      "loss: 1614.845337 [ 1160/ 1682]\n",
      "loss: 2091.103027 [ 1170/ 1682]\n",
      "loss: 2366.035645 [ 1180/ 1682]\n",
      "loss: 1494.129028 [ 1190/ 1682]\n",
      "loss: 563.295471 [ 1200/ 1682]\n",
      "loss: 912.667664 [ 1210/ 1682]\n",
      "loss: 2404.169922 [ 1220/ 1682]\n",
      "loss: 1448.989380 [ 1230/ 1682]\n",
      "loss: 1967.694702 [ 1240/ 1682]\n",
      "loss: 706.067139 [ 1250/ 1682]\n",
      "loss: 336.510681 [ 1260/ 1682]\n",
      "loss: 3291.626221 [ 1270/ 1682]\n",
      "loss: 2266.592529 [ 1280/ 1682]\n",
      "loss: 1257.402222 [ 1290/ 1682]\n",
      "loss: 1632.003296 [ 1300/ 1682]\n",
      "loss: 1287.941650 [ 1310/ 1682]\n",
      "loss: 4177.725098 [ 1320/ 1682]\n",
      "loss: 3089.469238 [ 1330/ 1682]\n",
      "loss: 427.905914 [ 1340/ 1682]\n",
      "loss: 182.282944 [ 1350/ 1682]\n",
      "loss: 3881.060547 [ 1360/ 1682]\n",
      "loss: 697.350037 [ 1370/ 1682]\n",
      "loss: 1328.117065 [ 1380/ 1682]\n",
      "loss: 3423.293701 [ 1390/ 1682]\n",
      "loss: 916.654419 [ 1400/ 1682]\n",
      "loss: 1261.090576 [ 1410/ 1682]\n",
      "loss: 3252.541016 [ 1420/ 1682]\n",
      "loss: 845.120789 [ 1430/ 1682]\n",
      "loss: 2474.573730 [ 1440/ 1682]\n",
      "loss: 861.601685 [ 1450/ 1682]\n",
      "loss: 1062.300537 [ 1460/ 1682]\n",
      "loss: 5608.511719 [ 1470/ 1682]\n",
      "loss: 6123.995605 [ 1480/ 1682]\n",
      "loss: 4326.624023 [ 1490/ 1682]\n",
      "loss: 2086.403320 [ 1500/ 1682]\n",
      "loss: 11597.981445 [ 1510/ 1682]\n",
      "loss: 3305.769531 [ 1520/ 1682]\n",
      "loss: 7188.259277 [ 1530/ 1682]\n",
      "loss: 6658.880371 [ 1540/ 1682]\n",
      "loss: 4416.289551 [ 1550/ 1682]\n",
      "loss: 3336.596191 [ 1560/ 1682]\n",
      "loss: 5526.418945 [ 1570/ 1682]\n",
      "loss: 994.766724 [ 1580/ 1682]\n",
      "loss: 1418.785522 [ 1590/ 1682]\n",
      "loss: 3821.697754 [ 1600/ 1682]\n",
      "loss: 3705.333252 [ 1610/ 1682]\n",
      "loss: 4007.455811 [ 1620/ 1682]\n",
      "loss: 3481.821533 [ 1630/ 1682]\n",
      "loss: 5230.041992 [ 1640/ 1682]\n",
      "loss: 2259.750488 [ 1650/ 1682]\n",
      "loss: 6494.689941 [ 1660/ 1682]\n",
      "loss: 5046.742676 [ 1670/ 1682]\n",
      "loss: 1709.549561 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 6426.463905 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1579.276001 [    0/ 1682]\n",
      "loss: 268.412292 [   10/ 1682]\n",
      "loss: 744.889099 [   20/ 1682]\n",
      "loss: 97.203606 [   30/ 1682]\n",
      "loss: 174.734192 [   40/ 1682]\n",
      "loss: 310.769562 [   50/ 1682]\n",
      "loss: 368.817078 [   60/ 1682]\n",
      "loss: 368.049835 [   70/ 1682]\n",
      "loss: 210.491058 [   80/ 1682]\n",
      "loss: 213.990555 [   90/ 1682]\n",
      "loss: 178.933136 [  100/ 1682]\n",
      "loss: 198.608688 [  110/ 1682]\n",
      "loss: 61.148865 [  120/ 1682]\n",
      "loss: 241.327103 [  130/ 1682]\n",
      "loss: 221.969696 [  140/ 1682]\n",
      "loss: 132.261673 [  150/ 1682]\n",
      "loss: 192.495392 [  160/ 1682]\n",
      "loss: 234.199677 [  170/ 1682]\n",
      "loss: 265.657379 [  180/ 1682]\n",
      "loss: 358.799072 [  190/ 1682]\n",
      "loss: 273.346222 [  200/ 1682]\n",
      "loss: 207.129807 [  210/ 1682]\n",
      "loss: 243.812668 [  220/ 1682]\n",
      "loss: 123.540039 [  230/ 1682]\n",
      "loss: 197.823853 [  240/ 1682]\n",
      "loss: 183.291428 [  250/ 1682]\n",
      "loss: 302.445068 [  260/ 1682]\n",
      "loss: 255.827835 [  270/ 1682]\n",
      "loss: 155.016571 [  280/ 1682]\n",
      "loss: 264.328827 [  290/ 1682]\n",
      "loss: 267.663177 [  300/ 1682]\n",
      "loss: 349.588501 [  310/ 1682]\n",
      "loss: 438.141846 [  320/ 1682]\n",
      "loss: 244.300446 [  330/ 1682]\n",
      "loss: 402.739319 [  340/ 1682]\n",
      "loss: 384.364807 [  350/ 1682]\n",
      "loss: 323.725983 [  360/ 1682]\n",
      "loss: 150.694168 [  370/ 1682]\n",
      "loss: 415.866547 [  380/ 1682]\n",
      "loss: 397.998169 [  390/ 1682]\n",
      "loss: 388.474426 [  400/ 1682]\n",
      "loss: 466.653564 [  410/ 1682]\n",
      "loss: 195.688614 [  420/ 1682]\n",
      "loss: 297.345886 [  430/ 1682]\n",
      "loss: 673.495056 [  440/ 1682]\n",
      "loss: 330.712708 [  450/ 1682]\n",
      "loss: 339.727356 [  460/ 1682]\n",
      "loss: 582.246277 [  470/ 1682]\n",
      "loss: 344.475281 [  480/ 1682]\n",
      "loss: 604.922424 [  490/ 1682]\n",
      "loss: 297.591248 [  500/ 1682]\n",
      "loss: 605.354370 [  510/ 1682]\n",
      "loss: 218.773590 [  520/ 1682]\n",
      "loss: 723.069031 [  530/ 1682]\n",
      "loss: 149.129196 [  540/ 1682]\n",
      "loss: 389.494354 [  550/ 1682]\n",
      "loss: 709.828735 [  560/ 1682]\n",
      "loss: 69.749336 [  570/ 1682]\n",
      "loss: 661.750916 [  580/ 1682]\n",
      "loss: 702.180298 [  590/ 1682]\n",
      "loss: 777.928345 [  600/ 1682]\n",
      "loss: 855.188660 [  610/ 1682]\n",
      "loss: 609.541870 [  620/ 1682]\n",
      "loss: 657.363892 [  630/ 1682]\n",
      "loss: 311.271271 [  640/ 1682]\n",
      "loss: 459.848083 [  650/ 1682]\n",
      "loss: 518.509460 [  660/ 1682]\n",
      "loss: 450.177429 [  670/ 1682]\n",
      "loss: 878.003784 [  680/ 1682]\n",
      "loss: 1147.362061 [  690/ 1682]\n",
      "loss: 324.977356 [  700/ 1682]\n",
      "loss: 477.123932 [  710/ 1682]\n",
      "loss: 386.480316 [  720/ 1682]\n",
      "loss: 447.876556 [  730/ 1682]\n",
      "loss: 58.476036 [  740/ 1682]\n",
      "loss: 316.183044 [  750/ 1682]\n",
      "loss: 171.977936 [  760/ 1682]\n",
      "loss: 573.655640 [  770/ 1682]\n",
      "loss: 417.806305 [  780/ 1682]\n",
      "loss: 772.275635 [  790/ 1682]\n",
      "loss: 454.179626 [  800/ 1682]\n",
      "loss: 590.540955 [  810/ 1682]\n",
      "loss: 523.460449 [  820/ 1682]\n",
      "loss: 59.740303 [  830/ 1682]\n",
      "loss: 583.479187 [  840/ 1682]\n",
      "loss: 744.574585 [  850/ 1682]\n",
      "loss: 115.607521 [  860/ 1682]\n",
      "loss: 437.118317 [  870/ 1682]\n",
      "loss: 284.990906 [  880/ 1682]\n",
      "loss: 353.318451 [  890/ 1682]\n",
      "loss: 293.292084 [  900/ 1682]\n",
      "loss: 973.256042 [  910/ 1682]\n",
      "loss: 1072.977173 [  920/ 1682]\n",
      "loss: 572.198853 [  930/ 1682]\n",
      "loss: 413.987396 [  940/ 1682]\n",
      "loss: 1303.884155 [  950/ 1682]\n",
      "loss: 800.175659 [  960/ 1682]\n",
      "loss: 94.447807 [  970/ 1682]\n",
      "loss: 1333.447144 [  980/ 1682]\n",
      "loss: 1591.191528 [  990/ 1682]\n",
      "loss: 1082.933960 [ 1000/ 1682]\n",
      "loss: 587.509216 [ 1010/ 1682]\n",
      "loss: 70.779793 [ 1020/ 1682]\n",
      "loss: 1082.171631 [ 1030/ 1682]\n",
      "loss: 645.867615 [ 1040/ 1682]\n",
      "loss: 1372.313965 [ 1050/ 1682]\n",
      "loss: 186.762283 [ 1060/ 1682]\n",
      "loss: 1220.376099 [ 1070/ 1682]\n",
      "loss: 1288.087646 [ 1080/ 1682]\n",
      "loss: 1531.456787 [ 1090/ 1682]\n",
      "loss: 1011.303589 [ 1100/ 1682]\n",
      "loss: 1657.231689 [ 1110/ 1682]\n",
      "loss: 2283.734619 [ 1120/ 1682]\n",
      "loss: 1022.895020 [ 1130/ 1682]\n",
      "loss: 4373.981934 [ 1140/ 1682]\n",
      "loss: 3489.890625 [ 1150/ 1682]\n",
      "loss: 2780.000488 [ 1160/ 1682]\n",
      "loss: 3901.209473 [ 1170/ 1682]\n",
      "loss: 1782.679688 [ 1180/ 1682]\n",
      "loss: 3173.847168 [ 1190/ 1682]\n",
      "loss: 3306.964355 [ 1200/ 1682]\n",
      "loss: 1579.301636 [ 1210/ 1682]\n",
      "loss: 3170.847168 [ 1220/ 1682]\n",
      "loss: 750.618896 [ 1230/ 1682]\n",
      "loss: 3172.859863 [ 1240/ 1682]\n",
      "loss: 1823.025757 [ 1250/ 1682]\n",
      "loss: 2127.706543 [ 1260/ 1682]\n",
      "loss: 4349.760254 [ 1270/ 1682]\n",
      "loss: 2012.189697 [ 1280/ 1682]\n",
      "loss: 1627.279053 [ 1290/ 1682]\n",
      "loss: 619.818848 [ 1300/ 1682]\n",
      "loss: 5898.718262 [ 1310/ 1682]\n",
      "loss: 3826.254639 [ 1320/ 1682]\n",
      "loss: 3357.882324 [ 1330/ 1682]\n",
      "loss: 4212.596680 [ 1340/ 1682]\n",
      "loss: 7472.439941 [ 1350/ 1682]\n",
      "loss: 5167.010254 [ 1360/ 1682]\n",
      "loss: 3415.318359 [ 1370/ 1682]\n",
      "loss: 3753.434082 [ 1380/ 1682]\n",
      "loss: 2782.935791 [ 1390/ 1682]\n",
      "loss: 1105.270264 [ 1400/ 1682]\n",
      "loss: 2570.707764 [ 1410/ 1682]\n",
      "loss: 103.524147 [ 1420/ 1682]\n",
      "loss: 1863.040039 [ 1430/ 1682]\n",
      "loss: 1855.024780 [ 1440/ 1682]\n",
      "loss: 7355.285156 [ 1450/ 1682]\n",
      "loss: 7031.604004 [ 1460/ 1682]\n",
      "loss: 1998.718506 [ 1470/ 1682]\n",
      "loss: 6813.997070 [ 1480/ 1682]\n",
      "loss: 7546.704102 [ 1490/ 1682]\n",
      "loss: 7076.110352 [ 1500/ 1682]\n",
      "loss: 9265.384766 [ 1510/ 1682]\n",
      "loss: 7436.883789 [ 1520/ 1682]\n",
      "loss: 606.147461 [ 1530/ 1682]\n",
      "loss: 3891.205811 [ 1540/ 1682]\n",
      "loss: 1892.812256 [ 1550/ 1682]\n",
      "loss: 9769.159180 [ 1560/ 1682]\n",
      "loss: 315.529480 [ 1570/ 1682]\n",
      "loss: 3740.511719 [ 1580/ 1682]\n",
      "loss: 2795.373047 [ 1590/ 1682]\n",
      "loss: 1850.759033 [ 1600/ 1682]\n",
      "loss: 3817.930176 [ 1610/ 1682]\n",
      "loss: 5551.399902 [ 1620/ 1682]\n",
      "loss: 2936.384766 [ 1630/ 1682]\n",
      "loss: 1905.336182 [ 1640/ 1682]\n",
      "loss: 5897.580078 [ 1650/ 1682]\n",
      "loss: 3090.232910 [ 1660/ 1682]\n",
      "loss: 4214.086426 [ 1670/ 1682]\n",
      "loss: 297.829407 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4634.172765 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1197.112549 [    0/ 1682]\n",
      "loss: 793.742371 [   10/ 1682]\n",
      "loss: 148.761780 [   20/ 1682]\n",
      "loss: 1158.568237 [   30/ 1682]\n",
      "loss: 209.500458 [   40/ 1682]\n",
      "loss: 394.972321 [   50/ 1682]\n",
      "loss: 1021.459961 [   60/ 1682]\n",
      "loss: 1261.304321 [   70/ 1682]\n",
      "loss: 132.519913 [   80/ 1682]\n",
      "loss: 204.339508 [   90/ 1682]\n",
      "loss: 51.601898 [  100/ 1682]\n",
      "loss: 81.321320 [  110/ 1682]\n",
      "loss: 113.541771 [  120/ 1682]\n",
      "loss: 91.494911 [  130/ 1682]\n",
      "loss: 155.021194 [  140/ 1682]\n",
      "loss: 208.434235 [  150/ 1682]\n",
      "loss: 94.463150 [  160/ 1682]\n",
      "loss: 85.137619 [  170/ 1682]\n",
      "loss: 51.003029 [  180/ 1682]\n",
      "loss: 142.316589 [  190/ 1682]\n",
      "loss: 182.687714 [  200/ 1682]\n",
      "loss: 185.923386 [  210/ 1682]\n",
      "loss: 58.962452 [  220/ 1682]\n",
      "loss: 107.268661 [  230/ 1682]\n",
      "loss: 143.613235 [  240/ 1682]\n",
      "loss: 46.311844 [  250/ 1682]\n",
      "loss: 272.572540 [  260/ 1682]\n",
      "loss: 119.710899 [  270/ 1682]\n",
      "loss: 118.815941 [  280/ 1682]\n",
      "loss: 270.173462 [  290/ 1682]\n",
      "loss: 163.345367 [  300/ 1682]\n",
      "loss: 97.730995 [  310/ 1682]\n",
      "loss: 61.866310 [  320/ 1682]\n",
      "loss: 70.636490 [  330/ 1682]\n",
      "loss: 76.297928 [  340/ 1682]\n",
      "loss: 138.200180 [  350/ 1682]\n",
      "loss: 127.141586 [  360/ 1682]\n",
      "loss: 101.407272 [  370/ 1682]\n",
      "loss: 94.965805 [  380/ 1682]\n",
      "loss: 87.775528 [  390/ 1682]\n",
      "loss: 80.895645 [  400/ 1682]\n",
      "loss: 123.728432 [  410/ 1682]\n",
      "loss: 96.585632 [  420/ 1682]\n",
      "loss: 82.688187 [  430/ 1682]\n",
      "loss: 26.371044 [  440/ 1682]\n",
      "loss: 128.330765 [  450/ 1682]\n",
      "loss: 168.945404 [  460/ 1682]\n",
      "loss: 131.881134 [  470/ 1682]\n",
      "loss: 105.602219 [  480/ 1682]\n",
      "loss: 48.772919 [  490/ 1682]\n",
      "loss: 27.407166 [  500/ 1682]\n",
      "loss: 81.523354 [  510/ 1682]\n",
      "loss: 93.654694 [  520/ 1682]\n",
      "loss: 98.372498 [  530/ 1682]\n",
      "loss: 42.987175 [  540/ 1682]\n",
      "loss: 67.674896 [  550/ 1682]\n",
      "loss: 81.711037 [  560/ 1682]\n",
      "loss: 61.047291 [  570/ 1682]\n",
      "loss: 70.852417 [  580/ 1682]\n",
      "loss: 144.933472 [  590/ 1682]\n",
      "loss: 185.207001 [  600/ 1682]\n",
      "loss: 183.255325 [  610/ 1682]\n",
      "loss: 85.389122 [  620/ 1682]\n",
      "loss: 33.909336 [  630/ 1682]\n",
      "loss: 18.736858 [  640/ 1682]\n",
      "loss: 101.448273 [  650/ 1682]\n",
      "loss: 60.118019 [  660/ 1682]\n",
      "loss: 82.337402 [  670/ 1682]\n",
      "loss: 100.908005 [  680/ 1682]\n",
      "loss: 212.271271 [  690/ 1682]\n",
      "loss: 123.746262 [  700/ 1682]\n",
      "loss: 171.750214 [  710/ 1682]\n",
      "loss: 86.805466 [  720/ 1682]\n",
      "loss: 52.057404 [  730/ 1682]\n",
      "loss: 62.232079 [  740/ 1682]\n",
      "loss: 30.247028 [  750/ 1682]\n",
      "loss: 106.678101 [  760/ 1682]\n",
      "loss: 204.925018 [  770/ 1682]\n",
      "loss: 175.260941 [  780/ 1682]\n",
      "loss: 146.512970 [  790/ 1682]\n",
      "loss: 291.569580 [  800/ 1682]\n",
      "loss: 217.049606 [  810/ 1682]\n",
      "loss: 144.346039 [  820/ 1682]\n",
      "loss: 144.557816 [  830/ 1682]\n",
      "loss: 61.129749 [  840/ 1682]\n",
      "loss: 98.795418 [  850/ 1682]\n",
      "loss: 82.214722 [  860/ 1682]\n",
      "loss: 44.552673 [  870/ 1682]\n",
      "loss: 184.757248 [  880/ 1682]\n",
      "loss: 172.451752 [  890/ 1682]\n",
      "loss: 103.364609 [  900/ 1682]\n",
      "loss: 36.049706 [  910/ 1682]\n",
      "loss: 89.657776 [  920/ 1682]\n",
      "loss: 74.344254 [  930/ 1682]\n",
      "loss: 328.461487 [  940/ 1682]\n",
      "loss: 131.445862 [  950/ 1682]\n",
      "loss: 236.043213 [  960/ 1682]\n",
      "loss: 280.246368 [  970/ 1682]\n",
      "loss: 388.537689 [  980/ 1682]\n",
      "loss: 144.311813 [  990/ 1682]\n",
      "loss: 1042.439941 [ 1000/ 1682]\n",
      "loss: 583.428101 [ 1010/ 1682]\n",
      "loss: 572.723999 [ 1020/ 1682]\n",
      "loss: 594.928162 [ 1030/ 1682]\n",
      "loss: 417.811188 [ 1040/ 1682]\n",
      "loss: 226.922699 [ 1050/ 1682]\n",
      "loss: 215.882782 [ 1060/ 1682]\n",
      "loss: 518.053589 [ 1070/ 1682]\n",
      "loss: 1184.544678 [ 1080/ 1682]\n",
      "loss: 1224.461670 [ 1090/ 1682]\n",
      "loss: 1396.314697 [ 1100/ 1682]\n",
      "loss: 1183.447021 [ 1110/ 1682]\n",
      "loss: 1351.963623 [ 1120/ 1682]\n",
      "loss: 891.431274 [ 1130/ 1682]\n",
      "loss: 405.062195 [ 1140/ 1682]\n",
      "loss: 1869.093018 [ 1150/ 1682]\n",
      "loss: 1566.295776 [ 1160/ 1682]\n",
      "loss: 864.795288 [ 1170/ 1682]\n",
      "loss: 339.782043 [ 1180/ 1682]\n",
      "loss: 976.055176 [ 1190/ 1682]\n",
      "loss: 533.955505 [ 1200/ 1682]\n",
      "loss: 992.363281 [ 1210/ 1682]\n",
      "loss: 1772.839478 [ 1220/ 1682]\n",
      "loss: 436.368713 [ 1230/ 1682]\n",
      "loss: 932.790344 [ 1240/ 1682]\n",
      "loss: 363.149017 [ 1250/ 1682]\n",
      "loss: 1543.558228 [ 1260/ 1682]\n",
      "loss: 358.506744 [ 1270/ 1682]\n",
      "loss: 14973.885742 [ 1280/ 1682]\n",
      "loss: 2039.380859 [ 1290/ 1682]\n",
      "loss: 4011.069824 [ 1300/ 1682]\n",
      "loss: 6514.214844 [ 1310/ 1682]\n",
      "loss: 5228.575684 [ 1320/ 1682]\n",
      "loss: 4617.065918 [ 1330/ 1682]\n",
      "loss: 1705.871338 [ 1340/ 1682]\n",
      "loss: 1744.759155 [ 1350/ 1682]\n",
      "loss: 3202.113281 [ 1360/ 1682]\n",
      "loss: 3895.690918 [ 1370/ 1682]\n",
      "loss: 3989.545410 [ 1380/ 1682]\n",
      "loss: 1982.811157 [ 1390/ 1682]\n",
      "loss: 3966.751221 [ 1400/ 1682]\n",
      "loss: 4594.956543 [ 1410/ 1682]\n",
      "loss: 2142.479492 [ 1420/ 1682]\n",
      "loss: 1138.520020 [ 1430/ 1682]\n",
      "loss: 2829.545410 [ 1440/ 1682]\n",
      "loss: 2794.999512 [ 1450/ 1682]\n",
      "loss: 2815.187500 [ 1460/ 1682]\n",
      "loss: 2130.923096 [ 1470/ 1682]\n",
      "loss: 3609.188232 [ 1480/ 1682]\n",
      "loss: 2099.667969 [ 1490/ 1682]\n",
      "loss: 1762.013062 [ 1500/ 1682]\n",
      "loss: 1372.539551 [ 1510/ 1682]\n",
      "loss: 2126.201660 [ 1520/ 1682]\n",
      "loss: 1218.130249 [ 1530/ 1682]\n",
      "loss: 2388.245605 [ 1540/ 1682]\n",
      "loss: 1732.081787 [ 1550/ 1682]\n",
      "loss: 5169.078125 [ 1560/ 1682]\n",
      "loss: 2055.404785 [ 1570/ 1682]\n",
      "loss: 1678.076782 [ 1580/ 1682]\n",
      "loss: 574.359680 [ 1590/ 1682]\n",
      "loss: 1150.218384 [ 1600/ 1682]\n",
      "loss: 1524.325317 [ 1610/ 1682]\n",
      "loss: 4562.650391 [ 1620/ 1682]\n",
      "loss: 838168.687500 [ 1630/ 1682]\n",
      "loss: 274.386932 [ 1640/ 1682]\n",
      "loss: 8712.391602 [ 1650/ 1682]\n",
      "loss: 34388.011719 [ 1660/ 1682]\n",
      "loss: 10556.948242 [ 1670/ 1682]\n",
      "loss: 3288.035156 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 182495.561523 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 9570.396484 [    0/ 1682]\n",
      "loss: 752331.250000 [   10/ 1682]\n",
      "loss: 175396.031250 [   20/ 1682]\n",
      "loss: 221408.125000 [   30/ 1682]\n",
      "loss: 192420.828125 [   40/ 1682]\n",
      "loss: 277038.843750 [   50/ 1682]\n",
      "loss: 13268.023438 [   60/ 1682]\n",
      "loss: 20691.027344 [   70/ 1682]\n",
      "loss: 20725.658203 [   80/ 1682]\n",
      "loss: 59679.570312 [   90/ 1682]\n",
      "loss: 12940.541992 [  100/ 1682]\n",
      "loss: 82666.859375 [  110/ 1682]\n",
      "loss: 108498.296875 [  120/ 1682]\n",
      "loss: 18660.332031 [  130/ 1682]\n",
      "loss: 13803.364258 [  140/ 1682]\n",
      "loss: 46598.285156 [  150/ 1682]\n",
      "loss: 41218.394531 [  160/ 1682]\n",
      "loss: 2331.486328 [  170/ 1682]\n",
      "loss: 15519.731445 [  180/ 1682]\n",
      "loss: 18391.287109 [  190/ 1682]\n",
      "loss: 12696.866211 [  200/ 1682]\n",
      "loss: 8021.638184 [  210/ 1682]\n",
      "loss: 977.205444 [  220/ 1682]\n",
      "loss: 2733.937744 [  230/ 1682]\n",
      "loss: 1810.692627 [  240/ 1682]\n",
      "loss: 3984.829590 [  250/ 1682]\n",
      "loss: 772.547119 [  260/ 1682]\n",
      "loss: 1140.105469 [  270/ 1682]\n",
      "loss: 1002.588074 [  280/ 1682]\n",
      "loss: 2346.186523 [  290/ 1682]\n",
      "loss: 1089.822876 [  300/ 1682]\n",
      "loss: 1181.419556 [  310/ 1682]\n",
      "loss: 67608.296875 [  320/ 1682]\n",
      "loss: 1213.607422 [  330/ 1682]\n",
      "loss: 1474.949707 [  340/ 1682]\n",
      "loss: 1234.150757 [  350/ 1682]\n",
      "loss: 1123.788330 [  360/ 1682]\n",
      "loss: 1103.658081 [  370/ 1682]\n",
      "loss: 63870.835938 [  380/ 1682]\n",
      "loss: 41662.996094 [  390/ 1682]\n",
      "loss: 1362.700073 [  400/ 1682]\n",
      "loss: 1414.859375 [  410/ 1682]\n",
      "loss: 1802.883057 [  420/ 1682]\n",
      "loss: 1275.274048 [  430/ 1682]\n",
      "loss: 1311.263428 [  440/ 1682]\n",
      "loss: 1431.112427 [  450/ 1682]\n",
      "loss: 1632.746826 [  460/ 1682]\n",
      "loss: 1608.394531 [  470/ 1682]\n",
      "loss: 2090.645508 [  480/ 1682]\n",
      "loss: 1608.020386 [  490/ 1682]\n",
      "loss: 1689.008423 [  500/ 1682]\n",
      "loss: 1553.198242 [  510/ 1682]\n",
      "loss: 1445.373535 [  520/ 1682]\n",
      "loss: 1672.426392 [  530/ 1682]\n",
      "loss: 1729.590454 [  540/ 1682]\n",
      "loss: 1567.221680 [  550/ 1682]\n",
      "loss: 1638.885498 [  560/ 1682]\n",
      "loss: 1528.468384 [  570/ 1682]\n",
      "loss: 8071.705566 [  580/ 1682]\n",
      "loss: 1915.690674 [  590/ 1682]\n",
      "loss: 2022.229126 [  600/ 1682]\n",
      "loss: 1924.342407 [  610/ 1682]\n",
      "loss: 1932.686890 [  620/ 1682]\n",
      "loss: 2030.135986 [  630/ 1682]\n",
      "loss: 2254.817871 [  640/ 1682]\n",
      "loss: 2289.802979 [  650/ 1682]\n",
      "loss: 2761.809082 [  660/ 1682]\n",
      "loss: 2719.020020 [  670/ 1682]\n",
      "loss: 2824.289062 [  680/ 1682]\n",
      "loss: 2697.911865 [  690/ 1682]\n",
      "loss: 2615.529053 [  700/ 1682]\n",
      "loss: 2205.830566 [  710/ 1682]\n",
      "loss: 1766.890381 [  720/ 1682]\n",
      "loss: 1586.199585 [  730/ 1682]\n",
      "loss: 1312.554443 [  740/ 1682]\n",
      "loss: 1275.461548 [  750/ 1682]\n",
      "loss: 1378.557129 [  760/ 1682]\n",
      "loss: 1655.404053 [  770/ 1682]\n",
      "loss: 1523.706299 [  780/ 1682]\n",
      "loss: 1807.196655 [  790/ 1682]\n",
      "loss: 2033.459961 [  800/ 1682]\n",
      "loss: 2217.458740 [  810/ 1682]\n",
      "loss: 2360.442139 [  820/ 1682]\n",
      "loss: 2181.834229 [  830/ 1682]\n",
      "loss: 1909.181274 [  840/ 1682]\n",
      "loss: 1829.806885 [  850/ 1682]\n",
      "loss: 2028.273071 [  860/ 1682]\n",
      "loss: 2310.377441 [  870/ 1682]\n",
      "loss: 2416.743164 [  880/ 1682]\n",
      "loss: 2349.932617 [  890/ 1682]\n",
      "loss: 2456.269043 [  900/ 1682]\n",
      "loss: 2485.349609 [  910/ 1682]\n",
      "loss: 2769.491699 [  920/ 1682]\n",
      "loss: 2824.422119 [  930/ 1682]\n",
      "loss: 3129.912598 [  940/ 1682]\n",
      "loss: 3521.500000 [  950/ 1682]\n",
      "loss: 3972.888672 [  960/ 1682]\n",
      "loss: 4022.605469 [  970/ 1682]\n",
      "loss: 4674.424805 [  980/ 1682]\n",
      "loss: 4861.976074 [  990/ 1682]\n",
      "loss: 5511.146973 [ 1000/ 1682]\n",
      "loss: 5833.250488 [ 1010/ 1682]\n",
      "loss: 6052.533691 [ 1020/ 1682]\n",
      "loss: 5157.585938 [ 1030/ 1682]\n",
      "loss: 4363.209473 [ 1040/ 1682]\n",
      "loss: 3508.820312 [ 1050/ 1682]\n",
      "loss: 4031.354004 [ 1060/ 1682]\n",
      "loss: 4581.002441 [ 1070/ 1682]\n",
      "loss: 5526.594727 [ 1080/ 1682]\n",
      "loss: 5831.184082 [ 1090/ 1682]\n",
      "loss: 6430.842773 [ 1100/ 1682]\n",
      "loss: 7372.549316 [ 1110/ 1682]\n",
      "loss: 8076.359375 [ 1120/ 1682]\n",
      "loss: 8713.802734 [ 1130/ 1682]\n",
      "loss: 10241.704102 [ 1140/ 1682]\n",
      "loss: 12629.068359 [ 1150/ 1682]\n",
      "loss: 15213.244141 [ 1160/ 1682]\n",
      "loss: 12043.097656 [ 1170/ 1682]\n",
      "loss: 12129.355469 [ 1180/ 1682]\n",
      "loss: 12309.181641 [ 1190/ 1682]\n",
      "loss: 12325.707031 [ 1200/ 1682]\n",
      "loss: 13088.418945 [ 1210/ 1682]\n",
      "loss: 13197.603516 [ 1220/ 1682]\n",
      "loss: 14463.046875 [ 1230/ 1682]\n",
      "loss: 16352.987305 [ 1240/ 1682]\n",
      "loss: 16038.568359 [ 1250/ 1682]\n",
      "loss: 17592.845703 [ 1260/ 1682]\n",
      "loss: 218681.750000 [ 1270/ 1682]\n",
      "loss: 15414.340820 [ 1280/ 1682]\n",
      "loss: 14069.166016 [ 1290/ 1682]\n",
      "loss: 14159.231445 [ 1300/ 1682]\n",
      "loss: 15562.494141 [ 1310/ 1682]\n",
      "loss: 17098.183594 [ 1320/ 1682]\n",
      "loss: 15223.632812 [ 1330/ 1682]\n",
      "loss: 15139.265625 [ 1340/ 1682]\n",
      "loss: 15087.384766 [ 1350/ 1682]\n",
      "loss: 15494.700195 [ 1360/ 1682]\n",
      "loss: 18321.128906 [ 1370/ 1682]\n",
      "loss: 20421.087891 [ 1380/ 1682]\n",
      "loss: 20645.458984 [ 1390/ 1682]\n",
      "loss: 20961.408203 [ 1400/ 1682]\n",
      "loss: 21659.632812 [ 1410/ 1682]\n",
      "loss: 21957.417969 [ 1420/ 1682]\n",
      "loss: 19865.650391 [ 1430/ 1682]\n",
      "loss: 19440.548828 [ 1440/ 1682]\n",
      "loss: 21349.433594 [ 1450/ 1682]\n",
      "loss: 23748.679688 [ 1460/ 1682]\n",
      "loss: 23882.509766 [ 1470/ 1682]\n",
      "loss: 27810.984375 [ 1480/ 1682]\n",
      "loss: 29615.027344 [ 1490/ 1682]\n",
      "loss: 30024.494141 [ 1500/ 1682]\n",
      "loss: 26771.255859 [ 1510/ 1682]\n",
      "loss: 28691.244141 [ 1520/ 1682]\n",
      "loss: 27243.863281 [ 1530/ 1682]\n",
      "loss: 25573.933594 [ 1540/ 1682]\n",
      "loss: 25482.087891 [ 1550/ 1682]\n",
      "loss: 29748.740234 [ 1560/ 1682]\n",
      "loss: 26955.728516 [ 1570/ 1682]\n",
      "loss: 24607.246094 [ 1580/ 1682]\n",
      "loss: 20427.880859 [ 1590/ 1682]\n",
      "loss: 20302.986328 [ 1600/ 1682]\n",
      "loss: 18391.878906 [ 1610/ 1682]\n",
      "loss: 18714.005859 [ 1620/ 1682]\n",
      "loss: 21079.792969 [ 1630/ 1682]\n",
      "loss: 24060.712891 [ 1640/ 1682]\n",
      "loss: 27783.792969 [ 1650/ 1682]\n",
      "loss: 26736.769531 [ 1660/ 1682]\n",
      "loss: 23605.841797 [ 1670/ 1682]\n",
      "loss: 22473.921875 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 25564.195125 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 432.156189 [    0/ 1682]\n",
      "loss: 415.544006 [   10/ 1682]\n",
      "loss: 410.810974 [   20/ 1682]\n",
      "loss: 443.554596 [   30/ 1682]\n",
      "loss: 486.039124 [   40/ 1682]\n",
      "loss: 530.720154 [   50/ 1682]\n",
      "loss: 553.672424 [   60/ 1682]\n",
      "loss: 481.900452 [   70/ 1682]\n",
      "loss: 384.763214 [   80/ 1682]\n",
      "loss: 421.058990 [   90/ 1682]\n",
      "loss: 436.852356 [  100/ 1682]\n",
      "loss: 416.125671 [  110/ 1682]\n",
      "loss: 419.970703 [  120/ 1682]\n",
      "loss: 444.710602 [  130/ 1682]\n",
      "loss: 523.720337 [  140/ 1682]\n",
      "loss: 565.215637 [  150/ 1682]\n",
      "loss: 532.496582 [  160/ 1682]\n",
      "loss: 570.686890 [  170/ 1682]\n",
      "loss: 613.180603 [  180/ 1682]\n",
      "loss: 648.365112 [  190/ 1682]\n",
      "loss: 613.431030 [  200/ 1682]\n",
      "loss: 520.005188 [  210/ 1682]\n",
      "loss: 578.557556 [  220/ 1682]\n",
      "loss: 612.751587 [  230/ 1682]\n",
      "loss: 657.638489 [  240/ 1682]\n",
      "loss: 662.534607 [  250/ 1682]\n",
      "loss: 710.011536 [  260/ 1682]\n",
      "loss: 849.029419 [  270/ 1682]\n",
      "loss: 920.218750 [  280/ 1682]\n",
      "loss: 883.051453 [  290/ 1682]\n",
      "loss: 981.527222 [  300/ 1682]\n",
      "loss: 907.980164 [  310/ 1682]\n",
      "loss: 1006.350464 [  320/ 1682]\n",
      "loss: 1137.639893 [  330/ 1682]\n",
      "loss: 1184.547729 [  340/ 1682]\n",
      "loss: 1157.380981 [  350/ 1682]\n",
      "loss: 1050.516479 [  360/ 1682]\n",
      "loss: 1031.004639 [  370/ 1682]\n",
      "loss: 1070.794556 [  380/ 1682]\n",
      "loss: 1198.027954 [  390/ 1682]\n",
      "loss: 1281.496094 [  400/ 1682]\n",
      "loss: 1331.953369 [  410/ 1682]\n",
      "loss: 1253.113037 [  420/ 1682]\n",
      "loss: 1196.396729 [  430/ 1682]\n",
      "loss: 1231.169434 [  440/ 1682]\n",
      "loss: 1347.333740 [  450/ 1682]\n",
      "loss: 1491.746460 [  460/ 1682]\n",
      "loss: 1519.284302 [  470/ 1682]\n",
      "loss: 1519.038330 [  480/ 1682]\n",
      "loss: 1518.788330 [  490/ 1682]\n",
      "loss: 1597.467041 [  500/ 1682]\n",
      "loss: 1465.451782 [  510/ 1682]\n",
      "loss: 1360.752441 [  520/ 1682]\n",
      "loss: 1581.210205 [  530/ 1682]\n",
      "loss: 1636.755371 [  540/ 1682]\n",
      "loss: 1362.617188 [  550/ 1682]\n",
      "loss: 1548.483398 [  560/ 1682]\n",
      "loss: 1461.003296 [  570/ 1682]\n",
      "loss: 1776.924438 [  580/ 1682]\n",
      "loss: 1793.518311 [  590/ 1682]\n",
      "loss: 1835.523438 [  600/ 1682]\n",
      "loss: 1826.201782 [  610/ 1682]\n",
      "loss: 1834.324829 [  620/ 1682]\n",
      "loss: 1929.284546 [  630/ 1682]\n",
      "loss: 2148.529541 [  640/ 1682]\n",
      "loss: 2422.646240 [  650/ 1682]\n",
      "loss: 2512.400146 [  660/ 1682]\n",
      "loss: 2602.071045 [  670/ 1682]\n",
      "loss: 2705.069092 [  680/ 1682]\n",
      "loss: 2343.184082 [  690/ 1682]\n",
      "loss: 2500.841064 [  700/ 1682]\n",
      "loss: 2100.668213 [  710/ 1682]\n",
      "loss: 1672.845093 [  720/ 1682]\n",
      "loss: 1497.146484 [  730/ 1682]\n",
      "loss: 1231.670898 [  740/ 1682]\n",
      "loss: 1195.714600 [  750/ 1682]\n",
      "loss: 1158.350830 [  760/ 1682]\n",
      "loss: 1557.613892 [  770/ 1682]\n",
      "loss: 1605.182617 [  780/ 1682]\n",
      "loss: 1712.023682 [  790/ 1682]\n",
      "loss: 1932.383423 [  800/ 1682]\n",
      "loss: 2111.840332 [  810/ 1682]\n",
      "loss: 2251.415527 [  820/ 1682]\n",
      "loss: 2180.167480 [  830/ 1682]\n",
      "loss: 1790.882080 [  840/ 1682]\n",
      "loss: 1873.580444 [  850/ 1682]\n",
      "loss: 2110.190430 [  860/ 1682]\n",
      "loss: 2152.410156 [  870/ 1682]\n",
      "loss: 2306.325439 [  880/ 1682]\n",
      "loss: 2241.099854 [  890/ 1682]\n",
      "loss: 2344.929932 [  900/ 1682]\n",
      "loss: 2373.322754 [  910/ 1682]\n",
      "loss: 2663.530762 [  920/ 1682]\n",
      "loss: 2704.880127 [  930/ 1682]\n",
      "loss: 3003.996582 [  940/ 1682]\n",
      "loss: 3387.842529 [  950/ 1682]\n",
      "loss: 3480.430176 [  960/ 1682]\n",
      "loss: 3879.607910 [  970/ 1682]\n",
      "loss: 4197.891602 [  980/ 1682]\n",
      "loss: 4704.633789 [  990/ 1682]\n",
      "loss: 5385.413086 [ 1000/ 1682]\n",
      "loss: 5082.371094 [ 1010/ 1682]\n",
      "loss: 5315.757812 [ 1020/ 1682]\n",
      "loss: 4995.688965 [ 1030/ 1682]\n",
      "loss: 4214.524414 [ 1040/ 1682]\n",
      "loss: 3375.395264 [ 1050/ 1682]\n",
      "loss: 3888.371582 [ 1060/ 1682]\n",
      "loss: 4095.541504 [ 1070/ 1682]\n",
      "loss: 5207.838867 [ 1080/ 1682]\n",
      "loss: 5658.613281 [ 1090/ 1682]\n",
      "loss: 6249.618164 [ 1100/ 1682]\n",
      "loss: 7178.339844 [ 1110/ 1682]\n",
      "loss: 7873.031250 [ 1120/ 1682]\n",
      "loss: 8502.490234 [ 1130/ 1682]\n",
      "loss: 10013.060547 [ 1140/ 1682]\n",
      "loss: 12374.424805 [ 1150/ 1682]\n",
      "loss: 14868.158203 [ 1160/ 1682]\n",
      "loss: 11794.318359 [ 1170/ 1682]\n",
      "loss: 11879.623047 [ 1180/ 1682]\n",
      "loss: 12994.837891 [ 1190/ 1682]\n",
      "loss: 12073.803711 [ 1200/ 1682]\n",
      "loss: 12828.682617 [ 1210/ 1682]\n",
      "loss: 12936.646484 [ 1220/ 1682]\n",
      "loss: 14189.674805 [ 1230/ 1682]\n",
      "loss: 15874.814453 [ 1240/ 1682]\n",
      "loss: 15750.403320 [ 1250/ 1682]\n",
      "loss: 17291.097656 [ 1260/ 1682]\n",
      "loss: 17233.253906 [ 1270/ 1682]\n",
      "loss: 14345.653320 [ 1280/ 1682]\n",
      "loss: 13799.471680 [ 1290/ 1682]\n",
      "loss: 12526.458984 [ 1300/ 1682]\n",
      "loss: 15279.150391 [ 1310/ 1682]\n",
      "loss: 16801.035156 [ 1320/ 1682]\n",
      "loss: 15833.048828 [ 1330/ 1682]\n",
      "loss: 14859.859375 [ 1340/ 1682]\n",
      "loss: 14808.481445 [ 1350/ 1682]\n",
      "loss: 16001.023438 [ 1360/ 1682]\n",
      "loss: 18013.755859 [ 1370/ 1682]\n",
      "loss: 20096.371094 [ 1380/ 1682]\n",
      "loss: 20025.839844 [ 1390/ 1682]\n",
      "loss: 20632.314453 [ 1400/ 1682]\n",
      "loss: 19685.722656 [ 1410/ 1682]\n",
      "loss: 21620.496094 [ 1420/ 1682]\n",
      "loss: 19629.136719 [ 1430/ 1682]\n",
      "loss: 19123.390625 [ 1440/ 1682]\n",
      "loss: 17959.896484 [ 1450/ 1682]\n",
      "loss: 21303.419922 [ 1460/ 1682]\n",
      "loss: 23530.746094 [ 1470/ 1682]\n",
      "loss: 26549.494141 [ 1480/ 1682]\n",
      "loss: 28169.056641 [ 1490/ 1682]\n",
      "loss: 29185.306641 [ 1500/ 1682]\n",
      "loss: 23592.396484 [ 1510/ 1682]\n",
      "loss: 28305.162109 [ 1520/ 1682]\n",
      "loss: 26275.177734 [ 1530/ 1682]\n",
      "loss: 22795.824219 [ 1540/ 1682]\n",
      "loss: 21652.203125 [ 1550/ 1682]\n",
      "loss: 28318.728516 [ 1560/ 1682]\n",
      "loss: 32600.052734 [ 1570/ 1682]\n",
      "loss: 21978.382812 [ 1580/ 1682]\n",
      "loss: 19478.875000 [ 1590/ 1682]\n",
      "loss: 17849.986328 [ 1600/ 1682]\n",
      "loss: 17883.720703 [ 1610/ 1682]\n",
      "loss: 19095.880859 [ 1620/ 1682]\n",
      "loss: 20810.216797 [ 1630/ 1682]\n",
      "loss: 20076.939453 [ 1640/ 1682]\n",
      "loss: 26308.933594 [ 1650/ 1682]\n",
      "loss: 16231.853516 [ 1660/ 1682]\n",
      "loss: 18187.433594 [ 1670/ 1682]\n",
      "loss: 21992.025391 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 30898.424166 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 11448.200195 [    0/ 1682]\n",
      "loss: 19666.033203 [   10/ 1682]\n",
      "loss: 19165.839844 [   20/ 1682]\n",
      "loss: 1271.769287 [   30/ 1682]\n",
      "loss: 278.423401 [   40/ 1682]\n",
      "loss: 646.561707 [   50/ 1682]\n",
      "loss: 1580.007080 [   60/ 1682]\n",
      "loss: 2534.035400 [   70/ 1682]\n",
      "loss: 342.062164 [   80/ 1682]\n",
      "loss: 5752.987305 [   90/ 1682]\n",
      "loss: 622.742554 [  100/ 1682]\n",
      "loss: 707.757019 [  110/ 1682]\n",
      "loss: 4672.739258 [  120/ 1682]\n",
      "loss: 2440.429688 [  130/ 1682]\n",
      "loss: 947.584778 [  140/ 1682]\n",
      "loss: 565.499023 [  150/ 1682]\n",
      "loss: 477.813477 [  160/ 1682]\n",
      "loss: 766.262024 [  170/ 1682]\n",
      "loss: 916.698120 [  180/ 1682]\n",
      "loss: 600.579590 [  190/ 1682]\n",
      "loss: 559.503784 [  200/ 1682]\n",
      "loss: 474.391510 [  210/ 1682]\n",
      "loss: 526.212891 [  220/ 1682]\n",
      "loss: 652.738953 [  230/ 1682]\n",
      "loss: 601.749512 [  240/ 1682]\n",
      "loss: 867.163269 [  250/ 1682]\n",
      "loss: 846.465149 [  260/ 1682]\n",
      "loss: 785.379089 [  270/ 1682]\n",
      "loss: 853.907410 [  280/ 1682]\n",
      "loss: 877.038452 [  290/ 1682]\n",
      "loss: 913.009277 [  300/ 1682]\n",
      "loss: 936.928833 [  310/ 1682]\n",
      "loss: 936.957703 [  320/ 1682]\n",
      "loss: 1096.784424 [  330/ 1682]\n",
      "loss: 1109.155151 [  340/ 1682]\n",
      "loss: 1082.884644 [  350/ 1682]\n",
      "loss: 979.588196 [  360/ 1682]\n",
      "loss: 960.749695 [  370/ 1682]\n",
      "loss: 1068.670532 [  380/ 1682]\n",
      "loss: 1122.214722 [  390/ 1682]\n",
      "loss: 1203.013916 [  400/ 1682]\n",
      "loss: 1251.916138 [  410/ 1682]\n",
      "loss: 1175.529053 [  420/ 1682]\n",
      "loss: 1120.598145 [  430/ 1682]\n",
      "loss: 1154.260498 [  440/ 1682]\n",
      "loss: 1266.864014 [  450/ 1682]\n",
      "loss: 1456.774536 [  460/ 1682]\n",
      "loss: 1433.691528 [  470/ 1682]\n",
      "loss: 1433.443970 [  480/ 1682]\n",
      "loss: 1433.192383 [  490/ 1682]\n",
      "loss: 1509.641724 [  500/ 1682]\n",
      "loss: 1381.413818 [  510/ 1682]\n",
      "loss: 1279.834717 [  520/ 1682]\n",
      "loss: 1493.827881 [  530/ 1682]\n",
      "loss: 1547.812256 [  540/ 1682]\n",
      "loss: 1351.598755 [  550/ 1682]\n",
      "loss: 1333.383789 [  560/ 1682]\n",
      "loss: 1357.846924 [  570/ 1682]\n",
      "loss: 1684.185181 [  580/ 1682]\n",
      "loss: 1723.910889 [  590/ 1682]\n",
      "loss: 1825.048584 [  600/ 1682]\n",
      "loss: 1732.105103 [  610/ 1682]\n",
      "loss: 1739.996094 [  620/ 1682]\n",
      "loss: 1832.492554 [  630/ 1682]\n",
      "loss: 2046.372314 [  640/ 1682]\n",
      "loss: 2314.006592 [  650/ 1682]\n",
      "loss: 2530.395996 [  660/ 1682]\n",
      "loss: 2489.395020 [  670/ 1682]\n",
      "loss: 2590.150391 [  680/ 1682]\n",
      "loss: 2469.163818 [  690/ 1682]\n",
      "loss: 2390.372070 [  700/ 1682]\n",
      "loss: 1999.577515 [  710/ 1682]\n",
      "loss: 1582.700806 [  720/ 1682]\n",
      "loss: 1411.915771 [  730/ 1682]\n",
      "loss: 1154.487061 [  740/ 1682]\n",
      "loss: 1119.647705 [  750/ 1682]\n",
      "loss: 1216.436035 [  760/ 1682]\n",
      "loss: 1535.565430 [  770/ 1682]\n",
      "loss: 1516.813232 [  780/ 1682]\n",
      "loss: 1620.748657 [  790/ 1682]\n",
      "loss: 1835.294312 [  800/ 1682]\n",
      "loss: 2010.278687 [  810/ 1682]\n",
      "loss: 2146.499023 [  820/ 1682]\n",
      "loss: 2077.043213 [  830/ 1682]\n",
      "loss: 1717.331299 [  840/ 1682]\n",
      "loss: 1778.069092 [  850/ 1682]\n",
      "loss: 2008.637695 [  860/ 1682]\n",
      "loss: 2098.675781 [  870/ 1682]\n",
      "loss: 2200.073975 [  880/ 1682]\n",
      "loss: 2136.415527 [  890/ 1682]\n",
      "loss: 3874.311279 [  900/ 1682]\n",
      "loss: 2437.578369 [  910/ 1682]\n",
      "loss: 2549.264893 [  920/ 1682]\n",
      "loss: 2589.743652 [  930/ 1682]\n",
      "loss: 2882.615723 [  940/ 1682]\n",
      "loss: 2738.064697 [  950/ 1682]\n",
      "loss: 3693.587158 [  960/ 1682]\n",
      "loss: 3741.516846 [  970/ 1682]\n",
      "loss: 5357.688477 [  980/ 1682]\n",
      "loss: 4552.474121 [  990/ 1682]\n",
      "loss: 5222.517090 [ 1000/ 1682]\n",
      "loss: 5483.970215 [ 1010/ 1682]\n",
      "loss: 5706.452148 [ 1020/ 1682]\n",
      "loss: 4839.028809 [ 1030/ 1682]\n",
      "loss: 4070.828125 [ 1040/ 1682]\n",
      "loss: 4068.784668 [ 1050/ 1682]\n",
      "loss: 3750.232910 [ 1060/ 1682]\n",
      "loss: 4120.881348 [ 1070/ 1682]\n",
      "loss: 5018.301270 [ 1080/ 1682]\n",
      "loss: 5491.274414 [ 1090/ 1682]\n",
      "loss: 6073.733398 [ 1100/ 1682]\n",
      "loss: 6621.648438 [ 1110/ 1682]\n",
      "loss: 7675.339844 [ 1120/ 1682]\n",
      "loss: 7979.559570 [ 1130/ 1682]\n",
      "loss: 9790.412109 [ 1140/ 1682]\n",
      "loss: 12126.158203 [ 1150/ 1682]\n",
      "loss: 14595.833008 [ 1160/ 1682]\n",
      "loss: 11551.807617 [ 1170/ 1682]\n",
      "loss: 11636.173828 [ 1180/ 1682]\n",
      "loss: 12740.111328 [ 1190/ 1682]\n",
      "loss: 11828.244141 [ 1200/ 1682]\n",
      "loss: 12575.432617 [ 1210/ 1682]\n",
      "loss: 12682.222656 [ 1220/ 1682]\n",
      "loss: 13615.607422 [ 1230/ 1682]\n",
      "loss: 15778.395508 [ 1240/ 1682]\n",
      "loss: 15469.268555 [ 1250/ 1682]\n",
      "loss: 16996.621094 [ 1260/ 1682]\n",
      "loss: 16938.927734 [ 1270/ 1682]\n",
      "loss: 14856.205078 [ 1280/ 1682]\n",
      "loss: 13536.090820 [ 1290/ 1682]\n",
      "loss: 13624.409180 [ 1300/ 1682]\n",
      "loss: 15001.846680 [ 1310/ 1682]\n",
      "loss: 16509.871094 [ 1320/ 1682]\n",
      "loss: 15550.361328 [ 1330/ 1682]\n",
      "loss: 13202.426758 [ 1340/ 1682]\n",
      "loss: 14534.890625 [ 1350/ 1682]\n",
      "loss: 15716.515625 [ 1360/ 1682]\n",
      "loss: 17711.771484 [ 1370/ 1682]\n",
      "loss: 19777.121094 [ 1380/ 1682]\n",
      "loss: 19997.751953 [ 1390/ 1682]\n",
      "loss: 20308.582031 [ 1400/ 1682]\n",
      "loss: 20995.757812 [ 1410/ 1682]\n",
      "loss: 21288.839844 [ 1420/ 1682]\n",
      "loss: 19313.003906 [ 1430/ 1682]\n",
      "loss: 18811.218750 [ 1440/ 1682]\n",
      "loss: 18619.171875 [ 1450/ 1682]\n",
      "loss: 20973.542969 [ 1460/ 1682]\n",
      "loss: 23183.939453 [ 1470/ 1682]\n",
      "loss: 27056.650391 [ 1480/ 1682]\n",
      "loss: 28835.865234 [ 1490/ 1682]\n",
      "loss: 29239.625000 [ 1500/ 1682]\n",
      "loss: 26030.384766 [ 1510/ 1682]\n",
      "loss: 27923.660156 [ 1520/ 1682]\n",
      "loss: 26495.636719 [ 1530/ 1682]\n",
      "loss: 24848.738281 [ 1540/ 1682]\n",
      "loss: 20324.269531 [ 1550/ 1682]\n",
      "loss: 28965.478516 [ 1560/ 1682]\n",
      "loss: 26043.035156 [ 1570/ 1682]\n",
      "loss: 23894.869141 [ 1580/ 1682]\n",
      "loss: 19779.359375 [ 1590/ 1682]\n",
      "loss: 19898.826172 [ 1600/ 1682]\n",
      "loss: 17597.039062 [ 1610/ 1682]\n",
      "loss: 18092.355469 [ 1620/ 1682]\n",
      "loss: 20419.425781 [ 1630/ 1682]\n",
      "loss: 23354.703125 [ 1640/ 1682]\n",
      "loss: 27024.300781 [ 1650/ 1682]\n",
      "loss: 25991.693359 [ 1660/ 1682]\n",
      "loss: 22905.496094 [ 1670/ 1682]\n",
      "loss: 21790.320312 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 24845.046950 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 341.871246 [    0/ 1682]\n",
      "loss: 327.119995 [   10/ 1682]\n",
      "loss: 322.867950 [   20/ 1682]\n",
      "loss: 351.949402 [   30/ 1682]\n",
      "loss: 389.847504 [   40/ 1682]\n",
      "loss: 429.939209 [   50/ 1682]\n",
      "loss: 450.602631 [   60/ 1682]\n",
      "loss: 555.041931 [   70/ 1682]\n",
      "loss: 299.696106 [   80/ 1682]\n",
      "loss: 331.862061 [   90/ 1682]\n",
      "loss: 345.828552 [  100/ 1682]\n",
      "loss: 327.423126 [  110/ 1682]\n",
      "loss: 330.822815 [  120/ 1682]\n",
      "loss: 404.906860 [  130/ 1682]\n",
      "loss: 423.537109 [  140/ 1682]\n",
      "loss: 460.897522 [  150/ 1682]\n",
      "loss: 427.173523 [  160/ 1682]\n",
      "loss: 465.936707 [  170/ 1682]\n",
      "loss: 504.293884 [  180/ 1682]\n",
      "loss: 536.263428 [  190/ 1682]\n",
      "loss: 622.203125 [  200/ 1682]\n",
      "loss: 451.551208 [  210/ 1682]\n",
      "loss: 1224.231689 [  220/ 1682]\n",
      "loss: 503.942780 [  230/ 1682]\n",
      "loss: 544.697876 [  240/ 1682]\n",
      "loss: 549.151184 [  250/ 1682]\n",
      "loss: 592.495422 [  260/ 1682]\n",
      "loss: 720.002258 [  270/ 1682]\n",
      "loss: 785.675476 [  280/ 1682]\n",
      "loss: 807.858459 [  290/ 1682]\n",
      "loss: 842.404785 [  300/ 1682]\n",
      "loss: 1233.409424 [  310/ 1682]\n",
      "loss: 865.417175 [  320/ 1682]\n",
      "loss: 987.503601 [  330/ 1682]\n",
      "loss: 1031.201904 [  340/ 1682]\n",
      "loss: 1005.895508 [  350/ 1682]\n",
      "loss: 858.585938 [  360/ 1682]\n",
      "loss: 888.313293 [  370/ 1682]\n",
      "loss: 992.196106 [  380/ 1682]\n",
      "loss: 1043.846680 [  390/ 1682]\n",
      "loss: 1121.799072 [  400/ 1682]\n",
      "loss: 1169.045288 [  410/ 1682]\n",
      "loss: 1095.290649 [  420/ 1682]\n",
      "loss: 1042.276611 [  430/ 1682]\n",
      "loss: 1074.758911 [  440/ 1682]\n",
      "loss: 1183.561890 [  450/ 1682]\n",
      "loss: 1367.289307 [  460/ 1682]\n",
      "loss: 1344.931030 [  470/ 1682]\n",
      "loss: 1344.693604 [  480/ 1682]\n",
      "loss: 1344.452148 [  490/ 1682]\n",
      "loss: 1418.528687 [  500/ 1682]\n",
      "loss: 1294.359497 [  510/ 1682]\n",
      "loss: 1196.123657 [  520/ 1682]\n",
      "loss: 1403.219238 [  530/ 1682]\n",
      "loss: 1455.545776 [  540/ 1682]\n",
      "loss: 1306.905029 [  550/ 1682]\n",
      "loss: 1372.362549 [  560/ 1682]\n",
      "loss: 1271.557983 [  570/ 1682]\n",
      "loss: 1587.899902 [  580/ 1682]\n",
      "loss: 1626.441528 [  590/ 1682]\n",
      "loss: 1572.799072 [  600/ 1682]\n",
      "loss: 1634.416138 [  610/ 1682]\n",
      "loss: 1642.080444 [  620/ 1682]\n",
      "loss: 1731.968384 [  630/ 1682]\n",
      "loss: 1940.142944 [  640/ 1682]\n",
      "loss: 2200.880859 [  650/ 1682]\n",
      "loss: 2412.041504 [  660/ 1682]\n",
      "loss: 2168.404785 [  670/ 1682]\n",
      "loss: 11177.560547 [  680/ 1682]\n",
      "loss: 4270.940430 [  690/ 1682]\n",
      "loss: 2275.480469 [  700/ 1682]\n",
      "loss: 1894.732056 [  710/ 1682]\n",
      "loss: 1489.553467 [  720/ 1682]\n",
      "loss: 1324.037964 [  730/ 1682]\n",
      "loss: 1075.199219 [  740/ 1682]\n",
      "loss: 1041.577637 [  750/ 1682]\n",
      "loss: 1135.068481 [  760/ 1682]\n",
      "loss: 1380.930908 [  770/ 1682]\n",
      "loss: 1425.755615 [  780/ 1682]\n",
      "loss: 1526.623535 [  790/ 1682]\n",
      "loss: 1735.015015 [  800/ 1682]\n",
      "loss: 1905.269287 [  810/ 1682]\n",
      "loss: 2037.946045 [  820/ 1682]\n",
      "loss: 1970.405518 [  830/ 1682]\n",
      "loss: 1620.439575 [  840/ 1682]\n",
      "loss: 1679.530273 [  850/ 1682]\n",
      "loss: 1903.704834 [  860/ 1682]\n",
      "loss: 1991.383423 [  870/ 1682]\n",
      "loss: 2090.184082 [  880/ 1682]\n",
      "loss: 2028.199219 [  890/ 1682]\n",
      "loss: 2126.968750 [  900/ 1682]\n",
      "loss: 2153.989990 [  910/ 1682]\n",
      "loss: 2430.833496 [  920/ 1682]\n",
      "loss: 2470.348877 [  930/ 1682]\n",
      "loss: 2756.559326 [  940/ 1682]\n",
      "loss: 2833.960938 [  950/ 1682]\n",
      "loss: 3263.395020 [  960/ 1682]\n",
      "loss: 3597.608154 [  970/ 1682]\n",
      "loss: 3904.369629 [  980/ 1682]\n",
      "loss: 4393.591309 [  990/ 1682]\n",
      "loss: 5052.231445 [ 1000/ 1682]\n",
      "loss: 5309.408691 [ 1010/ 1682]\n",
      "loss: 5528.325684 [ 1020/ 1682]\n",
      "loss: 4675.403809 [ 1030/ 1682]\n",
      "loss: 3920.992188 [ 1040/ 1682]\n",
      "loss: 3112.778809 [ 1050/ 1682]\n",
      "loss: 3606.354004 [ 1060/ 1682]\n",
      "loss: 4126.347656 [ 1070/ 1682]\n",
      "loss: 4851.375977 [ 1080/ 1682]\n",
      "loss: 5316.507812 [ 1090/ 1682]\n",
      "loss: 5889.936523 [ 1100/ 1682]\n",
      "loss: 6792.295898 [ 1110/ 1682]\n",
      "loss: 7468.475586 [ 1120/ 1682]\n",
      "loss: 8081.726562 [ 1130/ 1682]\n",
      "loss: 9557.127930 [ 1140/ 1682]\n",
      "loss: 11865.742188 [ 1150/ 1682]\n",
      "loss: 14309.942383 [ 1160/ 1682]\n",
      "loss: 11297.545898 [ 1170/ 1682]\n",
      "loss: 11380.942383 [ 1180/ 1682]\n",
      "loss: 12472.958984 [ 1190/ 1682]\n",
      "loss: 11570.833008 [ 1200/ 1682]\n",
      "loss: 12309.906250 [ 1210/ 1682]\n",
      "loss: 12415.483398 [ 1220/ 1682]\n",
      "loss: 13643.429688 [ 1230/ 1682]\n",
      "loss: 15480.634766 [ 1240/ 1682]\n",
      "loss: 15174.333984 [ 1250/ 1682]\n",
      "loss: 16687.589844 [ 1260/ 1682]\n",
      "loss: 16630.099609 [ 1270/ 1682]\n",
      "loss: 14567.127930 [ 1280/ 1682]\n",
      "loss: 13260.095703 [ 1290/ 1682]\n",
      "loss: 13347.426758 [ 1300/ 1682]\n",
      "loss: 14711.234375 [ 1310/ 1682]\n",
      "loss: 16204.666992 [ 1320/ 1682]\n",
      "loss: 15254.184570 [ 1330/ 1682]\n",
      "loss: 14298.979492 [ 1340/ 1682]\n",
      "loss: 14248.408203 [ 1350/ 1682]\n",
      "loss: 15418.510742 [ 1360/ 1682]\n",
      "loss: 17395.312500 [ 1370/ 1682]\n",
      "loss: 19442.431641 [ 1380/ 1682]\n",
      "loss: 19661.083984 [ 1390/ 1682]\n",
      "loss: 19969.222656 [ 1400/ 1682]\n",
      "loss: 20650.589844 [ 1410/ 1682]\n",
      "loss: 20941.212891 [ 1420/ 1682]\n",
      "loss: 18981.820312 [ 1430/ 1682]\n",
      "loss: 18484.287109 [ 1440/ 1682]\n",
      "loss: 20346.394531 [ 1450/ 1682]\n",
      "loss: 20628.003906 [ 1460/ 1682]\n",
      "loss: 22820.562500 [ 1470/ 1682]\n",
      "loss: 26663.916016 [ 1480/ 1682]\n",
      "loss: 28430.072266 [ 1490/ 1682]\n",
      "loss: 28830.832031 [ 1500/ 1682]\n",
      "loss: 25644.761719 [ 1510/ 1682]\n",
      "loss: 27524.000000 [ 1520/ 1682]\n",
      "loss: 26106.203125 [ 1530/ 1682]\n",
      "loss: 22992.082031 [ 1540/ 1682]\n",
      "loss: 23086.908203 [ 1550/ 1682]\n",
      "loss: 28557.675781 [ 1560/ 1682]\n",
      "loss: 25656.259766 [ 1570/ 1682]\n",
      "loss: 23524.332031 [ 1580/ 1682]\n",
      "loss: 19442.410156 [ 1590/ 1682]\n",
      "loss: 19560.574219 [ 1600/ 1682]\n",
      "loss: 17279.173828 [ 1610/ 1682]\n",
      "loss: 17769.582031 [ 1620/ 1682]\n",
      "loss: 20076.314453 [ 1630/ 1682]\n",
      "loss: 22987.623047 [ 1640/ 1682]\n",
      "loss: 26629.134766 [ 1650/ 1682]\n",
      "loss: 25604.146484 [ 1660/ 1682]\n",
      "loss: 22541.525391 [ 1670/ 1682]\n",
      "loss: 21435.203125 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 24733.387019 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 298.654541 [    0/ 1682]\n",
      "loss: 284.890320 [   10/ 1682]\n",
      "loss: 280.899048 [   20/ 1682]\n",
      "loss: 308.063446 [   30/ 1682]\n",
      "loss: 343.559174 [   40/ 1682]\n",
      "loss: 381.246399 [   50/ 1682]\n",
      "loss: 400.713959 [   60/ 1682]\n",
      "loss: 340.213135 [   70/ 1682]\n",
      "loss: 259.270020 [   80/ 1682]\n",
      "loss: 289.268127 [   90/ 1682]\n",
      "loss: 302.276398 [  100/ 1682]\n",
      "loss: 285.091370 [  110/ 1682]\n",
      "loss: 439.712799 [  120/ 1682]\n",
      "loss: 308.805389 [  130/ 1682]\n",
      "loss: 375.181183 [  140/ 1682]\n",
      "loss: 410.370667 [  150/ 1682]\n",
      "loss: 378.592712 [  160/ 1682]\n",
      "loss: 415.184509 [  170/ 1682]\n",
      "loss: 451.370178 [  180/ 1682]\n",
      "loss: 481.652496 [  190/ 1682]\n",
      "loss: 451.663971 [  200/ 1682]\n",
      "loss: 401.555939 [  210/ 1682]\n",
      "loss: 421.756775 [  220/ 1682]\n",
      "loss: 451.047180 [  230/ 1682]\n",
      "loss: 489.623230 [  240/ 1682]\n",
      "loss: 493.836121 [  250/ 1682]\n",
      "loss: 535.002014 [  260/ 1682]\n",
      "loss: 656.453247 [  270/ 1682]\n",
      "loss: 707.531006 [  280/ 1682]\n",
      "loss: 740.438416 [  290/ 1682]\n",
      "loss: 773.534607 [  300/ 1682]\n",
      "loss: 795.541565 [  310/ 1682]\n",
      "loss: 795.581665 [  320/ 1682]\n",
      "loss: 912.819702 [  330/ 1682]\n",
      "loss: 954.821777 [  340/ 1682]\n",
      "loss: 930.487183 [  350/ 1682]\n",
      "loss: 834.897583 [  360/ 1682]\n",
      "loss: 817.514954 [  370/ 1682]\n",
      "loss: 917.276184 [  380/ 1682]\n",
      "loss: 966.988281 [  390/ 1682]\n",
      "loss: 1042.032593 [  400/ 1682]\n",
      "loss: 1087.583984 [  410/ 1682]\n",
      "loss: 1016.501282 [  420/ 1682]\n",
      "loss: 965.432434 [  430/ 1682]\n",
      "loss: 996.706665 [  440/ 1682]\n",
      "loss: 1101.633545 [  450/ 1682]\n",
      "loss: 1279.059326 [  460/ 1682]\n",
      "loss: 1257.435303 [  470/ 1682]\n",
      "loss: 1257.203857 [  480/ 1682]\n",
      "loss: 1126.797974 [  490/ 1682]\n",
      "loss: 1328.630005 [  500/ 1682]\n",
      "loss: 1208.595093 [  510/ 1682]\n",
      "loss: 1113.765625 [  520/ 1682]\n",
      "loss: 1313.836060 [  530/ 1682]\n",
      "loss: 1260.482056 [  540/ 1682]\n",
      "loss: 1220.699585 [  550/ 1682]\n",
      "loss: 1283.987671 [  560/ 1682]\n",
      "loss: 1186.592285 [  570/ 1682]\n",
      "loss: 1492.760986 [  580/ 1682]\n",
      "loss: 1530.103638 [  590/ 1682]\n",
      "loss: 1625.492676 [  600/ 1682]\n",
      "loss: 1537.856812 [  610/ 1682]\n",
      "loss: 1545.286865 [  620/ 1682]\n",
      "loss: 1496.448242 [  630/ 1682]\n",
      "loss: 1834.883789 [  640/ 1682]\n",
      "loss: 2088.603516 [  650/ 1682]\n",
      "loss: 2294.442627 [  660/ 1682]\n",
      "loss: 2033.681885 [  670/ 1682]\n",
      "loss: 2514.128906 [  680/ 1682]\n",
      "loss: 2236.169434 [  690/ 1682]\n",
      "loss: 2161.246826 [  700/ 1682]\n",
      "loss: 1790.660767 [  710/ 1682]\n",
      "loss: 1397.355103 [  720/ 1682]\n",
      "loss: 1237.165527 [  730/ 1682]\n",
      "loss: 997.059875 [  740/ 1682]\n",
      "loss: 964.644714 [  750/ 1682]\n",
      "loss: 1054.726318 [  760/ 1682]\n",
      "loss: 1292.075439 [  770/ 1682]\n",
      "loss: 1335.436157 [  780/ 1682]\n",
      "loss: 1433.139160 [  790/ 1682]\n",
      "loss: 1635.204956 [  800/ 1682]\n",
      "loss: 1800.592773 [  810/ 1682]\n",
      "loss: 1929.619873 [  820/ 1682]\n",
      "loss: 1864.025024 [  830/ 1682]\n",
      "loss: 1524.035400 [  840/ 1682]\n",
      "loss: 1581.426025 [  850/ 1682]\n",
      "loss: 1799.034424 [  860/ 1682]\n",
      "loss: 1884.283447 [  870/ 1682]\n",
      "loss: 1980.412109 [  880/ 1682]\n",
      "loss: 1920.136108 [  890/ 1682]\n",
      "loss: 2016.234741 [  900/ 1682]\n",
      "loss: 2042.532227 [  910/ 1682]\n",
      "loss: 2312.324707 [  920/ 1682]\n",
      "loss: 2350.873535 [  930/ 1682]\n",
      "loss: 2630.275879 [  940/ 1682]\n",
      "loss: 2990.178467 [  950/ 1682]\n",
      "loss: 3407.082764 [  960/ 1682]\n",
      "loss: 3453.073730 [  970/ 1682]\n",
      "loss: 3753.743652 [  980/ 1682]\n",
      "loss: 4233.708008 [  990/ 1682]\n",
      "loss: 4880.654785 [ 1000/ 1682]\n",
      "loss: 5133.445312 [ 1010/ 1682]\n",
      "loss: 5348.706055 [ 1020/ 1682]\n",
      "loss: 4510.643555 [ 1030/ 1682]\n",
      "loss: 3770.362793 [ 1040/ 1682]\n",
      "loss: 2978.476074 [ 1050/ 1682]\n",
      "loss: 3461.822266 [ 1060/ 1682]\n",
      "loss: 3971.343262 [ 1070/ 1682]\n",
      "loss: 4683.217285 [ 1080/ 1682]\n",
      "loss: 5140.311523 [ 1090/ 1682]\n",
      "loss: 5704.482910 [ 1100/ 1682]\n",
      "loss: 6197.213867 [ 1110/ 1682]\n",
      "loss: 7259.392090 [ 1120/ 1682]\n",
      "loss: 7864.119629 [ 1130/ 1682]\n",
      "loss: 9321.002930 [ 1140/ 1682]\n",
      "loss: 11601.831055 [ 1150/ 1682]\n",
      "loss: 14019.947266 [ 1160/ 1682]\n",
      "loss: 11039.978516 [ 1170/ 1682]\n",
      "loss: 11122.402344 [ 1180/ 1682]\n",
      "loss: 12202.225586 [ 1190/ 1682]\n",
      "loss: 10708.141602 [ 1200/ 1682]\n",
      "loss: 12040.884766 [ 1210/ 1682]\n",
      "loss: 12145.248047 [ 1220/ 1682]\n",
      "loss: 13360.023438 [ 1230/ 1682]\n",
      "loss: 15178.690430 [ 1240/ 1682]\n",
      "loss: 14875.315430 [ 1250/ 1682]\n",
      "loss: 16374.169922 [ 1260/ 1682]\n",
      "loss: 16316.918945 [ 1270/ 1682]\n",
      "loss: 14274.193359 [ 1280/ 1682]\n",
      "loss: 12980.578125 [ 1290/ 1682]\n",
      "loss: 13066.927734 [ 1300/ 1682]\n",
      "loss: 14416.810547 [ 1310/ 1682]\n",
      "loss: 15895.340820 [ 1320/ 1682]\n",
      "loss: 14954.126953 [ 1330/ 1682]\n",
      "loss: 14008.434570 [ 1340/ 1682]\n",
      "loss: 13958.340820 [ 1350/ 1682]\n",
      "loss: 15116.706055 [ 1360/ 1682]\n",
      "loss: 17074.679688 [ 1370/ 1682]\n",
      "loss: 19103.214844 [ 1380/ 1682]\n",
      "loss: 19319.892578 [ 1390/ 1682]\n",
      "loss: 19625.312500 [ 1400/ 1682]\n",
      "loss: 20300.787109 [ 1410/ 1682]\n",
      "loss: 20297.089844 [ 1420/ 1682]\n",
      "loss: 18646.398438 [ 1430/ 1682]\n",
      "loss: 18153.248047 [ 1440/ 1682]\n",
      "loss: 19998.941406 [ 1450/ 1682]\n",
      "loss: 20278.074219 [ 1460/ 1682]\n",
      "loss: 22452.498047 [ 1470/ 1682]\n",
      "loss: 26265.957031 [ 1480/ 1682]\n",
      "loss: 28018.847656 [ 1490/ 1682]\n",
      "loss: 28416.628906 [ 1500/ 1682]\n",
      "loss: 25254.269531 [ 1510/ 1682]\n",
      "loss: 27119.253906 [ 1520/ 1682]\n",
      "loss: 25711.943359 [ 1530/ 1682]\n",
      "loss: 24089.644531 [ 1540/ 1682]\n",
      "loss: 24001.167969 [ 1550/ 1682]\n",
      "loss: 28144.833984 [ 1560/ 1682]\n",
      "loss: 25264.917969 [ 1570/ 1682]\n",
      "loss: 23149.603516 [ 1580/ 1682]\n",
      "loss: 19101.974609 [ 1590/ 1682]\n",
      "loss: 19192.894531 [ 1600/ 1682]\n",
      "loss: 16958.271484 [ 1610/ 1682]\n",
      "loss: 17443.730469 [ 1620/ 1682]\n",
      "loss: 19729.777344 [ 1630/ 1682]\n",
      "loss: 22616.708984 [ 1640/ 1682]\n",
      "loss: 26229.656250 [ 1650/ 1682]\n",
      "loss: 23115.591797 [ 1660/ 1682]\n",
      "loss: 20353.734375 [ 1670/ 1682]\n",
      "loss: 21076.759766 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 24396.196477 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 257.678223 [    0/ 1682]\n",
      "loss: 244.929764 [   10/ 1682]\n",
      "loss: 241.214233 [   20/ 1682]\n",
      "loss: 266.438782 [   30/ 1682]\n",
      "loss: 299.500793 [   40/ 1682]\n",
      "loss: 334.752533 [   50/ 1682]\n",
      "loss: 353.012573 [   60/ 1682]\n",
      "loss: 296.466888 [   70/ 1682]\n",
      "loss: 221.198898 [   80/ 1682]\n",
      "loss: 249.001907 [   90/ 1682]\n",
      "loss: 261.043488 [  100/ 1682]\n",
      "loss: 245.103790 [  110/ 1682]\n",
      "loss: 248.040131 [  120/ 1682]\n",
      "loss: 267.123962 [  130/ 1682]\n",
      "loss: 329.085510 [  140/ 1682]\n",
      "loss: 619.430969 [  150/ 1682]\n",
      "loss: 332.276215 [  160/ 1682]\n",
      "loss: 366.668671 [  170/ 1682]\n",
      "loss: 400.654572 [  180/ 1682]\n",
      "loss: 429.228760 [  190/ 1682]\n",
      "loss: 400.972839 [  200/ 1682]\n",
      "loss: 353.825867 [  210/ 1682]\n",
      "loss: 847.675415 [  220/ 1682]\n",
      "loss: 400.394104 [  230/ 1682]\n",
      "loss: 436.773102 [  240/ 1682]\n",
      "loss: 698.223938 [  250/ 1682]\n",
      "loss: 549.767639 [  260/ 1682]\n",
      "loss: 595.057495 [  270/ 1682]\n",
      "loss: 654.898560 [  280/ 1682]\n",
      "loss: 675.152771 [  290/ 1682]\n",
      "loss: 706.794434 [  300/ 1682]\n",
      "loss: 696.001282 [  310/ 1682]\n",
      "loss: 727.896851 [  320/ 1682]\n",
      "loss: 840.247925 [  330/ 1682]\n",
      "loss: 880.551880 [  340/ 1682]\n",
      "loss: 857.216187 [  350/ 1682]\n",
      "loss: 765.566406 [  360/ 1682]\n",
      "loss: 748.935486 [  370/ 1682]\n",
      "loss: 844.545227 [  380/ 1682]\n",
      "loss: 892.311890 [  390/ 1682]\n",
      "loss: 964.430542 [  400/ 1682]\n",
      "loss: 1008.281860 [  410/ 1682]\n",
      "loss: 939.909058 [  420/ 1682]\n",
      "loss: 890.814575 [  430/ 1682]\n",
      "loss: 920.877441 [  440/ 1682]\n",
      "loss: 1021.898621 [  450/ 1682]\n",
      "loss: 1192.969849 [  460/ 1682]\n",
      "loss: 1172.094360 [  470/ 1682]\n",
      "loss: 1171.876709 [  480/ 1682]\n",
      "loss: 1171.655884 [  490/ 1682]\n",
      "loss: 1240.878662 [  500/ 1682]\n",
      "loss: 1125.017334 [  510/ 1682]\n",
      "loss: 966.583191 [  520/ 1682]\n",
      "loss: 1226.608398 [  530/ 1682]\n",
      "loss: 1275.544922 [  540/ 1682]\n",
      "loss: 1136.675049 [  550/ 1682]\n",
      "loss: 1197.767944 [  560/ 1682]\n",
      "loss: 1103.808960 [  570/ 1682]\n",
      "loss: 1399.703491 [  580/ 1682]\n",
      "loss: 1435.831055 [  590/ 1682]\n",
      "loss: 1528.292114 [  600/ 1682]\n",
      "loss: 1443.359375 [  610/ 1682]\n",
      "loss: 1450.554932 [  620/ 1682]\n",
      "loss: 1535.101074 [  630/ 1682]\n",
      "loss: 1731.599365 [  640/ 1682]\n",
      "loss: 1978.224976 [  650/ 1682]\n",
      "loss: 2178.684814 [  660/ 1682]\n",
      "loss: 2140.616211 [  670/ 1682]\n",
      "loss: 2234.148193 [  680/ 1682]\n",
      "loss: 2121.898193 [  690/ 1682]\n",
      "loss: 2048.941650 [  700/ 1682]\n",
      "loss: 1688.648071 [  710/ 1682]\n",
      "loss: 1307.350830 [  720/ 1682]\n",
      "loss: 1152.557739 [  730/ 1682]\n",
      "loss: 921.277710 [  740/ 1682]\n",
      "loss: 890.096863 [  750/ 1682]\n",
      "loss: 976.759949 [  760/ 1682]\n",
      "loss: 1205.547119 [  770/ 1682]\n",
      "loss: 1247.449219 [  780/ 1682]\n",
      "loss: 1341.979004 [  790/ 1682]\n",
      "loss: 1537.688232 [  800/ 1682]\n",
      "loss: 1698.189453 [  810/ 1682]\n",
      "loss: 1823.554688 [  820/ 1682]\n",
      "loss: 1759.930664 [  830/ 1682]\n",
      "loss: 1429.992920 [  840/ 1682]\n",
      "loss: 1485.681152 [  850/ 1682]\n",
      "loss: 1696.690063 [  860/ 1682]\n",
      "loss: 1779.502686 [  870/ 1682]\n",
      "loss: 1872.949951 [  880/ 1682]\n",
      "loss: 1814.401001 [  890/ 1682]\n",
      "loss: 1907.818726 [  900/ 1682]\n",
      "loss: 1933.394775 [  910/ 1682]\n",
      "loss: 2196.099365 [  920/ 1682]\n",
      "loss: 2027.834961 [  930/ 1682]\n",
      "loss: 2506.249023 [  940/ 1682]\n",
      "loss: 2857.847900 [  950/ 1682]\n",
      "loss: 3265.713379 [  960/ 1682]\n",
      "loss: 3310.744629 [  970/ 1682]\n",
      "loss: 3605.313965 [  980/ 1682]\n",
      "loss: 4075.996826 [  990/ 1682]\n",
      "loss: 4711.213867 [ 1000/ 1682]\n",
      "loss: 4959.617188 [ 1010/ 1682]\n",
      "loss: 5171.224609 [ 1020/ 1682]\n",
      "loss: 4348.106934 [ 1030/ 1682]\n",
      "loss: 3622.038330 [ 1040/ 1682]\n",
      "loss: 2846.565918 [ 1050/ 1682]\n",
      "loss: 3319.648926 [ 1060/ 1682]\n",
      "loss: 3818.662598 [ 1070/ 1682]\n",
      "loss: 4119.008301 [ 1080/ 1682]\n",
      "loss: 4924.698242 [ 1090/ 1682]\n",
      "loss: 5521.272461 [ 1100/ 1682]\n",
      "loss: 6395.816406 [ 1110/ 1682]\n",
      "loss: 7052.482910 [ 1120/ 1682]\n",
      "loss: 7648.661133 [ 1130/ 1682]\n",
      "loss: 9086.958984 [ 1140/ 1682]\n",
      "loss: 11339.904297 [ 1150/ 1682]\n",
      "loss: 13731.843750 [ 1160/ 1682]\n",
      "loss: 10784.432617 [ 1170/ 1682]\n",
      "loss: 10333.207031 [ 1180/ 1682]\n",
      "loss: 11506.059570 [ 1190/ 1682]\n",
      "loss: 11051.416016 [ 1200/ 1682]\n",
      "loss: 11773.888672 [ 1210/ 1682]\n",
      "loss: 11877.034180 [ 1220/ 1682]\n",
      "loss: 13078.602539 [ 1230/ 1682]\n",
      "loss: 14878.666016 [ 1240/ 1682]\n",
      "loss: 14578.231445 [ 1250/ 1682]\n",
      "loss: 16062.640625 [ 1260/ 1682]\n",
      "loss: 16005.632812 [ 1270/ 1682]\n",
      "loss: 13983.234375 [ 1280/ 1682]\n",
      "loss: 12703.088867 [ 1290/ 1682]\n",
      "loss: 12788.459961 [ 1300/ 1682]\n",
      "loss: 14124.377930 [ 1310/ 1682]\n",
      "loss: 15587.962891 [ 1320/ 1682]\n",
      "loss: 14656.056641 [ 1330/ 1682]\n",
      "loss: 13719.918945 [ 1340/ 1682]\n",
      "loss: 13670.310547 [ 1350/ 1682]\n",
      "loss: 14786.818359 [ 1360/ 1682]\n",
      "loss: 16756.003906 [ 1370/ 1682]\n",
      "loss: 18765.896484 [ 1380/ 1682]\n",
      "loss: 17180.101562 [ 1390/ 1682]\n",
      "loss: 19283.347656 [ 1400/ 1682]\n",
      "loss: 19952.955078 [ 1410/ 1682]\n",
      "loss: 20238.662109 [ 1420/ 1682]\n",
      "loss: 18313.046875 [ 1430/ 1682]\n",
      "loss: 17824.308594 [ 1440/ 1682]\n",
      "loss: 19653.570312 [ 1450/ 1682]\n",
      "loss: 19930.251953 [ 1460/ 1682]\n",
      "loss: 22086.511719 [ 1470/ 1682]\n",
      "loss: 25870.025391 [ 1480/ 1682]\n",
      "loss: 27609.652344 [ 1490/ 1682]\n",
      "loss: 28004.462891 [ 1500/ 1682]\n",
      "loss: 24865.892578 [ 1510/ 1682]\n",
      "loss: 26716.615234 [ 1520/ 1682]\n",
      "loss: 25319.835938 [ 1530/ 1682]\n",
      "loss: 23710.039062 [ 1540/ 1682]\n",
      "loss: 23622.515625 [ 1550/ 1682]\n",
      "loss: 27734.228516 [ 1560/ 1682]\n",
      "loss: 24875.898438 [ 1570/ 1682]\n",
      "loss: 22777.261719 [ 1580/ 1682]\n",
      "loss: 18764.017578 [ 1590/ 1682]\n",
      "loss: 17198.519531 [ 1600/ 1682]\n",
      "loss: 16639.990234 [ 1610/ 1682]\n",
      "loss: 17120.544922 [ 1620/ 1682]\n",
      "loss: 19385.957031 [ 1630/ 1682]\n",
      "loss: 22248.562500 [ 1640/ 1682]\n",
      "loss: 23594.232422 [ 1650/ 1682]\n",
      "loss: 24823.712891 [ 1660/ 1682]\n",
      "loss: 21809.261719 [ 1670/ 1682]\n",
      "loss: 20721.113281 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 23808.028057 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 219.695023 [    0/ 1682]\n",
      "loss: 207.957245 [   10/ 1682]\n",
      "loss: 204.512192 [   20/ 1682]\n",
      "loss: 227.789597 [   30/ 1682]\n",
      "loss: 258.409790 [   40/ 1682]\n",
      "loss: 291.217865 [   50/ 1682]\n",
      "loss: 383.529724 [   60/ 1682]\n",
      "loss: 255.678253 [   70/ 1682]\n",
      "loss: 186.093414 [   80/ 1682]\n",
      "loss: 211.696045 [   90/ 1682]\n",
      "loss: 222.767868 [  100/ 1682]\n",
      "loss: 208.074631 [  110/ 1682]\n",
      "loss: 210.778610 [  120/ 1682]\n",
      "loss: 228.397705 [  130/ 1682]\n",
      "loss: 285.938293 [  140/ 1682]\n",
      "loss: 316.721588 [  150/ 1682]\n",
      "loss: 288.903900 [  160/ 1682]\n",
      "loss: 321.088440 [  170/ 1682]\n",
      "loss: 352.866241 [  180/ 1682]\n",
      "loss: 379.724976 [  190/ 1682]\n",
      "loss: 353.202087 [  200/ 1682]\n",
      "loss: 632.675842 [  210/ 1682]\n",
      "loss: 326.782806 [  220/ 1682]\n",
      "loss: 352.651367 [  230/ 1682]\n",
      "loss: 386.821442 [  240/ 1682]\n",
      "loss: 390.563965 [  250/ 1682]\n",
      "loss: 427.324036 [  260/ 1682]\n",
      "loss: 536.499695 [  270/ 1682]\n",
      "loss: 593.387512 [  280/ 1682]\n",
      "loss: 612.659607 [  290/ 1682]\n",
      "loss: 642.826965 [  300/ 1682]\n",
      "loss: 662.888855 [  310/ 1682]\n",
      "loss: 662.950989 [  320/ 1682]\n",
      "loss: 770.377502 [  330/ 1682]\n",
      "loss: 808.960327 [  340/ 1682]\n",
      "loss: 786.614929 [  350/ 1682]\n",
      "loss: 698.913086 [  360/ 1682]\n",
      "loss: 683.027222 [  370/ 1682]\n",
      "loss: 774.454102 [  380/ 1682]\n",
      "loss: 820.256531 [  390/ 1682]\n",
      "loss: 889.425598 [  400/ 1682]\n",
      "loss: 931.559753 [  410/ 1682]\n",
      "loss: 794.919312 [  420/ 1682]\n",
      "loss: 818.791992 [  430/ 1682]\n",
      "loss: 847.632690 [  440/ 1682]\n",
      "loss: 944.721802 [  450/ 1682]\n",
      "loss: 1109.397339 [  460/ 1682]\n",
      "loss: 1041.339478 [  470/ 1682]\n",
      "loss: 1089.065796 [  480/ 1682]\n",
      "loss: 2054.285645 [  490/ 1682]\n",
      "loss: 1155.637573 [  500/ 1682]\n",
      "loss: 1043.984131 [  510/ 1682]\n",
      "loss: 956.060425 [  520/ 1682]\n",
      "loss: 1141.919434 [  530/ 1682]\n",
      "loss: 1189.150635 [  540/ 1682]\n",
      "loss: 1055.221436 [  550/ 1682]\n",
      "loss: 1114.115479 [  560/ 1682]\n",
      "loss: 1023.618774 [  570/ 1682]\n",
      "loss: 1309.193359 [  580/ 1682]\n",
      "loss: 1344.107422 [  590/ 1682]\n",
      "loss: 1433.632202 [  600/ 1682]\n",
      "loss: 1351.421631 [  610/ 1682]\n",
      "loss: 1358.386841 [  620/ 1682]\n",
      "loss: 1440.243286 [  630/ 1682]\n",
      "loss: 1630.853760 [  640/ 1682]\n",
      "loss: 1870.362549 [  650/ 1682]\n",
      "loss: 1875.016846 [  660/ 1682]\n",
      "loss: 2028.364014 [  670/ 1682]\n",
      "loss: 2119.457031 [  680/ 1682]\n",
      "loss: 2010.177124 [  690/ 1682]\n",
      "loss: 1939.206421 [  700/ 1682]\n",
      "loss: 1589.256470 [  710/ 1682]\n",
      "loss: 1220.021484 [  720/ 1682]\n",
      "loss: 1070.652466 [  730/ 1682]\n",
      "loss: 848.236450 [  740/ 1682]\n",
      "loss: 818.299316 [  750/ 1682]\n",
      "loss: 901.536255 [  760/ 1682]\n",
      "loss: 1121.737061 [  770/ 1682]\n",
      "loss: 1162.179443 [  780/ 1682]\n",
      "loss: 1253.529663 [  790/ 1682]\n",
      "loss: 1442.865723 [  800/ 1682]\n",
      "loss: 1598.468506 [  810/ 1682]\n",
      "loss: 1720.163696 [  820/ 1682]\n",
      "loss: 1639.697266 [  830/ 1682]\n",
      "loss: 1338.675659 [  840/ 1682]\n",
      "loss: 1392.665771 [  850/ 1682]\n",
      "loss: 1597.064209 [  860/ 1682]\n",
      "loss: 1677.441772 [  870/ 1682]\n",
      "loss: 1768.208618 [  880/ 1682]\n",
      "loss: 1711.397461 [  890/ 1682]\n",
      "loss: 1802.134155 [  900/ 1682]\n",
      "loss: 1826.992554 [  910/ 1682]\n",
      "loss: 2082.598145 [  920/ 1682]\n",
      "loss: 2119.217529 [  930/ 1682]\n",
      "loss: 2276.996582 [  940/ 1682]\n",
      "loss: 2728.203369 [  950/ 1682]\n",
      "loss: 3127.011963 [  960/ 1682]\n",
      "loss: 3171.084229 [  970/ 1682]\n",
      "loss: 3459.542480 [  980/ 1682]\n",
      "loss: 3920.924316 [  990/ 1682]\n",
      "loss: 4544.388184 [ 1000/ 1682]\n",
      "loss: 4788.398438 [ 1010/ 1682]\n",
      "loss: 4996.347656 [ 1020/ 1682]\n",
      "loss: 4188.213867 [ 1030/ 1682]\n",
      "loss: 3476.390625 [ 1040/ 1682]\n",
      "loss: 2717.370117 [ 1050/ 1682]\n",
      "loss: 3180.171387 [ 1060/ 1682]\n",
      "loss: 3668.657715 [ 1070/ 1682]\n",
      "loss: 3949.554199 [ 1080/ 1682]\n",
      "loss: 4795.076172 [ 1090/ 1682]\n",
      "loss: 5340.672363 [ 1100/ 1682]\n",
      "loss: 6201.258789 [ 1110/ 1682]\n",
      "loss: 6848.135742 [ 1120/ 1682]\n",
      "loss: 7435.748535 [ 1130/ 1682]\n",
      "loss: 8855.430664 [ 1140/ 1682]\n",
      "loss: 11080.435547 [ 1150/ 1682]\n",
      "loss: 13446.156250 [ 1160/ 1682]\n",
      "loss: 10531.376953 [ 1170/ 1682]\n",
      "loss: 10611.865234 [ 1180/ 1682]\n",
      "loss: 11667.210938 [ 1190/ 1682]\n",
      "loss: 10795.197266 [ 1200/ 1682]\n",
      "loss: 11509.333008 [ 1210/ 1682]\n",
      "loss: 11611.257812 [ 1220/ 1682]\n",
      "loss: 12799.579102 [ 1230/ 1682]\n",
      "loss: 14580.998047 [ 1240/ 1682]\n",
      "loss: 13432.703125 [ 1250/ 1682]\n",
      "loss: 15753.458008 [ 1260/ 1682]\n",
      "loss: 15696.706055 [ 1270/ 1682]\n",
      "loss: 13694.684570 [ 1280/ 1682]\n",
      "loss: 12428.047852 [ 1290/ 1682]\n",
      "loss: 12512.445312 [ 1300/ 1682]\n",
      "loss: 13834.384766 [ 1310/ 1682]\n",
      "loss: 15283.002930 [ 1320/ 1682]\n",
      "loss: 14360.432617 [ 1330/ 1682]\n",
      "loss: 13433.875000 [ 1340/ 1682]\n",
      "loss: 13384.765625 [ 1350/ 1682]\n",
      "loss: 14519.583984 [ 1360/ 1682]\n",
      "loss: 16439.775391 [ 1370/ 1682]\n",
      "loss: 18431.003906 [ 1380/ 1682]\n",
      "loss: 18643.744141 [ 1390/ 1682]\n",
      "loss: 18943.759766 [ 1400/ 1682]\n",
      "loss: 19607.441406 [ 1410/ 1682]\n",
      "loss: 19890.660156 [ 1420/ 1682]\n",
      "loss: 17981.982422 [ 1430/ 1682]\n",
      "loss: 17497.646484 [ 1440/ 1682]\n",
      "loss: 19310.416016 [ 1450/ 1682]\n",
      "loss: 19584.615234 [ 1460/ 1682]\n",
      "loss: 21722.660156 [ 1470/ 1682]\n",
      "loss: 25476.140625 [ 1480/ 1682]\n",
      "loss: 27202.449219 [ 1490/ 1682]\n",
      "loss: 27594.285156 [ 1500/ 1682]\n",
      "loss: 24479.558594 [ 1510/ 1682]\n",
      "loss: 26315.980469 [ 1520/ 1682]\n",
      "loss: 24929.761719 [ 1530/ 1682]\n",
      "loss: 23332.494141 [ 1540/ 1682]\n",
      "loss: 23245.933594 [ 1550/ 1682]\n",
      "loss: 27325.593750 [ 1560/ 1682]\n",
      "loss: 24488.919922 [ 1570/ 1682]\n",
      "loss: 22407.015625 [ 1580/ 1682]\n",
      "loss: 16862.224609 [ 1590/ 1682]\n",
      "loss: 18542.660156 [ 1600/ 1682]\n",
      "loss: 16323.914062 [ 1610/ 1682]\n",
      "loss: 16799.509766 [ 1620/ 1682]\n",
      "loss: 19044.175781 [ 1630/ 1682]\n",
      "loss: 21882.330078 [ 1640/ 1682]\n",
      "loss: 25438.099609 [ 1650/ 1682]\n",
      "loss: 24015.339844 [ 1660/ 1682]\n",
      "loss: 21446.335938 [ 1670/ 1682]\n",
      "loss: 20367.273438 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 23347.039138 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 184.623383 [    0/ 1682]\n",
      "loss: 173.901306 [   10/ 1682]\n",
      "loss: 170.729095 [   20/ 1682]\n",
      "loss: 192.053726 [   30/ 1682]\n",
      "loss: 220.224701 [   40/ 1682]\n",
      "loss: 250.581696 [   50/ 1682]\n",
      "loss: 266.410950 [   60/ 1682]\n",
      "loss: 217.795807 [   70/ 1682]\n",
      "loss: 153.912079 [   80/ 1682]\n",
      "loss: 177.304535 [   90/ 1682]\n",
      "loss: 187.401337 [  100/ 1682]\n",
      "loss: 173.957520 [  110/ 1682]\n",
      "loss: 176.426788 [  120/ 1682]\n",
      "loss: 192.574280 [  130/ 1682]\n",
      "loss: 245.675385 [  140/ 1682]\n",
      "loss: 274.244232 [  150/ 1682]\n",
      "loss: 248.415527 [  160/ 1682]\n",
      "loss: 254.255341 [  170/ 1682]\n",
      "loss: 307.948273 [  180/ 1682]\n",
      "loss: 333.086884 [  190/ 1682]\n",
      "loss: 308.305115 [  200/ 1682]\n",
      "loss: 267.099579 [  210/ 1682]\n",
      "loss: 283.633606 [  220/ 1682]\n",
      "loss: 307.778595 [  230/ 1682]\n",
      "loss: 339.730804 [  240/ 1682]\n",
      "loss: 343.233459 [  250/ 1682]\n",
      "loss: 377.775970 [  260/ 1682]\n",
      "loss: 480.778992 [  270/ 1682]\n",
      "loss: 534.708252 [  280/ 1682]\n",
      "loss: 552.999939 [  290/ 1682]\n",
      "loss: 581.692688 [  300/ 1682]\n",
      "loss: 607.912537 [  310/ 1682]\n",
      "loss: 600.845947 [  320/ 1682]\n",
      "loss: 703.338318 [  330/ 1682]\n",
      "loss: 740.200806 [  340/ 1682]\n",
      "loss: 718.853882 [  350/ 1682]\n",
      "loss: 635.116333 [  360/ 1682]\n",
      "loss: 619.981995 [  370/ 1682]\n",
      "loss: 707.218201 [  380/ 1682]\n",
      "loss: 751.054199 [  390/ 1682]\n",
      "loss: 817.268982 [  400/ 1682]\n",
      "loss: 857.684753 [  410/ 1682]\n",
      "loss: 794.759521 [  420/ 1682]\n",
      "loss: 749.632324 [  430/ 1682]\n",
      "loss: 777.248108 [  440/ 1682]\n",
      "loss: 870.395325 [  450/ 1682]\n",
      "loss: 1028.659912 [  460/ 1682]\n",
      "loss: 1009.286804 [  470/ 1682]\n",
      "loss: 1009.091675 [  480/ 1682]\n",
      "loss: 1008.893860 [  490/ 1682]\n",
      "loss: 1208.377075 [  500/ 1682]\n",
      "loss: 965.760437 [  510/ 1682]\n",
      "loss: 881.303894 [  520/ 1682]\n",
      "loss: 1060.010010 [  530/ 1682]\n",
      "loss: 1105.522583 [  540/ 1682]\n",
      "loss: 976.542786 [  550/ 1682]\n",
      "loss: 1033.224976 [  560/ 1682]\n",
      "loss: 946.196472 [  570/ 1682]\n",
      "loss: 1221.410767 [  580/ 1682]\n",
      "loss: 1255.101685 [  590/ 1682]\n",
      "loss: 1341.675537 [  600/ 1682]\n",
      "loss: 1262.193604 [  610/ 1682]\n",
      "loss: 1826.992798 [  620/ 1682]\n",
      "loss: 1348.087524 [  630/ 1682]\n",
      "loss: 1532.795410 [  640/ 1682]\n",
      "loss: 1610.015869 [  650/ 1682]\n",
      "loss: 1954.833252 [  660/ 1682]\n",
      "loss: 1918.778564 [  670/ 1682]\n",
      "loss: 2007.426514 [  680/ 1682]\n",
      "loss: 1901.128540 [  690/ 1682]\n",
      "loss: 1832.151733 [  700/ 1682]\n",
      "loss: 1492.580200 [  710/ 1682]\n",
      "loss: 1135.447998 [  720/ 1682]\n",
      "loss: 991.521484 [  730/ 1682]\n",
      "loss: 761.281860 [  740/ 1682]\n",
      "loss: 749.312622 [  750/ 1682]\n",
      "loss: 829.116577 [  760/ 1682]\n",
      "loss: 1040.709229 [  770/ 1682]\n",
      "loss: 1079.690918 [  780/ 1682]\n",
      "loss: 1167.856201 [  790/ 1682]\n",
      "loss: 1245.780884 [  800/ 1682]\n",
      "loss: 1501.498901 [  810/ 1682]\n",
      "loss: 1619.519287 [  820/ 1682]\n",
      "loss: 1559.866699 [  830/ 1682]\n",
      "loss: 1250.132446 [  840/ 1682]\n",
      "loss: 1302.410400 [  850/ 1682]\n",
      "loss: 1453.178833 [  860/ 1682]\n",
      "loss: 1578.099731 [  870/ 1682]\n",
      "loss: 1666.170898 [  880/ 1682]\n",
      "loss: 1611.097778 [  890/ 1682]\n",
      "loss: 1699.139404 [  900/ 1682]\n",
      "loss: 1723.273682 [  910/ 1682]\n",
      "loss: 1971.753540 [  920/ 1682]\n",
      "loss: 2007.402710 [  930/ 1682]\n",
      "loss: 2266.231934 [  940/ 1682]\n",
      "loss: 2601.144043 [  950/ 1682]\n",
      "loss: 2990.849121 [  960/ 1682]\n",
      "loss: 3033.944824 [  970/ 1682]\n",
      "loss: 3316.255859 [  980/ 1682]\n",
      "loss: 3768.290283 [  990/ 1682]\n",
      "loss: 4379.944336 [ 1000/ 1682]\n",
      "loss: 4344.240723 [ 1010/ 1682]\n",
      "loss: 4823.805176 [ 1020/ 1682]\n",
      "loss: 4030.708984 [ 1030/ 1682]\n",
      "loss: 3333.185059 [ 1040/ 1682]\n",
      "loss: 2590.680420 [ 1050/ 1682]\n",
      "loss: 3043.155762 [ 1060/ 1682]\n",
      "loss: 3521.071045 [ 1070/ 1682]\n",
      "loss: 3751.604248 [ 1080/ 1682]\n",
      "loss: 4626.104492 [ 1090/ 1682]\n",
      "loss: 5162.365234 [ 1100/ 1682]\n",
      "loss: 6008.939453 [ 1110/ 1682]\n",
      "loss: 6645.988281 [ 1120/ 1682]\n",
      "loss: 7225.006836 [ 1130/ 1682]\n",
      "loss: 8625.998047 [ 1140/ 1682]\n",
      "loss: 10822.958984 [ 1150/ 1682]\n",
      "loss: 13162.356445 [ 1160/ 1682]\n",
      "loss: 10280.340820 [ 1170/ 1682]\n",
      "loss: 10359.865234 [ 1180/ 1682]\n",
      "loss: 11402.919922 [ 1190/ 1682]\n",
      "loss: 10541.017578 [ 1200/ 1682]\n",
      "loss: 11246.802734 [ 1210/ 1682]\n",
      "loss: 11347.512695 [ 1220/ 1682]\n",
      "loss: 12522.556641 [ 1230/ 1682]\n",
      "loss: 14285.279297 [ 1240/ 1682]\n",
      "loss: 13990.762695 [ 1250/ 1682]\n",
      "loss: 15446.182617 [ 1260/ 1682]\n",
      "loss: 15027.255859 [ 1270/ 1682]\n",
      "loss: 13408.107422 [ 1280/ 1682]\n",
      "loss: 12155.029297 [ 1290/ 1682]\n",
      "loss: 12238.451172 [ 1300/ 1682]\n",
      "loss: 13546.359375 [ 1310/ 1682]\n",
      "loss: 14979.960938 [ 1320/ 1682]\n",
      "loss: 14066.759766 [ 1330/ 1682]\n",
      "loss: 13149.822266 [ 1340/ 1682]\n",
      "loss: 13101.216797 [ 1350/ 1682]\n",
      "loss: 14224.216797 [ 1360/ 1682]\n",
      "loss: 16125.439453 [ 1370/ 1682]\n",
      "loss: 18097.949219 [ 1380/ 1682]\n",
      "loss: 18308.720703 [ 1390/ 1682]\n",
      "loss: 18606.027344 [ 1400/ 1682]\n",
      "loss: 19263.796875 [ 1410/ 1682]\n",
      "loss: 19544.562500 [ 1420/ 1682]\n",
      "loss: 17652.896484 [ 1430/ 1682]\n",
      "loss: 17173.005859 [ 1440/ 1682]\n",
      "loss: 18969.259766 [ 1450/ 1682]\n",
      "loss: 19240.996094 [ 1460/ 1682]\n",
      "loss: 21360.787109 [ 1470/ 1682]\n",
      "loss: 25084.169922 [ 1480/ 1682]\n",
      "loss: 26797.162109 [ 1490/ 1682]\n",
      "loss: 27186.031250 [ 1500/ 1682]\n",
      "loss: 24095.230469 [ 1510/ 1682]\n",
      "loss: 25917.333984 [ 1520/ 1682]\n",
      "loss: 24541.722656 [ 1530/ 1682]\n",
      "loss: 22957.041016 [ 1540/ 1682]\n",
      "loss: 22871.458984 [ 1550/ 1682]\n",
      "loss: 26919.021484 [ 1560/ 1682]\n",
      "loss: 24104.066406 [ 1570/ 1682]\n",
      "loss: 22038.949219 [ 1580/ 1682]\n",
      "loss: 18094.757812 [ 1590/ 1682]\n",
      "loss: 18207.908203 [ 1600/ 1682]\n",
      "loss: 16010.142578 [ 1610/ 1682]\n",
      "loss: 16480.781250 [ 1620/ 1682]\n",
      "loss: 18704.675781 [ 1630/ 1682]\n",
      "loss: 21518.337891 [ 1640/ 1682]\n",
      "loss: 25045.410156 [ 1650/ 1682]\n",
      "loss: 24051.859375 [ 1660/ 1682]\n",
      "loss: 21085.751953 [ 1670/ 1682]\n",
      "loss: 20015.824219 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 23172.847206 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 152.546661 [    0/ 1682]\n",
      "loss: 142.845566 [   10/ 1682]\n",
      "loss: 250.201630 [   20/ 1682]\n",
      "loss: 199.957138 [   30/ 1682]\n",
      "loss: 185.045715 [   40/ 1682]\n",
      "loss: 212.952789 [   50/ 1682]\n",
      "loss: 227.567902 [   60/ 1682]\n",
      "loss: 182.933456 [   70/ 1682]\n",
      "loss: 124.760864 [   80/ 1682]\n",
      "loss: 145.944901 [   90/ 1682]\n",
      "loss: 155.069336 [  100/ 1682]\n",
      "loss: 142.878830 [  110/ 1682]\n",
      "loss: 204.177109 [  120/ 1682]\n",
      "loss: 159.794556 [  130/ 1682]\n",
      "loss: 208.457977 [  140/ 1682]\n",
      "loss: 234.815216 [  150/ 1682]\n",
      "loss: 210.978729 [  160/ 1682]\n",
      "loss: 238.735596 [  170/ 1682]\n",
      "loss: 266.085663 [  180/ 1682]\n",
      "loss: 289.505371 [  190/ 1682]\n",
      "loss: 266.465851 [  200/ 1682]\n",
      "loss: 228.237885 [  210/ 1682]\n",
      "loss: 243.546509 [  220/ 1682]\n",
      "loss: 265.971771 [  230/ 1682]\n",
      "loss: 295.710022 [  240/ 1682]\n",
      "loss: 298.975861 [  250/ 1682]\n",
      "loss: 331.304504 [  260/ 1682]\n",
      "loss: 415.741760 [  270/ 1682]\n",
      "loss: 479.117432 [  280/ 1682]\n",
      "loss: 496.433289 [  290/ 1682]\n",
      "loss: 523.656433 [  300/ 1682]\n",
      "loss: 541.763123 [  310/ 1682]\n",
      "loss: 541.846497 [  320/ 1682]\n",
      "loss: 639.412720 [  330/ 1682]\n",
      "loss: 674.559448 [  340/ 1682]\n",
      "loss: 654.212769 [  350/ 1682]\n",
      "loss: 577.671509 [  360/ 1682]\n",
      "loss: 560.058167 [  370/ 1682]\n",
      "loss: 643.113159 [  380/ 1682]\n",
      "loss: 684.989624 [  390/ 1682]\n",
      "loss: 748.258118 [  400/ 1682]\n",
      "loss: 786.961487 [  410/ 1682]\n",
      "loss: 726.764587 [  420/ 1682]\n",
      "loss: 683.624634 [  430/ 1682]\n",
      "loss: 710.021118 [  440/ 1682]\n",
      "loss: 799.236328 [  450/ 1682]\n",
      "loss: 856.436401 [  460/ 1682]\n",
      "loss: 932.486328 [  470/ 1682]\n",
      "loss: 932.307312 [  480/ 1682]\n",
      "loss: 932.125000 [  490/ 1682]\n",
      "loss: 993.988647 [  500/ 1682]\n",
      "loss: 890.742065 [  510/ 1682]\n",
      "loss: 809.745972 [  520/ 1682]\n",
      "loss: 981.314941 [  530/ 1682]\n",
      "loss: 1025.113037 [  540/ 1682]\n",
      "loss: 901.073120 [  550/ 1682]\n",
      "loss: 955.548645 [  560/ 1682]\n",
      "loss: 871.981567 [  570/ 1682]\n",
      "loss: 1136.858643 [  580/ 1682]\n",
      "loss: 1169.329956 [  590/ 1682]\n",
      "loss: 1252.959717 [  600/ 1682]\n",
      "loss: 1176.200928 [  610/ 1682]\n",
      "loss: 1216.796143 [  620/ 1682]\n",
      "loss: 1259.154663 [  630/ 1682]\n",
      "loss: 1437.954834 [  640/ 1682]\n",
      "loss: 1663.188721 [  650/ 1682]\n",
      "loss: 1847.432861 [  660/ 1682]\n",
      "loss: 1812.366455 [  670/ 1682]\n",
      "loss: 1898.553955 [  680/ 1682]\n",
      "loss: 1795.218384 [  690/ 1682]\n",
      "loss: 1728.219971 [  700/ 1682]\n",
      "loss: 1399.007446 [  710/ 1682]\n",
      "loss: 1053.960571 [  720/ 1682]\n",
      "loss: 915.466003 [  730/ 1682]\n",
      "loss: 710.825562 [  740/ 1682]\n",
      "loss: 683.379272 [  750/ 1682]\n",
      "loss: 759.739746 [  760/ 1682]\n",
      "loss: 867.639160 [  770/ 1682]\n",
      "loss: 1000.226868 [  780/ 1682]\n",
      "loss: 1085.198975 [  790/ 1682]\n",
      "loss: 1261.749756 [  800/ 1682]\n",
      "loss: 1407.526489 [  810/ 1682]\n",
      "loss: 1521.861084 [  820/ 1682]\n",
      "loss: 1464.191650 [  830/ 1682]\n",
      "loss: 1164.574219 [  840/ 1682]\n",
      "loss: 1215.139282 [  850/ 1682]\n",
      "loss: 1406.258789 [  860/ 1682]\n",
      "loss: 1481.735107 [  870/ 1682]\n",
      "loss: 1567.108643 [  880/ 1682]\n",
      "loss: 1513.774170 [  890/ 1682]\n",
      "loss: 1599.119385 [  900/ 1682]\n",
      "loss: 1622.528564 [  910/ 1682]\n",
      "loss: 1863.877197 [  920/ 1682]\n",
      "loss: 1898.555908 [  930/ 1682]\n",
      "loss: 2150.501953 [  940/ 1682]\n",
      "loss: 2477.054199 [  950/ 1682]\n",
      "loss: 2857.660889 [  960/ 1682]\n",
      "loss: 2899.788330 [  970/ 1682]\n",
      "loss: 3175.958496 [  980/ 1682]\n",
      "loss: 3618.650879 [  990/ 1682]\n",
      "loss: 4218.500977 [ 1000/ 1682]\n",
      "loss: 4453.674805 [ 1010/ 1682]\n",
      "loss: 4654.268555 [ 1020/ 1682]\n",
      "loss: 3876.212891 [ 1030/ 1682]\n",
      "loss: 3192.990967 [ 1040/ 1682]\n",
      "loss: 2467.004395 [ 1050/ 1682]\n",
      "loss: 2909.153076 [ 1060/ 1682]\n",
      "loss: 3376.496094 [ 1070/ 1682]\n",
      "loss: 4035.385498 [ 1080/ 1682]\n",
      "loss: 4460.128906 [ 1090/ 1682]\n",
      "loss: 4987.038086 [ 1100/ 1682]\n",
      "loss: 5819.581543 [ 1110/ 1682]\n",
      "loss: 6446.788086 [ 1120/ 1682]\n",
      "loss: 7017.196289 [ 1130/ 1682]\n",
      "loss: 8399.476562 [ 1140/ 1682]\n",
      "loss: 10100.504883 [ 1150/ 1682]\n",
      "loss: 12881.423828 [ 1160/ 1682]\n",
      "loss: 10032.191406 [ 1170/ 1682]\n",
      "loss: 10110.748047 [ 1180/ 1682]\n",
      "loss: 10236.480469 [ 1190/ 1682]\n",
      "loss: 10289.735352 [ 1200/ 1682]\n",
      "loss: 10987.177734 [ 1210/ 1682]\n",
      "loss: 11086.683594 [ 1220/ 1682]\n",
      "loss: 11627.439453 [ 1230/ 1682]\n",
      "loss: 13992.495117 [ 1240/ 1682]\n",
      "loss: 13700.965820 [ 1250/ 1682]\n",
      "loss: 15141.887695 [ 1260/ 1682]\n",
      "loss: 15085.662109 [ 1270/ 1682]\n",
      "loss: 13124.559570 [ 1280/ 1682]\n",
      "loss: 10898.119141 [ 1290/ 1682]\n",
      "loss: 11967.528320 [ 1300/ 1682]\n",
      "loss: 13261.435547 [ 1310/ 1682]\n",
      "loss: 14680.046875 [ 1320/ 1682]\n",
      "loss: 13776.237305 [ 1330/ 1682]\n",
      "loss: 12868.935547 [ 1340/ 1682]\n",
      "loss: 12820.851562 [ 1350/ 1682]\n",
      "loss: 13932.059570 [ 1360/ 1682]\n",
      "loss: 15814.345703 [ 1370/ 1682]\n",
      "loss: 17768.160156 [ 1380/ 1682]\n",
      "loss: 17976.988281 [ 1390/ 1682]\n",
      "loss: 18271.605469 [ 1400/ 1682]\n",
      "loss: 18923.484375 [ 1410/ 1682]\n",
      "loss: 19201.804688 [ 1420/ 1682]\n",
      "loss: 17327.154297 [ 1430/ 1682]\n",
      "loss: 16851.712891 [ 1440/ 1682]\n",
      "loss: 17308.378906 [ 1450/ 1682]\n",
      "loss: 18900.781250 [ 1460/ 1682]\n",
      "loss: 21002.373047 [ 1470/ 1682]\n",
      "loss: 24620.316406 [ 1480/ 1682]\n",
      "loss: 26395.441406 [ 1490/ 1682]\n",
      "loss: 26781.371094 [ 1500/ 1682]\n",
      "loss: 23714.490234 [ 1510/ 1682]\n",
      "loss: 25522.324219 [ 1520/ 1682]\n",
      "loss: 24157.318359 [ 1530/ 1682]\n",
      "loss: 22585.218750 [ 1540/ 1682]\n",
      "loss: 20517.355469 [ 1550/ 1682]\n",
      "loss: 26516.199219 [ 1560/ 1682]\n",
      "loss: 23722.978516 [ 1570/ 1682]\n",
      "loss: 19972.265625 [ 1580/ 1682]\n",
      "loss: 15988.835938 [ 1590/ 1682]\n",
      "loss: 17877.035156 [ 1600/ 1682]\n",
      "loss: 15700.294922 [ 1610/ 1682]\n",
      "loss: 15923.629883 [ 1620/ 1682]\n",
      "loss: 18369.351562 [ 1630/ 1682]\n",
      "loss: 18260.843750 [ 1640/ 1682]\n",
      "loss: 24107.658203 [ 1650/ 1682]\n",
      "loss: 23671.833984 [ 1660/ 1682]\n",
      "loss: 18970.001953 [ 1670/ 1682]\n",
      "loss: 19669.380859 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 21499.006423 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 123.702065 [    0/ 1682]\n",
      "loss: 547.073608 [   10/ 1682]\n",
      "loss: 302.052887 [   20/ 1682]\n",
      "loss: 1871.079712 [   30/ 1682]\n",
      "loss: 212.708420 [   40/ 1682]\n",
      "loss: 180.964066 [   50/ 1682]\n",
      "loss: 220.686081 [   60/ 1682]\n",
      "loss: 291.693390 [   70/ 1682]\n",
      "loss: 98.940125 [   80/ 1682]\n",
      "loss: 138.508270 [   90/ 1682]\n",
      "loss: 108.325027 [  100/ 1682]\n",
      "loss: 193.641159 [  110/ 1682]\n",
      "loss: 117.210190 [  120/ 1682]\n",
      "loss: 130.014847 [  130/ 1682]\n",
      "loss: 174.745239 [  140/ 1682]\n",
      "loss: 187.986359 [  150/ 1682]\n",
      "loss: 168.430450 [  160/ 1682]\n",
      "loss: 194.010162 [  170/ 1682]\n",
      "loss: 227.834259 [  180/ 1682]\n",
      "loss: 240.142853 [  190/ 1682]\n",
      "loss: 217.373856 [  200/ 1682]\n",
      "loss: 191.191467 [  210/ 1682]\n",
      "loss: 207.073364 [  220/ 1682]\n",
      "loss: 212.909866 [  230/ 1682]\n",
      "loss: 257.578308 [  240/ 1682]\n",
      "loss: 258.415985 [  250/ 1682]\n",
      "loss: 288.572205 [  260/ 1682]\n",
      "loss: 404.262390 [  270/ 1682]\n",
      "loss: 427.422211 [  280/ 1682]\n",
      "loss: 480.891846 [  290/ 1682]\n",
      "loss: 498.722992 [  300/ 1682]\n",
      "loss: 486.710144 [  310/ 1682]\n",
      "loss: 496.357513 [  320/ 1682]\n",
      "loss: 574.814087 [  330/ 1682]\n",
      "loss: 613.331543 [  340/ 1682]\n",
      "loss: 591.904968 [  350/ 1682]\n",
      "loss: 507.462891 [  360/ 1682]\n",
      "loss: 504.112061 [  370/ 1682]\n",
      "loss: 532.993286 [  380/ 1682]\n",
      "loss: 573.523315 [  390/ 1682]\n",
      "loss: 621.341431 [  400/ 1682]\n",
      "loss: 572.127625 [  410/ 1682]\n",
      "loss: 604.955872 [  420/ 1682]\n",
      "loss: 523.518188 [  430/ 1682]\n",
      "loss: 646.890747 [  440/ 1682]\n",
      "loss: 619.471008 [  450/ 1682]\n",
      "loss: 715.347656 [  460/ 1682]\n",
      "loss: 625.327515 [  470/ 1682]\n",
      "loss: 831.343567 [  480/ 1682]\n",
      "loss: 859.645325 [  490/ 1682]\n",
      "loss: 714.230469 [  500/ 1682]\n",
      "loss: 820.006348 [  510/ 1682]\n",
      "loss: 742.423523 [  520/ 1682]\n",
      "loss: 805.429565 [  530/ 1682]\n",
      "loss: 872.411255 [  540/ 1682]\n",
      "loss: 651.467468 [  550/ 1682]\n",
      "loss: 840.489868 [  560/ 1682]\n",
      "loss: 802.132446 [  570/ 1682]\n",
      "loss: 999.911621 [  580/ 1682]\n",
      "loss: 1034.169678 [  590/ 1682]\n",
      "loss: 932.597046 [  600/ 1682]\n",
      "loss: 996.232422 [  610/ 1682]\n",
      "loss: 1079.699829 [  620/ 1682]\n",
      "loss: 1056.509033 [  630/ 1682]\n",
      "loss: 1157.042114 [  640/ 1682]\n",
      "loss: 1419.323242 [  650/ 1682]\n",
      "loss: 1694.752686 [  660/ 1682]\n",
      "loss: 1711.276367 [  670/ 1682]\n",
      "loss: 1226.682373 [  680/ 1682]\n",
      "loss: 1532.796143 [  690/ 1682]\n",
      "loss: 969.911133 [  700/ 1682]\n",
      "loss: 1254.132935 [  710/ 1682]\n",
      "loss: 820.033325 [  720/ 1682]\n",
      "loss: 1083.593506 [  730/ 1682]\n",
      "loss: 613.808533 [  740/ 1682]\n",
      "loss: 342.432709 [  750/ 1682]\n",
      "loss: 695.173645 [  760/ 1682]\n",
      "loss: 670.101196 [  770/ 1682]\n",
      "loss: 839.484070 [  780/ 1682]\n",
      "loss: 931.879517 [  790/ 1682]\n",
      "loss: 1091.187744 [  800/ 1682]\n",
      "loss: 1319.268799 [  810/ 1682]\n",
      "loss: 1225.958984 [  820/ 1682]\n",
      "loss: 1131.210693 [  830/ 1682]\n",
      "loss: 967.357056 [  840/ 1682]\n",
      "loss: 1133.515625 [  850/ 1682]\n",
      "loss: 1038.836182 [  860/ 1682]\n",
      "loss: 1318.121460 [  870/ 1682]\n",
      "loss: 1343.868896 [  880/ 1682]\n",
      "loss: 1324.612793 [  890/ 1682]\n",
      "loss: 1241.971680 [  900/ 1682]\n",
      "loss: 1337.878174 [  910/ 1682]\n",
      "loss: 1416.752686 [  920/ 1682]\n",
      "loss: 1632.606079 [  930/ 1682]\n",
      "loss: 1848.849243 [  940/ 1682]\n",
      "loss: 1658.383789 [  950/ 1682]\n",
      "loss: 1928.956299 [  960/ 1682]\n",
      "loss: 2505.626953 [  970/ 1682]\n",
      "loss: 2753.694824 [  980/ 1682]\n",
      "loss: 3477.487061 [  990/ 1682]\n",
      "loss: 3866.962158 [ 1000/ 1682]\n",
      "loss: 3863.784424 [ 1010/ 1682]\n",
      "loss: 3783.112061 [ 1020/ 1682]\n",
      "loss: 2414.213867 [ 1030/ 1682]\n",
      "loss: 2998.030518 [ 1040/ 1682]\n",
      "loss: 2149.057129 [ 1050/ 1682]\n",
      "loss: 2501.315430 [ 1060/ 1682]\n",
      "loss: 2353.251465 [ 1070/ 1682]\n",
      "loss: 3198.154297 [ 1080/ 1682]\n",
      "loss: 3743.962402 [ 1090/ 1682]\n",
      "loss: 4314.204590 [ 1100/ 1682]\n",
      "loss: 5641.749023 [ 1110/ 1682]\n",
      "loss: 6259.711914 [ 1120/ 1682]\n",
      "loss: 6027.622070 [ 1130/ 1682]\n",
      "loss: 6513.407715 [ 1140/ 1682]\n",
      "loss: 9407.153320 [ 1150/ 1682]\n",
      "loss: 7974.815430 [ 1160/ 1682]\n",
      "loss: 9254.115234 [ 1170/ 1682]\n",
      "loss: 7091.836914 [ 1180/ 1682]\n",
      "loss: 9461.852539 [ 1190/ 1682]\n",
      "loss: 9015.247070 [ 1200/ 1682]\n",
      "loss: 8812.988281 [ 1210/ 1682]\n",
      "loss: 8724.669922 [ 1220/ 1682]\n",
      "loss: 10113.334961 [ 1230/ 1682]\n",
      "loss: 8906.629883 [ 1240/ 1682]\n",
      "loss: 7380.235840 [ 1250/ 1682]\n",
      "loss: 6705.156250 [ 1260/ 1682]\n",
      "loss: 4453.668945 [ 1270/ 1682]\n",
      "loss: 8485.953125 [ 1280/ 1682]\n",
      "loss: 10823.658203 [ 1290/ 1682]\n",
      "loss: 13010.322266 [ 1300/ 1682]\n",
      "loss: 11242.978516 [ 1310/ 1682]\n",
      "loss: 4883.078125 [ 1320/ 1682]\n",
      "loss: 3168.327637 [ 1330/ 1682]\n",
      "loss: 2358.742920 [ 1340/ 1682]\n",
      "loss: 4825.586914 [ 1350/ 1682]\n",
      "loss: 3713.745361 [ 1360/ 1682]\n",
      "loss: 3433.637451 [ 1370/ 1682]\n",
      "loss: 2332.232178 [ 1380/ 1682]\n",
      "loss: 2333.935059 [ 1390/ 1682]\n",
      "loss: 5508.143555 [ 1400/ 1682]\n",
      "loss: 1861.298584 [ 1410/ 1682]\n",
      "loss: 2725.504150 [ 1420/ 1682]\n",
      "loss: 1291.229492 [ 1430/ 1682]\n",
      "loss: 2729.480957 [ 1440/ 1682]\n",
      "loss: 3054.442627 [ 1450/ 1682]\n",
      "loss: 2069.089355 [ 1460/ 1682]\n",
      "loss: 2391.936768 [ 1470/ 1682]\n",
      "loss: 1189.026611 [ 1480/ 1682]\n",
      "loss: 3456.924561 [ 1490/ 1682]\n",
      "loss: 3672.914795 [ 1500/ 1682]\n",
      "loss: 1409.813232 [ 1510/ 1682]\n",
      "loss: 3003.698486 [ 1520/ 1682]\n",
      "loss: 2363.093262 [ 1530/ 1682]\n",
      "loss: 2819.102539 [ 1540/ 1682]\n",
      "loss: 1856.856689 [ 1550/ 1682]\n",
      "loss: 1416.991455 [ 1560/ 1682]\n",
      "loss: 2129.441895 [ 1570/ 1682]\n",
      "loss: 2754.414062 [ 1580/ 1682]\n",
      "loss: 1934.114990 [ 1590/ 1682]\n",
      "loss: 3914.344482 [ 1600/ 1682]\n",
      "loss: 2784.203857 [ 1610/ 1682]\n",
      "loss: 917.745605 [ 1620/ 1682]\n",
      "loss: 1572.942139 [ 1630/ 1682]\n",
      "loss: 3674.294922 [ 1640/ 1682]\n",
      "loss: 2372.871582 [ 1650/ 1682]\n",
      "loss: 2152.821289 [ 1660/ 1682]\n",
      "loss: 1195.887695 [ 1670/ 1682]\n",
      "loss: 7279.225098 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3136.732013 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 6188.395508 [    0/ 1682]\n",
      "loss: 5068.334961 [   10/ 1682]\n",
      "loss: 1371.335205 [   20/ 1682]\n",
      "loss: 12.737780 [   30/ 1682]\n",
      "loss: 216.970047 [   40/ 1682]\n",
      "loss: 629.178772 [   50/ 1682]\n",
      "loss: 1126.418213 [   60/ 1682]\n",
      "loss: 1074.663330 [   70/ 1682]\n",
      "loss: 899.885925 [   80/ 1682]\n",
      "loss: 608.920654 [   90/ 1682]\n",
      "loss: 315.313171 [  100/ 1682]\n",
      "loss: 333.151367 [  110/ 1682]\n",
      "loss: 466.333801 [  120/ 1682]\n",
      "loss: 506.972748 [  130/ 1682]\n",
      "loss: 700.277466 [  140/ 1682]\n",
      "loss: 429.295898 [  150/ 1682]\n",
      "loss: 201.766922 [  160/ 1682]\n",
      "loss: 180.522751 [  170/ 1682]\n",
      "loss: 210.272369 [  180/ 1682]\n",
      "loss: 262.651520 [  190/ 1682]\n",
      "loss: 188.797897 [  200/ 1682]\n",
      "loss: 193.219925 [  210/ 1682]\n",
      "loss: 516.549194 [  220/ 1682]\n",
      "loss: 210.438675 [  230/ 1682]\n",
      "loss: 236.973068 [  240/ 1682]\n",
      "loss: 239.917938 [  250/ 1682]\n",
      "loss: 241.875046 [  260/ 1682]\n",
      "loss: 400.973541 [  270/ 1682]\n",
      "loss: 403.606628 [  280/ 1682]\n",
      "loss: 419.517090 [  290/ 1682]\n",
      "loss: 444.614014 [  300/ 1682]\n",
      "loss: 461.312195 [  310/ 1682]\n",
      "loss: 510.231018 [  320/ 1682]\n",
      "loss: 548.823608 [  330/ 1682]\n",
      "loss: 531.336304 [  340/ 1682]\n",
      "loss: 628.484497 [  350/ 1682]\n",
      "loss: 491.598877 [  360/ 1682]\n",
      "loss: 478.315857 [  370/ 1682]\n",
      "loss: 1084.597900 [  380/ 1682]\n",
      "loss: 544.694458 [  390/ 1682]\n",
      "loss: 578.223694 [  400/ 1682]\n",
      "loss: 629.318970 [  410/ 1682]\n",
      "loss: 670.149048 [  420/ 1682]\n",
      "loss: 493.019684 [  430/ 1682]\n",
      "loss: 569.111023 [  440/ 1682]\n",
      "loss: 627.546082 [  450/ 1682]\n",
      "loss: 794.379211 [  460/ 1682]\n",
      "loss: 1119.966553 [  470/ 1682]\n",
      "loss: 757.210571 [  480/ 1682]\n",
      "loss: 825.889282 [  490/ 1682]\n",
      "loss: 1128.583496 [  500/ 1682]\n",
      "loss: 646.787903 [  510/ 1682]\n",
      "loss: 700.143433 [  520/ 1682]\n",
      "loss: 872.313782 [  530/ 1682]\n",
      "loss: 913.630371 [  540/ 1682]\n",
      "loss: 720.183899 [  550/ 1682]\n",
      "loss: 795.904175 [  560/ 1682]\n",
      "loss: 700.096497 [  570/ 1682]\n",
      "loss: 956.436218 [  580/ 1682]\n",
      "loss: 1050.120972 [  590/ 1682]\n",
      "loss: 832.086731 [  600/ 1682]\n",
      "loss: 975.081726 [  610/ 1682]\n",
      "loss: 957.975098 [  620/ 1682]\n",
      "loss: 919.704285 [  630/ 1682]\n",
      "loss: 1224.987427 [  640/ 1682]\n",
      "loss: 1344.512207 [  650/ 1682]\n",
      "loss: 1377.619019 [  660/ 1682]\n",
      "loss: 1524.892456 [  670/ 1682]\n",
      "loss: 4323.268066 [  680/ 1682]\n",
      "loss: 1537.502563 [  690/ 1682]\n",
      "loss: 1582.972412 [  700/ 1682]\n",
      "loss: 1141.112915 [  710/ 1682]\n",
      "loss: 772.298218 [  720/ 1682]\n",
      "loss: 740.391479 [  730/ 1682]\n",
      "loss: 665.107178 [  740/ 1682]\n",
      "loss: 509.609375 [  750/ 1682]\n",
      "loss: 540.802673 [  760/ 1682]\n",
      "loss: 821.970215 [  770/ 1682]\n",
      "loss: 798.932373 [  780/ 1682]\n",
      "loss: 873.042358 [  790/ 1682]\n",
      "loss: 1092.119995 [  800/ 1682]\n",
      "loss: 954.251770 [  810/ 1682]\n",
      "loss: 1143.156250 [  820/ 1682]\n",
      "loss: 1226.799683 [  830/ 1682]\n",
      "loss: 851.103882 [  840/ 1682]\n",
      "loss: 1087.913818 [  850/ 1682]\n",
      "loss: 1275.918823 [  860/ 1682]\n",
      "loss: 1211.351562 [  870/ 1682]\n",
      "loss: 1215.515869 [  880/ 1682]\n",
      "loss: 1306.505493 [  890/ 1682]\n",
      "loss: 1339.082031 [  900/ 1682]\n",
      "loss: 1385.311523 [  910/ 1682]\n",
      "loss: 1473.840332 [  920/ 1682]\n",
      "loss: 1465.354248 [  930/ 1682]\n",
      "loss: 1988.743774 [  940/ 1682]\n",
      "loss: 2068.431641 [  950/ 1682]\n",
      "loss: 2269.646240 [  960/ 1682]\n",
      "loss: 1924.539795 [  970/ 1682]\n",
      "loss: 2385.792725 [  980/ 1682]\n",
      "loss: 2827.190186 [  990/ 1682]\n",
      "loss: 2838.151123 [ 1000/ 1682]\n",
      "loss: 3586.887939 [ 1010/ 1682]\n",
      "loss: 4046.777344 [ 1020/ 1682]\n",
      "loss: 3323.608154 [ 1030/ 1682]\n",
      "loss: 2777.248779 [ 1040/ 1682]\n",
      "loss: 1993.212891 [ 1050/ 1682]\n",
      "loss: 2555.363770 [ 1060/ 1682]\n",
      "loss: 2505.345215 [ 1070/ 1682]\n",
      "loss: 3215.774658 [ 1080/ 1682]\n",
      "loss: 3617.106689 [ 1090/ 1682]\n",
      "loss: 3132.336426 [ 1100/ 1682]\n",
      "loss: 3978.278564 [ 1110/ 1682]\n",
      "loss: 5155.535156 [ 1120/ 1682]\n",
      "loss: 5174.270996 [ 1130/ 1682]\n",
      "loss: 5848.278809 [ 1140/ 1682]\n",
      "loss: 6484.206543 [ 1150/ 1682]\n",
      "loss: 10793.629883 [ 1160/ 1682]\n",
      "loss: 2857.324707 [ 1170/ 1682]\n",
      "loss: 3359.623047 [ 1180/ 1682]\n",
      "loss: 7926.121094 [ 1190/ 1682]\n",
      "loss: 2482.603760 [ 1200/ 1682]\n",
      "loss: 7030.867188 [ 1210/ 1682]\n",
      "loss: 8516.117188 [ 1220/ 1682]\n",
      "loss: 6995.231934 [ 1230/ 1682]\n",
      "loss: 2271.084473 [ 1240/ 1682]\n",
      "loss: 4773.844238 [ 1250/ 1682]\n",
      "loss: 11559.966797 [ 1260/ 1682]\n",
      "loss: 1696.544678 [ 1270/ 1682]\n",
      "loss: 5874.717773 [ 1280/ 1682]\n",
      "loss: 8206.072266 [ 1290/ 1682]\n",
      "loss: 7723.777344 [ 1300/ 1682]\n",
      "loss: 8187.148438 [ 1310/ 1682]\n",
      "loss: 6179.327148 [ 1320/ 1682]\n",
      "loss: 1743.442383 [ 1330/ 1682]\n",
      "loss: 16782.947266 [ 1340/ 1682]\n",
      "loss: 2201.876709 [ 1350/ 1682]\n",
      "loss: 3749.056641 [ 1360/ 1682]\n",
      "loss: 10091.922852 [ 1370/ 1682]\n",
      "loss: 11559.159180 [ 1380/ 1682]\n",
      "loss: 9475.576172 [ 1390/ 1682]\n",
      "loss: 4241.264160 [ 1400/ 1682]\n",
      "loss: 2158.068848 [ 1410/ 1682]\n",
      "loss: 5987.282715 [ 1420/ 1682]\n",
      "loss: 3268.894287 [ 1430/ 1682]\n",
      "loss: 2258.181641 [ 1440/ 1682]\n",
      "loss: 4741.489258 [ 1450/ 1682]\n",
      "loss: 4490.371582 [ 1460/ 1682]\n",
      "loss: 4168.016113 [ 1470/ 1682]\n",
      "loss: 3362.718750 [ 1480/ 1682]\n",
      "loss: 3435.887939 [ 1490/ 1682]\n",
      "loss: 1592.827393 [ 1500/ 1682]\n",
      "loss: 4187.553223 [ 1510/ 1682]\n",
      "loss: 2061.948730 [ 1520/ 1682]\n",
      "loss: 4293.168945 [ 1530/ 1682]\n",
      "loss: 5264.074707 [ 1540/ 1682]\n",
      "loss: 3149.764648 [ 1550/ 1682]\n",
      "loss: 1269.022827 [ 1560/ 1682]\n",
      "loss: 5394.481934 [ 1570/ 1682]\n",
      "loss: 1875.630493 [ 1580/ 1682]\n",
      "loss: 2647.321533 [ 1590/ 1682]\n",
      "loss: 864.135071 [ 1600/ 1682]\n",
      "loss: 982.281738 [ 1610/ 1682]\n",
      "loss: 1405.003296 [ 1620/ 1682]\n",
      "loss: 1420.918701 [ 1630/ 1682]\n",
      "loss: 3802.655518 [ 1640/ 1682]\n",
      "loss: 1984.902588 [ 1650/ 1682]\n",
      "loss: 1946.789062 [ 1660/ 1682]\n",
      "loss: 6915.903320 [ 1670/ 1682]\n",
      "loss: 2438.094482 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3139.915779 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3181.048340 [    0/ 1682]\n",
      "loss: 750.437866 [   10/ 1682]\n",
      "loss: 149.766556 [   20/ 1682]\n",
      "loss: 27.607895 [   30/ 1682]\n",
      "loss: 195.995514 [   40/ 1682]\n",
      "loss: 420.980957 [   50/ 1682]\n",
      "loss: 558.816895 [   60/ 1682]\n",
      "loss: 572.631348 [   70/ 1682]\n",
      "loss: 345.836090 [   80/ 1682]\n",
      "loss: 470.938721 [   90/ 1682]\n",
      "loss: 395.426239 [  100/ 1682]\n",
      "loss: 336.368835 [  110/ 1682]\n",
      "loss: 358.609436 [  120/ 1682]\n",
      "loss: 247.896515 [  130/ 1682]\n",
      "loss: 328.626160 [  140/ 1682]\n",
      "loss: 268.390564 [  150/ 1682]\n",
      "loss: 165.566254 [  160/ 1682]\n",
      "loss: 236.738129 [  170/ 1682]\n",
      "loss: 260.948914 [  180/ 1682]\n",
      "loss: 388.993591 [  190/ 1682]\n",
      "loss: 327.643127 [  200/ 1682]\n",
      "loss: 185.537994 [  210/ 1682]\n",
      "loss: 174.559845 [  220/ 1682]\n",
      "loss: 232.732224 [  230/ 1682]\n",
      "loss: 254.682953 [  240/ 1682]\n",
      "loss: 295.749237 [  250/ 1682]\n",
      "loss: 250.025101 [  260/ 1682]\n",
      "loss: 334.938080 [  270/ 1682]\n",
      "loss: 381.798462 [  280/ 1682]\n",
      "loss: 533.770142 [  290/ 1682]\n",
      "loss: 499.001404 [  300/ 1682]\n",
      "loss: 436.242676 [  310/ 1682]\n",
      "loss: 472.978180 [  320/ 1682]\n",
      "loss: 831.188293 [  330/ 1682]\n",
      "loss: 556.198792 [  340/ 1682]\n",
      "loss: 559.515991 [  350/ 1682]\n",
      "loss: 594.975037 [  360/ 1682]\n",
      "loss: 538.731079 [  370/ 1682]\n",
      "loss: 624.365051 [  380/ 1682]\n",
      "loss: 593.535278 [  390/ 1682]\n",
      "loss: 721.723877 [  400/ 1682]\n",
      "loss: 714.203064 [  410/ 1682]\n",
      "loss: 603.697266 [  420/ 1682]\n",
      "loss: 564.384888 [  430/ 1682]\n",
      "loss: 654.253296 [  440/ 1682]\n",
      "loss: 714.136292 [  450/ 1682]\n",
      "loss: 812.400879 [  460/ 1682]\n",
      "loss: 1055.203735 [  470/ 1682]\n",
      "loss: 973.114136 [  480/ 1682]\n",
      "loss: 804.345093 [  490/ 1682]\n",
      "loss: 947.586243 [  500/ 1682]\n",
      "loss: 809.123413 [  510/ 1682]\n",
      "loss: 698.036987 [  520/ 1682]\n",
      "loss: 837.202759 [  530/ 1682]\n",
      "loss: 877.646790 [  540/ 1682]\n",
      "loss: 763.175110 [  550/ 1682]\n",
      "loss: 1013.471680 [  560/ 1682]\n",
      "loss: 743.290222 [  570/ 1682]\n",
      "loss: 988.526001 [  580/ 1682]\n",
      "loss: 1011.305298 [  590/ 1682]\n",
      "loss: 1089.181396 [  600/ 1682]\n",
      "loss: 1017.685425 [  610/ 1682]\n",
      "loss: 1023.698242 [  620/ 1682]\n",
      "loss: 1094.878540 [  630/ 1682]\n",
      "loss: 1262.170654 [  640/ 1682]\n",
      "loss: 1473.497314 [  650/ 1682]\n",
      "loss: 1647.184570 [  660/ 1682]\n",
      "loss: 1936.101807 [  670/ 1682]\n",
      "loss: 1695.399658 [  680/ 1682]\n",
      "loss: 1597.802979 [  690/ 1682]\n",
      "loss: 1534.626221 [  700/ 1682]\n",
      "loss: 1225.550537 [  710/ 1682]\n",
      "loss: 905.742188 [  720/ 1682]\n",
      "loss: 776.066528 [  730/ 1682]\n",
      "loss: 588.705444 [  740/ 1682]\n",
      "loss: 563.659729 [  750/ 1682]\n",
      "loss: 666.641479 [  760/ 1682]\n",
      "loss: 819.459656 [  770/ 1682]\n",
      "loss: 854.086609 [  780/ 1682]\n",
      "loss: 932.807739 [  790/ 1682]\n",
      "loss: 1096.859375 [  800/ 1682]\n",
      "loss: 1233.015869 [  810/ 1682]\n",
      "loss: 1340.131470 [  820/ 1682]\n",
      "loss: 1286.298584 [  830/ 1682]\n",
      "loss: 1006.386902 [  840/ 1682]\n",
      "loss: 1053.579712 [  850/ 1682]\n",
      "loss: 1231.706055 [  860/ 1682]\n",
      "loss: 1302.363403 [  870/ 1682]\n",
      "loss: 1382.435181 [  880/ 1682]\n",
      "loss: 1332.459229 [  890/ 1682]\n",
      "loss: 1412.500244 [  900/ 1682]\n",
      "loss: 1434.454468 [  910/ 1682]\n",
      "loss: 1661.828857 [  920/ 1682]\n",
      "loss: 1694.567383 [  930/ 1682]\n",
      "loss: 2007.395264 [  940/ 1682]\n",
      "loss: 2243.165527 [  950/ 1682]\n",
      "loss: 2605.920898 [  960/ 1682]\n",
      "loss: 2646.092285 [  970/ 1682]\n",
      "loss: 2910.180420 [  980/ 1682]\n",
      "loss: 3334.514893 [  990/ 1682]\n",
      "loss: 3911.173828 [ 1000/ 1682]\n",
      "loss: 4137.614258 [ 1010/ 1682]\n",
      "loss: 4330.913086 [ 1020/ 1682]\n",
      "loss: 3722.512451 [ 1030/ 1682]\n",
      "loss: 2926.962891 [ 1040/ 1682]\n",
      "loss: 2233.286133 [ 1050/ 1682]\n",
      "loss: 2655.105713 [ 1060/ 1682]\n",
      "loss: 3273.344482 [ 1070/ 1682]\n",
      "loss: 3734.356201 [ 1080/ 1682]\n",
      "loss: 4143.079590 [ 1090/ 1682]\n",
      "loss: 5308.007324 [ 1100/ 1682]\n",
      "loss: 5456.414551 [ 1110/ 1682]\n",
      "loss: 6064.161133 [ 1120/ 1682]\n",
      "loss: 6617.510742 [ 1130/ 1682]\n",
      "loss: 7962.853027 [ 1140/ 1682]\n",
      "loss: 10076.354492 [ 1150/ 1682]\n",
      "loss: 12337.349609 [ 1160/ 1682]\n",
      "loss: 9552.416016 [ 1170/ 1682]\n",
      "loss: 11624.842773 [ 1180/ 1682]\n",
      "loss: 10635.089844 [ 1190/ 1682]\n",
      "loss: 9802.986328 [ 1200/ 1682]\n",
      "loss: 10483.651367 [ 1210/ 1682]\n",
      "loss: 10580.467773 [ 1220/ 1682]\n",
      "loss: 11844.981445 [ 1230/ 1682]\n",
      "loss: 13422.419922 [ 1240/ 1682]\n",
      "loss: 13350.445312 [ 1250/ 1682]\n",
      "loss: 14548.240234 [ 1260/ 1682]\n",
      "loss: 14683.679688 [ 1270/ 1682]\n",
      "loss: 12571.236328 [ 1280/ 1682]\n",
      "loss: 11358.282227 [ 1290/ 1682]\n",
      "loss: 11608.595703 [ 1300/ 1682]\n",
      "loss: 12864.887695 [ 1310/ 1682]\n",
      "loss: 14092.541992 [ 1320/ 1682]\n",
      "loss: 13206.994141 [ 1330/ 1682]\n",
      "loss: 12811.882812 [ 1340/ 1682]\n",
      "loss: 12271.025391 [ 1350/ 1682]\n",
      "loss: 13358.307617 [ 1360/ 1682]\n",
      "loss: 15202.376953 [ 1370/ 1682]\n",
      "loss: 17118.410156 [ 1380/ 1682]\n",
      "loss: 17628.416016 [ 1390/ 1682]\n",
      "loss: 17611.644531 [ 1400/ 1682]\n",
      "loss: 18251.246094 [ 1410/ 1682]\n",
      "loss: 18298.246094 [ 1420/ 1682]\n",
      "loss: 16683.148438 [ 1430/ 1682]\n",
      "loss: 16216.177734 [ 1440/ 1682]\n",
      "loss: 17967.875000 [ 1450/ 1682]\n",
      "loss: 18260.394531 [ 1460/ 1682]\n",
      "loss: 20290.699219 [ 1470/ 1682]\n",
      "loss: 23923.041016 [ 1480/ 1682]\n",
      "loss: 25587.615234 [ 1490/ 1682]\n",
      "loss: 25974.746094 [ 1500/ 1682]\n",
      "loss: 22955.480469 [ 1510/ 1682]\n",
      "loss: 24733.869141 [ 1520/ 1682]\n",
      "loss: 23389.679688 [ 1530/ 1682]\n",
      "loss: 21842.412109 [ 1540/ 1682]\n",
      "loss: 21759.261719 [ 1550/ 1682]\n",
      "loss: 25709.308594 [ 1560/ 1682]\n",
      "loss: 22959.410156 [ 1570/ 1682]\n",
      "loss: 20793.667969 [ 1580/ 1682]\n",
      "loss: 17104.380859 [ 1590/ 1682]\n",
      "loss: 17213.318359 [ 1600/ 1682]\n",
      "loss: 15078.681641 [ 1610/ 1682]\n",
      "loss: 15533.929688 [ 1620/ 1682]\n",
      "loss: 17298.787109 [ 1630/ 1682]\n",
      "loss: 20433.503906 [ 1640/ 1682]\n",
      "loss: 23684.640625 [ 1650/ 1682]\n",
      "loss: 22903.019531 [ 1660/ 1682]\n",
      "loss: 19077.218750 [ 1670/ 1682]\n",
      "loss: 18967.335938 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 20692.502592 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 87.120827 [    0/ 1682]\n",
      "loss: 30854.724609 [   10/ 1682]\n",
      "loss: 66.951073 [   20/ 1682]\n",
      "loss: 75.763245 [   30/ 1682]\n",
      "loss: 96.898026 [   40/ 1682]\n",
      "loss: 109.192528 [   50/ 1682]\n",
      "loss: 128.254333 [   60/ 1682]\n",
      "loss: 88.861084 [   70/ 1682]\n",
      "loss: 54.905689 [   80/ 1682]\n",
      "loss: 69.363693 [   90/ 1682]\n",
      "loss: 75.521843 [  100/ 1682]\n",
      "loss: 118.656883 [  110/ 1682]\n",
      "loss: 68.661095 [  120/ 1682]\n",
      "loss: 78.858810 [  130/ 1682]\n",
      "loss: 114.004662 [  140/ 1682]\n",
      "loss: 133.618149 [  150/ 1682]\n",
      "loss: 115.835220 [  160/ 1682]\n",
      "loss: 136.845581 [  170/ 1682]\n",
      "loss: 157.447998 [  180/ 1682]\n",
      "loss: 175.625000 [  190/ 1682]\n",
      "loss: 157.881775 [  200/ 1682]\n",
      "loss: 128.713287 [  210/ 1682]\n",
      "loss: 141.388840 [  220/ 1682]\n",
      "loss: 157.462265 [  230/ 1682]\n",
      "loss: 180.449203 [  240/ 1682]\n",
      "loss: 182.985626 [  250/ 1682]\n",
      "loss: 208.562836 [  260/ 1682]\n",
      "loss: 286.604279 [  270/ 1682]\n",
      "loss: 328.570618 [  280/ 1682]\n",
      "loss: 342.897552 [  290/ 1682]\n",
      "loss: 365.626709 [  300/ 1682]\n",
      "loss: 390.401978 [  310/ 1682]\n",
      "loss: 380.849915 [  320/ 1682]\n",
      "loss: 474.030273 [  330/ 1682]\n",
      "loss: 493.286804 [  340/ 1682]\n",
      "loss: 475.972595 [  350/ 1682]\n",
      "loss: 408.260315 [  360/ 1682]\n",
      "loss: 369.720459 [  370/ 1682]\n",
      "loss: 509.057312 [  380/ 1682]\n",
      "loss: 502.315857 [  390/ 1682]\n",
      "loss: 556.573914 [  400/ 1682]\n",
      "loss: 590.029907 [  410/ 1682]\n",
      "loss: 538.131714 [  420/ 1682]\n",
      "loss: 489.961853 [  430/ 1682]\n",
      "loss: 526.709290 [  440/ 1682]\n",
      "loss: 600.883789 [  450/ 1682]\n",
      "loss: 733.206360 [  460/ 1682]\n",
      "loss: 716.866455 [  470/ 1682]\n",
      "loss: 733.295044 [  480/ 1682]\n",
      "loss: 716.545410 [  490/ 1682]\n",
      "loss: 770.903320 [  500/ 1682]\n",
      "loss: 680.471375 [  510/ 1682]\n",
      "loss: 610.032104 [  520/ 1682]\n",
      "loss: 759.796570 [  530/ 1682]\n",
      "loss: 798.347290 [  540/ 1682]\n",
      "loss: 689.380981 [  550/ 1682]\n",
      "loss: 737.104187 [  560/ 1682]\n",
      "loss: 664.095520 [  570/ 1682]\n",
      "loss: 900.417603 [  580/ 1682]\n",
      "loss: 926.111206 [  590/ 1682]\n",
      "loss: 1000.729858 [  600/ 1682]\n",
      "loss: 932.271484 [  610/ 1682]\n",
      "loss: 938.037415 [  620/ 1682]\n",
      "loss: 1006.234741 [  630/ 1682]\n",
      "loss: 1166.989380 [  640/ 1682]\n",
      "loss: 1370.413086 [  650/ 1682]\n",
      "loss: 1538.115234 [  660/ 1682]\n",
      "loss: 1506.078613 [  670/ 1682]\n",
      "loss: 1584.755127 [  680/ 1682]\n",
      "loss: 1490.471924 [  690/ 1682]\n",
      "loss: 1429.514282 [  700/ 1682]\n",
      "loss: 1131.944214 [  710/ 1682]\n",
      "loss: 823.813965 [  720/ 1682]\n",
      "loss: 701.907227 [  730/ 1682]\n",
      "loss: 538.806641 [  740/ 1682]\n",
      "loss: 500.741852 [  750/ 1682]\n",
      "loss: 566.581665 [  760/ 1682]\n",
      "loss: 743.216797 [  770/ 1682]\n",
      "loss: 776.231262 [  780/ 1682]\n",
      "loss: 852.326172 [  790/ 1682]\n",
      "loss: 1008.417175 [  800/ 1682]\n",
      "loss: 1194.423950 [  810/ 1682]\n",
      "loss: 1242.199585 [  820/ 1682]\n",
      "loss: 1190.574341 [  830/ 1682]\n",
      "loss: 985.014038 [  840/ 1682]\n",
      "loss: 967.173462 [  850/ 1682]\n",
      "loss: 1137.968628 [  860/ 1682]\n",
      "loss: 1213.161865 [  870/ 1682]\n",
      "loss: 1283.030640 [  880/ 1682]\n",
      "loss: 1234.988037 [  890/ 1682]\n",
      "loss: 1312.061523 [  900/ 1682]\n",
      "loss: 1333.226807 [  910/ 1682]\n",
      "loss: 1552.735596 [  920/ 1682]\n",
      "loss: 1593.621704 [  930/ 1682]\n",
      "loss: 1815.271484 [  940/ 1682]\n",
      "loss: 2116.207520 [  950/ 1682]\n",
      "loss: 2468.932617 [  960/ 1682]\n",
      "loss: 2508.052979 [  970/ 1682]\n",
      "loss: 2732.383301 [  980/ 1682]\n",
      "loss: 3198.580566 [  990/ 1682]\n",
      "loss: 3743.076660 [ 1000/ 1682]\n",
      "loss: 3964.669922 [ 1010/ 1682]\n",
      "loss: 4153.939453 [ 1020/ 1682]\n",
      "loss: 3421.887207 [ 1030/ 1682]\n",
      "loss: 2782.414551 [ 1040/ 1682]\n",
      "loss: 2106.979004 [ 1050/ 1682]\n",
      "loss: 2517.434570 [ 1060/ 1682]\n",
      "loss: 2952.321777 [ 1070/ 1682]\n",
      "loss: 3570.444580 [ 1080/ 1682]\n",
      "loss: 3970.262207 [ 1090/ 1682]\n",
      "loss: 4472.374023 [ 1100/ 1682]\n",
      "loss: 5257.937012 [ 1110/ 1682]\n",
      "loss: 5857.012207 [ 1120/ 1682]\n",
      "loss: 6398.827148 [ 1130/ 1682]\n",
      "loss: 7723.636719 [ 1140/ 1682]\n",
      "loss: 9806.312500 [ 1150/ 1682]\n",
      "loss: 12038.396484 [ 1160/ 1682]\n",
      "loss: 9289.584961 [ 1170/ 1682]\n",
      "loss: 9365.001953 [ 1180/ 1682]\n",
      "loss: 10357.822266 [ 1190/ 1682]\n",
      "loss: 9536.924805 [ 1200/ 1682]\n",
      "loss: 10208.491211 [ 1210/ 1682]\n",
      "loss: 10304.060547 [ 1220/ 1682]\n",
      "loss: 11424.778320 [ 1230/ 1682]\n",
      "loss: 13111.060547 [ 1240/ 1682]\n",
      "loss: 12828.418945 [ 1250/ 1682]\n",
      "loss: 14224.404297 [ 1260/ 1682]\n",
      "loss: 14168.706055 [ 1270/ 1682]\n",
      "loss: 12270.326172 [ 1280/ 1682]\n",
      "loss: 11072.337891 [ 1290/ 1682]\n",
      "loss: 10950.172852 [ 1300/ 1682]\n",
      "loss: 12401.962891 [ 1310/ 1682]\n",
      "loss: 13774.001953 [ 1320/ 1682]\n",
      "loss: 12898.818359 [ 1330/ 1682]\n",
      "loss: 12020.927734 [ 1340/ 1682]\n",
      "loss: 11974.135742 [ 1350/ 1682]\n",
      "loss: 13048.591797 [ 1360/ 1682]\n",
      "loss: 14872.021484 [ 1370/ 1682]\n",
      "loss: 16767.691406 [ 1380/ 1682]\n",
      "loss: 16970.128906 [ 1390/ 1682]\n",
      "loss: 17256.056641 [ 1400/ 1682]\n",
      "loss: 17889.322266 [ 1410/ 1682]\n",
      "loss: 18159.683594 [ 1420/ 1682]\n",
      "loss: 16337.354492 [ 1430/ 1682]\n",
      "loss: 15875.341797 [ 1440/ 1682]\n",
      "loss: 17603.625000 [ 1450/ 1682]\n",
      "loss: 17864.937500 [ 1460/ 1682]\n",
      "loss: 19909.603516 [ 1470/ 1682]\n",
      "loss: 23509.269531 [ 1480/ 1682]\n",
      "loss: 25167.224609 [ 1490/ 1682]\n",
      "loss: 25543.500000 [ 1500/ 1682]\n",
      "loss: 22550.439453 [ 1510/ 1682]\n",
      "loss: 24313.371094 [ 1520/ 1682]\n",
      "loss: 22980.863281 [ 1530/ 1682]\n",
      "loss: 21447.421875 [ 1540/ 1682]\n",
      "loss: 21572.128906 [ 1550/ 1682]\n",
      "loss: 25280.746094 [ 1560/ 1682]\n",
      "loss: 22554.599609 [ 1570/ 1682]\n",
      "loss: 20558.052734 [ 1580/ 1682]\n",
      "loss: 16755.619141 [ 1590/ 1682]\n",
      "loss: 16863.330078 [ 1600/ 1682]\n",
      "loss: 14751.599609 [ 1610/ 1682]\n",
      "loss: 15201.588867 [ 1620/ 1682]\n",
      "loss: 17339.732422 [ 1630/ 1682]\n",
      "loss: 19912.876953 [ 1640/ 1682]\n",
      "loss: 23460.943359 [ 1650/ 1682]\n",
      "loss: 22499.550781 [ 1660/ 1682]\n",
      "loss: 19632.849609 [ 1670/ 1682]\n",
      "loss: 18600.242188 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 21746.581280 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 52.657196 [    0/ 1682]\n",
      "loss: 47.157238 [   10/ 1682]\n",
      "loss: 45.394718 [   20/ 1682]\n",
      "loss: 56.708210 [   30/ 1682]\n",
      "loss: 72.317871 [   40/ 1682]\n",
      "loss: 90.100479 [   50/ 1682]\n",
      "loss: 99.686165 [   60/ 1682]\n",
      "loss: 71.458427 [   70/ 1682]\n",
      "loss: 36.840549 [   80/ 1682]\n",
      "loss: 48.897575 [   90/ 1682]\n",
      "loss: 53.995918 [  100/ 1682]\n",
      "loss: 46.968056 [  110/ 1682]\n",
      "loss: 48.235676 [  120/ 1682]\n",
      "loss: 56.832649 [  130/ 1682]\n",
      "loss: 87.151932 [  140/ 1682]\n",
      "loss: 104.356583 [  150/ 1682]\n",
      "loss: 88.733597 [  160/ 1682]\n",
      "loss: 107.334427 [  170/ 1682]\n",
      "loss: 125.527054 [  180/ 1682]\n",
      "loss: 141.831680 [  190/ 1682]\n",
      "loss: 125.979126 [  200/ 1682]\n",
      "loss: 100.045006 [  210/ 1682]\n",
      "loss: 109.242004 [  220/ 1682]\n",
      "loss: 125.586510 [  230/ 1682]\n",
      "loss: 146.163635 [  240/ 1682]\n",
      "loss: 146.908447 [  250/ 1682]\n",
      "loss: 171.608490 [  260/ 1682]\n",
      "loss: 242.942429 [  270/ 1682]\n",
      "loss: 281.695099 [  280/ 1682]\n",
      "loss: 294.958252 [  290/ 1682]\n",
      "loss: 361.672516 [  300/ 1682]\n",
      "loss: 330.141296 [  310/ 1682]\n",
      "loss: 330.257874 [  320/ 1682]\n",
      "loss: 407.434967 [  330/ 1682]\n",
      "loss: 437.449371 [  340/ 1682]\n",
      "loss: 419.241394 [  350/ 1682]\n",
      "loss: 355.837341 [  360/ 1682]\n",
      "loss: 344.553040 [  370/ 1682]\n",
      "loss: 410.284851 [  380/ 1682]\n",
      "loss: 444.027527 [  390/ 1682]\n",
      "loss: 495.079987 [  400/ 1682]\n",
      "loss: 526.672607 [  410/ 1682]\n",
      "loss: 477.740051 [  420/ 1682]\n",
      "loss: 442.801941 [  430/ 1682]\n",
      "loss: 464.130524 [  440/ 1682]\n",
      "loss: 537.051025 [  450/ 1682]\n",
      "loss: 662.418518 [  460/ 1682]\n",
      "loss: 578.989136 [  470/ 1682]\n",
      "loss: 649.386414 [  480/ 1682]\n",
      "loss: 646.615845 [  490/ 1682]\n",
      "loss: 698.312988 [  500/ 1682]\n",
      "loss: 612.454590 [  510/ 1682]\n",
      "loss: 545.784058 [  520/ 1682]\n",
      "loss: 687.799500 [  530/ 1682]\n",
      "loss: 724.494995 [  540/ 1682]\n",
      "loss: 620.902954 [  550/ 1682]\n",
      "loss: 666.235413 [  560/ 1682]\n",
      "loss: 596.993042 [  570/ 1682]\n",
      "loss: 819.057983 [  580/ 1682]\n",
      "loss: 846.469604 [  590/ 1682]\n",
      "loss: 917.898132 [  600/ 1682]\n",
      "loss: 852.403625 [  610/ 1682]\n",
      "loss: 857.923157 [  620/ 1682]\n",
      "loss: 923.199890 [  630/ 1682]\n",
      "loss: 1077.558838 [  640/ 1682]\n",
      "loss: 1273.251953 [  650/ 1682]\n",
      "loss: 1435.097046 [  660/ 1682]\n",
      "loss: 1404.155396 [  670/ 1682]\n",
      "loss: 1480.185425 [  680/ 1682]\n",
      "loss: 1389.134644 [  690/ 1682]\n",
      "loss: 1330.340332 [  700/ 1682]\n",
      "loss: 1057.065674 [  710/ 1682]\n",
      "loss: 748.995728 [  720/ 1682]\n",
      "loss: 632.985962 [  730/ 1682]\n",
      "loss: 465.111664 [  740/ 1682]\n",
      "loss: 442.808167 [  750/ 1682]\n",
      "loss: 504.929871 [  760/ 1682]\n",
      "loss: 672.243164 [  770/ 1682]\n",
      "loss: 703.677002 [  780/ 1682]\n",
      "loss: 775.428833 [  790/ 1682]\n",
      "loss: 925.500000 [  800/ 1682]\n",
      "loss: 1050.917236 [  810/ 1682]\n",
      "loss: 1140.094482 [  820/ 1682]\n",
      "loss: 1100.526123 [  830/ 1682]\n",
      "loss: 842.771118 [  840/ 1682]\n",
      "loss: 886.238098 [  850/ 1682]\n",
      "loss: 1049.860596 [  860/ 1682]\n",
      "loss: 1115.179810 [  870/ 1682]\n",
      "loss: 1189.376709 [  880/ 1682]\n",
      "loss: 1143.224976 [  890/ 1682]\n",
      "loss: 1217.393799 [  900/ 1682]\n",
      "loss: 1237.786255 [  910/ 1682]\n",
      "loss: 1449.598999 [  920/ 1682]\n",
      "loss: 1480.241577 [  930/ 1682]\n",
      "loss: 1703.671631 [  940/ 1682]\n",
      "loss: 1995.587524 [  950/ 1682]\n",
      "loss: 2338.498047 [  960/ 1682]\n",
      "loss: 2376.586670 [  970/ 1682]\n",
      "loss: 2641.858398 [  980/ 1682]\n",
      "loss: 3042.846924 [  990/ 1682]\n",
      "loss: 3582.190186 [ 1000/ 1682]\n",
      "loss: 3799.036621 [ 1010/ 1682]\n",
      "loss: 3984.360107 [ 1020/ 1682]\n",
      "loss: 3268.564453 [ 1030/ 1682]\n",
      "loss: 2632.003662 [ 1040/ 1682]\n",
      "loss: 1986.956299 [ 1050/ 1682]\n",
      "loss: 2386.287598 [ 1060/ 1682]\n",
      "loss: 2809.787598 [ 1070/ 1682]\n",
      "loss: 3413.604248 [ 1080/ 1682]\n",
      "loss: 3804.699219 [ 1090/ 1682]\n",
      "loss: 4292.852051 [ 1100/ 1682]\n",
      "loss: 5067.238281 [ 1110/ 1682]\n",
      "loss: 5653.630371 [ 1120/ 1682]\n",
      "loss: 6188.317871 [ 1130/ 1682]\n",
      "loss: 7493.013184 [ 1140/ 1682]\n",
      "loss: 9545.498047 [ 1150/ 1682]\n",
      "loss: 11749.256836 [ 1160/ 1682]\n",
      "loss: 9035.793945 [ 1170/ 1682]\n",
      "loss: 9110.215820 [ 1180/ 1682]\n",
      "loss: 10089.845703 [ 1190/ 1682]\n",
      "loss: 9279.886719 [ 1200/ 1682]\n",
      "loss: 9942.507812 [ 1210/ 1682]\n",
      "loss: 10036.821289 [ 1220/ 1682]\n",
      "loss: 11143.302734 [ 1230/ 1682]\n",
      "loss: 12809.521484 [ 1240/ 1682]\n",
      "loss: 12530.135742 [ 1250/ 1682]\n",
      "loss: 13910.573242 [ 1260/ 1682]\n",
      "loss: 13951.323242 [ 1270/ 1682]\n",
      "loss: 11978.904297 [ 1280/ 1682]\n",
      "loss: 10795.556641 [ 1290/ 1682]\n",
      "loss: 10873.798828 [ 1300/ 1682]\n",
      "loss: 12109.189453 [ 1310/ 1682]\n",
      "loss: 13465.159180 [ 1320/ 1682]\n",
      "loss: 12600.115234 [ 1330/ 1682]\n",
      "loss: 11732.623047 [ 1340/ 1682]\n",
      "loss: 11686.439453 [ 1350/ 1682]\n",
      "loss: 12748.277344 [ 1360/ 1682]\n",
      "loss: 14551.426758 [ 1370/ 1682]\n",
      "loss: 16427.085938 [ 1380/ 1682]\n",
      "loss: 16627.496094 [ 1390/ 1682]\n",
      "loss: 16910.611328 [ 1400/ 1682]\n",
      "loss: 17537.623047 [ 1410/ 1682]\n",
      "loss: 17662.738281 [ 1420/ 1682]\n",
      "loss: 16001.432617 [ 1430/ 1682]\n",
      "loss: 15544.270508 [ 1440/ 1682]\n",
      "loss: 17254.949219 [ 1450/ 1682]\n",
      "loss: 17513.716797 [ 1460/ 1682]\n",
      "loss: 19538.939453 [ 1470/ 1682]\n",
      "loss: 23106.496094 [ 1480/ 1682]\n",
      "loss: 24750.294922 [ 1490/ 1682]\n",
      "loss: 25123.515625 [ 1500/ 1682]\n",
      "loss: 22156.156250 [ 1510/ 1682]\n",
      "loss: 23110.214844 [ 1520/ 1682]\n",
      "loss: 22582.849609 [ 1530/ 1682]\n",
      "loss: 21062.974609 [ 1540/ 1682]\n",
      "loss: 20982.191406 [ 1550/ 1682]\n",
      "loss: 24863.306641 [ 1560/ 1682]\n",
      "loss: 22160.472656 [ 1570/ 1682]\n",
      "loss: 20181.964844 [ 1580/ 1682]\n",
      "loss: 16162.525391 [ 1590/ 1682]\n",
      "loss: 15637.783203 [ 1600/ 1682]\n",
      "loss: 11908.015625 [ 1610/ 1682]\n",
      "loss: 14878.693359 [ 1620/ 1682]\n",
      "loss: 16994.876953 [ 1630/ 1682]\n",
      "loss: 19681.566406 [ 1640/ 1682]\n",
      "loss: 22721.748047 [ 1650/ 1682]\n",
      "loss: 19201.132812 [ 1660/ 1682]\n",
      "loss: 16007.634766 [ 1670/ 1682]\n",
      "loss: 15394.822266 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11471.973670 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2559.844482 [    0/ 1682]\n",
      "loss: 19.159655 [   10/ 1682]\n",
      "loss: 9698.590820 [   20/ 1682]\n",
      "loss: 5174.836914 [   30/ 1682]\n",
      "loss: 251.036865 [   40/ 1682]\n",
      "loss: 378.100922 [   50/ 1682]\n",
      "loss: 471.474304 [   60/ 1682]\n",
      "loss: 132.829849 [   70/ 1682]\n",
      "loss: 872.790833 [   80/ 1682]\n",
      "loss: 67.965508 [   90/ 1682]\n",
      "loss: 742.097656 [  100/ 1682]\n",
      "loss: 1007.514343 [  110/ 1682]\n",
      "loss: 84.161484 [  120/ 1682]\n",
      "loss: 874.555847 [  130/ 1682]\n",
      "loss: 266.863373 [  140/ 1682]\n",
      "loss: 4952.232422 [  150/ 1682]\n",
      "loss: 1275.687744 [  160/ 1682]\n",
      "loss: 565.229614 [  170/ 1682]\n",
      "loss: 40.893005 [  180/ 1682]\n",
      "loss: 287.808380 [  190/ 1682]\n",
      "loss: 528.089600 [  200/ 1682]\n",
      "loss: 25.088461 [  210/ 1682]\n",
      "loss: 454.503357 [  220/ 1682]\n",
      "loss: 69.901962 [  230/ 1682]\n",
      "loss: 562.560608 [  240/ 1682]\n",
      "loss: 628.087891 [  250/ 1682]\n",
      "loss: 251.244064 [  260/ 1682]\n",
      "loss: 54.899403 [  270/ 1682]\n",
      "loss: 92.418945 [  280/ 1682]\n",
      "loss: 119.747620 [  290/ 1682]\n",
      "loss: 96.447922 [  300/ 1682]\n",
      "loss: 345.414368 [  310/ 1682]\n",
      "loss: 355.787781 [  320/ 1682]\n",
      "loss: 130.415894 [  330/ 1682]\n",
      "loss: 251.331497 [  340/ 1682]\n",
      "loss: 195.877991 [  350/ 1682]\n",
      "loss: 72.914436 [  360/ 1682]\n",
      "loss: 306.860535 [  370/ 1682]\n",
      "loss: 370.033875 [  380/ 1682]\n",
      "loss: 209.835526 [  390/ 1682]\n",
      "loss: 307.229858 [  400/ 1682]\n",
      "loss: 108.894630 [  410/ 1682]\n",
      "loss: 442.250061 [  420/ 1682]\n",
      "loss: 136.698807 [  430/ 1682]\n",
      "loss: 331.012177 [  440/ 1682]\n",
      "loss: 190.733673 [  450/ 1682]\n",
      "loss: 431.834412 [  460/ 1682]\n",
      "loss: 244.332565 [  470/ 1682]\n",
      "loss: 81.168861 [  480/ 1682]\n",
      "loss: 184.881393 [  490/ 1682]\n",
      "loss: 308.957092 [  500/ 1682]\n",
      "loss: 141.089630 [  510/ 1682]\n",
      "loss: 10235.843750 [  520/ 1682]\n",
      "loss: 144.231812 [  530/ 1682]\n",
      "loss: 134.485504 [  540/ 1682]\n",
      "loss: 420.650391 [  550/ 1682]\n",
      "loss: 1344.587158 [  560/ 1682]\n",
      "loss: 177.225189 [  570/ 1682]\n",
      "loss: 869.523071 [  580/ 1682]\n",
      "loss: 359.791687 [  590/ 1682]\n",
      "loss: 276.416687 [  600/ 1682]\n",
      "loss: 392.974915 [  610/ 1682]\n",
      "loss: 275.756042 [  620/ 1682]\n",
      "loss: 395.222229 [  630/ 1682]\n",
      "loss: 846.167358 [  640/ 1682]\n",
      "loss: 1502.334106 [  650/ 1682]\n",
      "loss: 464.310791 [  660/ 1682]\n",
      "loss: 1582.297119 [  670/ 1682]\n",
      "loss: 616.295471 [  680/ 1682]\n",
      "loss: 1265.078369 [  690/ 1682]\n",
      "loss: 400.859680 [  700/ 1682]\n",
      "loss: 809.280396 [  710/ 1682]\n",
      "loss: 629.241577 [  720/ 1682]\n",
      "loss: 329.935608 [  730/ 1682]\n",
      "loss: 139.978302 [  740/ 1682]\n",
      "loss: 463.747375 [  750/ 1682]\n",
      "loss: 311.263245 [  760/ 1682]\n",
      "loss: 168.281158 [  770/ 1682]\n",
      "loss: 324.987396 [  780/ 1682]\n",
      "loss: 300.469574 [  790/ 1682]\n",
      "loss: 642.948303 [  800/ 1682]\n",
      "loss: 432.485168 [  810/ 1682]\n",
      "loss: 879.188660 [  820/ 1682]\n",
      "loss: 713.053833 [  830/ 1682]\n",
      "loss: 317.028137 [  840/ 1682]\n",
      "loss: 423.966797 [  850/ 1682]\n",
      "loss: 530.646301 [  860/ 1682]\n",
      "loss: 473.492523 [  870/ 1682]\n",
      "loss: 524.379211 [  880/ 1682]\n",
      "loss: 714.546936 [  890/ 1682]\n",
      "loss: 599.150146 [  900/ 1682]\n",
      "loss: 536.114441 [  910/ 1682]\n",
      "loss: 407.727448 [  920/ 1682]\n",
      "loss: 705.896057 [  930/ 1682]\n",
      "loss: 575.358215 [  940/ 1682]\n",
      "loss: 572.022827 [  950/ 1682]\n",
      "loss: 536.377075 [  960/ 1682]\n",
      "loss: 950.441223 [  970/ 1682]\n",
      "loss: 227.059128 [  980/ 1682]\n",
      "loss: 212.923996 [  990/ 1682]\n",
      "loss: 1519.942383 [ 1000/ 1682]\n",
      "loss: 1356.897705 [ 1010/ 1682]\n",
      "loss: 1934.663452 [ 1020/ 1682]\n",
      "loss: 600.694641 [ 1030/ 1682]\n",
      "loss: 430.689270 [ 1040/ 1682]\n",
      "loss: 198.103424 [ 1050/ 1682]\n",
      "loss: 1216.893188 [ 1060/ 1682]\n",
      "loss: 1635.614990 [ 1070/ 1682]\n",
      "loss: 1386.913940 [ 1080/ 1682]\n",
      "loss: 958.577332 [ 1090/ 1682]\n",
      "loss: 801.019409 [ 1100/ 1682]\n",
      "loss: 873.200195 [ 1110/ 1682]\n",
      "loss: 1593.099609 [ 1120/ 1682]\n",
      "loss: 1614.713013 [ 1130/ 1682]\n",
      "loss: 3288.280762 [ 1140/ 1682]\n",
      "loss: 1299.729736 [ 1150/ 1682]\n",
      "loss: 4811.826660 [ 1160/ 1682]\n",
      "loss: 3363.684082 [ 1170/ 1682]\n",
      "loss: 1432.917236 [ 1180/ 1682]\n",
      "loss: 4998.836426 [ 1190/ 1682]\n",
      "loss: 1055.998291 [ 1200/ 1682]\n",
      "loss: 1890.233154 [ 1210/ 1682]\n",
      "loss: 2526.723145 [ 1220/ 1682]\n",
      "loss: 3220.802490 [ 1230/ 1682]\n",
      "loss: 3846.887939 [ 1240/ 1682]\n",
      "loss: 4941.665527 [ 1250/ 1682]\n",
      "loss: 4142.762695 [ 1260/ 1682]\n",
      "loss: 9673.597656 [ 1270/ 1682]\n",
      "loss: 2285.662354 [ 1280/ 1682]\n",
      "loss: 6442.041504 [ 1290/ 1682]\n",
      "loss: 5258.894531 [ 1300/ 1682]\n",
      "loss: 1818.260742 [ 1310/ 1682]\n",
      "loss: 3191.495117 [ 1320/ 1682]\n",
      "loss: 3477.025391 [ 1330/ 1682]\n",
      "loss: 2483.303955 [ 1340/ 1682]\n",
      "loss: 2854.034668 [ 1350/ 1682]\n",
      "loss: 3038.287109 [ 1360/ 1682]\n",
      "loss: 2980.713623 [ 1370/ 1682]\n",
      "loss: 2815.446777 [ 1380/ 1682]\n",
      "loss: 2721.856689 [ 1390/ 1682]\n",
      "loss: 5984.602051 [ 1400/ 1682]\n",
      "loss: 2415.016113 [ 1410/ 1682]\n",
      "loss: 4489.615234 [ 1420/ 1682]\n",
      "loss: 4771.120117 [ 1430/ 1682]\n",
      "loss: 2697.132080 [ 1440/ 1682]\n",
      "loss: 4619.559570 [ 1450/ 1682]\n",
      "loss: 4324.536133 [ 1460/ 1682]\n",
      "loss: 3323.428467 [ 1470/ 1682]\n",
      "loss: 4540.854980 [ 1480/ 1682]\n",
      "loss: 5018.981445 [ 1490/ 1682]\n",
      "loss: 3399.195801 [ 1500/ 1682]\n",
      "loss: 2683.272949 [ 1510/ 1682]\n",
      "loss: 3133.800049 [ 1520/ 1682]\n",
      "loss: 1558.247314 [ 1530/ 1682]\n",
      "loss: 2320.459229 [ 1540/ 1682]\n",
      "loss: 1374.539429 [ 1550/ 1682]\n",
      "loss: 3781.855469 [ 1560/ 1682]\n",
      "loss: 1227.025146 [ 1570/ 1682]\n",
      "loss: 5954.517090 [ 1580/ 1682]\n",
      "loss: 1878.166016 [ 1590/ 1682]\n",
      "loss: 3294.369873 [ 1600/ 1682]\n",
      "loss: 1323.435303 [ 1610/ 1682]\n",
      "loss: 3406.507324 [ 1620/ 1682]\n",
      "loss: 3603.107422 [ 1630/ 1682]\n",
      "loss: 3688.142578 [ 1640/ 1682]\n",
      "loss: 1661.359009 [ 1650/ 1682]\n",
      "loss: 2513.105957 [ 1660/ 1682]\n",
      "loss: 10346.298828 [ 1670/ 1682]\n",
      "loss: 4069.158447 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 4905.974215 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1712.757202 [    0/ 1682]\n",
      "loss: 132.595673 [   10/ 1682]\n",
      "loss: 32.646492 [   20/ 1682]\n",
      "loss: 187.205688 [   30/ 1682]\n",
      "loss: 235.740448 [   40/ 1682]\n",
      "loss: 132.311676 [   50/ 1682]\n",
      "loss: 223.149445 [   60/ 1682]\n",
      "loss: 111.400734 [   70/ 1682]\n",
      "loss: 105.145279 [   80/ 1682]\n",
      "loss: 28.937134 [   90/ 1682]\n",
      "loss: 132.275650 [  100/ 1682]\n",
      "loss: 27.494503 [  110/ 1682]\n",
      "loss: 95.728729 [  120/ 1682]\n",
      "loss: 109.705345 [  130/ 1682]\n",
      "loss: 59.856293 [  140/ 1682]\n",
      "loss: 74.252571 [  150/ 1682]\n",
      "loss: 61.250469 [  160/ 1682]\n",
      "loss: 77.042450 [  170/ 1682]\n",
      "loss: 92.430473 [  180/ 1682]\n",
      "loss: 375.694305 [  190/ 1682]\n",
      "loss: 92.988846 [  200/ 1682]\n",
      "loss: 70.900787 [  210/ 1682]\n",
      "loss: 79.590874 [  220/ 1682]\n",
      "loss: 155.334518 [  230/ 1682]\n",
      "loss: 110.491600 [  240/ 1682]\n",
      "loss: 112.486801 [  250/ 1682]\n",
      "loss: 154.213974 [  260/ 1682]\n",
      "loss: 196.335571 [  270/ 1682]\n",
      "loss: 231.342041 [  280/ 1682]\n",
      "loss: 243.378510 [  290/ 1682]\n",
      "loss: 556.632446 [  300/ 1682]\n",
      "loss: 275.479462 [  310/ 1682]\n",
      "loss: 275.627014 [  320/ 1682]\n",
      "loss: 572.774231 [  330/ 1682]\n",
      "loss: 375.833984 [  340/ 1682]\n",
      "loss: 525.770996 [  350/ 1682]\n",
      "loss: 299.125000 [  360/ 1682]\n",
      "loss: 288.809296 [  370/ 1682]\n",
      "loss: 349.235779 [  380/ 1682]\n",
      "loss: 380.496033 [  390/ 1682]\n",
      "loss: 427.811951 [  400/ 1682]\n",
      "loss: 457.234955 [  410/ 1682]\n",
      "loss: 411.770569 [  420/ 1682]\n",
      "loss: 379.358368 [  430/ 1682]\n",
      "loss: 399.140106 [  440/ 1682]\n",
      "loss: 467.069489 [  450/ 1682]\n",
      "loss: 584.315735 [  460/ 1682]\n",
      "loss: 569.753662 [  470/ 1682]\n",
      "loss: 569.628357 [  480/ 1682]\n",
      "loss: 552.584045 [  490/ 1682]\n",
      "loss: 618.084106 [  500/ 1682]\n",
      "loss: 537.560852 [  510/ 1682]\n",
      "loss: 545.149780 [  520/ 1682]\n",
      "loss: 608.241516 [  530/ 1682]\n",
      "loss: 642.760193 [  540/ 1682]\n",
      "loss: 645.629333 [  550/ 1682]\n",
      "loss: 587.970581 [  560/ 1682]\n",
      "loss: 523.122375 [  570/ 1682]\n",
      "loss: 732.066345 [  580/ 1682]\n",
      "loss: 757.928650 [  590/ 1682]\n",
      "loss: 825.618469 [  600/ 1682]\n",
      "loss: 763.576965 [  610/ 1682]\n",
      "loss: 768.796143 [  620/ 1682]\n",
      "loss: 830.645203 [  630/ 1682]\n",
      "loss: 977.510925 [  640/ 1682]\n",
      "loss: 1164.146851 [  650/ 1682]\n",
      "loss: 1319.122803 [  660/ 1682]\n",
      "loss: 1289.440918 [  670/ 1682]\n",
      "loss: 1362.352783 [  680/ 1682]\n",
      "loss: 1275.062256 [  690/ 1682]\n",
      "loss: 1201.291260 [  700/ 1682]\n",
      "loss: 945.595215 [  710/ 1682]\n",
      "loss: 665.910095 [  720/ 1682]\n",
      "loss: 556.790649 [  730/ 1682]\n",
      "loss: 400.186493 [  740/ 1682]\n",
      "loss: 379.456421 [  750/ 1682]\n",
      "loss: 437.207855 [  760/ 1682]\n",
      "loss: 651.320923 [  770/ 1682]\n",
      "loss: 684.216248 [  780/ 1682]\n",
      "loss: 690.851746 [  790/ 1682]\n",
      "loss: 832.805786 [  800/ 1682]\n",
      "loss: 951.982422 [  810/ 1682]\n",
      "loss: 1046.381592 [  820/ 1682]\n",
      "loss: 999.429871 [  830/ 1682]\n",
      "loss: 754.511536 [  840/ 1682]\n",
      "loss: 795.801270 [  850/ 1682]\n",
      "loss: 950.990112 [  860/ 1682]\n",
      "loss: 1013.192261 [  870/ 1682]\n",
      "loss: 1083.958740 [  880/ 1682]\n",
      "loss: 1040.006104 [  890/ 1682]\n",
      "loss: 1110.743164 [  900/ 1682]\n",
      "loss: 1130.205811 [  910/ 1682]\n",
      "loss: 1332.954468 [  920/ 1682]\n",
      "loss: 1362.353027 [  930/ 1682]\n",
      "loss: 1557.258789 [  940/ 1682]\n",
      "loss: 1858.312134 [  950/ 1682]\n",
      "loss: 2189.647705 [  960/ 1682]\n",
      "loss: 2226.486816 [  970/ 1682]\n",
      "loss: 2443.999512 [  980/ 1682]\n",
      "loss: 2861.448730 [  990/ 1682]\n",
      "loss: 3397.343018 [ 1000/ 1682]\n",
      "loss: 3608.546143 [ 1010/ 1682]\n",
      "loss: 3789.162598 [ 1020/ 1682]\n",
      "loss: 3268.694824 [ 1030/ 1682]\n",
      "loss: 2486.600098 [ 1040/ 1682]\n",
      "loss: 1849.990845 [ 1050/ 1682]\n",
      "loss: 1965.056030 [ 1060/ 1682]\n",
      "loss: 2646.178223 [ 1070/ 1682]\n",
      "loss: 2881.650879 [ 1080/ 1682]\n",
      "loss: 3613.818848 [ 1090/ 1682]\n",
      "loss: 4090.055908 [ 1100/ 1682]\n",
      "loss: 4846.563965 [ 1110/ 1682]\n",
      "loss: 5420.399902 [ 1120/ 1682]\n",
      "loss: 5944.090820 [ 1130/ 1682]\n",
      "loss: 7224.916992 [ 1140/ 1682]\n",
      "loss: 9241.594727 [ 1150/ 1682]\n",
      "loss: 11411.718750 [ 1160/ 1682]\n",
      "loss: 8739.947266 [ 1170/ 1682]\n",
      "loss: 8813.052734 [ 1180/ 1682]\n",
      "loss: 9158.745117 [ 1190/ 1682]\n",
      "loss: 8979.770508 [ 1200/ 1682]\n",
      "loss: 9631.636719 [ 1210/ 1682]\n",
      "loss: 9024.260742 [ 1220/ 1682]\n",
      "loss: 10813.730469 [ 1230/ 1682]\n",
      "loss: 12455.954102 [ 1240/ 1682]\n",
      "loss: 12180.251953 [ 1250/ 1682]\n",
      "loss: 13542.005859 [ 1260/ 1682]\n",
      "loss: 13486.853516 [ 1270/ 1682]\n",
      "loss: 11580.781250 [ 1280/ 1682]\n",
      "loss: 10470.463867 [ 1290/ 1682]\n",
      "loss: 10547.335938 [ 1300/ 1682]\n",
      "loss: 11764.625977 [ 1310/ 1682]\n",
      "loss: 13093.497070 [ 1320/ 1682]\n",
      "loss: 12341.840820 [ 1330/ 1682]\n",
      "loss: 11392.744141 [ 1340/ 1682]\n",
      "loss: 11347.063477 [ 1350/ 1682]\n",
      "loss: 12393.584961 [ 1360/ 1682]\n",
      "loss: 14172.216797 [ 1370/ 1682]\n",
      "loss: 16023.648438 [ 1380/ 1682]\n",
      "loss: 16221.353516 [ 1390/ 1682]\n",
      "loss: 15372.179688 [ 1400/ 1682]\n",
      "loss: 17120.035156 [ 1410/ 1682]\n",
      "loss: 17315.658203 [ 1420/ 1682]\n",
      "loss: 15602.228516 [ 1430/ 1682]\n",
      "loss: 14209.705078 [ 1440/ 1682]\n",
      "loss: 15573.708984 [ 1450/ 1682]\n",
      "loss: 17095.267578 [ 1460/ 1682]\n",
      "loss: 19096.777344 [ 1470/ 1682]\n",
      "loss: 22625.306641 [ 1480/ 1682]\n",
      "loss: 22655.804688 [ 1490/ 1682]\n",
      "loss: 24620.882812 [ 1500/ 1682]\n",
      "loss: 21684.246094 [ 1510/ 1682]\n",
      "loss: 23326.933594 [ 1520/ 1682]\n",
      "loss: 22105.714844 [ 1530/ 1682]\n",
      "loss: 20601.894531 [ 1540/ 1682]\n",
      "loss: 19866.316406 [ 1550/ 1682]\n",
      "loss: 24361.464844 [ 1560/ 1682]\n",
      "loss: 20217.740234 [ 1570/ 1682]\n",
      "loss: 18528.146484 [ 1580/ 1682]\n",
      "loss: 16008.857422 [ 1590/ 1682]\n",
      "loss: 16113.587891 [ 1600/ 1682]\n",
      "loss: 14051.455078 [ 1610/ 1682]\n",
      "loss: 14489.701172 [ 1620/ 1682]\n",
      "loss: 16578.675781 [ 1630/ 1682]\n",
      "loss: 19233.308594 [ 1640/ 1682]\n",
      "loss: 22573.994141 [ 1650/ 1682]\n",
      "loss: 21631.199219 [ 1660/ 1682]\n",
      "loss: 18821.718750 [ 1670/ 1682]\n",
      "loss: 17810.660156 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 20841.457933 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 18.779205 [    0/ 1682]\n",
      "loss: 15.709818 [   10/ 1682]\n",
      "loss: 14.613742 [   20/ 1682]\n",
      "loss: 21.314560 [   30/ 1682]\n",
      "loss: 28.444748 [   40/ 1682]\n",
      "loss: 43.115585 [   50/ 1682]\n",
      "loss: 49.826187 [   60/ 1682]\n",
      "loss: 31.025045 [   70/ 1682]\n",
      "loss: 9.933154 [   80/ 1682]\n",
      "loss: 16.759037 [   90/ 1682]\n",
      "loss: 19.552851 [  100/ 1682]\n",
      "loss: 27.734045 [  110/ 1682]\n",
      "loss: 16.207279 [  120/ 1682]\n",
      "loss: 21.318960 [  130/ 1682]\n",
      "loss: 41.102135 [  140/ 1682]\n",
      "loss: 53.075951 [  150/ 1682]\n",
      "loss: 42.167114 [  160/ 1682]\n",
      "loss: 55.518494 [  170/ 1682]\n",
      "loss: 572.648315 [  180/ 1682]\n",
      "loss: 80.688828 [  190/ 1682]\n",
      "loss: 63.592346 [  200/ 1682]\n",
      "loss: 206.981476 [  210/ 1682]\n",
      "loss: 566.682312 [  220/ 1682]\n",
      "loss: 68.637123 [  230/ 1682]\n",
      "loss: 83.966949 [  240/ 1682]\n",
      "loss: 85.680649 [  250/ 1682]\n",
      "loss: 103.602005 [  260/ 1682]\n",
      "loss: 160.325424 [  270/ 1682]\n",
      "loss: 192.078659 [  280/ 1682]\n",
      "loss: 203.024811 [  290/ 1682]\n",
      "loss: 220.666428 [  300/ 1682]\n",
      "loss: 232.405060 [  310/ 1682]\n",
      "loss: 232.545624 [  320/ 1682]\n",
      "loss: 298.049377 [  330/ 1682]\n",
      "loss: 322.014557 [  340/ 1682]\n",
      "loss: 308.151184 [  350/ 1682]\n",
      "loss: 254.126419 [  360/ 1682]\n",
      "loss: 244.619583 [  370/ 1682]\n",
      "loss: 300.437195 [  380/ 1682]\n",
      "loss: 329.528137 [  390/ 1682]\n",
      "loss: 373.591736 [  400/ 1682]\n",
      "loss: 401.118896 [  410/ 1682]\n",
      "loss: 358.972137 [  420/ 1682]\n",
      "loss: 328.400238 [  430/ 1682]\n",
      "loss: 346.832092 [  440/ 1682]\n",
      "loss: 410.430359 [  450/ 1682]\n",
      "loss: 520.635864 [  460/ 1682]\n",
      "loss: 703.074158 [  470/ 1682]\n",
      "loss: 506.785004 [  480/ 1682]\n",
      "loss: 506.670959 [  490/ 1682]\n",
      "loss: 539.424133 [  500/ 1682]\n",
      "loss: 709.990723 [  510/ 1682]\n",
      "loss: 430.902893 [  520/ 1682]\n",
      "loss: 543.312683 [  530/ 1682]\n",
      "loss: 575.958191 [  540/ 1682]\n",
      "loss: 484.072510 [  550/ 1682]\n",
      "loss: 524.186707 [  560/ 1682]\n",
      "loss: 463.145355 [  570/ 1682]\n",
      "loss: 646.494751 [  580/ 1682]\n",
      "loss: 685.269714 [  590/ 1682]\n",
      "loss: 739.259888 [  600/ 1682]\n",
      "loss: 715.096680 [  610/ 1682]\n",
      "loss: 695.669250 [  620/ 1682]\n",
      "loss: 754.572632 [  630/ 1682]\n",
      "loss: 877.184387 [  640/ 1682]\n",
      "loss: 1073.810791 [  650/ 1682]\n",
      "loss: 1222.875488 [  660/ 1682]\n",
      "loss: 1174.714722 [  670/ 1682]\n",
      "loss: 1264.550293 [  680/ 1682]\n",
      "loss: 1180.532471 [  690/ 1682]\n",
      "loss: 1126.438599 [  700/ 1682]\n",
      "loss: 864.619812 [  710/ 1682]\n",
      "loss: 598.181274 [  720/ 1682]\n",
      "loss: 495.020203 [  730/ 1682]\n",
      "loss: 348.146759 [  740/ 1682]\n",
      "loss: 328.785522 [  750/ 1682]\n",
      "loss: 508.241516 [  760/ 1682]\n",
      "loss: 529.749146 [  770/ 1682]\n",
      "loss: 557.724182 [  780/ 1682]\n",
      "loss: 621.941833 [  790/ 1682]\n",
      "loss: 756.911621 [  800/ 1682]\n",
      "loss: 870.722168 [  810/ 1682]\n",
      "loss: 943.897339 [  820/ 1682]\n",
      "loss: 916.328308 [  830/ 1682]\n",
      "loss: 682.469543 [  840/ 1682]\n",
      "loss: 721.896484 [  850/ 1682]\n",
      "loss: 869.841431 [  860/ 1682]\n",
      "loss: 929.376099 [  870/ 1682]\n",
      "loss: 997.207397 [  880/ 1682]\n",
      "loss: 955.162415 [  890/ 1682]\n",
      "loss: 1022.965820 [  900/ 1682]\n",
      "loss: 1041.647217 [  910/ 1682]\n",
      "loss: 1236.625244 [  920/ 1682]\n",
      "loss: 1264.975952 [  930/ 1682]\n",
      "loss: 1472.155273 [  940/ 1682]\n",
      "loss: 1744.331665 [  950/ 1682]\n",
      "loss: 2065.759277 [  960/ 1682]\n",
      "loss: 1919.646118 [  970/ 1682]\n",
      "loss: 2337.774902 [  980/ 1682]\n",
      "loss: 2719.691406 [  990/ 1682]\n",
      "loss: 3242.748535 [ 1000/ 1682]\n",
      "loss: 3449.171875 [ 1010/ 1682]\n",
      "loss: 3625.815186 [ 1020/ 1682]\n",
      "loss: 2945.529053 [ 1030/ 1682]\n",
      "loss: 2355.275879 [ 1040/ 1682]\n",
      "loss: 1736.679077 [ 1050/ 1682]\n",
      "loss: 2111.627930 [ 1060/ 1682]\n",
      "loss: 2510.164062 [ 1070/ 1682]\n",
      "loss: 3082.621582 [ 1080/ 1682]\n",
      "loss: 3419.308105 [ 1090/ 1682]\n",
      "loss: 3549.139160 [ 1100/ 1682]\n",
      "loss: 4661.950195 [ 1110/ 1682]\n",
      "loss: 5225.129395 [ 1120/ 1682]\n",
      "loss: 5739.508789 [ 1130/ 1682]\n",
      "loss: 7000.050781 [ 1140/ 1682]\n",
      "loss: 8986.284180 [ 1150/ 1682]\n",
      "loss: 11127.851562 [ 1160/ 1682]\n",
      "loss: 8491.748047 [ 1170/ 1682]\n",
      "loss: 8563.863281 [ 1180/ 1682]\n",
      "loss: 9514.425781 [ 1190/ 1682]\n",
      "loss: 8728.325195 [ 1200/ 1682]\n",
      "loss: 9311.945312 [ 1210/ 1682]\n",
      "loss: 9462.587891 [ 1220/ 1682]\n",
      "loss: 10537.648438 [ 1230/ 1682]\n",
      "loss: 12159.640625 [ 1240/ 1682]\n",
      "loss: 11887.224609 [ 1250/ 1682]\n",
      "loss: 13233.302734 [ 1260/ 1682]\n",
      "loss: 12252.004883 [ 1270/ 1682]\n",
      "loss: 11196.021484 [ 1280/ 1682]\n",
      "loss: 10199.174805 [ 1290/ 1682]\n",
      "loss: 10275.083008 [ 1300/ 1682]\n",
      "loss: 11477.265625 [ 1310/ 1682]\n",
      "loss: 12797.688477 [ 1320/ 1682]\n",
      "loss: 11886.940430 [ 1330/ 1682]\n",
      "loss: 11109.971680 [ 1340/ 1682]\n",
      "loss: 11064.934570 [ 1350/ 1682]\n",
      "loss: 11307.725586 [ 1360/ 1682]\n",
      "loss: 13857.013672 [ 1370/ 1682]\n",
      "loss: 15688.333008 [ 1380/ 1682]\n",
      "loss: 15884.037109 [ 1390/ 1682]\n",
      "loss: 16160.692383 [ 1400/ 1682]\n",
      "loss: 16691.109375 [ 1410/ 1682]\n",
      "loss: 17035.578125 [ 1420/ 1682]\n",
      "loss: 15271.779297 [ 1430/ 1682]\n",
      "loss: 14825.065430 [ 1440/ 1682]\n",
      "loss: 16496.568359 [ 1450/ 1682]\n",
      "loss: 16749.429688 [ 1460/ 1682]\n",
      "loss: 18731.357422 [ 1470/ 1682]\n",
      "loss: 22227.552734 [ 1480/ 1682]\n",
      "loss: 23839.697266 [ 1490/ 1682]\n",
      "loss: 23592.994141 [ 1500/ 1682]\n",
      "loss: 21295.013672 [ 1510/ 1682]\n",
      "loss: 23008.728516 [ 1520/ 1682]\n",
      "loss: 21712.705078 [ 1530/ 1682]\n",
      "loss: 20222.548828 [ 1540/ 1682]\n",
      "loss: 20057.580078 [ 1550/ 1682]\n",
      "loss: 23948.873047 [ 1560/ 1682]\n",
      "loss: 21106.896484 [ 1570/ 1682]\n",
      "loss: 19269.496094 [ 1580/ 1682]\n",
      "loss: 15675.117188 [ 1590/ 1682]\n",
      "loss: 14777.742188 [ 1600/ 1682]\n",
      "loss: 13739.114258 [ 1610/ 1682]\n",
      "loss: 14172.146484 [ 1620/ 1682]\n",
      "loss: 16238.939453 [ 1630/ 1682]\n",
      "loss: 18867.412109 [ 1640/ 1682]\n",
      "loss: 22177.447266 [ 1650/ 1682]\n",
      "loss: 21243.250000 [ 1660/ 1682]\n",
      "loss: 18368.816406 [ 1670/ 1682]\n",
      "loss: 17458.734375 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 20347.666804 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 9.078722 [    0/ 1682]\n",
      "loss: 7.114886 [   10/ 1682]\n",
      "loss: 436.303314 [   20/ 1682]\n",
      "loss: 10.944910 [   30/ 1682]\n",
      "loss: 18.149481 [   40/ 1682]\n",
      "loss: 27.518112 [   50/ 1682]\n",
      "loss: 32.935368 [   60/ 1682]\n",
      "loss: 18.395479 [   70/ 1682]\n",
      "loss: 3.412550 [   80/ 1682]\n",
      "loss: 7.879086 [   90/ 1682]\n",
      "loss: 9.634774 [  100/ 1682]\n",
      "loss: 6.914065 [  110/ 1682]\n",
      "loss: 7.381391 [  120/ 1682]\n",
      "loss: 10.922389 [  130/ 1682]\n",
      "loss: 25.979303 [  140/ 1682]\n",
      "loss: 35.573177 [  150/ 1682]\n",
      "loss: 26.790945 [  160/ 1682]\n",
      "loss: 37.777489 [  170/ 1682]\n",
      "loss: 48.355156 [  180/ 1682]\n",
      "loss: 197.108047 [  190/ 1682]\n",
      "loss: 48.875153 [  200/ 1682]\n",
      "loss: 33.170326 [  210/ 1682]\n",
      "loss: 39.185902 [  220/ 1682]\n",
      "loss: 48.575077 [  230/ 1682]\n",
      "loss: 57.197136 [  240/ 1682]\n",
      "loss: 62.991341 [  250/ 1682]\n",
      "loss: 78.543579 [  260/ 1682]\n",
      "loss: 128.675705 [  270/ 1682]\n",
      "loss: 157.269211 [  280/ 1682]\n",
      "loss: 167.167343 [  290/ 1682]\n",
      "loss: 183.233658 [  300/ 1682]\n",
      "loss: 193.925156 [  310/ 1682]\n",
      "loss: 186.237717 [  320/ 1682]\n",
      "loss: 233.620605 [  330/ 1682]\n",
      "loss: 276.441376 [  340/ 1682]\n",
      "loss: 253.002350 [  350/ 1682]\n",
      "loss: 213.849564 [  360/ 1682]\n",
      "loss: 205.144608 [  370/ 1682]\n",
      "loss: 256.492004 [  380/ 1682]\n",
      "loss: 283.486176 [  390/ 1682]\n",
      "loss: 501.563629 [  400/ 1682]\n",
      "loss: 350.098328 [  410/ 1682]\n",
      "loss: 310.532471 [  420/ 1682]\n",
      "loss: 282.415710 [  430/ 1682]\n",
      "loss: 299.545807 [  440/ 1682]\n",
      "loss: 358.946960 [  450/ 1682]\n",
      "loss: 462.324799 [  460/ 1682]\n",
      "loss: 449.391510 [  470/ 1682]\n",
      "loss: 449.292877 [  480/ 1682]\n",
      "loss: 449.191589 [  490/ 1682]\n",
      "loss: 492.461365 [  500/ 1682]\n",
      "loss: 421.043640 [  510/ 1682]\n",
      "loss: 366.270660 [  520/ 1682]\n",
      "loss: 483.771973 [  530/ 1682]\n",
      "loss: 514.584351 [  540/ 1682]\n",
      "loss: 391.945587 [  550/ 1682]\n",
      "loss: 465.727112 [  560/ 1682]\n",
      "loss: 408.379639 [  570/ 1682]\n",
      "loss: 594.944031 [  580/ 1682]\n",
      "loss: 618.172302 [  590/ 1682]\n",
      "loss: 679.495728 [  600/ 1682]\n",
      "loss: 623.357605 [  610/ 1682]\n",
      "loss: 628.079834 [  620/ 1682]\n",
      "loss: 684.100830 [  630/ 1682]\n",
      "loss: 818.210571 [  640/ 1682]\n",
      "loss: 989.429810 [  650/ 1682]\n",
      "loss: 1132.725342 [  660/ 1682]\n",
      "loss: 1105.222412 [  670/ 1682]\n",
      "loss: 1035.392944 [  680/ 1682]\n",
      "loss: 1092.011230 [  690/ 1682]\n",
      "loss: 1040.040894 [  700/ 1682]\n",
      "loss: 789.277527 [  710/ 1682]\n",
      "loss: 520.968018 [  720/ 1682]\n",
      "loss: 438.365967 [  730/ 1682]\n",
      "loss: 300.963837 [  740/ 1682]\n",
      "loss: 282.931824 [  750/ 1682]\n",
      "loss: 333.270813 [  760/ 1682]\n",
      "loss: 471.061859 [  770/ 1682]\n",
      "loss: 497.475769 [  780/ 1682]\n",
      "loss: 558.295776 [  790/ 1682]\n",
      "loss: 686.457642 [  800/ 1682]\n",
      "loss: 795.036377 [  810/ 1682]\n",
      "loss: 941.310852 [  820/ 1682]\n",
      "loss: 838.846863 [  830/ 1682]\n",
      "loss: 600.307068 [  840/ 1682]\n",
      "loss: 653.377563 [  850/ 1682]\n",
      "loss: 794.265503 [  860/ 1682]\n",
      "loss: 851.202148 [  870/ 1682]\n",
      "loss: 916.173950 [  880/ 1682]\n",
      "loss: 875.987488 [  890/ 1682]\n",
      "loss: 940.932922 [  900/ 1682]\n",
      "loss: 973.704590 [  910/ 1682]\n",
      "loss: 1146.267090 [  920/ 1682]\n",
      "loss: 1173.602417 [  930/ 1682]\n",
      "loss: 1349.646851 [  940/ 1682]\n",
      "loss: 1636.796143 [  950/ 1682]\n",
      "loss: 1919.832764 [  960/ 1682]\n",
      "loss: 1766.540771 [  970/ 1682]\n",
      "loss: 2213.085205 [  980/ 1682]\n",
      "loss: 2585.105957 [  990/ 1682]\n",
      "loss: 3095.656006 [ 1000/ 1682]\n",
      "loss: 3297.420410 [ 1010/ 1682]\n",
      "loss: 3470.189453 [ 1020/ 1682]\n",
      "loss: 2805.887207 [ 1030/ 1682]\n",
      "loss: 2230.827393 [ 1040/ 1682]\n",
      "loss: 1629.767700 [ 1050/ 1682]\n",
      "loss: 1993.787842 [ 1060/ 1682]\n",
      "loss: 2381.137939 [ 1070/ 1682]\n",
      "loss: 2902.233643 [ 1080/ 1682]\n",
      "loss: 3302.919189 [ 1090/ 1682]\n",
      "loss: 3757.403564 [ 1100/ 1682]\n",
      "loss: 4437.639648 [ 1110/ 1682]\n",
      "loss: 4555.331055 [ 1120/ 1682]\n",
      "loss: 5543.701172 [ 1130/ 1682]\n",
      "loss: 6784.489258 [ 1140/ 1682]\n",
      "loss: 8741.070312 [ 1150/ 1682]\n",
      "loss: 10854.824219 [ 1160/ 1682]\n",
      "loss: 8190.334473 [ 1170/ 1682]\n",
      "loss: 8324.605469 [ 1180/ 1682]\n",
      "loss: 8514.244141 [ 1190/ 1682]\n",
      "loss: 8486.890625 [ 1200/ 1682]\n",
      "loss: 9054.673828 [ 1210/ 1682]\n",
      "loss: 9211.182617 [ 1220/ 1682]\n",
      "loss: 10272.287109 [ 1230/ 1682]\n",
      "loss: 11874.603516 [ 1240/ 1682]\n",
      "loss: 11605.408203 [ 1250/ 1682]\n",
      "loss: 12936.246094 [ 1260/ 1682]\n",
      "loss: 12881.812500 [ 1270/ 1682]\n",
      "loss: 11075.462891 [ 1280/ 1682]\n",
      "loss: 9938.507812 [ 1290/ 1682]\n",
      "loss: 10013.454102 [ 1300/ 1682]\n",
      "loss: 11200.912109 [ 1310/ 1682]\n",
      "loss: 12505.573242 [ 1320/ 1682]\n",
      "loss: 10710.907227 [ 1330/ 1682]\n",
      "loss: 10838.041992 [ 1340/ 1682]\n",
      "loss: 10793.622070 [ 1350/ 1682]\n",
      "loss: 11815.103516 [ 1360/ 1682]\n",
      "loss: 13553.456055 [ 1370/ 1682]\n",
      "loss: 15365.156250 [ 1380/ 1682]\n",
      "loss: 15558.884766 [ 1390/ 1682]\n",
      "loss: 15832.784180 [ 1400/ 1682]\n",
      "loss: 16439.623047 [ 1410/ 1682]\n",
      "loss: 16699.060547 [ 1420/ 1682]\n",
      "loss: 14953.247070 [ 1430/ 1682]\n",
      "loss: 13327.320312 [ 1440/ 1682]\n",
      "loss: 16165.568359 [ 1450/ 1682]\n",
      "loss: 16415.964844 [ 1460/ 1682]\n",
      "loss: 18378.855469 [ 1470/ 1682]\n",
      "loss: 21843.601562 [ 1480/ 1682]\n",
      "loss: 21812.667969 [ 1490/ 1682]\n",
      "loss: 23805.048828 [ 1500/ 1682]\n",
      "loss: 20919.521484 [ 1510/ 1682]\n",
      "loss: 22618.384766 [ 1520/ 1682]\n",
      "loss: 21233.363281 [ 1530/ 1682]\n",
      "loss: 19856.785156 [ 1540/ 1682]\n",
      "loss: 19675.837891 [ 1550/ 1682]\n",
      "loss: 23550.837891 [ 1560/ 1682]\n",
      "loss: 20922.265625 [ 1570/ 1682]\n",
      "loss: 19001.156250 [ 1580/ 1682]\n",
      "loss: 15353.896484 [ 1590/ 1682]\n",
      "loss: 15456.200195 [ 1600/ 1682]\n",
      "loss: 13438.750000 [ 1610/ 1682]\n",
      "loss: 13866.718750 [ 1620/ 1682]\n",
      "loss: 15816.612305 [ 1630/ 1682]\n",
      "loss: 18418.705078 [ 1640/ 1682]\n",
      "loss: 21795.199219 [ 1650/ 1682]\n",
      "loss: 20869.363281 [ 1660/ 1682]\n",
      "loss: 18111.363281 [ 1670/ 1682]\n",
      "loss: 17119.871094 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 20072.044621 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.084391 [    0/ 1682]\n",
      "loss: 2.122735 [   10/ 1682]\n",
      "loss: 1.629297 [   20/ 1682]\n",
      "loss: 4.223074 [   30/ 1682]\n",
      "loss: 8.241274 [   40/ 1682]\n",
      "loss: 15.705458 [   50/ 1682]\n",
      "loss: 19.861658 [   60/ 1682]\n",
      "loss: 9.465961 [   70/ 1682]\n",
      "loss: 0.426435 [   80/ 1682]\n",
      "loss: 2.595172 [   90/ 1682]\n",
      "loss: 3.338818 [  100/ 1682]\n",
      "loss: 189.417343 [  110/ 1682]\n",
      "loss: 2.145709 [  120/ 1682]\n",
      "loss: 4.156343 [  130/ 1682]\n",
      "loss: 14.595263 [  140/ 1682]\n",
      "loss: 21.886461 [  150/ 1682]\n",
      "loss: 39.645306 [  160/ 1682]\n",
      "loss: 23.856457 [  170/ 1682]\n",
      "loss: 29.245037 [  180/ 1682]\n",
      "loss: 40.731197 [  190/ 1682]\n",
      "loss: 32.669941 [  200/ 1682]\n",
      "loss: 20.057127 [  210/ 1682]\n",
      "loss: 24.795593 [  220/ 1682]\n",
      "loss: 385.175354 [  230/ 1682]\n",
      "loss: 43.050812 [  240/ 1682]\n",
      "loss: 44.258244 [  250/ 1682]\n",
      "loss: 57.507729 [  260/ 1682]\n",
      "loss: 101.230301 [  270/ 1682]\n",
      "loss: 126.753014 [  280/ 1682]\n",
      "loss: 135.634293 [  290/ 1682]\n",
      "loss: 150.170822 [  300/ 1682]\n",
      "loss: 151.586456 [  310/ 1682]\n",
      "loss: 160.005463 [  320/ 1682]\n",
      "loss: 215.123688 [  330/ 1682]\n",
      "loss: 235.468414 [  340/ 1682]\n",
      "loss: 422.308289 [  350/ 1682]\n",
      "loss: 178.026169 [  360/ 1682]\n",
      "loss: 170.101669 [  370/ 1682]\n",
      "loss: 217.104446 [  380/ 1682]\n",
      "loss: 242.061234 [  390/ 1682]\n",
      "loss: 279.913391 [  400/ 1682]\n",
      "loss: 291.751862 [  410/ 1682]\n",
      "loss: 267.090454 [  420/ 1682]\n",
      "loss: 241.032272 [  430/ 1682]\n",
      "loss: 256.892609 [  440/ 1682]\n",
      "loss: 312.208588 [  450/ 1682]\n",
      "loss: 428.100647 [  460/ 1682]\n",
      "loss: 396.792236 [  470/ 1682]\n",
      "loss: 396.708435 [  480/ 1682]\n",
      "loss: 358.692108 [  490/ 1682]\n",
      "loss: 437.350281 [  500/ 1682]\n",
      "loss: 370.296265 [  510/ 1682]\n",
      "loss: 319.118225 [  520/ 1682]\n",
      "loss: 429.221191 [  530/ 1682]\n",
      "loss: 458.260590 [  540/ 1682]\n",
      "loss: 363.968353 [  550/ 1682]\n",
      "loss: 412.244537 [  560/ 1682]\n",
      "loss: 358.487610 [  570/ 1682]\n",
      "loss: 534.337646 [  580/ 1682]\n",
      "loss: 556.304504 [  590/ 1682]\n",
      "loss: 614.579529 [  600/ 1682]\n",
      "loss: 561.266235 [  610/ 1682]\n",
      "loss: 565.749512 [  620/ 1682]\n",
      "loss: 564.790405 [  630/ 1682]\n",
      "loss: 746.983337 [  640/ 1682]\n",
      "loss: 910.823547 [  650/ 1682]\n",
      "loss: 1048.528564 [  660/ 1682]\n",
      "loss: 984.932129 [  670/ 1682]\n",
      "loss: 1087.178833 [  680/ 1682]\n",
      "loss: 1009.419067 [  690/ 1682]\n",
      "loss: 959.511353 [  700/ 1682]\n",
      "loss: 719.476013 [  710/ 1682]\n",
      "loss: 478.436707 [  720/ 1682]\n",
      "loss: 386.696594 [  730/ 1682]\n",
      "loss: 258.482849 [  740/ 1682]\n",
      "loss: 241.740021 [  750/ 1682]\n",
      "loss: 288.528625 [  760/ 1682]\n",
      "loss: 417.422760 [  770/ 1682]\n",
      "loss: 442.325684 [  780/ 1682]\n",
      "loss: 499.853271 [  790/ 1682]\n",
      "loss: 621.414795 [  800/ 1682]\n",
      "loss: 724.922485 [  810/ 1682]\n",
      "loss: 807.588135 [  820/ 1682]\n",
      "loss: 766.985229 [  830/ 1682]\n",
      "loss: 511.416748 [  840/ 1682]\n",
      "loss: 590.196411 [  850/ 1682]\n",
      "loss: 724.234009 [  860/ 1682]\n",
      "loss: 778.645386 [  870/ 1682]\n",
      "loss: 840.838562 [  880/ 1682]\n",
      "loss: 802.450745 [  890/ 1682]\n",
      "loss: 863.081665 [  900/ 1682]\n",
      "loss: 881.800964 [  910/ 1682]\n",
      "loss: 1061.864014 [  920/ 1682]\n",
      "loss: 1088.203003 [  930/ 1682]\n",
      "loss: 1280.991455 [  940/ 1682]\n",
      "loss: 1535.688721 [  950/ 1682]\n",
      "loss: 1838.096313 [  960/ 1682]\n",
      "loss: 1871.896118 [  970/ 1682]\n",
      "loss: 2095.281738 [  980/ 1682]\n",
      "loss: 2457.672363 [  990/ 1682]\n",
      "loss: 2956.057373 [ 1000/ 1682]\n",
      "loss: 3153.276367 [ 1010/ 1682]\n",
      "loss: 3322.265137 [ 1020/ 1682]\n",
      "loss: 2673.477051 [ 1030/ 1682]\n",
      "loss: 2113.166992 [ 1040/ 1682]\n",
      "loss: 1529.139404 [ 1050/ 1682]\n",
      "loss: 1882.528076 [ 1060/ 1682]\n",
      "loss: 2258.996826 [ 1070/ 1682]\n",
      "loss: 2803.731689 [ 1080/ 1682]\n",
      "loss: 3158.770996 [ 1090/ 1682]\n",
      "loss: 3605.393066 [ 1100/ 1682]\n",
      "loss: 4317.434082 [ 1110/ 1682]\n",
      "loss: 4860.117676 [ 1120/ 1682]\n",
      "loss: 5356.582031 [ 1130/ 1682]\n",
      "loss: 6578.135254 [ 1140/ 1682]\n",
      "loss: 7086.878906 [ 1150/ 1682]\n",
      "loss: 10592.553711 [ 1160/ 1682]\n",
      "loss: 7959.625977 [ 1170/ 1682]\n",
      "loss: 8028.223633 [ 1180/ 1682]\n",
      "loss: 9020.189453 [ 1190/ 1682]\n",
      "loss: 8255.299805 [ 1200/ 1682]\n",
      "loss: 8880.838867 [ 1210/ 1682]\n",
      "loss: 8969.832031 [ 1220/ 1682]\n",
      "loss: 10017.326172 [ 1230/ 1682]\n",
      "loss: 11600.470703 [ 1240/ 1682]\n",
      "loss: 11334.377930 [ 1250/ 1682]\n",
      "loss: 12650.355469 [ 1260/ 1682]\n",
      "loss: 12596.248047 [ 1270/ 1682]\n",
      "loss: 10810.972656 [ 1280/ 1682]\n",
      "loss: 9688.000977 [ 1290/ 1682]\n",
      "loss: 9762.006836 [ 1300/ 1682]\n",
      "loss: 10935.123047 [ 1310/ 1682]\n",
      "loss: 12224.435547 [ 1320/ 1682]\n",
      "loss: 11401.149414 [ 1330/ 1682]\n",
      "loss: 10576.503906 [ 1340/ 1682]\n",
      "loss: 10455.501953 [ 1350/ 1682]\n",
      "loss: 11463.278320 [ 1360/ 1682]\n",
      "loss: 13261.028320 [ 1370/ 1682]\n",
      "loss: 15053.595703 [ 1380/ 1682]\n",
      "loss: 15245.379883 [ 1390/ 1682]\n",
      "loss: 15516.575195 [ 1400/ 1682]\n",
      "loss: 14900.648438 [ 1410/ 1682]\n",
      "loss: 16374.450195 [ 1420/ 1682]\n",
      "loss: 14646.162109 [ 1430/ 1682]\n",
      "loss: 14208.854492 [ 1440/ 1682]\n",
      "loss: 15846.291992 [ 1450/ 1682]\n",
      "loss: 16094.244141 [ 1460/ 1682]\n",
      "loss: 18038.548828 [ 1470/ 1682]\n",
      "loss: 21472.599609 [ 1480/ 1682]\n",
      "loss: 21833.248047 [ 1490/ 1682]\n",
      "loss: 23301.054688 [ 1500/ 1682]\n",
      "loss: 20556.599609 [ 1510/ 1682]\n",
      "loss: 22240.931641 [ 1520/ 1682]\n",
      "loss: 20967.099609 [ 1530/ 1682]\n",
      "loss: 19503.212891 [ 1540/ 1682]\n",
      "loss: 19426.919922 [ 1550/ 1682]\n",
      "loss: 23165.691406 [ 1560/ 1682]\n",
      "loss: 20559.390625 [ 1570/ 1682]\n",
      "loss: 18655.507812 [ 1580/ 1682]\n",
      "loss: 15043.617188 [ 1590/ 1682]\n",
      "loss: 15144.745117 [ 1600/ 1682]\n",
      "loss: 13148.793945 [ 1610/ 1682]\n",
      "loss: 13571.810547 [ 1620/ 1682]\n",
      "loss: 15595.960938 [ 1630/ 1682]\n",
      "loss: 18174.164062 [ 1640/ 1682]\n",
      "loss: 21195.339844 [ 1650/ 1682]\n",
      "loss: 20507.595703 [ 1660/ 1682]\n",
      "loss: 17774.349609 [ 1670/ 1682]\n",
      "loss: 16792.259766 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 19770.851638 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.293992 [    0/ 1682]\n",
      "loss: 0.451265 [   10/ 1682]\n",
      "loss: 0.246228 [   20/ 1682]\n",
      "loss: 0.861314 [   30/ 1682]\n",
      "loss: 3.037368 [   40/ 1682]\n",
      "loss: 7.372834 [   50/ 1682]\n",
      "loss: 10.297508 [   60/ 1682]\n",
      "loss: 293.578522 [   70/ 1682]\n",
      "loss: 0.708121 [   80/ 1682]\n",
      "loss: 0.633309 [   90/ 1682]\n",
      "loss: 0.388542 [  100/ 1682]\n",
      "loss: 0.241659 [  110/ 1682]\n",
      "loss: 0.228394 [  120/ 1682]\n",
      "loss: 0.744400 [  130/ 1682]\n",
      "loss: 6.377114 [  140/ 1682]\n",
      "loss: 10.596408 [  150/ 1682]\n",
      "loss: 7.022731 [  160/ 1682]\n",
      "loss: 13.167529 [  170/ 1682]\n",
      "loss: 19.480768 [  180/ 1682]\n",
      "loss: 26.333035 [  190/ 1682]\n",
      "loss: 20.036022 [  200/ 1682]\n",
      "loss: 10.440835 [  210/ 1682]\n",
      "loss: 13.931829 [  220/ 1682]\n",
      "loss: 19.780933 [  230/ 1682]\n",
      "loss: 172.051804 [  240/ 1682]\n",
      "loss: 29.148830 [  250/ 1682]\n",
      "loss: 40.147636 [  260/ 1682]\n",
      "loss: 77.608955 [  270/ 1682]\n",
      "loss: 100.130386 [  280/ 1682]\n",
      "loss: 108.016129 [  290/ 1682]\n",
      "loss: 121.056274 [  300/ 1682]\n",
      "loss: 206.660645 [  310/ 1682]\n",
      "loss: 129.904816 [  320/ 1682]\n",
      "loss: 180.022049 [  330/ 1682]\n",
      "loss: 198.623291 [  340/ 1682]\n",
      "loss: 187.873322 [  350/ 1682]\n",
      "loss: 137.683792 [  360/ 1682]\n",
      "loss: 139.039520 [  370/ 1682]\n",
      "loss: 181.793915 [  380/ 1682]\n",
      "loss: 204.756165 [  390/ 1682]\n",
      "loss: 239.613312 [  400/ 1682]\n",
      "loss: 261.786102 [  410/ 1682]\n",
      "loss: 227.809113 [  420/ 1682]\n",
      "loss: 203.761581 [  430/ 1682]\n",
      "loss: 218.380035 [  440/ 1682]\n",
      "loss: 269.703125 [  450/ 1682]\n",
      "loss: 359.944794 [  460/ 1682]\n",
      "loss: 348.552795 [  470/ 1682]\n",
      "loss: 348.478027 [  480/ 1682]\n",
      "loss: 348.401093 [  490/ 1682]\n",
      "loss: 386.637634 [  500/ 1682]\n",
      "loss: 323.843079 [  510/ 1682]\n",
      "loss: 276.174316 [  520/ 1682]\n",
      "loss: 379.037659 [  530/ 1682]\n",
      "loss: 406.337830 [  540/ 1682]\n",
      "loss: 316.419617 [  550/ 1682]\n",
      "loss: 442.994781 [  560/ 1682]\n",
      "loss: 312.847961 [  570/ 1682]\n",
      "loss: 461.487640 [  580/ 1682]\n",
      "loss: 498.952576 [  590/ 1682]\n",
      "loss: 554.246521 [  600/ 1682]\n",
      "loss: 503.696106 [  610/ 1682]\n",
      "loss: 507.945618 [  620/ 1682]\n",
      "loss: 558.445557 [  630/ 1682]\n",
      "loss: 773.728577 [  640/ 1682]\n",
      "loss: 837.102173 [  650/ 1682]\n",
      "loss: 969.344543 [  660/ 1682]\n",
      "loss: 943.909302 [  670/ 1682]\n",
      "loss: 1006.546265 [  680/ 1682]\n",
      "loss: 931.799622 [  690/ 1682]\n",
      "loss: 883.907104 [  700/ 1682]\n",
      "loss: 654.092407 [  710/ 1682]\n",
      "loss: 425.553864 [  720/ 1682]\n",
      "loss: 554.579102 [  730/ 1682]\n",
      "loss: 220.088257 [  740/ 1682]\n",
      "loss: 194.558075 [  750/ 1682]\n",
      "loss: 247.929489 [  760/ 1682]\n",
      "loss: 368.221985 [  770/ 1682]\n",
      "loss: 391.560059 [  780/ 1682]\n",
      "loss: 445.872894 [  790/ 1682]\n",
      "loss: 560.987732 [  800/ 1682]\n",
      "loss: 659.542847 [  810/ 1682]\n",
      "loss: 738.500732 [  820/ 1682]\n",
      "loss: 699.907654 [  830/ 1682]\n",
      "loss: 497.479553 [  840/ 1682]\n",
      "loss: 531.605591 [  850/ 1682]\n",
      "loss: 658.952820 [  860/ 1682]\n",
      "loss: 710.898438 [  870/ 1682]\n",
      "loss: 770.378296 [  880/ 1682]\n",
      "loss: 768.676453 [  890/ 1682]\n",
      "loss: 793.208618 [  900/ 1682]\n",
      "loss: 809.664917 [  910/ 1682]\n",
      "loss: 982.550171 [  920/ 1682]\n",
      "loss: 1007.917297 [  930/ 1682]\n",
      "loss: 1166.677490 [  940/ 1682]\n",
      "loss: 1440.065430 [  950/ 1682]\n",
      "loss: 1733.320068 [  960/ 1682]\n",
      "loss: 1766.154053 [  970/ 1682]\n",
      "loss: 1983.367188 [  980/ 1682]\n",
      "loss: 2110.473389 [  990/ 1682]\n",
      "loss: 2822.889893 [ 1000/ 1682]\n",
      "loss: 3015.688477 [ 1010/ 1682]\n",
      "loss: 3181.001709 [ 1020/ 1682]\n",
      "loss: 2547.364990 [ 1030/ 1682]\n",
      "loss: 2001.458618 [ 1040/ 1682]\n",
      "loss: 1434.057007 [ 1050/ 1682]\n",
      "loss: 1777.078857 [ 1060/ 1682]\n",
      "loss: 2142.936035 [ 1070/ 1682]\n",
      "loss: 2468.112793 [ 1080/ 1682]\n",
      "loss: 3021.255127 [ 1090/ 1682]\n",
      "loss: 3458.518066 [ 1100/ 1682]\n",
      "loss: 4156.502930 [ 1110/ 1682]\n",
      "loss: 4689.341309 [ 1120/ 1682]\n",
      "loss: 5177.199219 [ 1130/ 1682]\n",
      "loss: 6380.012207 [ 1140/ 1682]\n",
      "loss: 8279.605469 [ 1150/ 1682]\n",
      "loss: 10339.893555 [ 1160/ 1682]\n",
      "loss: 7805.211914 [ 1170/ 1682]\n",
      "loss: 7874.463379 [ 1180/ 1682]\n",
      "loss: 8787.167969 [ 1190/ 1682]\n",
      "loss: 8032.450195 [ 1200/ 1682]\n",
      "loss: 8649.630859 [ 1210/ 1682]\n",
      "loss: 8737.435547 [ 1220/ 1682]\n",
      "loss: 9771.639648 [ 1230/ 1682]\n",
      "loss: 11336.060547 [ 1240/ 1682]\n",
      "loss: 11072.987305 [ 1250/ 1682]\n",
      "loss: 12374.451172 [ 1260/ 1682]\n",
      "loss: 12320.652344 [ 1270/ 1682]\n",
      "loss: 10555.931641 [ 1280/ 1682]\n",
      "loss: 8696.466797 [ 1290/ 1682]\n",
      "loss: 9519.690430 [ 1300/ 1682]\n",
      "loss: 10678.815430 [ 1310/ 1682]\n",
      "loss: 11953.150391 [ 1320/ 1682]\n",
      "loss: 11139.314453 [ 1330/ 1682]\n",
      "loss: 10324.369141 [ 1340/ 1682]\n",
      "loss: 10281.081055 [ 1350/ 1682]\n",
      "loss: 11278.730469 [ 1360/ 1682]\n",
      "loss: 12978.781250 [ 1370/ 1682]\n",
      "loss: 14752.689453 [ 1380/ 1682]\n",
      "loss: 14942.578125 [ 1390/ 1682]\n",
      "loss: 15211.146484 [ 1400/ 1682]\n",
      "loss: 15806.169922 [ 1410/ 1682]\n",
      "loss: 16060.775391 [ 1420/ 1682]\n",
      "loss: 14349.548828 [ 1430/ 1682]\n",
      "loss: 13916.739258 [ 1440/ 1682]\n",
      "loss: 15537.734375 [ 1450/ 1682]\n",
      "loss: 15783.289062 [ 1460/ 1682]\n",
      "loss: 17709.435547 [ 1470/ 1682]\n",
      "loss: 21113.505859 [ 1480/ 1682]\n",
      "loss: 22685.058594 [ 1490/ 1682]\n",
      "loss: 23042.369141 [ 1500/ 1682]\n",
      "loss: 20205.345703 [ 1510/ 1682]\n",
      "loss: 21755.373047 [ 1520/ 1682]\n",
      "loss: 20612.281250 [ 1530/ 1682]\n",
      "loss: 19161.019531 [ 1540/ 1682]\n",
      "loss: 19085.787109 [ 1550/ 1682]\n",
      "loss: 22792.638672 [ 1560/ 1682]\n",
      "loss: 20208.068359 [ 1570/ 1682]\n",
      "loss: 18320.996094 [ 1580/ 1682]\n",
      "loss: 14743.627930 [ 1590/ 1682]\n",
      "loss: 14843.603516 [ 1600/ 1682]\n",
      "loss: 12868.634766 [ 1610/ 1682]\n",
      "loss: 13286.804688 [ 1620/ 1682]\n",
      "loss: 15290.367188 [ 1630/ 1682]\n",
      "loss: 17729.074219 [ 1640/ 1682]\n",
      "loss: 21066.962891 [ 1650/ 1682]\n",
      "loss: 20157.230469 [ 1660/ 1682]\n",
      "loss: 17448.160156 [ 1670/ 1682]\n",
      "loss: 16475.240234 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 19273.357159 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 43.821236 [    0/ 1682]\n",
      "loss: 1.874016 [   10/ 1682]\n",
      "loss: 1.949942 [   20/ 1682]\n",
      "loss: 0.630951 [   30/ 1682]\n",
      "loss: 0.378910 [   40/ 1682]\n",
      "loss: 2.284111 [   50/ 1682]\n",
      "loss: 4.004738 [   60/ 1682]\n",
      "loss: 1.605258 [   70/ 1682]\n",
      "loss: 4.033713 [   80/ 1682]\n",
      "loss: 1.765743 [   90/ 1682]\n",
      "loss: 0.554083 [  100/ 1682]\n",
      "loss: 1.648477 [  110/ 1682]\n",
      "loss: 3.819420 [  120/ 1682]\n",
      "loss: 0.456265 [  130/ 1682]\n",
      "loss: 1.979298 [  140/ 1682]\n",
      "loss: 4.822831 [  150/ 1682]\n",
      "loss: 2.099867 [  160/ 1682]\n",
      "loss: 6.333168 [  170/ 1682]\n",
      "loss: 10.157409 [  180/ 1682]\n",
      "loss: 15.299884 [  190/ 1682]\n",
      "loss: 10.726816 [  200/ 1682]\n",
      "loss: 4.080991 [  210/ 1682]\n",
      "loss: 6.351550 [  220/ 1682]\n",
      "loss: 8.745535 [  230/ 1682]\n",
      "loss: 263.933075 [  240/ 1682]\n",
      "loss: 17.416828 [  250/ 1682]\n",
      "loss: 26.214981 [  260/ 1682]\n",
      "loss: 57.555183 [  270/ 1682]\n",
      "loss: 77.142426 [  280/ 1682]\n",
      "loss: 84.054695 [  290/ 1682]\n",
      "loss: 95.631676 [  300/ 1682]\n",
      "loss: 103.339218 [  310/ 1682]\n",
      "loss: 103.513000 [  320/ 1682]\n",
      "loss: 148.738556 [  330/ 1682]\n",
      "loss: 165.632385 [  340/ 1682]\n",
      "loss: 155.868622 [  350/ 1682]\n",
      "loss: 118.124985 [  360/ 1682]\n",
      "loss: 111.699585 [  370/ 1682]\n",
      "loss: 150.299881 [  380/ 1682]\n",
      "loss: 171.311493 [  390/ 1682]\n",
      "loss: 203.239960 [  400/ 1682]\n",
      "loss: 223.708282 [  410/ 1682]\n",
      "loss: 192.430847 [  420/ 1682]\n",
      "loss: 400.390442 [  430/ 1682]\n",
      "loss: 183.754730 [  440/ 1682]\n",
      "loss: 231.176239 [  450/ 1682]\n",
      "loss: 315.072937 [  460/ 1682]\n",
      "loss: 304.426697 [  470/ 1682]\n",
      "loss: 304.364258 [  480/ 1682]\n",
      "loss: 304.299500 [  490/ 1682]\n",
      "loss: 340.105347 [  500/ 1682]\n",
      "loss: 281.475891 [  510/ 1682]\n",
      "loss: 237.238235 [  520/ 1682]\n",
      "loss: 333.031433 [  530/ 1682]\n",
      "loss: 358.635193 [  540/ 1682]\n",
      "loss: 287.050385 [  550/ 1682]\n",
      "loss: 318.100372 [  560/ 1682]\n",
      "loss: 271.281677 [  570/ 1682]\n",
      "loss: 426.413147 [  580/ 1682]\n",
      "loss: 445.936829 [  590/ 1682]\n",
      "loss: 498.313965 [  600/ 1682]\n",
      "loss: 450.458832 [  610/ 1682]\n",
      "loss: 454.476135 [  620/ 1682]\n",
      "loss: 502.305237 [  630/ 1682]\n",
      "loss: 618.492981 [  640/ 1682]\n",
      "loss: 746.446167 [  650/ 1682]\n",
      "loss: 894.936707 [  660/ 1682]\n",
      "loss: 870.484497 [  670/ 1682]\n",
      "loss: 930.691772 [  680/ 1682]\n",
      "loss: 790.107971 [  690/ 1682]\n",
      "loss: 812.954468 [  700/ 1682]\n",
      "loss: 593.663147 [  710/ 1682]\n",
      "loss: 376.813873 [  720/ 1682]\n",
      "loss: 295.948181 [  730/ 1682]\n",
      "loss: 185.506104 [  740/ 1682]\n",
      "loss: 171.252640 [  750/ 1682]\n",
      "loss: 211.167145 [  760/ 1682]\n",
      "loss: 320.367493 [  770/ 1682]\n",
      "loss: 344.117798 [  780/ 1682]\n",
      "loss: 395.974701 [  790/ 1682]\n",
      "loss: 504.764954 [  800/ 1682]\n",
      "loss: 598.459473 [  810/ 1682]\n",
      "loss: 673.776733 [  820/ 1682]\n",
      "loss: 637.148315 [  830/ 1682]\n",
      "loss: 444.724060 [  840/ 1682]\n",
      "loss: 477.159180 [  850/ 1682]\n",
      "loss: 540.257446 [  860/ 1682]\n",
      "loss: 627.606323 [  870/ 1682]\n",
      "loss: 636.753235 [  880/ 1682]\n",
      "loss: 669.386047 [  890/ 1682]\n",
      "loss: 726.185181 [  900/ 1682]\n",
      "loss: 741.927734 [  910/ 1682]\n",
      "loss: 907.772827 [  920/ 1682]\n",
      "loss: 932.184387 [  930/ 1682]\n",
      "loss: 1111.253418 [  940/ 1682]\n",
      "loss: 1349.287476 [  950/ 1682]\n",
      "loss: 1633.564087 [  960/ 1682]\n",
      "loss: 1665.447632 [  970/ 1682]\n",
      "loss: 1876.605469 [  980/ 1682]\n",
      "loss: 1995.486938 [  990/ 1682]\n",
      "loss: 2695.269043 [ 1000/ 1682]\n",
      "loss: 2883.718262 [ 1010/ 1682]\n",
      "loss: 3045.413574 [ 1020/ 1682]\n",
      "loss: 2426.629639 [ 1030/ 1682]\n",
      "loss: 1894.844360 [ 1040/ 1682]\n",
      "loss: 1343.747437 [ 1050/ 1682]\n",
      "loss: 1676.591431 [ 1060/ 1682]\n",
      "loss: 2032.029907 [ 1070/ 1682]\n",
      "loss: 2550.348633 [ 1080/ 1682]\n",
      "loss: 2889.274658 [ 1090/ 1682]\n",
      "loss: 3317.338623 [ 1100/ 1682]\n",
      "loss: 4001.514404 [ 1110/ 1682]\n",
      "loss: 4524.674316 [ 1120/ 1682]\n",
      "loss: 5004.070312 [ 1130/ 1682]\n",
      "loss: 6188.474121 [ 1140/ 1682]\n",
      "loss: 8060.440430 [ 1150/ 1682]\n",
      "loss: 10094.810547 [ 1160/ 1682]\n",
      "loss: 6993.635254 [ 1170/ 1682]\n",
      "loss: 7660.787598 [ 1180/ 1682]\n",
      "loss: 8561.419922 [ 1190/ 1682]\n",
      "loss: 7675.692871 [ 1200/ 1682]\n",
      "loss: 8425.707031 [ 1210/ 1682]\n",
      "loss: 8512.361328 [ 1220/ 1682]\n",
      "loss: 8893.476562 [ 1230/ 1682]\n",
      "loss: 11079.614258 [ 1240/ 1682]\n",
      "loss: 10819.528320 [ 1250/ 1682]\n",
      "loss: 12106.772461 [ 1260/ 1682]\n",
      "loss: 12053.291016 [ 1270/ 1682]\n",
      "loss: 9397.047852 [ 1280/ 1682]\n",
      "loss: 9212.819336 [ 1290/ 1682]\n",
      "loss: 9206.629883 [ 1300/ 1682]\n",
      "loss: 10430.409180 [ 1310/ 1682]\n",
      "loss: 11690.056641 [ 1320/ 1682]\n",
      "loss: 10885.487305 [ 1330/ 1682]\n",
      "loss: 10080.043945 [ 1340/ 1682]\n",
      "loss: 10037.308594 [ 1350/ 1682]\n",
      "loss: 11023.416992 [ 1360/ 1682]\n",
      "loss: 12704.917969 [ 1370/ 1682]\n",
      "loss: 14360.001953 [ 1380/ 1682]\n",
      "loss: 14648.551758 [ 1390/ 1682]\n",
      "loss: 14914.536133 [ 1400/ 1682]\n",
      "loss: 15503.833008 [ 1410/ 1682]\n",
      "loss: 15652.416992 [ 1420/ 1682]\n",
      "loss: 14061.619141 [ 1430/ 1682]\n",
      "loss: 13633.232422 [ 1440/ 1682]\n",
      "loss: 15238.117188 [ 1450/ 1682]\n",
      "loss: 15481.330078 [ 1460/ 1682]\n",
      "loss: 17389.675781 [ 1470/ 1682]\n",
      "loss: 20764.373047 [ 1480/ 1682]\n",
      "loss: 22322.964844 [ 1490/ 1682]\n",
      "loss: 22677.464844 [ 1500/ 1682]\n",
      "loss: 19863.933594 [ 1510/ 1682]\n",
      "loss: 21520.125000 [ 1520/ 1682]\n",
      "loss: 20267.406250 [ 1530/ 1682]\n",
      "loss: 18714.609375 [ 1540/ 1682]\n",
      "loss: 18754.341797 [ 1550/ 1682]\n",
      "loss: 22429.902344 [ 1560/ 1682]\n",
      "loss: 19866.646484 [ 1570/ 1682]\n",
      "loss: 17996.058594 [ 1580/ 1682]\n",
      "loss: 14452.529297 [ 1590/ 1682]\n",
      "loss: 14551.372070 [ 1600/ 1682]\n",
      "loss: 12499.791016 [ 1610/ 1682]\n",
      "loss: 12349.550781 [ 1620/ 1682]\n",
      "loss: 14993.783203 [ 1630/ 1682]\n",
      "loss: 17523.933594 [ 1640/ 1682]\n",
      "loss: 20718.724609 [ 1650/ 1682]\n",
      "loss: 19816.791016 [ 1660/ 1682]\n",
      "loss: 17015.705078 [ 1670/ 1682]\n",
      "loss: 16167.492188 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 18976.632212 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 297.831085 [    0/ 1682]\n",
      "loss: 6.198496 [   10/ 1682]\n",
      "loss: 6.548844 [   20/ 1682]\n",
      "loss: 6.175345 [   30/ 1682]\n",
      "loss: 0.701467 [   40/ 1682]\n",
      "loss: 0.224463 [   50/ 1682]\n",
      "loss: 0.764511 [   60/ 1682]\n",
      "loss: 2.235937 [   70/ 1682]\n",
      "loss: 10.216299 [   80/ 1682]\n",
      "loss: 5.798687 [   90/ 1682]\n",
      "loss: 3.638978 [  100/ 1682]\n",
      "loss: 5.948534 [  110/ 1682]\n",
      "loss: 5.472739 [  120/ 1682]\n",
      "loss: 3.094451 [  130/ 1682]\n",
      "loss: 0.299561 [  140/ 1682]\n",
      "loss: 0.988657 [  150/ 1682]\n",
      "loss: 0.196548 [  160/ 1682]\n",
      "loss: 2.274501 [  170/ 1682]\n",
      "loss: 3.943347 [  180/ 1682]\n",
      "loss: 7.410924 [  190/ 1682]\n",
      "loss: 4.525612 [  200/ 1682]\n",
      "loss: 0.767409 [  210/ 1682]\n",
      "loss: 1.842252 [  220/ 1682]\n",
      "loss: 4.304523 [  230/ 1682]\n",
      "loss: 237.969803 [  240/ 1682]\n",
      "loss: 8.839984 [  250/ 1682]\n",
      "loss: 15.482386 [  260/ 1682]\n",
      "loss: 40.827827 [  270/ 1682]\n",
      "loss: 57.540962 [  280/ 1682]\n",
      "loss: 63.499062 [  290/ 1682]\n",
      "loss: 73.642395 [  300/ 1682]\n",
      "loss: 74.338005 [  310/ 1682]\n",
      "loss: 80.576248 [  320/ 1682]\n",
      "loss: 121.012474 [  330/ 1682]\n",
      "loss: 136.234924 [  340/ 1682]\n",
      "loss: 127.437149 [  350/ 1682]\n",
      "loss: 93.535355 [  360/ 1682]\n",
      "loss: 87.835457 [  370/ 1682]\n",
      "loss: 122.368912 [  380/ 1682]\n",
      "loss: 141.471039 [  390/ 1682]\n",
      "loss: 170.532501 [  400/ 1682]\n",
      "loss: 189.332153 [  410/ 1682]\n",
      "loss: 160.697250 [  420/ 1682]\n",
      "loss: 140.538513 [  430/ 1682]\n",
      "loss: 152.753052 [  440/ 1682]\n",
      "loss: 196.349823 [  450/ 1682]\n",
      "loss: 274.028809 [  460/ 1682]\n",
      "loss: 264.108093 [  470/ 1682]\n",
      "loss: 264.053223 [  480/ 1682]\n",
      "loss: 263.996521 [  490/ 1682]\n",
      "loss: 297.417664 [  500/ 1682]\n",
      "loss: 242.864868 [  510/ 1682]\n",
      "loss: 202.012924 [  520/ 1682]\n",
      "loss: 290.849426 [  530/ 1682]\n",
      "loss: 314.788147 [  540/ 1682]\n",
      "loss: 247.996735 [  550/ 1682]\n",
      "loss: 276.903809 [  560/ 1682]\n",
      "loss: 233.442657 [  570/ 1682]\n",
      "loss: 519.270447 [  580/ 1682]\n",
      "loss: 396.884338 [  590/ 1682]\n",
      "loss: 446.407776 [  600/ 1682]\n",
      "loss: 401.196198 [  610/ 1682]\n",
      "loss: 404.988831 [  620/ 1682]\n",
      "loss: 450.204498 [  630/ 1682]\n",
      "loss: 560.674744 [  640/ 1682]\n",
      "loss: 703.325684 [  650/ 1682]\n",
      "loss: 824.972534 [  660/ 1682]\n",
      "loss: 801.492798 [  670/ 1682]\n",
      "loss: 778.086365 [  680/ 1682]\n",
      "loss: 767.320862 [  690/ 1682]\n",
      "loss: 746.404602 [  700/ 1682]\n",
      "loss: 537.150940 [  710/ 1682]\n",
      "loss: 332.006042 [  720/ 1682]\n",
      "loss: 256.401520 [  730/ 1682]\n",
      "loss: 294.748718 [  740/ 1682]\n",
      "loss: 141.509323 [  750/ 1682]\n",
      "loss: 178.099136 [  760/ 1682]\n",
      "loss: 281.445709 [  770/ 1682]\n",
      "loss: 302.004486 [  780/ 1682]\n",
      "loss: 350.073486 [  790/ 1682]\n",
      "loss: 452.680176 [  800/ 1682]\n",
      "loss: 541.622070 [  810/ 1682]\n",
      "loss: 613.379089 [  820/ 1682]\n",
      "loss: 578.669373 [  830/ 1682]\n",
      "loss: 396.022522 [  840/ 1682]\n",
      "loss: 426.803284 [  850/ 1682]\n",
      "loss: 541.173218 [  860/ 1682]\n",
      "loss: 588.332153 [  870/ 1682]\n",
      "loss: 642.545471 [  880/ 1682]\n",
      "loss: 609.316284 [  890/ 1682]\n",
      "loss: 663.508179 [  900/ 1682]\n",
      "loss: 678.548462 [  910/ 1682]\n",
      "loss: 837.504211 [  920/ 1682]\n",
      "loss: 844.439575 [  930/ 1682]\n",
      "loss: 1033.403320 [  940/ 1682]\n",
      "loss: 1263.368652 [  950/ 1682]\n",
      "loss: 1355.585083 [  960/ 1682]\n",
      "loss: 1569.826782 [  970/ 1682]\n",
      "loss: 1775.069092 [  980/ 1682]\n",
      "loss: 2109.848389 [  990/ 1682]\n",
      "loss: 2573.345947 [ 1000/ 1682]\n",
      "loss: 2757.545166 [ 1010/ 1682]\n",
      "loss: 2915.704834 [ 1020/ 1682]\n",
      "loss: 2311.444336 [ 1030/ 1682]\n",
      "loss: 1793.465454 [ 1040/ 1682]\n",
      "loss: 1258.308960 [ 1050/ 1682]\n",
      "loss: 1581.201904 [ 1060/ 1682]\n",
      "loss: 1735.415283 [ 1070/ 1682]\n",
      "loss: 2431.985840 [ 1080/ 1682]\n",
      "loss: 2763.113281 [ 1090/ 1682]\n",
      "loss: 3182.193848 [ 1100/ 1682]\n",
      "loss: 3852.880371 [ 1110/ 1682]\n",
      "loss: 4366.589355 [ 1120/ 1682]\n",
      "loss: 4336.699219 [ 1130/ 1682]\n",
      "loss: 6004.156250 [ 1140/ 1682]\n",
      "loss: 7849.146973 [ 1150/ 1682]\n",
      "loss: 9858.208984 [ 1160/ 1682]\n",
      "loss: 6361.068848 [ 1170/ 1682]\n",
      "loss: 7454.894531 [ 1180/ 1682]\n",
      "loss: 8343.744141 [ 1190/ 1682]\n",
      "loss: 7608.808594 [ 1200/ 1682]\n",
      "loss: 8209.813477 [ 1210/ 1682]\n",
      "loss: 8295.343750 [ 1220/ 1682]\n",
      "loss: 9303.791992 [ 1230/ 1682]\n",
      "loss: 10165.006836 [ 1240/ 1682]\n",
      "loss: 10574.745117 [ 1250/ 1682]\n",
      "loss: 11848.088867 [ 1260/ 1682]\n",
      "loss: 11794.909180 [ 1270/ 1682]\n",
      "loss: 10070.066406 [ 1280/ 1682]\n",
      "loss: 8987.194336 [ 1290/ 1682]\n",
      "loss: 9058.485352 [ 1300/ 1682]\n",
      "loss: 10190.441406 [ 1310/ 1682]\n",
      "loss: 11435.705078 [ 1320/ 1682]\n",
      "loss: 10546.959961 [ 1330/ 1682]\n",
      "loss: 9843.999023 [ 1340/ 1682]\n",
      "loss: 9801.787109 [ 1350/ 1682]\n",
      "loss: 10776.595703 [ 1360/ 1682]\n",
      "loss: 12439.940430 [ 1370/ 1682]\n",
      "loss: 14177.627930 [ 1380/ 1682]\n",
      "loss: 14363.820312 [ 1390/ 1682]\n",
      "loss: 13852.439453 [ 1400/ 1682]\n",
      "loss: 15210.953125 [ 1410/ 1682]\n",
      "loss: 15460.923828 [ 1420/ 1682]\n",
      "loss: 13782.817383 [ 1430/ 1682]\n",
      "loss: 13358.748047 [ 1440/ 1682]\n",
      "loss: 13977.495117 [ 1450/ 1682]\n",
      "loss: 15188.798828 [ 1460/ 1682]\n",
      "loss: 17079.750000 [ 1470/ 1682]\n",
      "loss: 20425.712891 [ 1480/ 1682]\n",
      "loss: 21971.636719 [ 1490/ 1682]\n",
      "loss: 22323.378906 [ 1500/ 1682]\n",
      "loss: 18270.634766 [ 1510/ 1682]\n",
      "loss: 21175.433594 [ 1520/ 1682]\n",
      "loss: 19807.429688 [ 1530/ 1682]\n",
      "loss: 18506.220703 [ 1540/ 1682]\n",
      "loss: 18433.062500 [ 1550/ 1682]\n",
      "loss: 21942.431641 [ 1560/ 1682]\n",
      "loss: 19535.636719 [ 1570/ 1682]\n",
      "loss: 17681.167969 [ 1580/ 1682]\n",
      "loss: 14170.736328 [ 1590/ 1682]\n",
      "loss: 14268.471680 [ 1600/ 1682]\n",
      "loss: 11618.530273 [ 1610/ 1682]\n",
      "loss: 12742.965820 [ 1620/ 1682]\n",
      "loss: 14706.627930 [ 1630/ 1682]\n",
      "loss: 17213.515625 [ 1640/ 1682]\n",
      "loss: 20381.041016 [ 1650/ 1682]\n",
      "loss: 19486.712891 [ 1660/ 1682]\n",
      "loss: 16824.496094 [ 1670/ 1682]\n",
      "loss: 15869.356445 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 18708.754357 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 10.073970 [    0/ 1682]\n",
      "loss: 13.233890 [   10/ 1682]\n",
      "loss: 13.852835 [   20/ 1682]\n",
      "loss: 8.782796 [   30/ 1682]\n",
      "loss: 213.197067 [   40/ 1682]\n",
      "loss: 4.414910 [   50/ 1682]\n",
      "loss: 0.397310 [   60/ 1682]\n",
      "loss: 5.655631 [   70/ 1682]\n",
      "loss: 395.148315 [   80/ 1682]\n",
      "loss: 12.545752 [   90/ 1682]\n",
      "loss: 9.457289 [  100/ 1682]\n",
      "loss: 12.954727 [  110/ 1682]\n",
      "loss: 115.160423 [  120/ 1682]\n",
      "loss: 8.472396 [  130/ 1682]\n",
      "loss: 1.452202 [  140/ 1682]\n",
      "loss: 2.519647 [  150/ 1682]\n",
      "loss: 1.128665 [  160/ 1682]\n",
      "loss: 1.097537 [  170/ 1682]\n",
      "loss: 84.282455 [  180/ 1682]\n",
      "loss: 2.486345 [  190/ 1682]\n",
      "loss: 1.251023 [  200/ 1682]\n",
      "loss: 0.315913 [  210/ 1682]\n",
      "loss: 0.220733 [  220/ 1682]\n",
      "loss: 1.043537 [  230/ 1682]\n",
      "loss: 2.983215 [  240/ 1682]\n",
      "loss: 3.239228 [  250/ 1682]\n",
      "loss: 7.773066 [  260/ 1682]\n",
      "loss: 27.256235 [  270/ 1682]\n",
      "loss: 41.158295 [  280/ 1682]\n",
      "loss: 46.182579 [  290/ 1682]\n",
      "loss: 54.923267 [  300/ 1682]\n",
      "loss: 60.744011 [  310/ 1682]\n",
      "loss: 60.928722 [  320/ 1682]\n",
      "loss: 96.680801 [  330/ 1682]\n",
      "loss: 110.267860 [  340/ 1682]\n",
      "loss: 102.413574 [  350/ 1682]\n",
      "loss: 72.267532 [  360/ 1682]\n",
      "loss: 114.094215 [  370/ 1682]\n",
      "loss: 97.832626 [  380/ 1682]\n",
      "loss: 115.067459 [  390/ 1682]\n",
      "loss: 141.325272 [  400/ 1682]\n",
      "loss: 158.493240 [  410/ 1682]\n",
      "loss: 132.441818 [  420/ 1682]\n",
      "loss: 114.163452 [  430/ 1682]\n",
      "loss: 116.223923 [  440/ 1682]\n",
      "loss: 165.074448 [  450/ 1682]\n",
      "loss: 236.677322 [  460/ 1682]\n",
      "loss: 227.467743 [  470/ 1682]\n",
      "loss: 227.422073 [  480/ 1682]\n",
      "loss: 227.374634 [  490/ 1682]\n",
      "loss: 258.466248 [  500/ 1682]\n",
      "loss: 207.898483 [  510/ 1682]\n",
      "loss: 170.301559 [  520/ 1682]\n",
      "loss: 252.394867 [  530/ 1682]\n",
      "loss: 274.707092 [  540/ 1682]\n",
      "loss: 212.599930 [  550/ 1682]\n",
      "loss: 239.413361 [  560/ 1682]\n",
      "loss: 199.232941 [  570/ 1682]\n",
      "loss: 334.529510 [  580/ 1682]\n",
      "loss: 351.711823 [  590/ 1682]\n",
      "loss: 398.440186 [  600/ 1682]\n",
      "loss: 355.806366 [  610/ 1682]\n",
      "loss: 359.374329 [  620/ 1682]\n",
      "loss: 402.030334 [  630/ 1682]\n",
      "loss: 486.445984 [  640/ 1682]\n",
      "loss: 642.797119 [  650/ 1682]\n",
      "loss: 759.320190 [  660/ 1682]\n",
      "loss: 736.787292 [  670/ 1682]\n",
      "loss: 792.304382 [  680/ 1682]\n",
      "loss: 726.190918 [  690/ 1682]\n",
      "loss: 684.068054 [  700/ 1682]\n",
      "loss: 484.627014 [  710/ 1682]\n",
      "loss: 290.927124 [  720/ 1682]\n",
      "loss: 220.466156 [  730/ 1682]\n",
      "loss: 127.030991 [  740/ 1682]\n",
      "loss: 108.138329 [  750/ 1682]\n",
      "loss: 148.487732 [  760/ 1682]\n",
      "loss: 243.677155 [  770/ 1682]\n",
      "loss: 262.844604 [  780/ 1682]\n",
      "loss: 307.889954 [  790/ 1682]\n",
      "loss: 371.436584 [  800/ 1682]\n",
      "loss: 488.731110 [  810/ 1682]\n",
      "loss: 557.002563 [  820/ 1682]\n",
      "loss: 524.171021 [  830/ 1682]\n",
      "loss: 533.631226 [  840/ 1682]\n",
      "loss: 380.259491 [  850/ 1682]\n",
      "loss: 488.353088 [  860/ 1682]\n",
      "loss: 533.198486 [  870/ 1682]\n",
      "loss: 584.866089 [  880/ 1682]\n",
      "loss: 553.284241 [  890/ 1682]\n",
      "loss: 582.621277 [  900/ 1682]\n",
      "loss: 619.290161 [  910/ 1682]\n",
      "loss: 771.512390 [  920/ 1682]\n",
      "loss: 794.072876 [  930/ 1682]\n",
      "loss: 959.995972 [  940/ 1682]\n",
      "loss: 1182.064697 [  950/ 1682]\n",
      "loss: 1448.968994 [  960/ 1682]\n",
      "loss: 1479.010986 [  970/ 1682]\n",
      "loss: 1678.449585 [  980/ 1682]\n",
      "loss: 2004.403320 [  990/ 1682]\n",
      "loss: 2456.750000 [ 1000/ 1682]\n",
      "loss: 2438.258789 [ 1010/ 1682]\n",
      "loss: 2791.475342 [ 1020/ 1682]\n",
      "loss: 2201.425293 [ 1030/ 1682]\n",
      "loss: 1696.957642 [ 1040/ 1682]\n",
      "loss: 1177.403076 [ 1050/ 1682]\n",
      "loss: 1490.552002 [ 1060/ 1682]\n",
      "loss: 1825.831421 [ 1070/ 1682]\n",
      "loss: 2318.826660 [ 1080/ 1682]\n",
      "loss: 2642.302246 [ 1090/ 1682]\n",
      "loss: 3052.571533 [ 1100/ 1682]\n",
      "loss: 3710.033936 [ 1110/ 1682]\n",
      "loss: 3916.737061 [ 1120/ 1682]\n",
      "loss: 4677.504883 [ 1130/ 1682]\n",
      "loss: 5762.773438 [ 1140/ 1682]\n",
      "loss: 6925.470215 [ 1150/ 1682]\n",
      "loss: 9629.078125 [ 1160/ 1682]\n",
      "loss: 7189.250977 [ 1170/ 1682]\n",
      "loss: 7255.804688 [ 1180/ 1682]\n",
      "loss: 8133.071777 [ 1190/ 1682]\n",
      "loss: 7407.690430 [ 1200/ 1682]\n",
      "loss: 8000.831055 [ 1210/ 1682]\n",
      "loss: 8085.233398 [ 1220/ 1682]\n",
      "loss: 9081.178711 [ 1230/ 1682]\n",
      "loss: 10591.703125 [ 1240/ 1682]\n",
      "loss: 10337.335938 [ 1250/ 1682]\n",
      "loss: 11597.012695 [ 1260/ 1682]\n",
      "loss: 11544.097656 [ 1270/ 1682]\n",
      "loss: 9298.127930 [ 1280/ 1682]\n",
      "loss: 8768.506836 [ 1290/ 1682]\n",
      "loss: 8838.913086 [ 1300/ 1682]\n",
      "loss: 9957.686523 [ 1310/ 1682]\n",
      "loss: 11188.833984 [ 1320/ 1682]\n",
      "loss: 10402.170898 [ 1330/ 1682]\n",
      "loss: 9615.099609 [ 1340/ 1682]\n",
      "loss: 9573.399414 [ 1350/ 1682]\n",
      "loss: 10537.115234 [ 1360/ 1682]\n",
      "loss: 12082.710938 [ 1370/ 1682]\n",
      "loss: 13902.750000 [ 1380/ 1682]\n",
      "loss: 14087.138672 [ 1390/ 1682]\n",
      "loss: 14348.080078 [ 1400/ 1682]\n",
      "loss: 14926.247070 [ 1410/ 1682]\n",
      "loss: 15173.945312 [ 1420/ 1682]\n",
      "loss: 13511.885742 [ 1430/ 1682]\n",
      "loss: 13092.039062 [ 1440/ 1682]\n",
      "loss: 14665.661133 [ 1450/ 1682]\n",
      "loss: 14904.307617 [ 1460/ 1682]\n",
      "loss: 16778.130859 [ 1470/ 1682]\n",
      "loss: 20095.832031 [ 1480/ 1682]\n",
      "loss: 21629.269531 [ 1490/ 1682]\n",
      "loss: 21978.275391 [ 1500/ 1682]\n",
      "loss: 19210.269531 [ 1510/ 1682]\n",
      "loss: 20839.445312 [ 1520/ 1682]\n",
      "loss: 18359.441406 [ 1530/ 1682]\n",
      "loss: 18192.105469 [ 1540/ 1682]\n",
      "loss: 16914.562500 [ 1550/ 1682]\n",
      "loss: 21734.833984 [ 1560/ 1682]\n",
      "loss: 19212.896484 [ 1570/ 1682]\n",
      "loss: 17374.265625 [ 1580/ 1682]\n",
      "loss: 13781.554688 [ 1590/ 1682]\n",
      "loss: 13993.004883 [ 1600/ 1682]\n",
      "loss: 12078.487305 [ 1610/ 1682]\n",
      "loss: 12482.682617 [ 1620/ 1682]\n",
      "loss: 14426.923828 [ 1630/ 1682]\n",
      "loss: 16910.917969 [ 1640/ 1682]\n",
      "loss: 20051.605469 [ 1650/ 1682]\n",
      "loss: 19164.750000 [ 1660/ 1682]\n",
      "loss: 16525.304688 [ 1670/ 1682]\n",
      "loss: 15578.792969 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 18288.272611 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 26.426035 [    0/ 1682]\n",
      "loss: 22.861526 [   10/ 1682]\n",
      "loss: 23.744091 [   20/ 1682]\n",
      "loss: 16.848978 [   30/ 1682]\n",
      "loss: 9.596663 [   40/ 1682]\n",
      "loss: 4.494754 [   50/ 1682]\n",
      "loss: 2.741488 [   60/ 1682]\n",
      "loss: 11.724882 [   70/ 1682]\n",
      "loss: 30.481327 [   80/ 1682]\n",
      "loss: 21.889399 [   90/ 1682]\n",
      "loss: 17.887695 [  100/ 1682]\n",
      "loss: 22.554943 [  110/ 1682]\n",
      "loss: 21.632381 [  120/ 1682]\n",
      "loss: 16.470860 [  130/ 1682]\n",
      "loss: 5.289971 [  140/ 1682]\n",
      "loss: 1.804870 [  150/ 1682]\n",
      "loss: 4.748683 [  160/ 1682]\n",
      "loss: 2.639197 [  170/ 1682]\n",
      "loss: 0.120369 [  180/ 1682]\n",
      "loss: 0.333210 [  190/ 1682]\n",
      "loss: 0.723085 [  200/ 1682]\n",
      "loss: 2.570230 [  210/ 1682]\n",
      "loss: 1.320836 [  220/ 1682]\n",
      "loss: 0.526218 [  230/ 1682]\n",
      "loss: 0.385450 [  240/ 1682]\n",
      "loss: 0.412789 [  250/ 1682]\n",
      "loss: 2.866277 [  260/ 1682]\n",
      "loss: 16.566650 [  270/ 1682]\n",
      "loss: 27.695135 [  280/ 1682]\n",
      "loss: 31.797398 [  290/ 1682]\n",
      "loss: 39.153599 [  300/ 1682]\n",
      "loss: 44.052849 [  310/ 1682]\n",
      "loss: 44.241531 [  320/ 1682]\n",
      "loss: 75.372185 [  330/ 1682]\n",
      "loss: 87.345207 [  340/ 1682]\n",
      "loss: 80.420776 [  350/ 1682]\n",
      "loss: 53.978798 [  360/ 1682]\n",
      "loss: 44.548439 [  370/ 1682]\n",
      "loss: 76.316544 [  380/ 1682]\n",
      "loss: 91.706841 [  390/ 1682]\n",
      "loss: 115.196289 [  400/ 1682]\n",
      "loss: 130.751938 [  410/ 1682]\n",
      "loss: 107.247765 [  420/ 1682]\n",
      "loss: 90.823181 [  430/ 1682]\n",
      "loss: 100.724953 [  440/ 1682]\n",
      "loss: 136.894516 [  450/ 1682]\n",
      "loss: 191.357773 [  460/ 1682]\n",
      "loss: 193.989227 [  470/ 1682]\n",
      "loss: 193.950912 [  480/ 1682]\n",
      "loss: 193.910889 [  490/ 1682]\n",
      "loss: 222.701782 [  500/ 1682]\n",
      "loss: 176.065948 [  510/ 1682]\n",
      "loss: 141.707672 [  520/ 1682]\n",
      "loss: 217.116486 [  530/ 1682]\n",
      "loss: 237.821991 [  540/ 1682]\n",
      "loss: 180.337433 [  550/ 1682]\n",
      "loss: 205.083008 [  560/ 1682]\n",
      "loss: 168.140106 [  570/ 1682]\n",
      "loss: 293.759338 [  580/ 1682]\n",
      "loss: 309.798248 [  590/ 1682]\n",
      "loss: 353.769897 [  600/ 1682]\n",
      "loss: 313.682190 [  610/ 1682]\n",
      "loss: 317.029816 [  620/ 1682]\n",
      "loss: 357.161194 [  630/ 1682]\n",
      "loss: 456.517639 [  640/ 1682]\n",
      "loss: 543.445862 [  650/ 1682]\n",
      "loss: 697.206238 [  660/ 1682]\n",
      "loss: 675.608765 [  670/ 1682]\n",
      "loss: 728.837097 [  680/ 1682]\n",
      "loss: 665.501770 [  690/ 1682]\n",
      "loss: 625.235413 [  700/ 1682]\n",
      "loss: 671.961121 [  710/ 1682]\n",
      "loss: 253.073929 [  720/ 1682]\n",
      "loss: 187.690063 [  730/ 1682]\n",
      "loss: 102.550636 [  740/ 1682]\n",
      "loss: 91.837250 [  750/ 1682]\n",
      "loss: 121.958534 [  760/ 1682]\n",
      "loss: 209.109985 [  770/ 1682]\n",
      "loss: 462.154114 [  780/ 1682]\n",
      "loss: 268.981018 [  790/ 1682]\n",
      "loss: 359.572723 [  800/ 1682]\n",
      "loss: 420.217133 [  810/ 1682]\n",
      "loss: 504.122070 [  820/ 1682]\n",
      "loss: 473.143463 [  830/ 1682]\n",
      "loss: 309.497131 [  840/ 1682]\n",
      "loss: 337.064178 [  850/ 1682]\n",
      "loss: 438.968903 [  860/ 1682]\n",
      "loss: 463.502136 [  870/ 1682]\n",
      "loss: 530.683899 [  880/ 1682]\n",
      "loss: 500.720856 [  890/ 1682]\n",
      "loss: 549.855713 [  900/ 1682]\n",
      "loss: 563.538757 [  910/ 1682]\n",
      "loss: 709.119995 [  920/ 1682]\n",
      "loss: 730.776672 [  930/ 1682]\n",
      "loss: 890.291138 [  940/ 1682]\n",
      "loss: 1104.576538 [  950/ 1682]\n",
      "loss: 1328.017212 [  960/ 1682]\n",
      "loss: 1392.155518 [  970/ 1682]\n",
      "loss: 1549.872925 [  980/ 1682]\n",
      "loss: 1903.143921 [  990/ 1682]\n",
      "loss: 2344.510254 [ 1000/ 1682]\n",
      "loss: 2520.439209 [ 1010/ 1682]\n",
      "loss: 2671.717041 [ 1020/ 1682]\n",
      "loss: 2095.668213 [ 1030/ 1682]\n",
      "loss: 1604.511841 [ 1040/ 1682]\n",
      "loss: 1100.328857 [ 1050/ 1682]\n",
      "loss: 1403.875244 [ 1060/ 1682]\n",
      "loss: 1729.324951 [ 1070/ 1682]\n",
      "loss: 2209.973389 [ 1080/ 1682]\n",
      "loss: 2525.915527 [ 1090/ 1682]\n",
      "loss: 2927.508057 [ 1100/ 1682]\n",
      "loss: 3571.944824 [ 1110/ 1682]\n",
      "loss: 4067.252441 [ 1120/ 1682]\n",
      "loss: 4461.081055 [ 1130/ 1682]\n",
      "loss: 5653.721680 [ 1140/ 1682]\n",
      "loss: 7446.185059 [ 1150/ 1682]\n",
      "loss: 9405.958008 [ 1160/ 1682]\n",
      "loss: 6994.641602 [ 1170/ 1682]\n",
      "loss: 7058.366211 [ 1180/ 1682]\n",
      "loss: 7928.096680 [ 1190/ 1682]\n",
      "loss: 7212.126465 [ 1200/ 1682]\n",
      "loss: 7797.512695 [ 1210/ 1682]\n",
      "loss: 7880.799316 [ 1220/ 1682]\n",
      "loss: 8864.416992 [ 1230/ 1682]\n",
      "loss: 10357.580078 [ 1240/ 1682]\n",
      "loss: 10105.995117 [ 1250/ 1682]\n",
      "loss: 11352.205078 [ 1260/ 1682]\n",
      "loss: 10257.527344 [ 1270/ 1682]\n",
      "loss: 9613.074219 [ 1280/ 1682]\n",
      "loss: 8555.646484 [ 1290/ 1682]\n",
      "loss: 8616.034180 [ 1300/ 1682]\n",
      "loss: 9730.961914 [ 1310/ 1682]\n",
      "loss: 10051.123047 [ 1320/ 1682]\n",
      "loss: 10170.298828 [ 1330/ 1682]\n",
      "loss: 8852.338867 [ 1340/ 1682]\n",
      "loss: 9351.034180 [ 1350/ 1682]\n",
      "loss: 10303.843750 [ 1360/ 1682]\n",
      "loss: 11931.839844 [ 1370/ 1682]\n",
      "loss: 13634.645508 [ 1380/ 1682]\n",
      "loss: 13817.273438 [ 1390/ 1682]\n",
      "loss: 14075.765625 [ 1400/ 1682]\n",
      "loss: 14648.517578 [ 1410/ 1682]\n",
      "loss: 14893.992188 [ 1420/ 1682]\n",
      "loss: 13191.380859 [ 1430/ 1682]\n",
      "loss: 12832.080078 [ 1440/ 1682]\n",
      "loss: 14390.463867 [ 1450/ 1682]\n",
      "loss: 13066.101562 [ 1460/ 1682]\n",
      "loss: 16483.908203 [ 1470/ 1682]\n",
      "loss: 19773.861328 [ 1480/ 1682]\n",
      "loss: 21295.072266 [ 1490/ 1682]\n",
      "loss: 21641.425781 [ 1500/ 1682]\n",
      "loss: 18895.638672 [ 1510/ 1682]\n",
      "loss: 20511.662109 [ 1520/ 1682]\n",
      "loss: 19289.078125 [ 1530/ 1682]\n",
      "loss: 17870.183594 [ 1540/ 1682]\n",
      "loss: 17679.398438 [ 1550/ 1682]\n",
      "loss: 21399.984375 [ 1560/ 1682]\n",
      "loss: 18898.152344 [ 1570/ 1682]\n",
      "loss: 15191.807617 [ 1580/ 1682]\n",
      "loss: 13524.453125 [ 1590/ 1682]\n",
      "loss: 13577.807617 [ 1600/ 1682]\n",
      "loss: 10812.685547 [ 1610/ 1682]\n",
      "loss: 12229.424805 [ 1620/ 1682]\n",
      "loss: 13816.500000 [ 1630/ 1682]\n",
      "loss: 16616.150391 [ 1640/ 1682]\n",
      "loss: 19730.519531 [ 1650/ 1682]\n",
      "loss: 17343.597656 [ 1660/ 1682]\n",
      "loss: 15790.510742 [ 1670/ 1682]\n",
      "loss: 15296.039062 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 17211.401480 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 193.565308 [    0/ 1682]\n",
      "loss: 291.567932 [   10/ 1682]\n",
      "loss: 149.420624 [   20/ 1682]\n",
      "loss: 27.372395 [   30/ 1682]\n",
      "loss: 177.299500 [   40/ 1682]\n",
      "loss: 288.758148 [   50/ 1682]\n",
      "loss: 98.050751 [   60/ 1682]\n",
      "loss: 20.273687 [   70/ 1682]\n",
      "loss: 458.837494 [   80/ 1682]\n",
      "loss: 371.778992 [   90/ 1682]\n",
      "loss: 28.731487 [  100/ 1682]\n",
      "loss: 34.628746 [  110/ 1682]\n",
      "loss: 151.413132 [  120/ 1682]\n",
      "loss: 62.009777 [  130/ 1682]\n",
      "loss: 28.977650 [  140/ 1682]\n",
      "loss: 17.291433 [  150/ 1682]\n",
      "loss: 47.377300 [  160/ 1682]\n",
      "loss: 10.942368 [  170/ 1682]\n",
      "loss: 30.158905 [  180/ 1682]\n",
      "loss: 119.465660 [  190/ 1682]\n",
      "loss: 622.067200 [  200/ 1682]\n",
      "loss: 7.339301 [  210/ 1682]\n",
      "loss: 4.963887 [  220/ 1682]\n",
      "loss: 2.593771 [  230/ 1682]\n",
      "loss: 0.424651 [  240/ 1682]\n",
      "loss: 0.210573 [  250/ 1682]\n",
      "loss: 0.843303 [  260/ 1682]\n",
      "loss: 8.745919 [  270/ 1682]\n",
      "loss: 17.181431 [  280/ 1682]\n",
      "loss: 26.114771 [  290/ 1682]\n",
      "loss: 28.125134 [  300/ 1682]\n",
      "loss: 34.202862 [  310/ 1682]\n",
      "loss: 36.234699 [  320/ 1682]\n",
      "loss: 62.343433 [  330/ 1682]\n",
      "loss: 70.278763 [  340/ 1682]\n",
      "loss: 61.635845 [  350/ 1682]\n",
      "loss: 40.696304 [  360/ 1682]\n",
      "loss: 36.749012 [  370/ 1682]\n",
      "loss: 52.849342 [  380/ 1682]\n",
      "loss: 74.722305 [  390/ 1682]\n",
      "loss: 92.404015 [  400/ 1682]\n",
      "loss: 114.902420 [  410/ 1682]\n",
      "loss: 113.241257 [  420/ 1682]\n",
      "loss: 76.668968 [  430/ 1682]\n",
      "loss: 83.752846 [  440/ 1682]\n",
      "loss: 303.611542 [  450/ 1682]\n",
      "loss: 174.442993 [  460/ 1682]\n",
      "loss: 164.075989 [  470/ 1682]\n",
      "loss: 164.045990 [  480/ 1682]\n",
      "loss: 245.358643 [  490/ 1682]\n",
      "loss: 219.030640 [  500/ 1682]\n",
      "loss: 147.754730 [  510/ 1682]\n",
      "loss: 120.585388 [  520/ 1682]\n",
      "loss: 185.468674 [  530/ 1682]\n",
      "loss: 224.372192 [  540/ 1682]\n",
      "loss: 174.044769 [  550/ 1682]\n",
      "loss: 174.356201 [  560/ 1682]\n",
      "loss: 153.113800 [  570/ 1682]\n",
      "loss: 244.440521 [  580/ 1682]\n",
      "loss: 271.724213 [  590/ 1682]\n",
      "loss: 297.432556 [  600/ 1682]\n",
      "loss: 275.404541 [  610/ 1682]\n",
      "loss: 278.538940 [  620/ 1682]\n",
      "loss: 316.224335 [  630/ 1682]\n",
      "loss: 410.233643 [  640/ 1682]\n",
      "loss: 532.992615 [  650/ 1682]\n",
      "loss: 639.562561 [  660/ 1682]\n",
      "loss: 618.870483 [  670/ 1682]\n",
      "loss: 669.591248 [  680/ 1682]\n",
      "loss: 608.758118 [  690/ 1682]\n",
      "loss: 570.164429 [  700/ 1682]\n",
      "loss: 390.390289 [  710/ 1682]\n",
      "loss: 218.922394 [  720/ 1682]\n",
      "loss: 158.452576 [  730/ 1682]\n",
      "loss: 80.829117 [  740/ 1682]\n",
      "loss: 66.531807 [  750/ 1682]\n",
      "loss: 98.763763 [  760/ 1682]\n",
      "loss: 177.160980 [  770/ 1682]\n",
      "loss: 193.921387 [  780/ 1682]\n",
      "loss: 233.766754 [  790/ 1682]\n",
      "loss: 306.510895 [  800/ 1682]\n",
      "loss: 392.567078 [  810/ 1682]\n",
      "loss: 438.822601 [  820/ 1682]\n",
      "loss: 426.140778 [  830/ 1682]\n",
      "loss: 271.641266 [  840/ 1682]\n",
      "loss: 297.656158 [  850/ 1682]\n",
      "loss: 393.552094 [  860/ 1682]\n",
      "loss: 410.794922 [  870/ 1682]\n",
      "loss: 480.606445 [  880/ 1682]\n",
      "loss: 423.375671 [  890/ 1682]\n",
      "loss: 680.960938 [  900/ 1682]\n",
      "loss: 507.150879 [  910/ 1682]\n",
      "loss: 651.078125 [  920/ 1682]\n",
      "loss: 671.861145 [  930/ 1682]\n",
      "loss: 825.159363 [  940/ 1682]\n",
      "loss: 1031.895142 [  950/ 1682]\n",
      "loss: 1230.839478 [  960/ 1682]\n",
      "loss: 1276.077637 [  970/ 1682]\n",
      "loss: 1438.476685 [  980/ 1682]\n",
      "loss: 1793.611084 [  990/ 1682]\n",
      "loss: 2235.472412 [ 1000/ 1682]\n",
      "loss: 2410.076172 [ 1010/ 1682]\n",
      "loss: 2558.046387 [ 1020/ 1682]\n",
      "loss: 1964.354492 [ 1030/ 1682]\n",
      "loss: 1517.345337 [ 1040/ 1682]\n",
      "loss: 955.784302 [ 1050/ 1682]\n",
      "loss: 1322.312134 [ 1060/ 1682]\n",
      "loss: 1638.232544 [ 1070/ 1682]\n",
      "loss: 1932.898193 [ 1080/ 1682]\n",
      "loss: 2368.707520 [ 1090/ 1682]\n",
      "loss: 2647.918457 [ 1100/ 1682]\n",
      "loss: 3011.147461 [ 1110/ 1682]\n",
      "loss: 3837.836426 [ 1120/ 1682]\n",
      "loss: 3970.757812 [ 1130/ 1682]\n",
      "loss: 5488.966309 [ 1140/ 1682]\n",
      "loss: 6127.920898 [ 1150/ 1682]\n",
      "loss: 8454.242188 [ 1160/ 1682]\n",
      "loss: 5942.014160 [ 1170/ 1682]\n",
      "loss: 5748.786621 [ 1180/ 1682]\n",
      "loss: 7047.869629 [ 1190/ 1682]\n",
      "loss: 6300.129883 [ 1200/ 1682]\n",
      "loss: 7603.537598 [ 1210/ 1682]\n",
      "loss: 7033.920898 [ 1220/ 1682]\n",
      "loss: 7999.797852 [ 1230/ 1682]\n",
      "loss: 9765.234375 [ 1240/ 1682]\n",
      "loss: 8967.787109 [ 1250/ 1682]\n",
      "loss: 8302.411133 [ 1260/ 1682]\n",
      "loss: 8775.075195 [ 1270/ 1682]\n",
      "loss: 7608.038086 [ 1280/ 1682]\n",
      "loss: 7438.854004 [ 1290/ 1682]\n",
      "loss: 8423.039062 [ 1300/ 1682]\n",
      "loss: 8592.821289 [ 1310/ 1682]\n",
      "loss: 9726.994141 [ 1320/ 1682]\n",
      "loss: 8488.220703 [ 1330/ 1682]\n",
      "loss: 8484.037109 [ 1340/ 1682]\n",
      "loss: 8374.757812 [ 1350/ 1682]\n",
      "loss: 10084.439453 [ 1360/ 1682]\n",
      "loss: 11695.958008 [ 1370/ 1682]\n",
      "loss: 12207.732422 [ 1380/ 1682]\n",
      "loss: 12329.449219 [ 1390/ 1682]\n",
      "loss: 12513.271484 [ 1400/ 1682]\n",
      "loss: 14387.854492 [ 1410/ 1682]\n",
      "loss: 14521.248047 [ 1420/ 1682]\n",
      "loss: 13000.317383 [ 1430/ 1682]\n",
      "loss: 11557.202148 [ 1440/ 1682]\n",
      "loss: 13950.028320 [ 1450/ 1682]\n",
      "loss: 13464.232422 [ 1460/ 1682]\n",
      "loss: 15251.635742 [ 1470/ 1682]\n",
      "loss: 15832.151367 [ 1480/ 1682]\n",
      "loss: 19326.105469 [ 1490/ 1682]\n",
      "loss: 6890.136719 [ 1500/ 1682]\n",
      "loss: 17941.931641 [ 1510/ 1682]\n",
      "loss: 21623.550781 [ 1520/ 1682]\n",
      "loss: 13243.335938 [ 1530/ 1682]\n",
      "loss: 13133.653320 [ 1540/ 1682]\n",
      "loss: 6776.264648 [ 1550/ 1682]\n",
      "loss: 9198.217773 [ 1560/ 1682]\n",
      "loss: 8191.014160 [ 1570/ 1682]\n",
      "loss: 12363.001953 [ 1580/ 1682]\n",
      "loss: 6966.743652 [ 1590/ 1682]\n",
      "loss: 7442.818848 [ 1600/ 1682]\n",
      "loss: 2126.584229 [ 1610/ 1682]\n",
      "loss: 3494.121826 [ 1620/ 1682]\n",
      "loss: 4036.873535 [ 1630/ 1682]\n",
      "loss: 3951.211670 [ 1640/ 1682]\n",
      "loss: 4829.075195 [ 1650/ 1682]\n",
      "loss: 3654.511230 [ 1660/ 1682]\n",
      "loss: 3693.656738 [ 1670/ 1682]\n",
      "loss: 600.902893 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 3447.846769 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 3590.315918 [    0/ 1682]\n",
      "loss: 3235.507812 [   10/ 1682]\n",
      "loss: 1933.005493 [   20/ 1682]\n",
      "loss: 1296.753174 [   30/ 1682]\n",
      "loss: 597.407837 [   40/ 1682]\n",
      "loss: 284.761719 [   50/ 1682]\n",
      "loss: 88.458923 [   60/ 1682]\n",
      "loss: 55.981018 [   70/ 1682]\n",
      "loss: 37.582989 [   80/ 1682]\n",
      "loss: 9.939374 [   90/ 1682]\n",
      "loss: 6.821969 [  100/ 1682]\n",
      "loss: 13.309084 [  110/ 1682]\n",
      "loss: 12.642455 [  120/ 1682]\n",
      "loss: 49.867287 [  130/ 1682]\n",
      "loss: 55.432690 [  140/ 1682]\n",
      "loss: 96.089287 [  150/ 1682]\n",
      "loss: 41.253834 [  160/ 1682]\n",
      "loss: 123.211807 [  170/ 1682]\n",
      "loss: 76.821983 [  180/ 1682]\n",
      "loss: 80.630600 [  190/ 1682]\n",
      "loss: 66.283936 [  200/ 1682]\n",
      "loss: 53.543018 [  210/ 1682]\n",
      "loss: 57.349770 [  220/ 1682]\n",
      "loss: 42.472832 [  230/ 1682]\n",
      "loss: 50.243793 [  240/ 1682]\n",
      "loss: 9.086802 [  250/ 1682]\n",
      "loss: 31.048084 [  260/ 1682]\n",
      "loss: 55.597881 [  270/ 1682]\n",
      "loss: 46.129894 [  280/ 1682]\n",
      "loss: 47.218536 [  290/ 1682]\n",
      "loss: 35.294968 [  300/ 1682]\n",
      "loss: 42.641514 [  310/ 1682]\n",
      "loss: 27.993341 [  320/ 1682]\n",
      "loss: 58.839703 [  330/ 1682]\n",
      "loss: 63.678596 [  340/ 1682]\n",
      "loss: 49.252125 [  350/ 1682]\n",
      "loss: 35.589939 [  360/ 1682]\n",
      "loss: 32.948158 [  370/ 1682]\n",
      "loss: 45.947548 [  380/ 1682]\n",
      "loss: 55.386517 [  390/ 1682]\n",
      "loss: 77.004066 [  400/ 1682]\n",
      "loss: 89.824669 [  410/ 1682]\n",
      "loss: 72.168068 [  420/ 1682]\n",
      "loss: 62.509369 [  430/ 1682]\n",
      "loss: 66.768692 [  440/ 1682]\n",
      "loss: 392.308411 [  450/ 1682]\n",
      "loss: 150.649734 [  460/ 1682]\n",
      "loss: 143.325928 [  470/ 1682]\n",
      "loss: 151.886963 [  480/ 1682]\n",
      "loss: 132.342651 [  490/ 1682]\n",
      "loss: 157.236420 [  500/ 1682]\n",
      "loss: 121.679825 [  510/ 1682]\n",
      "loss: 104.251076 [  520/ 1682]\n",
      "loss: 163.394485 [  530/ 1682]\n",
      "loss: 181.374664 [  540/ 1682]\n",
      "loss: 131.724655 [  550/ 1682]\n",
      "loss: 152.962967 [  560/ 1682]\n",
      "loss: 125.063431 [  570/ 1682]\n",
      "loss: 249.387421 [  580/ 1682]\n",
      "loss: 244.815750 [  590/ 1682]\n",
      "loss: 311.307800 [  600/ 1682]\n",
      "loss: 248.336639 [  610/ 1682]\n",
      "loss: 452.404480 [  620/ 1682]\n",
      "loss: 287.157776 [  630/ 1682]\n",
      "loss: 357.477966 [  640/ 1682]\n",
      "loss: 495.067322 [  650/ 1682]\n",
      "loss: 597.962463 [  660/ 1682]\n",
      "loss: 553.894409 [  670/ 1682]\n",
      "loss: 627.292419 [  680/ 1682]\n",
      "loss: 568.663879 [  690/ 1682]\n",
      "loss: 536.141541 [  700/ 1682]\n",
      "loss: 345.663361 [  710/ 1682]\n",
      "loss: 194.938263 [  720/ 1682]\n",
      "loss: 138.156143 [  730/ 1682]\n",
      "loss: 184.980865 [  740/ 1682]\n",
      "loss: 54.085693 [  750/ 1682]\n",
      "loss: 83.004227 [  760/ 1682]\n",
      "loss: 156.515869 [  770/ 1682]\n",
      "loss: 171.990219 [  780/ 1682]\n",
      "loss: 209.003952 [  790/ 1682]\n",
      "loss: 274.258606 [  800/ 1682]\n",
      "loss: 361.392426 [  810/ 1682]\n",
      "loss: 451.443024 [  820/ 1682]\n",
      "loss: 404.761902 [  830/ 1682]\n",
      "loss: 244.913895 [  840/ 1682]\n",
      "loss: 286.283752 [  850/ 1682]\n",
      "loss: 361.166351 [  860/ 1682]\n",
      "loss: 520.356934 [  870/ 1682]\n",
      "loss: 444.736328 [  880/ 1682]\n",
      "loss: 400.163635 [  890/ 1682]\n",
      "loss: 462.392578 [  900/ 1682]\n",
      "loss: 474.928406 [  910/ 1682]\n",
      "loss: 609.236633 [  920/ 1682]\n",
      "loss: 605.451782 [  930/ 1682]\n",
      "loss: 777.993286 [  940/ 1682]\n",
      "loss: 1007.965820 [  950/ 1682]\n",
      "loss: 1223.122070 [  960/ 1682]\n",
      "loss: 1157.216064 [  970/ 1682]\n",
      "loss: 1278.904541 [  980/ 1682]\n",
      "loss: 1737.270874 [  990/ 1682]\n",
      "loss: 2177.680176 [ 1000/ 1682]\n",
      "loss: 2328.959473 [ 1010/ 1682]\n",
      "loss: 2474.438721 [ 1020/ 1682]\n",
      "loss: 1955.337158 [ 1030/ 1682]\n",
      "loss: 1485.496704 [ 1040/ 1682]\n",
      "loss: 975.496887 [ 1050/ 1682]\n",
      "loss: 1262.737183 [ 1060/ 1682]\n",
      "loss: 1571.493652 [ 1070/ 1682]\n",
      "loss: 2031.172485 [ 1080/ 1682]\n",
      "loss: 2283.123779 [ 1090/ 1682]\n",
      "loss: 2721.160645 [ 1100/ 1682]\n",
      "loss: 3343.466309 [ 1110/ 1682]\n",
      "loss: 3833.955078 [ 1120/ 1682]\n",
      "loss: 4195.941895 [ 1130/ 1682]\n",
      "loss: 5366.627930 [ 1140/ 1682]\n",
      "loss: 7114.799316 [ 1150/ 1682]\n",
      "loss: 9032.999023 [ 1160/ 1682]\n",
      "loss: 6675.367188 [ 1170/ 1682]\n",
      "loss: 6739.494629 [ 1180/ 1682]\n",
      "loss: 7585.919434 [ 1190/ 1682]\n",
      "loss: 6885.911621 [ 1200/ 1682]\n",
      "loss: 6836.743164 [ 1210/ 1682]\n",
      "loss: 7563.845215 [ 1220/ 1682]\n",
      "loss: 8502.115234 [ 1230/ 1682]\n",
      "loss: 9965.747070 [ 1240/ 1682]\n",
      "loss: 9718.847656 [ 1250/ 1682]\n",
      "loss: 10985.521484 [ 1260/ 1682]\n",
      "loss: 10936.232422 [ 1270/ 1682]\n",
      "loss: 9235.675781 [ 1280/ 1682]\n",
      "loss: 8199.642578 [ 1290/ 1682]\n",
      "loss: 8267.622070 [ 1300/ 1682]\n",
      "loss: 9382.943359 [ 1310/ 1682]\n",
      "loss: 10544.706055 [ 1320/ 1682]\n",
      "loss: 9812.650391 [ 1330/ 1682]\n",
      "loss: 9018.638672 [ 1340/ 1682]\n",
      "loss: 8978.215820 [ 1350/ 1682]\n",
      "loss: 9806.517578 [ 1360/ 1682]\n",
      "loss: 11510.268555 [ 1370/ 1682]\n",
      "loss: 12295.126953 [ 1380/ 1682]\n",
      "loss: 13362.916016 [ 1390/ 1682]\n",
      "loss: 13617.099609 [ 1400/ 1682]\n",
      "loss: 14183.642578 [ 1410/ 1682]\n",
      "loss: 13343.794922 [ 1420/ 1682]\n",
      "loss: 11967.699219 [ 1430/ 1682]\n",
      "loss: 12394.051758 [ 1440/ 1682]\n",
      "loss: 13324.043945 [ 1450/ 1682]\n",
      "loss: 12009.412109 [ 1460/ 1682]\n",
      "loss: 15987.000000 [ 1470/ 1682]\n",
      "loss: 19229.345703 [ 1480/ 1682]\n",
      "loss: 20729.472656 [ 1490/ 1682]\n",
      "loss: 20913.109375 [ 1500/ 1682]\n",
      "loss: 16105.232422 [ 1510/ 1682]\n",
      "loss: 18259.900391 [ 1520/ 1682]\n",
      "loss: 17529.794922 [ 1530/ 1682]\n",
      "loss: 17367.613281 [ 1540/ 1682]\n",
      "loss: 16260.715820 [ 1550/ 1682]\n",
      "loss: 18637.968750 [ 1560/ 1682]\n",
      "loss: 18365.181641 [ 1570/ 1682]\n",
      "loss: 15071.720703 [ 1580/ 1682]\n",
      "loss: 12197.976562 [ 1590/ 1682]\n",
      "loss: 12152.518555 [ 1600/ 1682]\n",
      "loss: 11409.388672 [ 1610/ 1682]\n",
      "loss: 11668.577148 [ 1620/ 1682]\n",
      "loss: 13552.909180 [ 1630/ 1682]\n",
      "loss: 16116.607422 [ 1640/ 1682]\n",
      "loss: 19185.652344 [ 1650/ 1682]\n",
      "loss: 18318.675781 [ 1660/ 1682]\n",
      "loss: 15739.926758 [ 1670/ 1682]\n",
      "loss: 14816.375000 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 17067.459398 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 655.550476 [    0/ 1682]\n",
      "loss: 76.266098 [   10/ 1682]\n",
      "loss: 63.301476 [   20/ 1682]\n",
      "loss: 51.533852 [   30/ 1682]\n",
      "loss: 38.163643 [   40/ 1682]\n",
      "loss: 26.936853 [   50/ 1682]\n",
      "loss: 915.394165 [   60/ 1682]\n",
      "loss: 41.070221 [   70/ 1682]\n",
      "loss: 74.089020 [   80/ 1682]\n",
      "loss: 873.983582 [   90/ 1682]\n",
      "loss: 53.519184 [  100/ 1682]\n",
      "loss: 61.300529 [  110/ 1682]\n",
      "loss: 59.779839 [  120/ 1682]\n",
      "loss: 50.928089 [  130/ 1682]\n",
      "loss: 28.642694 [  140/ 1682]\n",
      "loss: 19.601912 [  150/ 1682]\n",
      "loss: 780.605835 [  160/ 1682]\n",
      "loss: 28.316412 [  170/ 1682]\n",
      "loss: 20.243507 [  180/ 1682]\n",
      "loss: 14.861380 [  190/ 1682]\n",
      "loss: 723.684326 [  200/ 1682]\n",
      "loss: 21.661366 [  210/ 1682]\n",
      "loss: 17.330927 [  220/ 1682]\n",
      "loss: 20.610693 [  230/ 1682]\n",
      "loss: 6.536101 [  240/ 1682]\n",
      "loss: 5.950448 [  250/ 1682]\n",
      "loss: 2.860709 [  260/ 1682]\n",
      "loss: 641.585144 [  270/ 1682]\n",
      "loss: 4.904881 [  280/ 1682]\n",
      "loss: 6.551869 [  290/ 1682]\n",
      "loss: 10.222246 [  300/ 1682]\n",
      "loss: 12.668260 [  310/ 1682]\n",
      "loss: 12.866481 [  320/ 1682]\n",
      "loss: 629.002869 [  330/ 1682]\n",
      "loss: 39.378994 [  340/ 1682]\n",
      "loss: 481.346283 [  350/ 1682]\n",
      "loss: 837.333679 [  360/ 1682]\n",
      "loss: 897.330688 [  370/ 1682]\n",
      "loss: 32.103035 [  380/ 1682]\n",
      "loss: 253.878632 [  390/ 1682]\n",
      "loss: 136.023331 [  400/ 1682]\n",
      "loss: 64.495255 [  410/ 1682]\n",
      "loss: 53.284191 [  420/ 1682]\n",
      "loss: 465.601868 [  430/ 1682]\n",
      "loss: 48.643105 [  440/ 1682]\n",
      "loss: 75.021622 [  450/ 1682]\n",
      "loss: 124.709557 [  460/ 1682]\n",
      "loss: 108.916542 [  470/ 1682]\n",
      "loss: 118.051804 [  480/ 1682]\n",
      "loss: 426.046448 [  490/ 1682]\n",
      "loss: 421.385101 [  500/ 1682]\n",
      "loss: 229.437164 [  510/ 1682]\n",
      "loss: 265.893738 [  520/ 1682]\n",
      "loss: 136.459183 [  530/ 1682]\n",
      "loss: 152.912354 [  540/ 1682]\n",
      "loss: 642.716797 [  550/ 1682]\n",
      "loss: 217.368042 [  560/ 1682]\n",
      "loss: 98.607597 [  570/ 1682]\n",
      "loss: 287.489441 [  580/ 1682]\n",
      "loss: 366.179016 [  590/ 1682]\n",
      "loss: 248.296555 [  600/ 1682]\n",
      "loss: 214.963303 [  610/ 1682]\n",
      "loss: 384.113556 [  620/ 1682]\n",
      "loss: 346.173767 [  630/ 1682]\n",
      "loss: 335.952667 [  640/ 1682]\n",
      "loss: 447.532623 [  650/ 1682]\n",
      "loss: 492.523529 [  660/ 1682]\n",
      "loss: 543.160645 [  670/ 1682]\n",
      "loss: 573.713867 [  680/ 1682]\n",
      "loss: 465.985168 [  690/ 1682]\n",
      "loss: 451.091949 [  700/ 1682]\n",
      "loss: 318.268646 [  710/ 1682]\n",
      "loss: 165.731964 [  720/ 1682]\n",
      "loss: 113.769836 [  730/ 1682]\n",
      "loss: 50.564480 [  740/ 1682]\n",
      "loss: 42.916420 [  750/ 1682]\n",
      "loss: 64.544174 [  760/ 1682]\n",
      "loss: 336.285461 [  770/ 1682]\n",
      "loss: 306.558014 [  780/ 1682]\n",
      "loss: 164.242630 [  790/ 1682]\n",
      "loss: 253.632248 [  800/ 1682]\n",
      "loss: 321.220825 [  810/ 1682]\n",
      "loss: 376.983429 [  820/ 1682]\n",
      "loss: 350.899689 [  830/ 1682]\n",
      "loss: 197.704803 [  840/ 1682]\n",
      "loss: 235.533905 [  850/ 1682]\n",
      "loss: 374.706757 [  860/ 1682]\n",
      "loss: 357.602356 [  870/ 1682]\n",
      "loss: 400.115875 [  880/ 1682]\n",
      "loss: 374.436310 [  890/ 1682]\n",
      "loss: 381.531738 [  900/ 1682]\n",
      "loss: 452.106140 [  910/ 1682]\n",
      "loss: 453.073914 [  920/ 1682]\n",
      "loss: 478.505615 [  930/ 1682]\n",
      "loss: 603.595764 [  940/ 1682]\n",
      "loss: 912.475281 [  950/ 1682]\n",
      "loss: 1148.556152 [  960/ 1682]\n",
      "loss: 1175.344116 [  970/ 1682]\n",
      "loss: 1228.226074 [  980/ 1682]\n",
      "loss: 1648.316650 [  990/ 1682]\n",
      "loss: 2011.431641 [ 1000/ 1682]\n",
      "loss: 2044.575928 [ 1010/ 1682]\n",
      "loss: 2368.084473 [ 1020/ 1682]\n",
      "loss: 1829.025757 [ 1030/ 1682]\n",
      "loss: 1170.420166 [ 1040/ 1682]\n",
      "loss: 909.445312 [ 1050/ 1682]\n",
      "loss: 1075.910645 [ 1060/ 1682]\n",
      "loss: 1487.163574 [ 1070/ 1682]\n",
      "loss: 1935.231812 [ 1080/ 1682]\n",
      "loss: 2029.773804 [ 1090/ 1682]\n",
      "loss: 2368.866943 [ 1100/ 1682]\n",
      "loss: 2705.989502 [ 1110/ 1682]\n",
      "loss: 3691.305908 [ 1120/ 1682]\n",
      "loss: 4125.303711 [ 1130/ 1682]\n",
      "loss: 5142.129883 [ 1140/ 1682]\n",
      "loss: 6934.624023 [ 1150/ 1682]\n",
      "loss: 8829.879883 [ 1160/ 1682]\n",
      "loss: 6500.928223 [ 1170/ 1682]\n",
      "loss: 6564.262695 [ 1180/ 1682]\n",
      "loss: 7400.006348 [ 1190/ 1682]\n",
      "loss: 6166.669434 [ 1200/ 1682]\n",
      "loss: 7273.821289 [ 1210/ 1682]\n",
      "loss: 6753.305664 [ 1220/ 1682]\n",
      "loss: 8305.291016 [ 1230/ 1682]\n",
      "loss: 9752.672852 [ 1240/ 1682]\n",
      "loss: 9508.418945 [ 1250/ 1682]\n",
      "loss: 10048.490234 [ 1260/ 1682]\n",
      "loss: 10667.146484 [ 1270/ 1682]\n",
      "loss: 9030.818359 [ 1280/ 1682]\n",
      "loss: 7906.524902 [ 1290/ 1682]\n",
      "loss: 8073.835938 [ 1300/ 1682]\n",
      "loss: 8197.908203 [ 1310/ 1682]\n",
      "loss: 10325.772461 [ 1320/ 1682]\n",
      "loss: 8867.291992 [ 1330/ 1682]\n",
      "loss: 8816.374023 [ 1340/ 1682]\n",
      "loss: 8169.812500 [ 1350/ 1682]\n",
      "loss: 9091.799805 [ 1360/ 1682]\n",
      "loss: 10584.857422 [ 1370/ 1682]\n",
      "loss: 12938.957031 [ 1380/ 1682]\n",
      "loss: 13116.833008 [ 1390/ 1682]\n",
      "loss: 11888.207031 [ 1400/ 1682]\n",
      "loss: 13927.106445 [ 1410/ 1682]\n",
      "loss: 14166.629883 [ 1420/ 1682]\n",
      "loss: 12431.972656 [ 1430/ 1682]\n",
      "loss: 12157.384766 [ 1440/ 1682]\n",
      "loss: 13675.390625 [ 1450/ 1682]\n",
      "loss: 13905.833984 [ 1460/ 1682]\n",
      "loss: 15718.197266 [ 1470/ 1682]\n",
      "loss: 18934.488281 [ 1480/ 1682]\n",
      "loss: 20423.091797 [ 1490/ 1682]\n",
      "loss: 20762.205078 [ 1500/ 1682]\n",
      "loss: 18075.000000 [ 1510/ 1682]\n",
      "loss: 19655.988281 [ 1520/ 1682]\n",
      "loss: 18459.384766 [ 1530/ 1682]\n",
      "loss: 15140.010742 [ 1540/ 1682]\n",
      "loss: 14053.122070 [ 1550/ 1682]\n",
      "loss: 20525.066406 [ 1560/ 1682]\n",
      "loss: 18076.496094 [ 1570/ 1682]\n",
      "loss: 16294.575195 [ 1580/ 1682]\n",
      "loss: 11901.051758 [ 1590/ 1682]\n",
      "loss: 12229.411133 [ 1600/ 1682]\n",
      "loss: 10408.306641 [ 1610/ 1682]\n",
      "loss: 10638.691406 [ 1620/ 1682]\n",
      "loss: 13444.440430 [ 1630/ 1682]\n",
      "loss: 15846.153320 [ 1640/ 1682]\n",
      "loss: 17654.585938 [ 1650/ 1682]\n",
      "loss: 18030.333984 [ 1660/ 1682]\n",
      "loss: 15472.615234 [ 1670/ 1682]\n",
      "loss: 14557.041016 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 16932.768066 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 727.376831 [    0/ 1682]\n",
      "loss: 759.316040 [   10/ 1682]\n",
      "loss: 98.479630 [   20/ 1682]\n",
      "loss: 67.980621 [   30/ 1682]\n",
      "loss: 66.881058 [   40/ 1682]\n",
      "loss: 651.486816 [   50/ 1682]\n",
      "loss: 869.551086 [   60/ 1682]\n",
      "loss: 55.656158 [   70/ 1682]\n",
      "loss: 93.598083 [   80/ 1682]\n",
      "loss: 77.559998 [   90/ 1682]\n",
      "loss: 70.267288 [  100/ 1682]\n",
      "loss: 79.123459 [  110/ 1682]\n",
      "loss: 77.395218 [  120/ 1682]\n",
      "loss: 672.285583 [  130/ 1682]\n",
      "loss: 41.143158 [  140/ 1682]\n",
      "loss: 30.184620 [  150/ 1682]\n",
      "loss: 39.801346 [  160/ 1682]\n",
      "loss: 30.227575 [  170/ 1682]\n",
      "loss: 532.761963 [  180/ 1682]\n",
      "loss: 14.655058 [  190/ 1682]\n",
      "loss: 344.385315 [  200/ 1682]\n",
      "loss: 32.678429 [  210/ 1682]\n",
      "loss: 27.282461 [  220/ 1682]\n",
      "loss: 20.682507 [  230/ 1682]\n",
      "loss: 266.584045 [  240/ 1682]\n",
      "loss: 20.653921 [  250/ 1682]\n",
      "loss: 7.272582 [  260/ 1682]\n",
      "loss: 191.044495 [  270/ 1682]\n",
      "loss: 1.440272 [  280/ 1682]\n",
      "loss: 4.257025 [  290/ 1682]\n",
      "loss: 4.631171 [  300/ 1682]\n",
      "loss: 343.459137 [  310/ 1682]\n",
      "loss: 240.389236 [  320/ 1682]\n",
      "loss: 21.009441 [  330/ 1682]\n",
      "loss: 27.202850 [  340/ 1682]\n",
      "loss: 23.605526 [  350/ 1682]\n",
      "loss: 193.328827 [  360/ 1682]\n",
      "loss: 339.800507 [  370/ 1682]\n",
      "loss: 21.204330 [  380/ 1682]\n",
      "loss: 78.253036 [  390/ 1682]\n",
      "loss: 43.581074 [  400/ 1682]\n",
      "loss: 53.370522 [  410/ 1682]\n",
      "loss: 38.978706 [  420/ 1682]\n",
      "loss: 29.183935 [  430/ 1682]\n",
      "loss: 36.301815 [  440/ 1682]\n",
      "loss: 282.977325 [  450/ 1682]\n",
      "loss: 189.620728 [  460/ 1682]\n",
      "loss: 96.094872 [  470/ 1682]\n",
      "loss: 224.694366 [  480/ 1682]\n",
      "loss: 96.071823 [  490/ 1682]\n",
      "loss: 116.638916 [  500/ 1682]\n",
      "loss: 84.058075 [  510/ 1682]\n",
      "loss: 274.725311 [  520/ 1682]\n",
      "loss: 112.792213 [  530/ 1682]\n",
      "loss: 127.755661 [  540/ 1682]\n",
      "loss: 89.230515 [  550/ 1682]\n",
      "loss: 104.144936 [  560/ 1682]\n",
      "loss: 241.927200 [  570/ 1682]\n",
      "loss: 227.767166 [  580/ 1682]\n",
      "loss: 241.416946 [  590/ 1682]\n",
      "loss: 311.301514 [  600/ 1682]\n",
      "loss: 184.900925 [  610/ 1682]\n",
      "loss: 218.106781 [  620/ 1682]\n",
      "loss: 218.580780 [  630/ 1682]\n",
      "loss: 298.226685 [  640/ 1682]\n",
      "loss: 384.357239 [  650/ 1682]\n",
      "loss: 473.243652 [  660/ 1682]\n",
      "loss: 478.794983 [  670/ 1682]\n",
      "loss: 523.851685 [  680/ 1682]\n",
      "loss: 428.285828 [  690/ 1682]\n",
      "loss: 414.235016 [  700/ 1682]\n",
      "loss: 281.624451 [  710/ 1682]\n",
      "loss: 249.925995 [  720/ 1682]\n",
      "loss: 171.876801 [  730/ 1682]\n",
      "loss: 36.737175 [  740/ 1682]\n",
      "loss: 30.160105 [  750/ 1682]\n",
      "loss: 48.811180 [  760/ 1682]\n",
      "loss: 107.251686 [  770/ 1682]\n",
      "loss: 120.157814 [  780/ 1682]\n",
      "loss: 151.588043 [  790/ 1682]\n",
      "loss: 206.472504 [  800/ 1682]\n",
      "loss: 322.678436 [  810/ 1682]\n",
      "loss: 312.300262 [  820/ 1682]\n",
      "loss: 317.729889 [  830/ 1682]\n",
      "loss: 244.032806 [  840/ 1682]\n",
      "loss: 189.938599 [  850/ 1682]\n",
      "loss: 333.825745 [  860/ 1682]\n",
      "loss: 318.507599 [  870/ 1682]\n",
      "loss: 353.080933 [  880/ 1682]\n",
      "loss: 319.058105 [  890/ 1682]\n",
      "loss: 373.523651 [  900/ 1682]\n",
      "loss: 385.960754 [  910/ 1682]\n",
      "loss: 507.838074 [  920/ 1682]\n",
      "loss: 501.782776 [  930/ 1682]\n",
      "loss: 547.046509 [  940/ 1682]\n",
      "loss: 755.492310 [  950/ 1682]\n",
      "loss: 1077.646484 [  960/ 1682]\n",
      "loss: 1007.116028 [  970/ 1682]\n",
      "loss: 1276.938354 [  980/ 1682]\n",
      "loss: 1563.191772 [  990/ 1682]\n",
      "loss: 1965.376953 [ 1000/ 1682]\n",
      "loss: 2126.672363 [ 1010/ 1682]\n",
      "loss: 1903.563721 [ 1020/ 1682]\n",
      "loss: 1739.701538 [ 1030/ 1682]\n",
      "loss: 1296.053345 [ 1040/ 1682]\n",
      "loss: 846.720215 [ 1050/ 1682]\n",
      "loss: 1116.011108 [ 1060/ 1682]\n",
      "loss: 1255.669678 [ 1070/ 1682]\n",
      "loss: 1689.561157 [ 1080/ 1682]\n",
      "loss: 1525.796265 [ 1090/ 1682]\n",
      "loss: 2251.926270 [ 1100/ 1682]\n",
      "loss: 3100.728516 [ 1110/ 1682]\n",
      "loss: 3264.634277 [ 1120/ 1682]\n",
      "loss: 3364.759277 [ 1130/ 1682]\n",
      "loss: 4728.538574 [ 1140/ 1682]\n",
      "loss: 6759.166504 [ 1150/ 1682]\n",
      "loss: 8631.773438 [ 1160/ 1682]\n",
      "loss: 6331.102539 [ 1170/ 1682]\n",
      "loss: 5632.679199 [ 1180/ 1682]\n",
      "loss: 7124.144531 [ 1190/ 1682]\n",
      "loss: 6536.475586 [ 1200/ 1682]\n",
      "loss: 7094.273438 [ 1210/ 1682]\n",
      "loss: 7173.625000 [ 1220/ 1682]\n",
      "loss: 7614.563965 [ 1230/ 1682]\n",
      "loss: 9023.202148 [ 1240/ 1682]\n",
      "loss: 9303.018555 [ 1250/ 1682]\n",
      "loss: 10501.264648 [ 1260/ 1682]\n",
      "loss: 9421.679688 [ 1270/ 1682]\n",
      "loss: 8830.884766 [ 1280/ 1682]\n",
      "loss: 7305.113281 [ 1290/ 1682]\n",
      "loss: 7884.824219 [ 1300/ 1682]\n",
      "loss: 8548.611328 [ 1310/ 1682]\n",
      "loss: 10111.905273 [ 1320/ 1682]\n",
      "loss: 8745.397461 [ 1330/ 1682]\n",
      "loss: 8618.892578 [ 1340/ 1682]\n",
      "loss: 8192.517578 [ 1350/ 1682]\n",
      "loss: 9380.470703 [ 1360/ 1682]\n",
      "loss: 10577.434570 [ 1370/ 1682]\n",
      "loss: 12699.538086 [ 1380/ 1682]\n",
      "loss: 12875.750977 [ 1390/ 1682]\n",
      "loss: 12560.012695 [ 1400/ 1682]\n",
      "loss: 13537.239258 [ 1410/ 1682]\n",
      "loss: 13328.330078 [ 1420/ 1682]\n",
      "loss: 12326.224609 [ 1430/ 1682]\n",
      "loss: 11925.328125 [ 1440/ 1682]\n",
      "loss: 13429.192383 [ 1450/ 1682]\n",
      "loss: 13657.557617 [ 1460/ 1682]\n",
      "loss: 14698.754883 [ 1470/ 1682]\n",
      "loss: 18644.851562 [ 1480/ 1682]\n",
      "loss: 19946.605469 [ 1490/ 1682]\n",
      "loss: 20458.712891 [ 1500/ 1682]\n",
      "loss: 17622.828125 [ 1510/ 1682]\n",
      "loss: 19360.857422 [ 1520/ 1682]\n",
      "loss: 17374.183594 [ 1530/ 1682]\n",
      "loss: 16811.884766 [ 1540/ 1682]\n",
      "loss: 16744.003906 [ 1550/ 1682]\n",
      "loss: 20223.300781 [ 1560/ 1682]\n",
      "loss: 16780.820312 [ 1570/ 1682]\n",
      "loss: 16025.793945 [ 1580/ 1682]\n",
      "loss: 12694.168945 [ 1590/ 1682]\n",
      "loss: 12785.709961 [ 1600/ 1682]\n",
      "loss: 10960.065430 [ 1610/ 1682]\n",
      "loss: 11343.499023 [ 1620/ 1682]\n",
      "loss: 13200.145508 [ 1630/ 1682]\n",
      "loss: 15580.859375 [ 1640/ 1682]\n",
      "loss: 17248.113281 [ 1650/ 1682]\n",
      "loss: 17747.208984 [ 1660/ 1682]\n",
      "loss: 15210.244141 [ 1670/ 1682]\n",
      "loss: 14302.507812 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 16707.086463 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 91.126457 [    0/ 1682]\n",
      "loss: 99.593491 [   10/ 1682]\n",
      "loss: 101.667976 [   20/ 1682]\n",
      "loss: 684.608398 [   30/ 1682]\n",
      "loss: 68.961594 [   40/ 1682]\n",
      "loss: 412.773438 [   50/ 1682]\n",
      "loss: 46.628380 [   60/ 1682]\n",
      "loss: 534.088989 [   70/ 1682]\n",
      "loss: 115.238853 [   80/ 1682]\n",
      "loss: 97.304916 [   90/ 1682]\n",
      "loss: 510.010803 [  100/ 1682]\n",
      "loss: 99.101677 [  110/ 1682]\n",
      "loss: 97.168549 [  120/ 1682]\n",
      "loss: 85.773582 [  130/ 1682]\n",
      "loss: 55.835716 [  140/ 1682]\n",
      "loss: 341.601654 [  150/ 1682]\n",
      "loss: 54.291157 [  160/ 1682]\n",
      "loss: 56.036797 [  170/ 1682]\n",
      "loss: 370.053192 [  180/ 1682]\n",
      "loss: 23.846281 [  190/ 1682]\n",
      "loss: 31.543377 [  200/ 1682]\n",
      "loss: 45.904408 [  210/ 1682]\n",
      "loss: 38.229546 [  220/ 1682]\n",
      "loss: 31.363926 [  230/ 1682]\n",
      "loss: 381.618988 [  240/ 1682]\n",
      "loss: 20.839197 [  250/ 1682]\n",
      "loss: 13.922676 [  260/ 1682]\n",
      "loss: 7.978363 [  270/ 1682]\n",
      "loss: 0.456806 [  280/ 1682]\n",
      "loss: 0.189205 [  290/ 1682]\n",
      "loss: 1.311200 [  300/ 1682]\n",
      "loss: 2.059174 [  310/ 1682]\n",
      "loss: 2.259098 [  320/ 1682]\n",
      "loss: 12.598606 [  330/ 1682]\n",
      "loss: 123.919029 [  340/ 1682]\n",
      "loss: 15.637026 [  350/ 1682]\n",
      "loss: 4.766545 [  360/ 1682]\n",
      "loss: 3.605256 [  370/ 1682]\n",
      "loss: 12.581326 [  380/ 1682]\n",
      "loss: 19.674549 [  390/ 1682]\n",
      "loss: 32.136150 [  400/ 1682]\n",
      "loss: 365.447968 [  410/ 1682]\n",
      "loss: 201.628387 [  420/ 1682]\n",
      "loss: 234.530075 [  430/ 1682]\n",
      "loss: 23.597672 [  440/ 1682]\n",
      "loss: 43.180260 [  450/ 1682]\n",
      "loss: 251.591110 [  460/ 1682]\n",
      "loss: 76.460045 [  470/ 1682]\n",
      "loss: 200.022064 [  480/ 1682]\n",
      "loss: 76.445053 [  490/ 1682]\n",
      "loss: 94.897354 [  500/ 1682]\n",
      "loss: 57.941181 [  510/ 1682]\n",
      "loss: 41.975964 [  520/ 1682]\n",
      "loss: 91.487381 [  530/ 1682]\n",
      "loss: 104.972061 [  540/ 1682]\n",
      "loss: 62.837666 [  550/ 1682]\n",
      "loss: 83.699211 [  560/ 1682]\n",
      "loss: 61.291389 [  570/ 1682]\n",
      "loss: 176.681702 [  580/ 1682]\n",
      "loss: 299.614258 [  590/ 1682]\n",
      "loss: 188.638794 [  600/ 1682]\n",
      "loss: 157.278015 [  610/ 1682]\n",
      "loss: 210.293137 [  620/ 1682]\n",
      "loss: 245.653641 [  630/ 1682]\n",
      "loss: 263.004333 [  640/ 1682]\n",
      "loss: 362.280914 [  650/ 1682]\n",
      "loss: 429.208740 [  660/ 1682]\n",
      "loss: 394.166199 [  670/ 1682]\n",
      "loss: 476.607849 [  680/ 1682]\n",
      "loss: 425.741547 [  690/ 1682]\n",
      "loss: 393.806030 [  700/ 1682]\n",
      "loss: 247.511475 [  710/ 1682]\n",
      "loss: 122.485413 [  720/ 1682]\n",
      "loss: 73.173592 [  730/ 1682]\n",
      "loss: 25.260128 [  740/ 1682]\n",
      "loss: 19.743919 [  750/ 1682]\n",
      "loss: 35.438805 [  760/ 1682]\n",
      "loss: 86.483444 [  770/ 1682]\n",
      "loss: 209.613129 [  780/ 1682]\n",
      "loss: 193.914307 [  790/ 1682]\n",
      "loss: 221.801636 [  800/ 1682]\n",
      "loss: 253.021240 [  810/ 1682]\n",
      "loss: 299.134674 [  820/ 1682]\n",
      "loss: 294.631927 [  830/ 1682]\n",
      "loss: 144.810516 [  840/ 1682]\n",
      "loss: 165.210297 [  850/ 1682]\n",
      "loss: 249.662277 [  860/ 1682]\n",
      "loss: 282.551208 [  870/ 1682]\n",
      "loss: 319.832672 [  880/ 1682]\n",
      "loss: 278.788147 [  890/ 1682]\n",
      "loss: 314.675720 [  900/ 1682]\n",
      "loss: 316.874756 [  910/ 1682]\n",
      "loss: 436.743073 [  920/ 1682]\n",
      "loss: 479.016663 [  930/ 1682]\n",
      "loss: 581.997559 [  940/ 1682]\n",
      "loss: 720.726990 [  950/ 1682]\n",
      "loss: 913.557312 [  960/ 1682]\n",
      "loss: 923.690063 [  970/ 1682]\n",
      "loss: 1202.745972 [  980/ 1682]\n",
      "loss: 1481.003296 [  990/ 1682]\n",
      "loss: 1873.086670 [ 1000/ 1682]\n",
      "loss: 2030.607788 [ 1010/ 1682]\n",
      "loss: 2166.568115 [ 1020/ 1682]\n",
      "loss: 1653.366821 [ 1030/ 1682]\n",
      "loss: 1221.956299 [ 1040/ 1682]\n",
      "loss: 786.757263 [ 1050/ 1682]\n",
      "loss: 1047.211548 [ 1060/ 1682]\n",
      "loss: 1189.400635 [ 1070/ 1682]\n",
      "loss: 1615.788574 [ 1080/ 1682]\n",
      "loss: 1651.069092 [ 1090/ 1682]\n",
      "loss: 2165.418701 [ 1100/ 1682]\n",
      "loss: 2744.145508 [ 1110/ 1682]\n",
      "loss: 3438.963623 [ 1120/ 1682]\n",
      "loss: 3550.911621 [ 1130/ 1682]\n",
      "loss: 4911.729492 [ 1140/ 1682]\n",
      "loss: 6491.055664 [ 1150/ 1682]\n",
      "loss: 8437.369141 [ 1160/ 1682]\n",
      "loss: 5809.170898 [ 1170/ 1682]\n",
      "loss: 5833.068848 [ 1180/ 1682]\n",
      "loss: 7041.141602 [ 1190/ 1682]\n",
      "loss: 6278.145508 [ 1200/ 1682]\n",
      "loss: 6814.793945 [ 1210/ 1682]\n",
      "loss: 6553.885742 [ 1220/ 1682]\n",
      "loss: 7924.779785 [ 1230/ 1682]\n",
      "loss: 9224.660156 [ 1240/ 1682]\n",
      "loss: 9101.031250 [ 1250/ 1682]\n",
      "loss: 10166.913086 [ 1260/ 1682]\n",
      "loss: 10235.424805 [ 1270/ 1682]\n",
      "loss: 8634.228516 [ 1280/ 1682]\n",
      "loss: 7104.654785 [ 1290/ 1682]\n",
      "loss: 7295.522461 [ 1300/ 1682]\n",
      "loss: 8157.885254 [ 1310/ 1682]\n",
      "loss: 9901.237305 [ 1320/ 1682]\n",
      "loss: 9162.449219 [ 1330/ 1682]\n",
      "loss: 8307.907227 [ 1340/ 1682]\n",
      "loss: 8385.509766 [ 1350/ 1682]\n",
      "loss: 9289.216797 [ 1360/ 1682]\n",
      "loss: 10711.311523 [ 1370/ 1682]\n",
      "loss: 11666.783203 [ 1380/ 1682]\n",
      "loss: 12637.896484 [ 1390/ 1682]\n",
      "loss: 12163.363281 [ 1400/ 1682]\n",
      "loss: 12015.357422 [ 1410/ 1682]\n",
      "loss: 13668.914062 [ 1420/ 1682]\n",
      "loss: 12093.619141 [ 1430/ 1682]\n",
      "loss: 11696.560547 [ 1440/ 1682]\n",
      "loss: 13186.375000 [ 1450/ 1682]\n",
      "loss: 13412.687500 [ 1460/ 1682]\n",
      "loss: 15193.912109 [ 1470/ 1682]\n",
      "loss: 18358.820312 [ 1480/ 1682]\n",
      "loss: 19824.734375 [ 1490/ 1682]\n",
      "loss: 20158.871094 [ 1500/ 1682]\n",
      "loss: 17512.693359 [ 1510/ 1682]\n",
      "loss: 19069.300781 [ 1520/ 1682]\n",
      "loss: 17890.933594 [ 1530/ 1682]\n",
      "loss: 15839.343750 [ 1540/ 1682]\n",
      "loss: 16473.179688 [ 1550/ 1682]\n",
      "loss: 19732.703125 [ 1560/ 1682]\n",
      "loss: 17327.792969 [ 1570/ 1682]\n",
      "loss: 15760.562500 [ 1580/ 1682]\n",
      "loss: 12458.435547 [ 1590/ 1682]\n",
      "loss: 12548.962891 [ 1600/ 1682]\n",
      "loss: 10741.246094 [ 1610/ 1682]\n",
      "loss: 9793.429688 [ 1620/ 1682]\n",
      "loss: 12150.843750 [ 1630/ 1682]\n",
      "loss: 15146.700195 [ 1640/ 1682]\n",
      "loss: 18131.166016 [ 1650/ 1682]\n",
      "loss: 16424.878906 [ 1660/ 1682]\n",
      "loss: 13994.903320 [ 1670/ 1682]\n",
      "loss: 14052.171875 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 16503.527006 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 133.254196 [    0/ 1682]\n",
      "loss: 432.593414 [   10/ 1682]\n",
      "loss: 837.401672 [   20/ 1682]\n",
      "loss: 107.151794 [   30/ 1682]\n",
      "loss: 87.484863 [   40/ 1682]\n",
      "loss: 69.954880 [   50/ 1682]\n",
      "loss: 78.949707 [   60/ 1682]\n",
      "loss: 91.177032 [   70/ 1682]\n",
      "loss: 138.856461 [   80/ 1682]\n",
      "loss: 453.237732 [   90/ 1682]\n",
      "loss: 110.082176 [  100/ 1682]\n",
      "loss: 121.064964 [  110/ 1682]\n",
      "loss: 118.927933 [  120/ 1682]\n",
      "loss: 578.047485 [  130/ 1682]\n",
      "loss: 72.567596 [  140/ 1682]\n",
      "loss: 57.819023 [  150/ 1682]\n",
      "loss: 354.463409 [  160/ 1682]\n",
      "loss: 57.450123 [  170/ 1682]\n",
      "loss: 43.673756 [  180/ 1682]\n",
      "loss: 411.343994 [  190/ 1682]\n",
      "loss: 308.100769 [  200/ 1682]\n",
      "loss: 310.944977 [  210/ 1682]\n",
      "loss: 299.508728 [  220/ 1682]\n",
      "loss: 44.110058 [  230/ 1682]\n",
      "loss: 378.324188 [  240/ 1682]\n",
      "loss: 31.484989 [  250/ 1682]\n",
      "loss: 239.914917 [  260/ 1682]\n",
      "loss: 278.653809 [  270/ 1682]\n",
      "loss: 1.260309 [  280/ 1682]\n",
      "loss: 6.566503 [  290/ 1682]\n",
      "loss: 0.240087 [  300/ 1682]\n",
      "loss: 0.152916 [  310/ 1682]\n",
      "loss: 0.353158 [  320/ 1682]\n",
      "loss: 112.763748 [  330/ 1682]\n",
      "loss: 9.769793 [  340/ 1682]\n",
      "loss: 104.707176 [  350/ 1682]\n",
      "loss: 1.403035 [  360/ 1682]\n",
      "loss: 4.024012 [  370/ 1682]\n",
      "loss: 8.012053 [  380/ 1682]\n",
      "loss: 11.731398 [  390/ 1682]\n",
      "loss: 73.419365 [  400/ 1682]\n",
      "loss: 27.123775 [  410/ 1682]\n",
      "loss: 17.359320 [  420/ 1682]\n",
      "loss: 157.053589 [  430/ 1682]\n",
      "loss: 14.622027 [  440/ 1682]\n",
      "loss: 30.875988 [  450/ 1682]\n",
      "loss: 64.112106 [  460/ 1682]\n",
      "loss: 52.498981 [  470/ 1682]\n",
      "loss: 59.366161 [  480/ 1682]\n",
      "loss: 59.359058 [  490/ 1682]\n",
      "loss: 75.731529 [  500/ 1682]\n",
      "loss: 209.390549 [  510/ 1682]\n",
      "loss: 33.398201 [  520/ 1682]\n",
      "loss: 72.744698 [  530/ 1682]\n",
      "loss: 84.776123 [  540/ 1682]\n",
      "loss: 183.297974 [  550/ 1682]\n",
      "loss: 65.799225 [  560/ 1682]\n",
      "loss: 147.328094 [  570/ 1682]\n",
      "loss: 119.730202 [  580/ 1682]\n",
      "loss: 129.596298 [  590/ 1682]\n",
      "loss: 158.695038 [  600/ 1682]\n",
      "loss: 132.328949 [  610/ 1682]\n",
      "loss: 195.701828 [  620/ 1682]\n",
      "loss: 195.405060 [  630/ 1682]\n",
      "loss: 230.581909 [  640/ 1682]\n",
      "loss: 323.836090 [  650/ 1682]\n",
      "loss: 408.043152 [  660/ 1682]\n",
      "loss: 391.481720 [  670/ 1682]\n",
      "loss: 434.145935 [  680/ 1682]\n",
      "loss: 383.997559 [  690/ 1682]\n",
      "loss: 328.998596 [  700/ 1682]\n",
      "loss: 198.195587 [  710/ 1682]\n",
      "loss: 94.633545 [  720/ 1682]\n",
      "loss: 159.133606 [  730/ 1682]\n",
      "loss: 129.566040 [  740/ 1682]\n",
      "loss: 11.692116 [  750/ 1682]\n",
      "loss: 23.263100 [  760/ 1682]\n",
      "loss: 68.277283 [  770/ 1682]\n",
      "loss: 78.682373 [  780/ 1682]\n",
      "loss: 104.680984 [  790/ 1682]\n",
      "loss: 163.091187 [  800/ 1682]\n",
      "loss: 218.062866 [  810/ 1682]\n",
      "loss: 264.369781 [  820/ 1682]\n",
      "loss: 243.359222 [  830/ 1682]\n",
      "loss: 130.559784 [  840/ 1682]\n",
      "loss: 149.505188 [  850/ 1682]\n",
      "loss: 218.020844 [  860/ 1682]\n",
      "loss: 242.329544 [  870/ 1682]\n",
      "loss: 283.845764 [  880/ 1682]\n",
      "loss: 262.609375 [  890/ 1682]\n",
      "loss: 277.749329 [  900/ 1682]\n",
      "loss: 260.706573 [  910/ 1682]\n",
      "loss: 417.980164 [  920/ 1682]\n",
      "loss: 386.331726 [  930/ 1682]\n",
      "loss: 559.709656 [  940/ 1682]\n",
      "loss: 732.020203 [  950/ 1682]\n",
      "loss: 944.782898 [  960/ 1682]\n",
      "loss: 969.075195 [  970/ 1682]\n",
      "loss: 1140.169678 [  980/ 1682]\n",
      "loss: 1402.370239 [  990/ 1682]\n",
      "loss: 1784.508545 [ 1000/ 1682]\n",
      "loss: 1938.305054 [ 1010/ 1682]\n",
      "loss: 1500.689697 [ 1020/ 1682]\n",
      "loss: 1519.779053 [ 1030/ 1682]\n",
      "loss: 1151.262085 [ 1040/ 1682]\n",
      "loss: 729.965942 [ 1050/ 1682]\n",
      "loss: 981.737305 [ 1060/ 1682]\n",
      "loss: 1208.814209 [ 1070/ 1682]\n",
      "loss: 1668.266968 [ 1080/ 1682]\n",
      "loss: 1749.039307 [ 1090/ 1682]\n",
      "loss: 2114.666016 [ 1100/ 1682]\n",
      "loss: 2799.447266 [ 1110/ 1682]\n",
      "loss: 3318.692871 [ 1120/ 1682]\n",
      "loss: 3730.697266 [ 1130/ 1682]\n",
      "loss: 4768.489258 [ 1140/ 1682]\n",
      "loss: 6420.447754 [ 1150/ 1682]\n",
      "loss: 8248.375000 [ 1160/ 1682]\n",
      "loss: 5574.939941 [ 1170/ 1682]\n",
      "loss: 6064.201660 [ 1180/ 1682]\n",
      "loss: 6868.548340 [ 1190/ 1682]\n",
      "loss: 6203.335938 [ 1200/ 1682]\n",
      "loss: 6321.232910 [ 1210/ 1682]\n",
      "loss: 6824.276367 [ 1220/ 1682]\n",
      "loss: 7324.768555 [ 1230/ 1682]\n",
      "loss: 9141.120117 [ 1240/ 1682]\n",
      "loss: 8904.532227 [ 1250/ 1682]\n",
      "loss: 9620.524414 [ 1260/ 1682]\n",
      "loss: 10026.976562 [ 1270/ 1682]\n",
      "loss: 7838.965820 [ 1280/ 1682]\n",
      "loss: 6803.672852 [ 1290/ 1682]\n",
      "loss: 7518.381348 [ 1300/ 1682]\n",
      "loss: 8554.079102 [ 1310/ 1682]\n",
      "loss: 9038.552734 [ 1320/ 1682]\n",
      "loss: 8360.655273 [ 1330/ 1682]\n",
      "loss: 7733.438965 [ 1340/ 1682]\n",
      "loss: 8197.101562 [ 1350/ 1682]\n",
      "loss: 9090.932617 [ 1360/ 1682]\n",
      "loss: 10624.200195 [ 1370/ 1682]\n",
      "loss: 11656.338867 [ 1380/ 1682]\n",
      "loss: 12253.865234 [ 1390/ 1682]\n",
      "loss: 11956.322266 [ 1400/ 1682]\n",
      "loss: 12355.164062 [ 1410/ 1682]\n",
      "loss: 13428.340820 [ 1420/ 1682]\n",
      "loss: 11867.354492 [ 1430/ 1682]\n",
      "loss: 11474.059570 [ 1440/ 1682]\n",
      "loss: 12950.066406 [ 1450/ 1682]\n",
      "loss: 13174.354492 [ 1460/ 1682]\n",
      "loss: 14056.856445 [ 1470/ 1682]\n",
      "loss: 15351.026367 [ 1480/ 1682]\n",
      "loss: 17801.490234 [ 1490/ 1682]\n",
      "loss: 19120.000000 [ 1500/ 1682]\n",
      "loss: 16328.598633 [ 1510/ 1682]\n",
      "loss: 18785.509766 [ 1520/ 1682]\n",
      "loss: 17616.146484 [ 1530/ 1682]\n",
      "loss: 15111.432617 [ 1540/ 1682]\n",
      "loss: 16209.981445 [ 1550/ 1682]\n",
      "loss: 19635.275391 [ 1560/ 1682]\n",
      "loss: 16018.763672 [ 1570/ 1682]\n",
      "loss: 15503.049805 [ 1580/ 1682]\n",
      "loss: 12229.869141 [ 1590/ 1682]\n",
      "loss: 10676.508789 [ 1600/ 1682]\n",
      "loss: 10529.352539 [ 1610/ 1682]\n",
      "loss: 10208.708984 [ 1620/ 1682]\n",
      "loss: 12560.505859 [ 1630/ 1682]\n",
      "loss: 15066.104492 [ 1640/ 1682]\n",
      "loss: 18037.644531 [ 1650/ 1682]\n",
      "loss: 17197.777344 [ 1660/ 1682]\n",
      "loss: 14701.697266 [ 1670/ 1682]\n",
      "loss: 13809.464844 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 16091.966046 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 135.134537 [    0/ 1682]\n",
      "loss: 503.517395 [   10/ 1682]\n",
      "loss: 147.869339 [   20/ 1682]\n",
      "loss: 152.550415 [   30/ 1682]\n",
      "loss: 684.372803 [   40/ 1682]\n",
      "loss: 88.198891 [   50/ 1682]\n",
      "loss: 512.155945 [   60/ 1682]\n",
      "loss: 728.722534 [   70/ 1682]\n",
      "loss: 164.130661 [   80/ 1682]\n",
      "loss: 815.162537 [   90/ 1682]\n",
      "loss: 132.694153 [  100/ 1682]\n",
      "loss: 144.707733 [  110/ 1682]\n",
      "loss: 142.367233 [  120/ 1682]\n",
      "loss: 128.487045 [  130/ 1682]\n",
      "loss: 91.081467 [  140/ 1682]\n",
      "loss: 74.486801 [  150/ 1682]\n",
      "loss: 89.129227 [  160/ 1682]\n",
      "loss: 1122.185913 [  170/ 1682]\n",
      "loss: 58.289818 [  180/ 1682]\n",
      "loss: 48.311604 [  190/ 1682]\n",
      "loss: 58.910156 [  200/ 1682]\n",
      "loss: 97.951004 [  210/ 1682]\n",
      "loss: 339.709534 [  220/ 1682]\n",
      "loss: 58.719879 [  230/ 1682]\n",
      "loss: 45.472301 [  240/ 1682]\n",
      "loss: 60.501026 [  250/ 1682]\n",
      "loss: 33.393505 [  260/ 1682]\n",
      "loss: 10.710496 [  270/ 1682]\n",
      "loss: 438.397797 [  280/ 1682]\n",
      "loss: 2.444059 [  290/ 1682]\n",
      "loss: 340.257263 [  300/ 1682]\n",
      "loss: 334.439575 [  310/ 1682]\n",
      "loss: 0.601244 [  320/ 1682]\n",
      "loss: 185.573090 [  330/ 1682]\n",
      "loss: 4.484889 [  340/ 1682]\n",
      "loss: 3.392250 [  350/ 1682]\n",
      "loss: 367.441956 [  360/ 1682]\n",
      "loss: 0.293463 [  370/ 1682]\n",
      "loss: 2.248408 [  380/ 1682]\n",
      "loss: 6.040488 [  390/ 1682]\n",
      "loss: 311.717560 [  400/ 1682]\n",
      "loss: 252.986237 [  410/ 1682]\n",
      "loss: 10.036322 [  420/ 1682]\n",
      "loss: 5.244765 [  430/ 1682]\n",
      "loss: 383.321228 [  440/ 1682]\n",
      "loss: 438.878601 [  450/ 1682]\n",
      "loss: 243.425507 [  460/ 1682]\n",
      "loss: 44.715481 [  470/ 1682]\n",
      "loss: 39.286629 [  480/ 1682]\n",
      "loss: 44.716698 [  490/ 1682]\n",
      "loss: 59.057789 [  500/ 1682]\n",
      "loss: 203.219437 [  510/ 1682]\n",
      "loss: 23.052353 [  520/ 1682]\n",
      "loss: 49.566948 [  530/ 1682]\n",
      "loss: 174.521713 [  540/ 1682]\n",
      "loss: 38.627235 [  550/ 1682]\n",
      "loss: 46.454277 [  560/ 1682]\n",
      "loss: 162.633774 [  570/ 1682]\n",
      "loss: 98.637741 [  580/ 1682]\n",
      "loss: 107.494705 [  590/ 1682]\n",
      "loss: 167.739014 [  600/ 1682]\n",
      "loss: 110.039635 [  610/ 1682]\n",
      "loss: 176.916092 [  620/ 1682]\n",
      "loss: 136.281372 [  630/ 1682]\n",
      "loss: 201.007233 [  640/ 1682]\n",
      "loss: 288.383209 [  650/ 1682]\n",
      "loss: 401.435211 [  660/ 1682]\n",
      "loss: 352.400330 [  670/ 1682]\n",
      "loss: 412.437073 [  680/ 1682]\n",
      "loss: 325.131287 [  690/ 1682]\n",
      "loss: 316.723083 [  700/ 1682]\n",
      "loss: 265.611542 [  710/ 1682]\n",
      "loss: 76.081909 [  720/ 1682]\n",
      "loss: 42.508324 [  730/ 1682]\n",
      "loss: 99.347023 [  740/ 1682]\n",
      "loss: 5.914302 [  750/ 1682]\n",
      "loss: 15.879666 [  760/ 1682]\n",
      "loss: 52.590546 [  770/ 1682]\n",
      "loss: 61.786324 [  780/ 1682]\n",
      "loss: 160.279373 [  790/ 1682]\n",
      "loss: 138.317337 [  800/ 1682]\n",
      "loss: 169.130737 [  810/ 1682]\n",
      "loss: 232.535767 [  820/ 1682]\n",
      "loss: 278.058655 [  830/ 1682]\n",
      "loss: 108.649979 [  840/ 1682]\n",
      "loss: 126.190140 [  850/ 1682]\n",
      "loss: 189.261383 [  860/ 1682]\n",
      "loss: 283.914124 [  870/ 1682]\n",
      "loss: 250.867477 [  880/ 1682]\n",
      "loss: 278.488831 [  890/ 1682]\n",
      "loss: 256.683289 [  900/ 1682]\n",
      "loss: 275.352264 [  910/ 1682]\n",
      "loss: 377.796051 [  920/ 1682]\n",
      "loss: 361.804138 [  930/ 1682]\n",
      "loss: 513.105469 [  940/ 1682]\n",
      "loss: 678.579468 [  950/ 1682]\n",
      "loss: 845.016724 [  960/ 1682]\n",
      "loss: 907.408081 [  970/ 1682]\n",
      "loss: 1065.302368 [  980/ 1682]\n",
      "loss: 1204.458252 [  990/ 1682]\n",
      "loss: 1359.971924 [ 1000/ 1682]\n",
      "loss: 1850.763916 [ 1010/ 1682]\n",
      "loss: 1980.639648 [ 1020/ 1682]\n",
      "loss: 1492.394287 [ 1030/ 1682]\n",
      "loss: 1084.708374 [ 1040/ 1682]\n",
      "loss: 676.901794 [ 1050/ 1682]\n",
      "loss: 920.243469 [ 1060/ 1682]\n",
      "loss: 1184.065796 [ 1070/ 1682]\n",
      "loss: 1464.385376 [ 1080/ 1682]\n",
      "loss: 1856.011963 [ 1090/ 1682]\n",
      "loss: 2002.472900 [ 1100/ 1682]\n",
      "loss: 2288.253662 [ 1110/ 1682]\n",
      "loss: 3204.062012 [ 1120/ 1682]\n",
      "loss: 3609.068359 [ 1130/ 1682]\n",
      "loss: 4320.075684 [ 1140/ 1682]\n",
      "loss: 5888.328125 [ 1150/ 1682]\n",
      "loss: 7674.587402 [ 1160/ 1682]\n",
      "loss: 5037.163086 [ 1170/ 1682]\n",
      "loss: 5909.087402 [ 1180/ 1682]\n",
      "loss: 6703.465820 [ 1190/ 1682]\n",
      "loss: 6046.534668 [ 1200/ 1682]\n",
      "loss: 6466.621582 [ 1210/ 1682]\n",
      "loss: 5740.375977 [ 1220/ 1682]\n",
      "loss: 7566.213379 [ 1230/ 1682]\n",
      "loss: 8950.682617 [ 1240/ 1682]\n",
      "loss: 8716.556641 [ 1250/ 1682]\n",
      "loss: 8552.457031 [ 1260/ 1682]\n",
      "loss: 9827.511719 [ 1270/ 1682]\n",
      "loss: 8260.224609 [ 1280/ 1682]\n",
      "loss: 7281.854492 [ 1290/ 1682]\n",
      "loss: 7345.891602 [ 1300/ 1682]\n",
      "loss: 7679.704590 [ 1310/ 1682]\n",
      "loss: 9500.297852 [ 1320/ 1682]\n",
      "loss: 8162.249023 [ 1330/ 1682]\n",
      "loss: 7432.666992 [ 1340/ 1682]\n",
      "loss: 8017.002441 [ 1350/ 1682]\n",
      "loss: 8901.270508 [ 1360/ 1682]\n",
      "loss: 10419.175781 [ 1370/ 1682]\n",
      "loss: 12013.329102 [ 1380/ 1682]\n",
      "loss: 12184.722656 [ 1390/ 1682]\n",
      "loss: 12427.675781 [ 1400/ 1682]\n",
      "loss: 12966.267578 [ 1410/ 1682]\n",
      "loss: 12476.093750 [ 1420/ 1682]\n",
      "loss: 11650.467773 [ 1430/ 1682]\n",
      "loss: 11260.797852 [ 1440/ 1682]\n",
      "loss: 12723.419922 [ 1450/ 1682]\n",
      "loss: 11365.311523 [ 1460/ 1682]\n",
      "loss: 13782.560547 [ 1470/ 1682]\n",
      "loss: 17812.355469 [ 1480/ 1682]\n",
      "loss: 18092.679688 [ 1490/ 1682]\n",
      "loss: 18638.082031 [ 1500/ 1682]\n",
      "loss: 16979.218750 [ 1510/ 1682]\n",
      "loss: 18512.380859 [ 1520/ 1682]\n",
      "loss: 17351.623047 [ 1530/ 1682]\n",
      "loss: 16021.718750 [ 1540/ 1682]\n",
      "loss: 14965.637695 [ 1550/ 1682]\n",
      "loss: 19355.722656 [ 1560/ 1682]\n",
      "loss: 15817.181641 [ 1570/ 1682]\n",
      "loss: 15254.757812 [ 1580/ 1682]\n",
      "loss: 12009.631836 [ 1590/ 1682]\n",
      "loss: 12098.237305 [ 1600/ 1682]\n",
      "loss: 9474.100586 [ 1610/ 1682]\n",
      "loss: 10696.412109 [ 1620/ 1682]\n",
      "loss: 12501.412109 [ 1630/ 1682]\n",
      "loss: 14821.244141 [ 1640/ 1682]\n",
      "loss: 16578.347656 [ 1650/ 1682]\n",
      "loss: 16722.333984 [ 1660/ 1682]\n",
      "loss: 14459.706055 [ 1670/ 1682]\n",
      "loss: 13574.921875 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 15850.083120 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 446.791656 [    0/ 1682]\n",
      "loss: 610.976135 [   10/ 1682]\n",
      "loss: 173.227921 [   20/ 1682]\n",
      "loss: 153.244308 [   30/ 1682]\n",
      "loss: 129.559662 [   40/ 1682]\n",
      "loss: 108.006447 [   50/ 1682]\n",
      "loss: 121.581894 [   60/ 1682]\n",
      "loss: 133.757248 [   70/ 1682]\n",
      "loss: 504.864838 [   80/ 1682]\n",
      "loss: 693.661316 [   90/ 1682]\n",
      "loss: 654.740051 [  100/ 1682]\n",
      "loss: 685.749207 [  110/ 1682]\n",
      "loss: 167.263458 [  120/ 1682]\n",
      "loss: 152.846939 [  130/ 1682]\n",
      "loss: 413.650635 [  140/ 1682]\n",
      "loss: 454.051697 [  150/ 1682]\n",
      "loss: 109.015427 [  160/ 1682]\n",
      "loss: 91.997543 [  170/ 1682]\n",
      "loss: 74.570976 [  180/ 1682]\n",
      "loss: 63.192238 [  190/ 1682]\n",
      "loss: 75.191452 [  200/ 1682]\n",
      "loss: 96.925690 [  210/ 1682]\n",
      "loss: 87.391373 [  220/ 1682]\n",
      "loss: 74.999176 [  230/ 1682]\n",
      "loss: 59.950237 [  240/ 1682]\n",
      "loss: 76.607277 [  250/ 1682]\n",
      "loss: 366.619202 [  260/ 1682]\n",
      "loss: 18.182482 [  270/ 1682]\n",
      "loss: 9.453345 [  280/ 1682]\n",
      "loss: 6.942431 [  290/ 1682]\n",
      "loss: 4.378610 [  300/ 1682]\n",
      "loss: 253.085236 [  310/ 1682]\n",
      "loss: 272.691559 [  320/ 1682]\n",
      "loss: 295.313904 [  330/ 1682]\n",
      "loss: 173.649811 [  340/ 1682]\n",
      "loss: 1.056744 [  350/ 1682]\n",
      "loss: 1.076940 [  360/ 1682]\n",
      "loss: 1.754414 [  370/ 1682]\n",
      "loss: 0.310745 [  380/ 1682]\n",
      "loss: 2.503489 [  390/ 1682]\n",
      "loss: 395.334534 [  400/ 1682]\n",
      "loss: 154.017593 [  410/ 1682]\n",
      "loss: 101.792046 [  420/ 1682]\n",
      "loss: 1.707289 [  430/ 1682]\n",
      "loss: 3.380112 [  440/ 1682]\n",
      "loss: 13.172325 [  450/ 1682]\n",
      "loss: 193.880814 [  460/ 1682]\n",
      "loss: 210.839874 [  470/ 1682]\n",
      "loss: 32.389652 [  480/ 1682]\n",
      "loss: 88.079750 [  490/ 1682]\n",
      "loss: 224.286057 [  500/ 1682]\n",
      "loss: 26.167135 [  510/ 1682]\n",
      "loss: 17.148958 [  520/ 1682]\n",
      "loss: 134.176041 [  530/ 1682]\n",
      "loss: 51.800240 [  540/ 1682]\n",
      "loss: 27.308691 [  550/ 1682]\n",
      "loss: 37.274204 [  560/ 1682]\n",
      "loss: 24.092506 [  570/ 1682]\n",
      "loss: 80.796234 [  580/ 1682]\n",
      "loss: 250.163208 [  590/ 1682]\n",
      "loss: 112.081177 [  600/ 1682]\n",
      "loss: 251.677643 [  610/ 1682]\n",
      "loss: 91.919472 [  620/ 1682]\n",
      "loss: 114.016724 [  630/ 1682]\n",
      "loss: 164.003632 [  640/ 1682]\n",
      "loss: 255.566330 [  650/ 1682]\n",
      "loss: 330.941406 [  660/ 1682]\n",
      "loss: 327.089050 [  670/ 1682]\n",
      "loss: 352.886292 [  680/ 1682]\n",
      "loss: 364.997925 [  690/ 1682]\n",
      "loss: 282.338013 [  700/ 1682]\n",
      "loss: 161.675003 [  710/ 1682]\n",
      "loss: 170.505554 [  720/ 1682]\n",
      "loss: 30.651346 [  730/ 1682]\n",
      "loss: 4.687245 [  740/ 1682]\n",
      "loss: 2.229733 [  750/ 1682]\n",
      "loss: 9.410655 [  760/ 1682]\n",
      "loss: 39.157063 [  770/ 1682]\n",
      "loss: 47.162392 [  780/ 1682]\n",
      "loss: 117.637428 [  790/ 1682]\n",
      "loss: 115.940811 [  800/ 1682]\n",
      "loss: 150.026871 [  810/ 1682]\n",
      "loss: 181.539520 [  820/ 1682]\n",
      "loss: 195.635742 [  830/ 1682]\n",
      "loss: 89.079002 [  840/ 1682]\n",
      "loss: 135.540680 [  850/ 1682]\n",
      "loss: 162.941086 [  860/ 1682]\n",
      "loss: 189.183563 [  870/ 1682]\n",
      "loss: 220.385712 [  880/ 1682]\n",
      "loss: 201.971146 [  890/ 1682]\n",
      "loss: 233.160919 [  900/ 1682]\n",
      "loss: 263.268982 [  910/ 1682]\n",
      "loss: 349.636719 [  920/ 1682]\n",
      "loss: 355.376617 [  930/ 1682]\n",
      "loss: 424.745453 [  940/ 1682]\n",
      "loss: 627.866760 [  950/ 1682]\n",
      "loss: 750.317566 [  960/ 1682]\n",
      "loss: 848.574402 [  970/ 1682]\n",
      "loss: 1001.517700 [  980/ 1682]\n",
      "loss: 1256.739014 [  990/ 1682]\n",
      "loss: 1619.721680 [ 1000/ 1682]\n",
      "loss: 1766.362671 [ 1010/ 1682]\n",
      "loss: 1706.397705 [ 1020/ 1682]\n",
      "loss: 1370.256714 [ 1030/ 1682]\n",
      "loss: 831.651367 [ 1040/ 1682]\n",
      "loss: 567.966187 [ 1050/ 1682]\n",
      "loss: 732.806396 [ 1060/ 1682]\n",
      "loss: 1116.846680 [ 1070/ 1682]\n",
      "loss: 1509.404541 [ 1080/ 1682]\n",
      "loss: 1771.594727 [ 1090/ 1682]\n",
      "loss: 2111.279297 [ 1100/ 1682]\n",
      "loss: 2662.783691 [ 1110/ 1682]\n",
      "loss: 3092.950439 [ 1120/ 1682]\n",
      "loss: 3491.035645 [ 1130/ 1682]\n",
      "loss: 4498.552246 [ 1140/ 1682]\n",
      "loss: 5774.915039 [ 1150/ 1682]\n",
      "loss: 7374.833008 [ 1160/ 1682]\n",
      "loss: 5698.437500 [ 1170/ 1682]\n",
      "loss: 5757.815430 [ 1180/ 1682]\n",
      "loss: 6542.286133 [ 1190/ 1682]\n",
      "loss: 5487.154785 [ 1200/ 1682]\n",
      "loss: 6187.600586 [ 1210/ 1682]\n",
      "loss: 6084.015137 [ 1220/ 1682]\n",
      "loss: 7394.781250 [ 1230/ 1682]\n",
      "loss: 8764.197266 [ 1240/ 1682]\n",
      "loss: 8532.465820 [ 1250/ 1682]\n",
      "loss: 9142.689453 [ 1260/ 1682]\n",
      "loss: 9198.126953 [ 1270/ 1682]\n",
      "loss: 8081.129883 [ 1280/ 1682]\n",
      "loss: 6338.570312 [ 1290/ 1682]\n",
      "loss: 7176.960938 [ 1300/ 1682]\n",
      "loss: 7176.188965 [ 1310/ 1682]\n",
      "loss: 8752.302734 [ 1320/ 1682]\n",
      "loss: 8061.352539 [ 1330/ 1682]\n",
      "loss: 7382.324219 [ 1340/ 1682]\n",
      "loss: 6733.911621 [ 1350/ 1682]\n",
      "loss: 8327.599609 [ 1360/ 1682]\n",
      "loss: 10218.075195 [ 1370/ 1682]\n",
      "loss: 11797.238281 [ 1380/ 1682]\n",
      "loss: 11967.115234 [ 1390/ 1682]\n",
      "loss: 11361.339844 [ 1400/ 1682]\n",
      "loss: 12741.869141 [ 1410/ 1682]\n",
      "loss: 12971.330078 [ 1420/ 1682]\n",
      "loss: 11437.875977 [ 1430/ 1682]\n",
      "loss: 11051.820312 [ 1440/ 1682]\n",
      "loss: 12501.238281 [ 1450/ 1682]\n",
      "loss: 12721.620117 [ 1460/ 1682]\n",
      "loss: 12698.192383 [ 1470/ 1682]\n",
      "loss: 17549.527344 [ 1480/ 1682]\n",
      "loss: 17859.464844 [ 1490/ 1682]\n",
      "loss: 19310.003906 [ 1500/ 1682]\n",
      "loss: 16722.621094 [ 1510/ 1682]\n",
      "loss: 18244.333984 [ 1520/ 1682]\n",
      "loss: 16062.947266 [ 1530/ 1682]\n",
      "loss: 14747.395508 [ 1540/ 1682]\n",
      "loss: 15707.919922 [ 1550/ 1682]\n",
      "loss: 18073.691406 [ 1560/ 1682]\n",
      "loss: 16723.447266 [ 1570/ 1682]\n",
      "loss: 15011.500000 [ 1580/ 1682]\n",
      "loss: 10972.568359 [ 1590/ 1682]\n",
      "loss: 11881.764648 [ 1600/ 1682]\n",
      "loss: 10125.541992 [ 1610/ 1682]\n",
      "loss: 10492.870117 [ 1620/ 1682]\n",
      "loss: 12108.676758 [ 1630/ 1682]\n",
      "loss: 14581.570312 [ 1640/ 1682]\n",
      "loss: 16231.049805 [ 1650/ 1682]\n",
      "loss: 15821.825195 [ 1660/ 1682]\n",
      "loss: 14033.396484 [ 1670/ 1682]\n",
      "loss: 13345.492188 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 15445.818510 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 454.316315 [    0/ 1682]\n",
      "loss: 197.223663 [   10/ 1682]\n",
      "loss: 1132.383423 [   20/ 1682]\n",
      "loss: 1024.710205 [   30/ 1682]\n",
      "loss: 153.022049 [   40/ 1682]\n",
      "loss: 129.503525 [   50/ 1682]\n",
      "loss: 118.606857 [   60/ 1682]\n",
      "loss: 189.131714 [   70/ 1682]\n",
      "loss: 751.941284 [   80/ 1682]\n",
      "loss: 565.044617 [   90/ 1682]\n",
      "loss: 1148.483765 [  100/ 1682]\n",
      "loss: 225.559967 [  110/ 1682]\n",
      "loss: 193.721146 [  120/ 1682]\n",
      "loss: 177.455597 [  130/ 1682]\n",
      "loss: 158.534821 [  140/ 1682]\n",
      "loss: 365.795715 [  150/ 1682]\n",
      "loss: 372.308044 [  160/ 1682]\n",
      "loss: 594.765930 [  170/ 1682]\n",
      "loss: 92.543503 [  180/ 1682]\n",
      "loss: 451.125885 [  190/ 1682]\n",
      "loss: 93.153641 [  200/ 1682]\n",
      "loss: 117.242271 [  210/ 1682]\n",
      "loss: 106.718056 [  220/ 1682]\n",
      "loss: 92.943497 [  230/ 1682]\n",
      "loss: 76.119995 [  240/ 1682]\n",
      "loss: 74.287010 [  250/ 1682]\n",
      "loss: 260.650818 [  260/ 1682]\n",
      "loss: 240.622238 [  270/ 1682]\n",
      "loss: 231.710007 [  280/ 1682]\n",
      "loss: 220.483521 [  290/ 1682]\n",
      "loss: 20.266411 [  300/ 1682]\n",
      "loss: 6.834763 [  310/ 1682]\n",
      "loss: 262.359589 [  320/ 1682]\n",
      "loss: 1.175412 [  330/ 1682]\n",
      "loss: 72.517242 [  340/ 1682]\n",
      "loss: 0.706861 [  350/ 1682]\n",
      "loss: 3.860274 [  360/ 1682]\n",
      "loss: 285.822418 [  370/ 1682]\n",
      "loss: 211.816895 [  380/ 1682]\n",
      "loss: 0.974945 [  390/ 1682]\n",
      "loss: 243.289917 [  400/ 1682]\n",
      "loss: 4.968000 [  410/ 1682]\n",
      "loss: 1.805203 [  420/ 1682]\n",
      "loss: 0.176271 [  430/ 1682]\n",
      "loss: 0.870650 [  440/ 1682]\n",
      "loss: 7.533616 [  450/ 1682]\n",
      "loss: 25.187897 [  460/ 1682]\n",
      "loss: 22.252745 [  470/ 1682]\n",
      "loss: 22.253641 [  480/ 1682]\n",
      "loss: 22.253744 [  490/ 1682]\n",
      "loss: 187.711060 [  500/ 1682]\n",
      "loss: 17.399570 [  510/ 1682]\n",
      "loss: 90.921204 [  520/ 1682]\n",
      "loss: 30.881886 [  530/ 1682]\n",
      "loss: 38.732262 [  540/ 1682]\n",
      "loss: 18.151981 [  550/ 1682]\n",
      "loss: 26.358692 [  560/ 1682]\n",
      "loss: 15.252995 [  570/ 1682]\n",
      "loss: 228.897781 [  580/ 1682]\n",
      "loss: 70.441521 [  590/ 1682]\n",
      "loss: 92.376373 [  600/ 1682]\n",
      "loss: 65.583847 [  610/ 1682]\n",
      "loss: 74.171890 [  620/ 1682]\n",
      "loss: 94.119919 [  630/ 1682]\n",
      "loss: 149.377075 [  640/ 1682]\n",
      "loss: 225.312866 [  650/ 1682]\n",
      "loss: 338.663513 [  660/ 1682]\n",
      "loss: 282.242249 [  670/ 1682]\n",
      "loss: 317.171387 [  680/ 1682]\n",
      "loss: 275.999023 [  690/ 1682]\n",
      "loss: 307.488464 [  700/ 1682]\n",
      "loss: 138.084686 [  710/ 1682]\n",
      "loss: 45.827671 [  720/ 1682]\n",
      "loss: 20.943596 [  730/ 1682]\n",
      "loss: 2.018572 [  740/ 1682]\n",
      "loss: 0.540903 [  750/ 1682]\n",
      "loss: 4.987729 [  760/ 1682]\n",
      "loss: 25.629826 [  770/ 1682]\n",
      "loss: 125.584984 [  780/ 1682]\n",
      "loss: 52.983875 [  790/ 1682]\n",
      "loss: 240.319870 [  800/ 1682]\n",
      "loss: 180.226639 [  810/ 1682]\n",
      "loss: 176.333557 [  820/ 1682]\n",
      "loss: 160.117630 [  830/ 1682]\n",
      "loss: 71.788193 [  840/ 1682]\n",
      "loss: 173.496094 [  850/ 1682]\n",
      "loss: 172.070618 [  860/ 1682]\n",
      "loss: 163.319168 [  870/ 1682]\n",
      "loss: 176.036163 [  880/ 1682]\n",
      "loss: 175.341507 [  890/ 1682]\n",
      "loss: 204.393341 [  900/ 1682]\n",
      "loss: 199.472504 [  910/ 1682]\n",
      "loss: 305.182556 [  920/ 1682]\n",
      "loss: 319.608246 [  930/ 1682]\n",
      "loss: 385.454712 [  940/ 1682]\n",
      "loss: 446.893616 [  950/ 1682]\n",
      "loss: 770.806519 [  960/ 1682]\n",
      "loss: 628.021362 [  970/ 1682]\n",
      "loss: 940.853333 [  980/ 1682]\n",
      "loss: 1192.064575 [  990/ 1682]\n",
      "loss: 1542.343018 [ 1000/ 1682]\n",
      "loss: 1522.491699 [ 1010/ 1682]\n",
      "loss: 1637.552368 [ 1020/ 1682]\n",
      "loss: 1192.011475 [ 1030/ 1682]\n",
      "loss: 960.449402 [ 1040/ 1682]\n",
      "loss: 579.015137 [ 1050/ 1682]\n",
      "loss: 805.881775 [ 1060/ 1682]\n",
      "loss: 1052.841064 [ 1070/ 1682]\n",
      "loss: 1318.556885 [ 1080/ 1682]\n",
      "loss: 1635.545776 [ 1090/ 1682]\n",
      "loss: 2022.990234 [ 1100/ 1682]\n",
      "loss: 2342.167725 [ 1110/ 1682]\n",
      "loss: 2985.832520 [ 1120/ 1682]\n",
      "loss: 3128.818359 [ 1130/ 1682]\n",
      "loss: 4004.082764 [ 1140/ 1682]\n",
      "loss: 5844.435059 [ 1150/ 1682]\n",
      "loss: 7718.865723 [ 1160/ 1682]\n",
      "loss: 4934.864258 [ 1170/ 1682]\n",
      "loss: 5564.508301 [ 1180/ 1682]\n",
      "loss: 6386.236816 [ 1190/ 1682]\n",
      "loss: 5745.446289 [ 1200/ 1682]\n",
      "loss: 5792.314453 [ 1210/ 1682]\n",
      "loss: 5944.092285 [ 1220/ 1682]\n",
      "loss: 6615.252441 [ 1230/ 1682]\n",
      "loss: 8583.496094 [ 1240/ 1682]\n",
      "loss: 7859.383789 [ 1250/ 1682]\n",
      "loss: 8951.802734 [ 1260/ 1682]\n",
      "loss: 8925.115234 [ 1270/ 1682]\n",
      "loss: 7421.299805 [ 1280/ 1682]\n",
      "loss: 6951.116211 [ 1290/ 1682]\n",
      "loss: 7013.661621 [ 1300/ 1682]\n",
      "loss: 8015.711914 [ 1310/ 1682]\n",
      "loss: 9121.938477 [ 1320/ 1682]\n",
      "loss: 7008.934570 [ 1330/ 1682]\n",
      "loss: 7179.055664 [ 1340/ 1682]\n",
      "loss: 7669.838379 [ 1350/ 1682]\n",
      "loss: 7772.735352 [ 1360/ 1682]\n",
      "loss: 10023.173828 [ 1370/ 1682]\n",
      "loss: 10622.075195 [ 1380/ 1682]\n",
      "loss: 11755.995117 [ 1390/ 1682]\n",
      "loss: 11002.875000 [ 1400/ 1682]\n",
      "loss: 11769.638672 [ 1410/ 1682]\n",
      "loss: 11853.708008 [ 1420/ 1682]\n",
      "loss: 11231.622070 [ 1430/ 1682]\n",
      "loss: 10849.119141 [ 1440/ 1682]\n",
      "loss: 12285.620117 [ 1450/ 1682]\n",
      "loss: 12321.331055 [ 1460/ 1682]\n",
      "loss: 14226.458008 [ 1470/ 1682]\n",
      "loss: 17294.164062 [ 1480/ 1682]\n",
      "loss: 17847.607422 [ 1490/ 1682]\n",
      "loss: 19041.912109 [ 1500/ 1682]\n",
      "loss: 16473.333984 [ 1510/ 1682]\n",
      "loss: 17983.833984 [ 1520/ 1682]\n",
      "loss: 16839.964844 [ 1530/ 1682]\n",
      "loss: 15530.083008 [ 1540/ 1682]\n",
      "loss: 15452.583008 [ 1550/ 1682]\n",
      "loss: 16394.970703 [ 1560/ 1682]\n",
      "loss: 16249.770508 [ 1570/ 1682]\n",
      "loss: 14775.106445 [ 1580/ 1682]\n",
      "loss: 11202.598633 [ 1590/ 1682]\n",
      "loss: 11163.584961 [ 1600/ 1682]\n",
      "loss: 9163.979492 [ 1610/ 1682]\n",
      "loss: 9373.932617 [ 1620/ 1682]\n",
      "loss: 12067.565430 [ 1630/ 1682]\n",
      "loss: 14348.723633 [ 1640/ 1682]\n",
      "loss: 16029.379883 [ 1650/ 1682]\n",
      "loss: 14422.802734 [ 1660/ 1682]\n",
      "loss: 13085.548828 [ 1670/ 1682]\n",
      "loss: 13122.779297 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 15294.978497 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 212.668854 [    0/ 1682]\n",
      "loss: 629.620728 [   10/ 1682]\n",
      "loss: 871.268250 [   20/ 1682]\n",
      "loss: 520.170776 [   30/ 1682]\n",
      "loss: 1061.165771 [   40/ 1682]\n",
      "loss: 185.581787 [   50/ 1682]\n",
      "loss: 171.855515 [   60/ 1682]\n",
      "loss: 182.529495 [   70/ 1682]\n",
      "loss: 248.572876 [   80/ 1682]\n",
      "loss: 221.617096 [   90/ 1682]\n",
      "loss: 739.651794 [  100/ 1682]\n",
      "loss: 224.474091 [  110/ 1682]\n",
      "loss: 221.551361 [  120/ 1682]\n",
      "loss: 614.518188 [  130/ 1682]\n",
      "loss: 156.093277 [  140/ 1682]\n",
      "loss: 134.183060 [  150/ 1682]\n",
      "loss: 153.548798 [  160/ 1682]\n",
      "loss: 535.117126 [  170/ 1682]\n",
      "loss: 653.994995 [  180/ 1682]\n",
      "loss: 97.960464 [  190/ 1682]\n",
      "loss: 318.467102 [  200/ 1682]\n",
      "loss: 139.090179 [  210/ 1682]\n",
      "loss: 127.596756 [  220/ 1682]\n",
      "loss: 591.442810 [  230/ 1682]\n",
      "loss: 364.955475 [  240/ 1682]\n",
      "loss: 118.964188 [  250/ 1682]\n",
      "loss: 75.898468 [  260/ 1682]\n",
      "loss: 299.097473 [  270/ 1682]\n",
      "loss: 25.059040 [  280/ 1682]\n",
      "loss: 20.984560 [  290/ 1682]\n",
      "loss: 31.600037 [  300/ 1682]\n",
      "loss: 281.471100 [  310/ 1682]\n",
      "loss: 316.324585 [  320/ 1682]\n",
      "loss: 3.286940 [  330/ 1682]\n",
      "loss: 0.977483 [  340/ 1682]\n",
      "loss: 2.236133 [  350/ 1682]\n",
      "loss: 8.472208 [  360/ 1682]\n",
      "loss: 10.312033 [  370/ 1682]\n",
      "loss: 9.579996 [  380/ 1682]\n",
      "loss: 1.341879 [  390/ 1682]\n",
      "loss: 0.369221 [  400/ 1682]\n",
      "loss: 9.422525 [  410/ 1682]\n",
      "loss: 0.626766 [  420/ 1682]\n",
      "loss: 0.538992 [  430/ 1682]\n",
      "loss: 0.268782 [  440/ 1682]\n",
      "loss: 3.846974 [  450/ 1682]\n",
      "loss: 16.489298 [  460/ 1682]\n",
      "loss: 516.304321 [  470/ 1682]\n",
      "loss: 129.634247 [  480/ 1682]\n",
      "loss: 14.136848 [  490/ 1682]\n",
      "loss: 22.602539 [  500/ 1682]\n",
      "loss: 10.634151 [  510/ 1682]\n",
      "loss: 168.637909 [  520/ 1682]\n",
      "loss: 21.231991 [  530/ 1682]\n",
      "loss: 27.737326 [  540/ 1682]\n",
      "loss: 13.080763 [  550/ 1682]\n",
      "loss: 162.019226 [  560/ 1682]\n",
      "loss: 251.574707 [  570/ 1682]\n",
      "loss: 197.449722 [  580/ 1682]\n",
      "loss: 55.233879 [  590/ 1682]\n",
      "loss: 208.341339 [  600/ 1682]\n",
      "loss: 57.214119 [  610/ 1682]\n",
      "loss: 58.595539 [  620/ 1682]\n",
      "loss: 213.169235 [  630/ 1682]\n",
      "loss: 127.095482 [  640/ 1682]\n",
      "loss: 197.475983 [  650/ 1682]\n",
      "loss: 264.343689 [  660/ 1682]\n",
      "loss: 250.971802 [  670/ 1682]\n",
      "loss: 283.992493 [  680/ 1682]\n",
      "loss: 226.143387 [  690/ 1682]\n",
      "loss: 180.050018 [  700/ 1682]\n",
      "loss: 116.812119 [  710/ 1682]\n",
      "loss: 33.948547 [  720/ 1682]\n",
      "loss: 13.283127 [  730/ 1682]\n",
      "loss: 1.256546 [  740/ 1682]\n",
      "loss: 9.884893 [  750/ 1682]\n",
      "loss: 2.507339 [  760/ 1682]\n",
      "loss: 131.555267 [  770/ 1682]\n",
      "loss: 24.408436 [  780/ 1682]\n",
      "loss: 40.177273 [  790/ 1682]\n",
      "loss: 78.116463 [  800/ 1682]\n",
      "loss: 117.350540 [  810/ 1682]\n",
      "loss: 151.863739 [  820/ 1682]\n",
      "loss: 137.180374 [  830/ 1682]\n",
      "loss: 212.986420 [  840/ 1682]\n",
      "loss: 202.567352 [  850/ 1682]\n",
      "loss: 117.436722 [  860/ 1682]\n",
      "loss: 139.826111 [  870/ 1682]\n",
      "loss: 166.790176 [  880/ 1682]\n",
      "loss: 189.396027 [  890/ 1682]\n",
      "loss: 221.881317 [  900/ 1682]\n",
      "loss: 215.994308 [  910/ 1682]\n",
      "loss: 272.743591 [  920/ 1682]\n",
      "loss: 286.416687 [  930/ 1682]\n",
      "loss: 396.277405 [  940/ 1682]\n",
      "loss: 535.016357 [  950/ 1682]\n",
      "loss: 619.509827 [  960/ 1682]\n",
      "loss: 688.978699 [  970/ 1682]\n",
      "loss: 788.604248 [  980/ 1682]\n",
      "loss: 1013.924927 [  990/ 1682]\n",
      "loss: 1468.414551 [ 1000/ 1682]\n",
      "loss: 1446.244751 [ 1010/ 1682]\n",
      "loss: 1558.174316 [ 1020/ 1682]\n",
      "loss: 1167.891235 [ 1030/ 1682]\n",
      "loss: 903.007507 [ 1040/ 1682]\n",
      "loss: 492.022064 [ 1050/ 1682]\n",
      "loss: 753.244751 [ 1060/ 1682]\n",
      "loss: 992.038757 [ 1070/ 1682]\n",
      "loss: 1239.359131 [ 1080/ 1682]\n",
      "loss: 1613.371704 [ 1090/ 1682]\n",
      "loss: 1767.510742 [ 1100/ 1682]\n",
      "loss: 2388.445312 [ 1110/ 1682]\n",
      "loss: 2424.949463 [ 1120/ 1682]\n",
      "loss: 3267.620361 [ 1130/ 1682]\n",
      "loss: 4245.982422 [ 1140/ 1682]\n",
      "loss: 5665.158691 [ 1150/ 1682]\n",
      "loss: 7113.442871 [ 1160/ 1682]\n",
      "loss: 5412.250977 [ 1170/ 1682]\n",
      "loss: 5180.249512 [ 1180/ 1682]\n",
      "loss: 6235.499512 [ 1190/ 1682]\n",
      "loss: 5602.527344 [ 1200/ 1682]\n",
      "loss: 6119.676758 [ 1210/ 1682]\n",
      "loss: 6193.204590 [ 1220/ 1682]\n",
      "loss: 6613.280273 [ 1230/ 1682]\n",
      "loss: 8408.572266 [ 1240/ 1682]\n",
      "loss: 8181.510742 [ 1250/ 1682]\n",
      "loss: 9309.134766 [ 1260/ 1682]\n",
      "loss: 9258.841797 [ 1270/ 1682]\n",
      "loss: 7739.942871 [ 1280/ 1682]\n",
      "loss: 6793.664062 [ 1290/ 1682]\n",
      "loss: 6855.442383 [ 1300/ 1682]\n",
      "loss: 7846.653809 [ 1310/ 1682]\n",
      "loss: 7763.447754 [ 1320/ 1682]\n",
      "loss: 8240.255859 [ 1330/ 1682]\n",
      "loss: 7390.688965 [ 1340/ 1682]\n",
      "loss: 7504.214844 [ 1350/ 1682]\n",
      "loss: 8360.607422 [ 1360/ 1682]\n",
      "loss: 9833.757812 [ 1370/ 1682]\n",
      "loss: 11383.736328 [ 1380/ 1682]\n",
      "loss: 11550.549805 [ 1390/ 1682]\n",
      "loss: 11787.181641 [ 1400/ 1682]\n",
      "loss: 11580.275391 [ 1410/ 1682]\n",
      "loss: 10767.347656 [ 1420/ 1682]\n",
      "loss: 11030.649414 [ 1430/ 1682]\n",
      "loss: 10473.513672 [ 1440/ 1682]\n",
      "loss: 11484.897461 [ 1450/ 1682]\n",
      "loss: 11563.638672 [ 1460/ 1682]\n",
      "loss: 14000.189453 [ 1470/ 1682]\n",
      "loss: 17044.669922 [ 1480/ 1682]\n",
      "loss: 18457.400391 [ 1490/ 1682]\n",
      "loss: 17794.550781 [ 1500/ 1682]\n",
      "loss: 16229.826172 [ 1510/ 1682]\n",
      "loss: 16668.128906 [ 1520/ 1682]\n",
      "loss: 16593.683594 [ 1530/ 1682]\n",
      "loss: 15293.570312 [ 1540/ 1682]\n",
      "loss: 15230.732422 [ 1550/ 1682]\n",
      "loss: 17543.167969 [ 1560/ 1682]\n",
      "loss: 15878.732422 [ 1570/ 1682]\n",
      "loss: 13594.591797 [ 1580/ 1682]\n",
      "loss: 11380.927734 [ 1590/ 1682]\n",
      "loss: 11260.095703 [ 1600/ 1682]\n",
      "loss: 9743.236328 [ 1610/ 1682]\n",
      "loss: 9925.966797 [ 1620/ 1682]\n",
      "loss: 10984.724609 [ 1630/ 1682]\n",
      "loss: 14121.445312 [ 1640/ 1682]\n",
      "loss: 17002.320312 [ 1650/ 1682]\n",
      "loss: 15413.462891 [ 1660/ 1682]\n",
      "loss: 12754.359375 [ 1670/ 1682]\n",
      "loss: 12905.238281 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14892.292030 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 699.003784 [    0/ 1682]\n",
      "loss: 254.828415 [   10/ 1682]\n",
      "loss: 258.255676 [   20/ 1682]\n",
      "loss: 272.160645 [   30/ 1682]\n",
      "loss: 279.346680 [   40/ 1682]\n",
      "loss: 176.910004 [   50/ 1682]\n",
      "loss: 164.123856 [   60/ 1682]\n",
      "loss: 209.138794 [   70/ 1682]\n",
      "loss: 653.882446 [   80/ 1682]\n",
      "loss: 250.929611 [   90/ 1682]\n",
      "loss: 238.051178 [  100/ 1682]\n",
      "loss: 253.998581 [  110/ 1682]\n",
      "loss: 250.893555 [  120/ 1682]\n",
      "loss: 553.841431 [  130/ 1682]\n",
      "loss: 180.861282 [  140/ 1682]\n",
      "loss: 157.234879 [  150/ 1682]\n",
      "loss: 635.838745 [  160/ 1682]\n",
      "loss: 155.881134 [  170/ 1682]\n",
      "loss: 563.618591 [  180/ 1682]\n",
      "loss: 445.175690 [  190/ 1682]\n",
      "loss: 178.621506 [  200/ 1682]\n",
      "loss: 986.741577 [  210/ 1682]\n",
      "loss: 150.068604 [  220/ 1682]\n",
      "loss: 441.637207 [  230/ 1682]\n",
      "loss: 113.303185 [  240/ 1682]\n",
      "loss: 363.950134 [  250/ 1682]\n",
      "loss: 569.212769 [  260/ 1682]\n",
      "loss: 51.176262 [  270/ 1682]\n",
      "loss: 35.473141 [  280/ 1682]\n",
      "loss: 30.631409 [  290/ 1682]\n",
      "loss: 24.578562 [  300/ 1682]\n",
      "loss: 20.541735 [  310/ 1682]\n",
      "loss: 20.732018 [  320/ 1682]\n",
      "loss: 7.201859 [  330/ 1682]\n",
      "loss: 3.556035 [  340/ 1682]\n",
      "loss: 5.575377 [  350/ 1682]\n",
      "loss: 44.307381 [  360/ 1682]\n",
      "loss: 17.269016 [  370/ 1682]\n",
      "loss: 5.974500 [  380/ 1682]\n",
      "loss: 3.530692 [  390/ 1682]\n",
      "loss: 266.179169 [  400/ 1682]\n",
      "loss: 136.327972 [  410/ 1682]\n",
      "loss: 1.287044 [  420/ 1682]\n",
      "loss: 2.721414 [  430/ 1682]\n",
      "loss: 277.286804 [  440/ 1682]\n",
      "loss: 2.029551 [  450/ 1682]\n",
      "loss: 192.591324 [  460/ 1682]\n",
      "loss: 169.010315 [  470/ 1682]\n",
      "loss: 7.943064 [  480/ 1682]\n",
      "loss: 220.733444 [  490/ 1682]\n",
      "loss: 14.511896 [  500/ 1682]\n",
      "loss: 5.775352 [  510/ 1682]\n",
      "loss: 169.976532 [  520/ 1682]\n",
      "loss: 27.026699 [  530/ 1682]\n",
      "loss: 18.704334 [  540/ 1682]\n",
      "loss: 5.772985 [  550/ 1682]\n",
      "loss: 11.029394 [  560/ 1682]\n",
      "loss: 4.790441 [  570/ 1682]\n",
      "loss: 37.051239 [  580/ 1682]\n",
      "loss: 42.033813 [  590/ 1682]\n",
      "loss: 59.391380 [  600/ 1682]\n",
      "loss: 38.609657 [  610/ 1682]\n",
      "loss: 45.017105 [  620/ 1682]\n",
      "loss: 157.770752 [  630/ 1682]\n",
      "loss: 106.878662 [  640/ 1682]\n",
      "loss: 171.758926 [  650/ 1682]\n",
      "loss: 234.455276 [  660/ 1682]\n",
      "loss: 201.564911 [  670/ 1682]\n",
      "loss: 252.972946 [  680/ 1682]\n",
      "loss: 196.121429 [  690/ 1682]\n",
      "loss: 220.276413 [  700/ 1682]\n",
      "loss: 162.605530 [  710/ 1682]\n",
      "loss: 19.207558 [  720/ 1682]\n",
      "loss: 150.989899 [  730/ 1682]\n",
      "loss: 27.555573 [  740/ 1682]\n",
      "loss: 2.738854 [  750/ 1682]\n",
      "loss: 176.082596 [  760/ 1682]\n",
      "loss: 11.442049 [  770/ 1682]\n",
      "loss: 159.788055 [  780/ 1682]\n",
      "loss: 29.313053 [  790/ 1682]\n",
      "loss: 129.054626 [  800/ 1682]\n",
      "loss: 97.796677 [  810/ 1682]\n",
      "loss: 129.481949 [  820/ 1682]\n",
      "loss: 157.610748 [  830/ 1682]\n",
      "loss: 42.082546 [  840/ 1682]\n",
      "loss: 55.729847 [  850/ 1682]\n",
      "loss: 97.917145 [  860/ 1682]\n",
      "loss: 118.422585 [  870/ 1682]\n",
      "loss: 143.314575 [  880/ 1682]\n",
      "loss: 153.594986 [  890/ 1682]\n",
      "loss: 169.845413 [  900/ 1682]\n",
      "loss: 160.973846 [  910/ 1682]\n",
      "loss: 220.579910 [  920/ 1682]\n",
      "loss: 296.876160 [  930/ 1682]\n",
      "loss: 327.515869 [  940/ 1682]\n",
      "loss: 492.329590 [  950/ 1682]\n",
      "loss: 669.089600 [  960/ 1682]\n",
      "loss: 647.342590 [  970/ 1682]\n",
      "loss: 828.170044 [  980/ 1682]\n",
      "loss: 1061.608643 [  990/ 1682]\n",
      "loss: 1258.657593 [ 1000/ 1682]\n",
      "loss: 1377.947510 [ 1010/ 1682]\n",
      "loss: 1437.740967 [ 1020/ 1682]\n",
      "loss: 1044.850342 [ 1030/ 1682]\n",
      "loss: 847.917786 [ 1040/ 1682]\n",
      "loss: 491.915924 [ 1050/ 1682]\n",
      "loss: 702.886353 [ 1060/ 1682]\n",
      "loss: 839.785339 [ 1070/ 1682]\n",
      "loss: 1196.032227 [ 1080/ 1682]\n",
      "loss: 1474.041016 [ 1090/ 1682]\n",
      "loss: 1662.961304 [ 1100/ 1682]\n",
      "loss: 2375.352783 [ 1110/ 1682]\n",
      "loss: 2695.770996 [ 1120/ 1682]\n",
      "loss: 3160.751953 [ 1130/ 1682]\n",
      "loss: 4124.789062 [ 1140/ 1682]\n",
      "loss: 5666.071289 [ 1150/ 1682]\n",
      "loss: 7390.153809 [ 1160/ 1682]\n",
      "loss: 5274.483398 [ 1170/ 1682]\n",
      "loss: 5331.645508 [ 1180/ 1682]\n",
      "loss: 5526.081055 [ 1190/ 1682]\n",
      "loss: 5329.913574 [ 1200/ 1682]\n",
      "loss: 5505.721191 [ 1210/ 1682]\n",
      "loss: 5583.110840 [ 1220/ 1682]\n",
      "loss: 6910.638184 [ 1230/ 1682]\n",
      "loss: 7717.924805 [ 1240/ 1682]\n",
      "loss: 7552.767090 [ 1250/ 1682]\n",
      "loss: 9128.434570 [ 1260/ 1682]\n",
      "loss: 8459.327148 [ 1270/ 1682]\n",
      "loss: 7575.186035 [ 1280/ 1682]\n",
      "loss: 6231.648926 [ 1290/ 1682]\n",
      "loss: 6700.409180 [ 1300/ 1682]\n",
      "loss: 7680.942871 [ 1310/ 1682]\n",
      "loss: 8245.033203 [ 1320/ 1682]\n",
      "loss: 8070.312500 [ 1330/ 1682]\n",
      "loss: 6941.053711 [ 1340/ 1682]\n",
      "loss: 6842.947754 [ 1350/ 1682]\n",
      "loss: 7433.430664 [ 1360/ 1682]\n",
      "loss: 9648.215820 [ 1370/ 1682]\n",
      "loss: 10433.163086 [ 1380/ 1682]\n",
      "loss: 11349.359375 [ 1390/ 1682]\n",
      "loss: 11583.999023 [ 1400/ 1682]\n",
      "loss: 12104.258789 [ 1410/ 1682]\n",
      "loss: 11779.282227 [ 1420/ 1682]\n",
      "loss: 10834.201172 [ 1430/ 1682]\n",
      "loss: 10265.001953 [ 1440/ 1682]\n",
      "loss: 10362.847656 [ 1450/ 1682]\n",
      "loss: 12084.500000 [ 1460/ 1682]\n",
      "loss: 12825.494141 [ 1470/ 1682]\n",
      "loss: 15799.135742 [ 1480/ 1682]\n",
      "loss: 18203.150391 [ 1490/ 1682]\n",
      "loss: 18523.449219 [ 1500/ 1682]\n",
      "loss: 15991.654297 [ 1510/ 1682]\n",
      "loss: 16486.339844 [ 1520/ 1682]\n",
      "loss: 16352.773438 [ 1530/ 1682]\n",
      "loss: 15062.278320 [ 1540/ 1682]\n",
      "loss: 14198.768555 [ 1550/ 1682]\n",
      "loss: 18299.546875 [ 1560/ 1682]\n",
      "loss: 15991.900391 [ 1570/ 1682]\n",
      "loss: 14318.939453 [ 1580/ 1682]\n",
      "loss: 11181.723633 [ 1590/ 1682]\n",
      "loss: 11266.599609 [ 1600/ 1682]\n",
      "loss: 9559.087891 [ 1610/ 1682]\n",
      "loss: 9164.658203 [ 1620/ 1682]\n",
      "loss: 11447.951172 [ 1630/ 1682]\n",
      "loss: 13137.210938 [ 1640/ 1682]\n",
      "loss: 16758.328125 [ 1650/ 1682]\n",
      "loss: 15949.662109 [ 1660/ 1682]\n",
      "loss: 13548.840820 [ 1670/ 1682]\n",
      "loss: 12692.666016 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14992.505747 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 829.557434 [    0/ 1682]\n",
      "loss: 285.692688 [   10/ 1682]\n",
      "loss: 289.336304 [   20/ 1682]\n",
      "loss: 733.620972 [   30/ 1682]\n",
      "loss: 231.974319 [   40/ 1682]\n",
      "loss: 202.786789 [   50/ 1682]\n",
      "loss: 360.980896 [   60/ 1682]\n",
      "loss: 237.108444 [   70/ 1682]\n",
      "loss: 311.893585 [   80/ 1682]\n",
      "loss: 537.692810 [   90/ 1682]\n",
      "loss: 267.922852 [  100/ 1682]\n",
      "loss: 284.813263 [  110/ 1682]\n",
      "loss: 829.303040 [  120/ 1682]\n",
      "loss: 261.833679 [  130/ 1682]\n",
      "loss: 700.425903 [  140/ 1682]\n",
      "loss: 429.706635 [  150/ 1682]\n",
      "loss: 204.053864 [  160/ 1682]\n",
      "loss: 180.109894 [  170/ 1682]\n",
      "loss: 155.756577 [  180/ 1682]\n",
      "loss: 711.218323 [  190/ 1682]\n",
      "loss: 198.167267 [  200/ 1682]\n",
      "loss: 187.300415 [  210/ 1682]\n",
      "loss: 425.958435 [  220/ 1682]\n",
      "loss: 195.478760 [  230/ 1682]\n",
      "loss: 491.802734 [  240/ 1682]\n",
      "loss: 131.715530 [  250/ 1682]\n",
      "loss: 112.333656 [  260/ 1682]\n",
      "loss: 245.398438 [  270/ 1682]\n",
      "loss: 47.480957 [  280/ 1682]\n",
      "loss: 384.918732 [  290/ 1682]\n",
      "loss: 34.705154 [  300/ 1682]\n",
      "loss: 337.482361 [  310/ 1682]\n",
      "loss: 52.211037 [  320/ 1682]\n",
      "loss: 30.732721 [  330/ 1682]\n",
      "loss: 7.859128 [  340/ 1682]\n",
      "loss: 10.624548 [  350/ 1682]\n",
      "loss: 317.729675 [  360/ 1682]\n",
      "loss: 45.666225 [  370/ 1682]\n",
      "loss: 11.388390 [  380/ 1682]\n",
      "loss: 7.444963 [  390/ 1682]\n",
      "loss: 13.398376 [  400/ 1682]\n",
      "loss: 0.593456 [  410/ 1682]\n",
      "loss: 3.698942 [  420/ 1682]\n",
      "loss: 6.627004 [  430/ 1682]\n",
      "loss: 4.467188 [  440/ 1682]\n",
      "loss: 2.007308 [  450/ 1682]\n",
      "loss: 4.842425 [  460/ 1682]\n",
      "loss: 3.617229 [  470/ 1682]\n",
      "loss: 216.695953 [  480/ 1682]\n",
      "loss: 85.476120 [  490/ 1682]\n",
      "loss: 8.316805 [  500/ 1682]\n",
      "loss: 2.756985 [  510/ 1682]\n",
      "loss: 2.223554 [  520/ 1682]\n",
      "loss: 7.702836 [  530/ 1682]\n",
      "loss: 14.641078 [  540/ 1682]\n",
      "loss: 2.378535 [  550/ 1682]\n",
      "loss: 142.938599 [  560/ 1682]\n",
      "loss: 228.207031 [  570/ 1682]\n",
      "loss: 26.742514 [  580/ 1682]\n",
      "loss: 30.792881 [  590/ 1682]\n",
      "loss: 42.865192 [  600/ 1682]\n",
      "loss: 133.173325 [  610/ 1682]\n",
      "loss: 33.410912 [  620/ 1682]\n",
      "loss: 44.052837 [  630/ 1682]\n",
      "loss: 77.644531 [  640/ 1682]\n",
      "loss: 148.219101 [  650/ 1682]\n",
      "loss: 206.816574 [  660/ 1682]\n",
      "loss: 194.959152 [  670/ 1682]\n",
      "loss: 203.117065 [  680/ 1682]\n",
      "loss: 189.890213 [  690/ 1682]\n",
      "loss: 136.801437 [  700/ 1682]\n",
      "loss: 80.398636 [  710/ 1682]\n",
      "loss: 15.968883 [  720/ 1682]\n",
      "loss: 3.581555 [  730/ 1682]\n",
      "loss: 5.093361 [  740/ 1682]\n",
      "loss: 6.458297 [  750/ 1682]\n",
      "loss: 2.968140 [  760/ 1682]\n",
      "loss: 6.029818 [  770/ 1682]\n",
      "loss: 9.470442 [  780/ 1682]\n",
      "loss: 20.359831 [  790/ 1682]\n",
      "loss: 48.537449 [  800/ 1682]\n",
      "loss: 80.266037 [  810/ 1682]\n",
      "loss: 166.619034 [  820/ 1682]\n",
      "loss: 97.485374 [  830/ 1682]\n",
      "loss: 32.405407 [  840/ 1682]\n",
      "loss: 43.240192 [  850/ 1682]\n",
      "loss: 80.394547 [  860/ 1682]\n",
      "loss: 99.037674 [  870/ 1682]\n",
      "loss: 121.881424 [  880/ 1682]\n",
      "loss: 108.819542 [  890/ 1682]\n",
      "loss: 131.654831 [  900/ 1682]\n",
      "loss: 138.242905 [  910/ 1682]\n",
      "loss: 246.958038 [  920/ 1682]\n",
      "loss: 264.336639 [  930/ 1682]\n",
      "loss: 319.012177 [  940/ 1682]\n",
      "loss: 438.206879 [  950/ 1682]\n",
      "loss: 621.759705 [  960/ 1682]\n",
      "loss: 581.934692 [  970/ 1682]\n",
      "loss: 724.203186 [  980/ 1682]\n",
      "loss: 918.206909 [  990/ 1682]\n",
      "loss: 1328.354858 [ 1000/ 1682]\n",
      "loss: 1313.729858 [ 1010/ 1682]\n",
      "loss: 1576.954224 [ 1020/ 1682]\n",
      "loss: 1147.278931 [ 1030/ 1682]\n",
      "loss: 795.273804 [ 1040/ 1682]\n",
      "loss: 451.760437 [ 1050/ 1682]\n",
      "loss: 604.302002 [ 1060/ 1682]\n",
      "loss: 877.611206 [ 1070/ 1682]\n",
      "loss: 1229.195068 [ 1080/ 1682]\n",
      "loss: 1172.036377 [ 1090/ 1682]\n",
      "loss: 1777.265015 [ 1100/ 1682]\n",
      "loss: 2285.553223 [ 1110/ 1682]\n",
      "loss: 2685.425781 [ 1120/ 1682]\n",
      "loss: 2765.073730 [ 1130/ 1682]\n",
      "loss: 4006.936035 [ 1140/ 1682]\n",
      "loss: 5527.029297 [ 1150/ 1682]\n",
      "loss: 7231.229492 [ 1160/ 1682]\n",
      "loss: 4411.537109 [ 1170/ 1682]\n",
      "loss: 5196.799316 [ 1180/ 1682]\n",
      "loss: 5598.049805 [ 1190/ 1682]\n",
      "loss: 5325.892578 [ 1200/ 1682]\n",
      "loss: 5830.362305 [ 1210/ 1682]\n",
      "loss: 5465.947266 [ 1220/ 1682]\n",
      "loss: 6756.987305 [ 1230/ 1682]\n",
      "loss: 7502.694336 [ 1240/ 1682]\n",
      "loss: 7846.384277 [ 1250/ 1682]\n",
      "loss: 8951.991211 [ 1260/ 1682]\n",
      "loss: 8721.681641 [ 1270/ 1682]\n",
      "loss: 7414.389160 [ 1280/ 1682]\n",
      "loss: 5852.070801 [ 1290/ 1682]\n",
      "loss: 6080.668457 [ 1300/ 1682]\n",
      "loss: 7017.497070 [ 1310/ 1682]\n",
      "loss: 8591.002930 [ 1320/ 1682]\n",
      "loss: 7393.862305 [ 1330/ 1682]\n",
      "loss: 7219.746094 [ 1340/ 1682]\n",
      "loss: 6697.772461 [ 1350/ 1682]\n",
      "loss: 7844.324219 [ 1360/ 1682]\n",
      "loss: 9466.690430 [ 1370/ 1682]\n",
      "loss: 10988.337891 [ 1380/ 1682]\n",
      "loss: 11152.249023 [ 1390/ 1682]\n",
      "loss: 11384.855469 [ 1400/ 1682]\n",
      "loss: 11900.666016 [ 1410/ 1682]\n",
      "loss: 11423.827148 [ 1420/ 1682]\n",
      "loss: 10087.827148 [ 1430/ 1682]\n",
      "loss: 10071.561523 [ 1440/ 1682]\n",
      "loss: 11668.072266 [ 1450/ 1682]\n",
      "loss: 11881.004883 [ 1460/ 1682]\n",
      "loss: 12819.411133 [ 1470/ 1682]\n",
      "loss: 16560.562500 [ 1480/ 1682]\n",
      "loss: 17953.160156 [ 1490/ 1682]\n",
      "loss: 16233.739258 [ 1500/ 1682]\n",
      "loss: 14829.979492 [ 1510/ 1682]\n",
      "loss: 17235.382812 [ 1520/ 1682]\n",
      "loss: 16115.903320 [ 1530/ 1682]\n",
      "loss: 14834.918945 [ 1540/ 1682]\n",
      "loss: 14773.640625 [ 1550/ 1682]\n",
      "loss: 16314.720703 [ 1560/ 1682]\n",
      "loss: 13779.468750 [ 1570/ 1682]\n",
      "loss: 13177.650391 [ 1580/ 1682]\n",
      "loss: 10986.232422 [ 1590/ 1682]\n",
      "loss: 10851.521484 [ 1600/ 1682]\n",
      "loss: 9378.618164 [ 1610/ 1682]\n",
      "loss: 9730.932617 [ 1620/ 1682]\n",
      "loss: 11230.482422 [ 1630/ 1682]\n",
      "loss: 13681.206055 [ 1640/ 1682]\n",
      "loss: 15332.392578 [ 1650/ 1682]\n",
      "loss: 15716.135742 [ 1660/ 1682]\n",
      "loss: 12436.289062 [ 1670/ 1682]\n",
      "loss: 12484.339844 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14739.012508 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 790.469116 [    0/ 1682]\n",
      "loss: 815.485596 [   10/ 1682]\n",
      "loss: 948.450562 [   20/ 1682]\n",
      "loss: 294.228668 [   30/ 1682]\n",
      "loss: 261.075989 [   40/ 1682]\n",
      "loss: 230.043533 [   50/ 1682]\n",
      "loss: 215.412964 [   60/ 1682]\n",
      "loss: 314.329895 [   70/ 1682]\n",
      "loss: 345.472443 [   80/ 1682]\n",
      "loss: 313.461853 [   90/ 1682]\n",
      "loss: 1297.625488 [  100/ 1682]\n",
      "loss: 316.911835 [  110/ 1682]\n",
      "loss: 313.431030 [  120/ 1682]\n",
      "loss: 292.629425 [  130/ 1682]\n",
      "loss: 234.441559 [  140/ 1682]\n",
      "loss: 207.456085 [  150/ 1682]\n",
      "loss: 231.330856 [  160/ 1682]\n",
      "loss: 740.490601 [  170/ 1682]\n",
      "loss: 448.991882 [  180/ 1682]\n",
      "loss: 592.828918 [  190/ 1682]\n",
      "loss: 180.289185 [  200/ 1682]\n",
      "loss: 213.446075 [  210/ 1682]\n",
      "loss: 199.122452 [  220/ 1682]\n",
      "loss: 336.952209 [  230/ 1682]\n",
      "loss: 156.384796 [  240/ 1682]\n",
      "loss: 1069.538818 [  250/ 1682]\n",
      "loss: 504.771484 [  260/ 1682]\n",
      "loss: 393.196838 [  270/ 1682]\n",
      "loss: 61.021900 [  280/ 1682]\n",
      "loss: 54.682304 [  290/ 1682]\n",
      "loss: 73.189224 [  300/ 1682]\n",
      "loss: 40.855053 [  310/ 1682]\n",
      "loss: 92.165726 [  320/ 1682]\n",
      "loss: 20.062984 [  330/ 1682]\n",
      "loss: 13.808533 [  340/ 1682]\n",
      "loss: 17.308277 [  350/ 1682]\n",
      "loss: 32.530903 [  360/ 1682]\n",
      "loss: 36.047920 [  370/ 1682]\n",
      "loss: 18.432547 [  380/ 1682]\n",
      "loss: 311.807556 [  390/ 1682]\n",
      "loss: 271.889160 [  400/ 1682]\n",
      "loss: 2.654475 [  410/ 1682]\n",
      "loss: 177.254791 [  420/ 1682]\n",
      "loss: 200.034470 [  430/ 1682]\n",
      "loss: 28.530499 [  440/ 1682]\n",
      "loss: 3.690992 [  450/ 1682]\n",
      "loss: 1.742644 [  460/ 1682]\n",
      "loss: 11.627841 [  470/ 1682]\n",
      "loss: 298.136414 [  480/ 1682]\n",
      "loss: 391.428406 [  490/ 1682]\n",
      "loss: 129.573196 [  500/ 1682]\n",
      "loss: 181.104965 [  510/ 1682]\n",
      "loss: 410.082581 [  520/ 1682]\n",
      "loss: 77.003746 [  530/ 1682]\n",
      "loss: 6.275792 [  540/ 1682]\n",
      "loss: 0.747409 [  550/ 1682]\n",
      "loss: 111.523819 [  560/ 1682]\n",
      "loss: 94.992401 [  570/ 1682]\n",
      "loss: 82.294983 [  580/ 1682]\n",
      "loss: 21.484343 [  590/ 1682]\n",
      "loss: 71.213028 [  600/ 1682]\n",
      "loss: 22.917435 [  610/ 1682]\n",
      "loss: 23.746284 [  620/ 1682]\n",
      "loss: 35.446293 [  630/ 1682]\n",
      "loss: 72.698792 [  640/ 1682]\n",
      "loss: 115.572021 [  650/ 1682]\n",
      "loss: 205.600342 [  660/ 1682]\n",
      "loss: 182.296173 [  670/ 1682]\n",
      "loss: 197.808960 [  680/ 1682]\n",
      "loss: 165.667603 [  690/ 1682]\n",
      "loss: 146.232971 [  700/ 1682]\n",
      "loss: 65.300766 [  710/ 1682]\n",
      "loss: 9.801031 [  720/ 1682]\n",
      "loss: 11.275612 [  730/ 1682]\n",
      "loss: 9.491099 [  740/ 1682]\n",
      "loss: 11.766615 [  750/ 1682]\n",
      "loss: 5.728880 [  760/ 1682]\n",
      "loss: 2.423193 [  770/ 1682]\n",
      "loss: 4.774173 [  780/ 1682]\n",
      "loss: 13.301264 [  790/ 1682]\n",
      "loss: 36.753677 [  800/ 1682]\n",
      "loss: 61.992695 [  810/ 1682]\n",
      "loss: 91.015335 [  820/ 1682]\n",
      "loss: 80.803574 [  830/ 1682]\n",
      "loss: 23.177326 [  840/ 1682]\n",
      "loss: 32.742352 [  850/ 1682]\n",
      "loss: 119.677246 [  860/ 1682]\n",
      "loss: 81.823715 [  870/ 1682]\n",
      "loss: 102.673630 [  880/ 1682]\n",
      "loss: 99.117325 [  890/ 1682]\n",
      "loss: 189.341568 [  900/ 1682]\n",
      "loss: 117.781616 [  910/ 1682]\n",
      "loss: 188.635712 [  920/ 1682]\n",
      "loss: 200.096512 [  930/ 1682]\n",
      "loss: 285.005402 [  940/ 1682]\n",
      "loss: 411.548401 [  950/ 1682]\n",
      "loss: 551.080872 [  960/ 1682]\n",
      "loss: 553.703186 [  970/ 1682]\n",
      "loss: 665.968079 [  980/ 1682]\n",
      "loss: 945.345337 [  990/ 1682]\n",
      "loss: 1263.196411 [ 1000/ 1682]\n",
      "loss: 941.835815 [ 1010/ 1682]\n",
      "loss: 1505.849365 [ 1020/ 1682]\n",
      "loss: 1034.097656 [ 1030/ 1682]\n",
      "loss: 745.759644 [ 1040/ 1682]\n",
      "loss: 414.392761 [ 1050/ 1682]\n",
      "loss: 493.007172 [ 1060/ 1682]\n",
      "loss: 824.888550 [ 1070/ 1682]\n",
      "loss: 903.949036 [ 1080/ 1682]\n",
      "loss: 1261.677979 [ 1090/ 1682]\n",
      "loss: 1393.480469 [ 1100/ 1682]\n",
      "loss: 1613.700928 [ 1110/ 1682]\n",
      "loss: 2399.912842 [ 1120/ 1682]\n",
      "loss: 2856.049561 [ 1130/ 1682]\n",
      "loss: 3581.078857 [ 1140/ 1682]\n",
      "loss: 5393.830078 [ 1150/ 1682]\n",
      "loss: 7078.783691 [ 1160/ 1682]\n",
      "loss: 5011.938965 [ 1170/ 1682]\n",
      "loss: 5067.717773 [ 1180/ 1682]\n",
      "loss: 5805.387207 [ 1190/ 1682]\n",
      "loss: 4881.812500 [ 1200/ 1682]\n",
      "loss: 5209.007324 [ 1210/ 1682]\n",
      "loss: 5764.475586 [ 1220/ 1682]\n",
      "loss: 6609.690430 [ 1230/ 1682]\n",
      "loss: 7438.822754 [ 1240/ 1682]\n",
      "loss: 7179.715820 [ 1250/ 1682]\n",
      "loss: 8782.638672 [ 1260/ 1682]\n",
      "loss: 8543.038086 [ 1270/ 1682]\n",
      "loss: 7260.250977 [ 1280/ 1682]\n",
      "loss: 6344.571289 [ 1290/ 1682]\n",
      "loss: 6238.996094 [ 1300/ 1682]\n",
      "loss: 7363.958496 [ 1310/ 1682]\n",
      "loss: 8424.881836 [ 1320/ 1682]\n",
      "loss: 7745.001465 [ 1330/ 1682]\n",
      "loss: 7067.506348 [ 1340/ 1682]\n",
      "loss: 7031.849121 [ 1350/ 1682]\n",
      "loss: 7861.723633 [ 1360/ 1682]\n",
      "loss: 9292.283203 [ 1370/ 1682]\n",
      "loss: 10302.591797 [ 1380/ 1682]\n",
      "loss: 10962.710938 [ 1390/ 1682]\n",
      "loss: 11193.341797 [ 1400/ 1682]\n",
      "loss: 11704.836914 [ 1410/ 1682]\n",
      "loss: 11925.021484 [ 1420/ 1682]\n",
      "loss: 9130.175781 [ 1430/ 1682]\n",
      "loss: 10087.418945 [ 1440/ 1682]\n",
      "loss: 11060.880859 [ 1450/ 1682]\n",
      "loss: 11685.262695 [ 1460/ 1682]\n",
      "loss: 13352.578125 [ 1470/ 1682]\n",
      "loss: 15553.666016 [ 1480/ 1682]\n",
      "loss: 16785.138672 [ 1490/ 1682]\n",
      "loss: 18028.333984 [ 1500/ 1682]\n",
      "loss: 15532.119141 [ 1510/ 1682]\n",
      "loss: 16999.541016 [ 1520/ 1682]\n",
      "loss: 15887.807617 [ 1530/ 1682]\n",
      "loss: 13808.291016 [ 1540/ 1682]\n",
      "loss: 13789.287109 [ 1550/ 1682]\n",
      "loss: 17807.210938 [ 1560/ 1682]\n",
      "loss: 15531.864258 [ 1570/ 1682]\n",
      "loss: 13883.875000 [ 1580/ 1682]\n",
      "loss: 9266.820312 [ 1590/ 1682]\n",
      "loss: 10660.215820 [ 1600/ 1682]\n",
      "loss: 9204.876953 [ 1610/ 1682]\n",
      "loss: 9553.591797 [ 1620/ 1682]\n",
      "loss: 10563.210938 [ 1630/ 1682]\n",
      "loss: 13470.840820 [ 1640/ 1682]\n",
      "loss: 15277.338867 [ 1650/ 1682]\n",
      "loss: 13552.809570 [ 1660/ 1682]\n",
      "loss: 13125.840820 [ 1670/ 1682]\n",
      "loss: 12283.328125 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14475.197904 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 391.834656 [    0/ 1682]\n",
      "loss: 350.915314 [   10/ 1682]\n",
      "loss: 793.174438 [   20/ 1682]\n",
      "loss: 326.023438 [   30/ 1682]\n",
      "loss: 291.085388 [   40/ 1682]\n",
      "loss: 634.791626 [   50/ 1682]\n",
      "loss: 457.990631 [   60/ 1682]\n",
      "loss: 837.237183 [   70/ 1682]\n",
      "loss: 665.782227 [   80/ 1682]\n",
      "loss: 346.227386 [   90/ 1682]\n",
      "loss: 854.225281 [  100/ 1682]\n",
      "loss: 349.857483 [  110/ 1682]\n",
      "loss: 664.302063 [  120/ 1682]\n",
      "loss: 324.307465 [  130/ 1682]\n",
      "loss: 262.867279 [  140/ 1682]\n",
      "loss: 283.909882 [  150/ 1682]\n",
      "loss: 259.566742 [  160/ 1682]\n",
      "loss: 232.328690 [  170/ 1682]\n",
      "loss: 204.680939 [  180/ 1682]\n",
      "loss: 185.348907 [  190/ 1682]\n",
      "loss: 205.269379 [  200/ 1682]\n",
      "loss: 240.588913 [  210/ 1682]\n",
      "loss: 516.532410 [  220/ 1682]\n",
      "loss: 205.008667 [  230/ 1682]\n",
      "loss: 179.732803 [  240/ 1682]\n",
      "loss: 176.941147 [  250/ 1682]\n",
      "loss: 154.266327 [  260/ 1682]\n",
      "loss: 98.236694 [  270/ 1682]\n",
      "loss: 75.894676 [  280/ 1682]\n",
      "loss: 68.833694 [  290/ 1682]\n",
      "loss: 59.456871 [  300/ 1682]\n",
      "loss: 53.201080 [  310/ 1682]\n",
      "loss: 53.381767 [  320/ 1682]\n",
      "loss: 28.797592 [  330/ 1682]\n",
      "loss: 21.278214 [  340/ 1682]\n",
      "loss: 25.498156 [  350/ 1682]\n",
      "loss: 241.211624 [  360/ 1682]\n",
      "loss: 47.662716 [  370/ 1682]\n",
      "loss: 207.925705 [  380/ 1682]\n",
      "loss: 20.109291 [  390/ 1682]\n",
      "loss: 176.439972 [  400/ 1682]\n",
      "loss: 6.318494 [  410/ 1682]\n",
      "loss: 13.430242 [  420/ 1682]\n",
      "loss: 463.943695 [  430/ 1682]\n",
      "loss: 162.334305 [  440/ 1682]\n",
      "loss: 451.618652 [  450/ 1682]\n",
      "loss: 132.656113 [  460/ 1682]\n",
      "loss: 0.210635 [  470/ 1682]\n",
      "loss: 141.389297 [  480/ 1682]\n",
      "loss: 0.210742 [  490/ 1682]\n",
      "loss: 212.069870 [  500/ 1682]\n",
      "loss: 1.890236 [  510/ 1682]\n",
      "loss: 123.922691 [  520/ 1682]\n",
      "loss: 1.379894 [  530/ 1682]\n",
      "loss: 127.174805 [  540/ 1682]\n",
      "loss: 0.779047 [  550/ 1682]\n",
      "loss: 0.597134 [  560/ 1682]\n",
      "loss: 2.544827 [  570/ 1682]\n",
      "loss: 87.074982 [  580/ 1682]\n",
      "loss: 159.504425 [  590/ 1682]\n",
      "loss: 24.709410 [  600/ 1682]\n",
      "loss: 15.195767 [  610/ 1682]\n",
      "loss: 133.987091 [  620/ 1682]\n",
      "loss: 164.095642 [  630/ 1682]\n",
      "loss: 138.375946 [  640/ 1682]\n",
      "loss: 107.461143 [  650/ 1682]\n",
      "loss: 158.094650 [  660/ 1682]\n",
      "loss: 174.920090 [  670/ 1682]\n",
      "loss: 151.766373 [  680/ 1682]\n",
      "loss: 127.402489 [  690/ 1682]\n",
      "loss: 153.881927 [  700/ 1682]\n",
      "loss: 52.023338 [  710/ 1682]\n",
      "loss: 146.749466 [  720/ 1682]\n",
      "loss: 129.678253 [  730/ 1682]\n",
      "loss: 200.256073 [  740/ 1682]\n",
      "loss: 18.628180 [  750/ 1682]\n",
      "loss: 10.075041 [  760/ 1682]\n",
      "loss: 0.487106 [  770/ 1682]\n",
      "loss: 1.762927 [  780/ 1682]\n",
      "loss: 7.960164 [  790/ 1682]\n",
      "loss: 26.753531 [  800/ 1682]\n",
      "loss: 48.645885 [  810/ 1682]\n",
      "loss: 74.751427 [  820/ 1682]\n",
      "loss: 65.978104 [  830/ 1682]\n",
      "loss: 15.699155 [  840/ 1682]\n",
      "loss: 24.013947 [  850/ 1682]\n",
      "loss: 51.435402 [  860/ 1682]\n",
      "loss: 66.479721 [  870/ 1682]\n",
      "loss: 85.365448 [  880/ 1682]\n",
      "loss: 67.767189 [  890/ 1682]\n",
      "loss: 93.715836 [  900/ 1682]\n",
      "loss: 99.232803 [  910/ 1682]\n",
      "loss: 173.399628 [  920/ 1682]\n",
      "loss: 175.654526 [  930/ 1682]\n",
      "loss: 233.326569 [  940/ 1682]\n",
      "loss: 362.587433 [  950/ 1682]\n",
      "loss: 535.260986 [  960/ 1682]\n",
      "loss: 553.557373 [  970/ 1682]\n",
      "loss: 569.386475 [  980/ 1682]\n",
      "loss: 891.243835 [  990/ 1682]\n",
      "loss: 1082.127197 [ 1000/ 1682]\n",
      "loss: 1183.532227 [ 1010/ 1682]\n",
      "loss: 1437.281860 [ 1020/ 1682]\n",
      "loss: 923.090149 [ 1030/ 1682]\n",
      "loss: 698.479492 [ 1040/ 1682]\n",
      "loss: 347.060852 [ 1050/ 1682]\n",
      "loss: 567.169250 [ 1060/ 1682]\n",
      "loss: 774.416870 [ 1070/ 1682]\n",
      "loss: 899.501282 [ 1080/ 1682]\n",
      "loss: 1212.151123 [ 1090/ 1682]\n",
      "loss: 1629.204956 [ 1100/ 1682]\n",
      "loss: 2117.032715 [ 1110/ 1682]\n",
      "loss: 2502.561768 [ 1120/ 1682]\n",
      "loss: 2609.799316 [ 1130/ 1682]\n",
      "loss: 3784.252686 [ 1140/ 1682]\n",
      "loss: 5119.404297 [ 1150/ 1682]\n",
      "loss: 6929.221191 [ 1160/ 1682]\n",
      "loss: 4886.170410 [ 1170/ 1682]\n",
      "loss: 4666.286133 [ 1180/ 1682]\n",
      "loss: 5520.971680 [ 1190/ 1682]\n",
      "loss: 5067.172852 [ 1200/ 1682]\n",
      "loss: 5152.372559 [ 1210/ 1682]\n",
      "loss: 5629.434570 [ 1220/ 1682]\n",
      "loss: 6464.995117 [ 1230/ 1682]\n",
      "loss: 7495.093750 [ 1240/ 1682]\n",
      "loss: 7349.575195 [ 1250/ 1682]\n",
      "loss: 8615.947266 [ 1260/ 1682]\n",
      "loss: 8566.505859 [ 1270/ 1682]\n",
      "loss: 7108.610352 [ 1280/ 1682]\n",
      "loss: 6202.790039 [ 1290/ 1682]\n",
      "loss: 5783.029785 [ 1300/ 1682]\n",
      "loss: 7211.283691 [ 1310/ 1682]\n",
      "loss: 7870.756348 [ 1320/ 1682]\n",
      "loss: 7588.256348 [ 1330/ 1682]\n",
      "loss: 6472.344727 [ 1340/ 1682]\n",
      "loss: 6440.392578 [ 1350/ 1682]\n",
      "loss: 7703.838379 [ 1360/ 1682]\n",
      "loss: 8565.546875 [ 1370/ 1682]\n",
      "loss: 9629.225586 [ 1380/ 1682]\n",
      "loss: 10776.177734 [ 1390/ 1682]\n",
      "loss: 10524.212891 [ 1400/ 1682]\n",
      "loss: 11512.177734 [ 1410/ 1682]\n",
      "loss: 11134.673828 [ 1420/ 1682]\n",
      "loss: 9780.096680 [ 1430/ 1682]\n",
      "loss: 9253.849609 [ 1440/ 1682]\n",
      "loss: 10792.060547 [ 1450/ 1682]\n",
      "loss: 10843.298828 [ 1460/ 1682]\n",
      "loss: 13147.078125 [ 1470/ 1682]\n",
      "loss: 14572.990234 [ 1480/ 1682]\n",
      "loss: 16665.195312 [ 1490/ 1682]\n",
      "loss: 17789.482422 [ 1500/ 1682]\n",
      "loss: 15310.650391 [ 1510/ 1682]\n",
      "loss: 16767.781250 [ 1520/ 1682]\n",
      "loss: 15663.784180 [ 1530/ 1682]\n",
      "loss: 13499.723633 [ 1540/ 1682]\n",
      "loss: 12798.333008 [ 1550/ 1682]\n",
      "loss: 17569.992188 [ 1560/ 1682]\n",
      "loss: 15310.401367 [ 1570/ 1682]\n",
      "loss: 12993.041016 [ 1580/ 1682]\n",
      "loss: 10613.780273 [ 1590/ 1682]\n",
      "loss: 9734.332031 [ 1600/ 1682]\n",
      "loss: 8828.115234 [ 1610/ 1682]\n",
      "loss: 9380.166016 [ 1620/ 1682]\n",
      "loss: 10395.845703 [ 1630/ 1682]\n",
      "loss: 13264.873047 [ 1640/ 1682]\n",
      "loss: 16060.895508 [ 1650/ 1682]\n",
      "loss: 15269.760742 [ 1660/ 1682]\n",
      "loss: 12922.422852 [ 1670/ 1682]\n",
      "loss: 12086.502930 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14332.414682 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 433.468262 [    0/ 1682]\n",
      "loss: 1048.395264 [   10/ 1682]\n",
      "loss: 389.364197 [   20/ 1682]\n",
      "loss: 924.584839 [   30/ 1682]\n",
      "loss: 322.306274 [   40/ 1682]\n",
      "loss: 287.714661 [   50/ 1682]\n",
      "loss: 271.315125 [   60/ 1682]\n",
      "loss: 802.753235 [   70/ 1682]\n",
      "loss: 415.415436 [   80/ 1682]\n",
      "loss: 380.184448 [   90/ 1682]\n",
      "loss: 364.393250 [  100/ 1682]\n",
      "loss: 384.011322 [  110/ 1682]\n",
      "loss: 380.178894 [  120/ 1682]\n",
      "loss: 357.227966 [  130/ 1682]\n",
      "loss: 349.798737 [  140/ 1682]\n",
      "loss: 262.365509 [  150/ 1682]\n",
      "loss: 820.765747 [  160/ 1682]\n",
      "loss: 314.752136 [  170/ 1682]\n",
      "loss: 231.005936 [  180/ 1682]\n",
      "loss: 210.421509 [  190/ 1682]\n",
      "loss: 542.077759 [  200/ 1682]\n",
      "loss: 269.040344 [  210/ 1682]\n",
      "loss: 252.914841 [  220/ 1682]\n",
      "loss: 231.307739 [  230/ 1682]\n",
      "loss: 204.421753 [  240/ 1682]\n",
      "loss: 201.446152 [  250/ 1682]\n",
      "loss: 180.049652 [  260/ 1682]\n",
      "loss: 116.670822 [  270/ 1682]\n",
      "loss: 92.185463 [  280/ 1682]\n",
      "loss: 122.828224 [  290/ 1682]\n",
      "loss: 73.956070 [  300/ 1682]\n",
      "loss: 440.409485 [  310/ 1682]\n",
      "loss: 67.157158 [  320/ 1682]\n",
      "loss: 39.007114 [  330/ 1682]\n",
      "loss: 30.237101 [  340/ 1682]\n",
      "loss: 533.608643 [  350/ 1682]\n",
      "loss: 227.155792 [  360/ 1682]\n",
      "loss: 60.693756 [  370/ 1682]\n",
      "loss: 36.979633 [  380/ 1682]\n",
      "loss: 170.916061 [  390/ 1682]\n",
      "loss: 16.661127 [  400/ 1682]\n",
      "loss: 11.512800 [  410/ 1682]\n",
      "loss: 20.573904 [  420/ 1682]\n",
      "loss: 27.829844 [  430/ 1682]\n",
      "loss: 267.080261 [  440/ 1682]\n",
      "loss: 301.945618 [  450/ 1682]\n",
      "loss: 260.435272 [  460/ 1682]\n",
      "loss: 199.708527 [  470/ 1682]\n",
      "loss: 0.951018 [  480/ 1682]\n",
      "loss: 0.949180 [  490/ 1682]\n",
      "loss: 15.311287 [  500/ 1682]\n",
      "loss: 3.866998 [  510/ 1682]\n",
      "loss: 200.060852 [  520/ 1682]\n",
      "loss: 198.045807 [  530/ 1682]\n",
      "loss: 49.406414 [  540/ 1682]\n",
      "loss: 146.553619 [  550/ 1682]\n",
      "loss: 0.615859 [  560/ 1682]\n",
      "loss: 5.043551 [  570/ 1682]\n",
      "loss: 6.761723 [  580/ 1682]\n",
      "loss: 8.112754 [  590/ 1682]\n",
      "loss: 16.752508 [  600/ 1682]\n",
      "loss: 9.187382 [  610/ 1682]\n",
      "loss: 9.658930 [  620/ 1682]\n",
      "loss: 171.321259 [  630/ 1682]\n",
      "loss: 134.291779 [  640/ 1682]\n",
      "loss: 94.052994 [  650/ 1682]\n",
      "loss: 122.638344 [  660/ 1682]\n",
      "loss: 126.996300 [  670/ 1682]\n",
      "loss: 150.887161 [  680/ 1682]\n",
      "loss: 123.037254 [  690/ 1682]\n",
      "loss: 97.161102 [  700/ 1682]\n",
      "loss: 40.532066 [  710/ 1682]\n",
      "loss: 2.524989 [  720/ 1682]\n",
      "loss: 2.000850 [  730/ 1682]\n",
      "loss: 22.915108 [  740/ 1682]\n",
      "loss: 273.776154 [  750/ 1682]\n",
      "loss: 42.020428 [  760/ 1682]\n",
      "loss: 0.155272 [  770/ 1682]\n",
      "loss: 0.368595 [  780/ 1682]\n",
      "loss: 87.090660 [  790/ 1682]\n",
      "loss: 18.452782 [  800/ 1682]\n",
      "loss: 147.712585 [  810/ 1682]\n",
      "loss: 60.256611 [  820/ 1682]\n",
      "loss: 156.380249 [  830/ 1682]\n",
      "loss: 19.304930 [  840/ 1682]\n",
      "loss: 16.962326 [  850/ 1682]\n",
      "loss: 100.776817 [  860/ 1682]\n",
      "loss: 144.495728 [  870/ 1682]\n",
      "loss: 69.837112 [  880/ 1682]\n",
      "loss: 60.551289 [  890/ 1682]\n",
      "loss: 77.492874 [  900/ 1682]\n",
      "loss: 82.485878 [  910/ 1682]\n",
      "loss: 180.599609 [  920/ 1682]\n",
      "loss: 153.094147 [  930/ 1682]\n",
      "loss: 230.563309 [  940/ 1682]\n",
      "loss: 345.220551 [  950/ 1682]\n",
      "loss: 455.714752 [  960/ 1682]\n",
      "loss: 512.856384 [  970/ 1682]\n",
      "loss: 580.737915 [  980/ 1682]\n",
      "loss: 839.454468 [  990/ 1682]\n",
      "loss: 1030.091675 [ 1000/ 1682]\n",
      "loss: 1135.585693 [ 1010/ 1682]\n",
      "loss: 1371.256470 [ 1020/ 1682]\n",
      "loss: 902.420898 [ 1030/ 1682]\n",
      "loss: 538.527954 [ 1040/ 1682]\n",
      "loss: 345.889526 [ 1050/ 1682]\n",
      "loss: 526.574829 [ 1060/ 1682]\n",
      "loss: 655.720032 [ 1070/ 1682]\n",
      "loss: 947.924622 [ 1080/ 1682]\n",
      "loss: 1268.515625 [ 1090/ 1682]\n",
      "loss: 1559.100464 [ 1100/ 1682]\n",
      "loss: 2036.914429 [ 1110/ 1682]\n",
      "loss: 2415.423096 [ 1120/ 1682]\n",
      "loss: 2768.338379 [ 1130/ 1682]\n",
      "loss: 3342.860840 [ 1140/ 1682]\n",
      "loss: 5136.759766 [ 1150/ 1682]\n",
      "loss: 6783.765625 [ 1160/ 1682]\n",
      "loss: 4009.410889 [ 1170/ 1682]\n",
      "loss: 4492.170898 [ 1180/ 1682]\n",
      "loss: 5039.045410 [ 1190/ 1682]\n",
      "loss: 4942.970215 [ 1200/ 1682]\n",
      "loss: 5083.689941 [ 1210/ 1682]\n",
      "loss: 5065.641602 [ 1220/ 1682]\n",
      "loss: 6324.593750 [ 1230/ 1682]\n",
      "loss: 7595.938477 [ 1240/ 1682]\n",
      "loss: 6838.656250 [ 1250/ 1682]\n",
      "loss: 8454.104492 [ 1260/ 1682]\n",
      "loss: 7207.156250 [ 1270/ 1682]\n",
      "loss: 6961.627441 [ 1280/ 1682]\n",
      "loss: 6065.516113 [ 1290/ 1682]\n",
      "loss: 6123.830078 [ 1300/ 1682]\n",
      "loss: 6748.899902 [ 1310/ 1682]\n",
      "loss: 8102.799805 [ 1320/ 1682]\n",
      "loss: 7016.350586 [ 1330/ 1682]\n",
      "loss: 6277.810547 [ 1340/ 1682]\n",
      "loss: 6195.364258 [ 1350/ 1682]\n",
      "loss: 7550.964844 [ 1360/ 1682]\n",
      "loss: 8399.633789 [ 1370/ 1682]\n",
      "loss: 10435.500977 [ 1380/ 1682]\n",
      "loss: 10595.243164 [ 1390/ 1682]\n",
      "loss: 10822.075195 [ 1400/ 1682]\n",
      "loss: 11325.166016 [ 1410/ 1682]\n",
      "loss: 11541.897461 [ 1420/ 1682]\n",
      "loss: 9566.145508 [ 1430/ 1682]\n",
      "loss: 9183.174805 [ 1440/ 1682]\n",
      "loss: 9661.539062 [ 1450/ 1682]\n",
      "loss: 11305.995117 [ 1460/ 1682]\n",
      "loss: 12947.224609 [ 1470/ 1682]\n",
      "loss: 15881.103516 [ 1480/ 1682]\n",
      "loss: 17244.960938 [ 1490/ 1682]\n",
      "loss: 17556.707031 [ 1500/ 1682]\n",
      "loss: 15094.853516 [ 1510/ 1682]\n",
      "loss: 16541.816406 [ 1520/ 1682]\n",
      "loss: 15445.346680 [ 1530/ 1682]\n",
      "loss: 12267.060547 [ 1540/ 1682]\n",
      "loss: 12299.732422 [ 1550/ 1682]\n",
      "loss: 16575.957031 [ 1560/ 1682]\n",
      "loss: 15094.337891 [ 1570/ 1682]\n",
      "loss: 11861.884766 [ 1580/ 1682]\n",
      "loss: 10434.267578 [ 1590/ 1682]\n",
      "loss: 10515.706055 [ 1600/ 1682]\n",
      "loss: 8869.557617 [ 1610/ 1682]\n",
      "loss: 7805.352539 [ 1620/ 1682]\n",
      "loss: 10400.240234 [ 1630/ 1682]\n",
      "loss: 12153.976562 [ 1640/ 1682]\n",
      "loss: 15839.809570 [ 1650/ 1682]\n",
      "loss: 15054.343750 [ 1660/ 1682]\n",
      "loss: 12724.225586 [ 1670/ 1682]\n",
      "loss: 11894.837891 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 14161.367826 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 851.957703 [    0/ 1682]\n",
      "loss: 420.197601 [   10/ 1682]\n",
      "loss: 424.651215 [   20/ 1682]\n",
      "loss: 644.990967 [   30/ 1682]\n",
      "loss: 354.478607 [   40/ 1682]\n",
      "loss: 879.488098 [   50/ 1682]\n",
      "loss: 300.885345 [   60/ 1682]\n",
      "loss: 360.465027 [   70/ 1682]\n",
      "loss: 1613.611084 [   80/ 1682]\n",
      "loss: 1024.635498 [   90/ 1682]\n",
      "loss: 398.514832 [  100/ 1682]\n",
      "loss: 418.998535 [  110/ 1682]\n",
      "loss: 414.982971 [  120/ 1682]\n",
      "loss: 950.563354 [  130/ 1682]\n",
      "loss: 323.175781 [  140/ 1682]\n",
      "loss: 751.718872 [  150/ 1682]\n",
      "loss: 319.504395 [  160/ 1682]\n",
      "loss: 654.510010 [  170/ 1682]\n",
      "loss: 258.243073 [  180/ 1682]\n",
      "loss: 236.428589 [  190/ 1682]\n",
      "loss: 601.630066 [  200/ 1682]\n",
      "loss: 806.523560 [  210/ 1682]\n",
      "loss: 341.458099 [  220/ 1682]\n",
      "loss: 258.484528 [  230/ 1682]\n",
      "loss: 480.928070 [  240/ 1682]\n",
      "loss: 226.857300 [  250/ 1682]\n",
      "loss: 200.996185 [  260/ 1682]\n",
      "loss: 136.153732 [  270/ 1682]\n",
      "loss: 452.541901 [  280/ 1682]\n",
      "loss: 101.092758 [  290/ 1682]\n",
      "loss: 89.592094 [  300/ 1682]\n",
      "loss: 81.916855 [  310/ 1682]\n",
      "loss: 82.084335 [  320/ 1682]\n",
      "loss: 50.463501 [  330/ 1682]\n",
      "loss: 40.475777 [  340/ 1682]\n",
      "loss: 46.090134 [  350/ 1682]\n",
      "loss: 69.804245 [  360/ 1682]\n",
      "loss: 74.902672 [  370/ 1682]\n",
      "loss: 48.242657 [  380/ 1682]\n",
      "loss: 38.561466 [  390/ 1682]\n",
      "loss: 279.810425 [  400/ 1682]\n",
      "loss: 304.580170 [  410/ 1682]\n",
      "loss: 155.610474 [  420/ 1682]\n",
      "loss: 37.689857 [  430/ 1682]\n",
      "loss: 31.942087 [  440/ 1682]\n",
      "loss: 56.817150 [  450/ 1682]\n",
      "loss: 2.278552 [  460/ 1682]\n",
      "loss: 3.191029 [  470/ 1682]\n",
      "loss: 3.187171 [  480/ 1682]\n",
      "loss: 3.183591 [  490/ 1682]\n",
      "loss: 0.733378 [  500/ 1682]\n",
      "loss: 7.308854 [  510/ 1682]\n",
      "loss: 16.765726 [  520/ 1682]\n",
      "loss: 1.538122 [  530/ 1682]\n",
      "loss: 0.408484 [  540/ 1682]\n",
      "loss: 190.180908 [  550/ 1682]\n",
      "loss: 22.120108 [  560/ 1682]\n",
      "loss: 215.215332 [  570/ 1682]\n",
      "loss: 3.448095 [  580/ 1682]\n",
      "loss: 215.011917 [  590/ 1682]\n",
      "loss: 111.947533 [  600/ 1682]\n",
      "loss: 12.439921 [  610/ 1682]\n",
      "loss: 5.133728 [  620/ 1682]\n",
      "loss: 11.003133 [  630/ 1682]\n",
      "loss: 193.483093 [  640/ 1682]\n",
      "loss: 69.711555 [  650/ 1682]\n",
      "loss: 231.592499 [  660/ 1682]\n",
      "loss: 108.309242 [  670/ 1682]\n",
      "loss: 230.499756 [  680/ 1682]\n",
      "loss: 104.706284 [  690/ 1682]\n",
      "loss: 209.653000 [  700/ 1682]\n",
      "loss: 118.988571 [  710/ 1682]\n",
      "loss: 1.290841 [  720/ 1682]\n",
      "loss: 79.356827 [  730/ 1682]\n",
      "loss: 198.351074 [  740/ 1682]\n",
      "loss: 36.597427 [  750/ 1682]\n",
      "loss: 44.771358 [  760/ 1682]\n",
      "loss: 1.336152 [  770/ 1682]\n",
      "loss: 14.872217 [  780/ 1682]\n",
      "loss: 2.174556 [  790/ 1682]\n",
      "loss: 122.965866 [  800/ 1682]\n",
      "loss: 29.429193 [  810/ 1682]\n",
      "loss: 47.681885 [  820/ 1682]\n",
      "loss: 41.705997 [  830/ 1682]\n",
      "loss: 112.274498 [  840/ 1682]\n",
      "loss: 53.355885 [  850/ 1682]\n",
      "loss: 29.623236 [  860/ 1682]\n",
      "loss: 147.389725 [  870/ 1682]\n",
      "loss: 56.247765 [  880/ 1682]\n",
      "loss: 48.166840 [  890/ 1682]\n",
      "loss: 63.222572 [  900/ 1682]\n",
      "loss: 67.703789 [  910/ 1682]\n",
      "loss: 212.386917 [  920/ 1682]\n",
      "loss: 132.657272 [  930/ 1682]\n",
      "loss: 205.328583 [  940/ 1682]\n",
      "loss: 312.959717 [  950/ 1682]\n",
      "loss: 457.862366 [  960/ 1682]\n",
      "loss: 474.788757 [  970/ 1682]\n",
      "loss: 533.681824 [  980/ 1682]\n",
      "loss: 713.403687 [  990/ 1682]\n",
      "loss: 1083.229248 [ 1000/ 1682]\n",
      "loss: 1203.565186 [ 1010/ 1682]\n",
      "loss: 1308.579224 [ 1020/ 1682]\n",
      "loss: 922.105835 [ 1030/ 1682]\n",
      "loss: 591.321350 [ 1040/ 1682]\n",
      "loss: 312.613312 [ 1050/ 1682]\n",
      "loss: 462.604675 [ 1060/ 1682]\n",
      "loss: 605.618164 [ 1070/ 1682]\n",
      "loss: 994.390320 [ 1080/ 1682]\n",
      "loss: 1208.302490 [ 1090/ 1682]\n",
      "loss: 1218.619629 [ 1100/ 1682]\n",
      "loss: 1871.404297 [ 1110/ 1682]\n",
      "loss: 2332.152832 [ 1120/ 1682]\n",
      "loss: 2679.102295 [ 1130/ 1682]\n",
      "loss: 3575.408203 [ 1140/ 1682]\n",
      "loss: 5015.064941 [ 1150/ 1682]\n",
      "loss: 6643.790527 [ 1160/ 1682]\n",
      "loss: 4321.064941 [ 1170/ 1682]\n",
      "loss: 4700.647461 [ 1180/ 1682]\n",
      "loss: 5412.082031 [ 1190/ 1682]\n",
      "loss: 4823.559082 [ 1200/ 1682]\n",
      "loss: 4345.029785 [ 1210/ 1682]\n",
      "loss: 5372.403320 [ 1220/ 1682]\n",
      "loss: 5840.463867 [ 1230/ 1682]\n",
      "loss: 7447.661133 [ 1240/ 1682]\n",
      "loss: 7233.743652 [ 1250/ 1682]\n",
      "loss: 8297.813477 [ 1260/ 1682]\n",
      "loss: 8248.778320 [ 1270/ 1682]\n",
      "loss: 6819.706055 [ 1280/ 1682]\n",
      "loss: 5485.040527 [ 1290/ 1682]\n",
      "loss: 5990.642578 [ 1300/ 1682]\n",
      "loss: 6513.416992 [ 1310/ 1682]\n",
      "loss: 7949.434570 [ 1320/ 1682]\n",
      "loss: 7289.610840 [ 1330/ 1682]\n",
      "loss: 6632.693848 [ 1340/ 1682]\n",
      "loss: 6598.173340 [ 1350/ 1682]\n",
      "loss: 7059.196777 [ 1360/ 1682]\n",
      "loss: 8574.797852 [ 1370/ 1682]\n",
      "loss: 9671.828125 [ 1380/ 1682]\n",
      "loss: 9791.669922 [ 1390/ 1682]\n",
      "loss: 10227.500977 [ 1400/ 1682]\n",
      "loss: 10545.349609 [ 1410/ 1682]\n",
      "loss: 11358.626953 [ 1420/ 1682]\n",
      "loss: 9926.401367 [ 1430/ 1682]\n",
      "loss: 9566.924805 [ 1440/ 1682]\n",
      "loss: 10918.518555 [ 1450/ 1682]\n",
      "loss: 10493.497070 [ 1460/ 1682]\n",
      "loss: 12510.189453 [ 1470/ 1682]\n",
      "loss: 14993.260742 [ 1480/ 1682]\n",
      "loss: 16071.489258 [ 1490/ 1682]\n",
      "loss: 17030.761719 [ 1500/ 1682]\n",
      "loss: 14608.857422 [ 1510/ 1682]\n",
      "loss: 16322.265625 [ 1520/ 1682]\n",
      "loss: 15233.210938 [ 1530/ 1682]\n",
      "loss: 13988.309570 [ 1540/ 1682]\n",
      "loss: 13929.950195 [ 1550/ 1682]\n",
      "loss: 16387.175781 [ 1560/ 1682]\n",
      "loss: 14884.416016 [ 1570/ 1682]\n",
      "loss: 13272.215820 [ 1580/ 1682]\n",
      "loss: 10259.965820 [ 1590/ 1682]\n",
      "loss: 10340.517578 [ 1600/ 1682]\n",
      "loss: 8708.941406 [ 1610/ 1682]\n",
      "loss: 8825.626953 [ 1620/ 1682]\n",
      "loss: 10712.874023 [ 1630/ 1682]\n",
      "loss: 12603.201172 [ 1640/ 1682]\n",
      "loss: 15624.208984 [ 1650/ 1682]\n",
      "loss: 14844.195312 [ 1660/ 1682]\n",
      "loss: 11569.594727 [ 1670/ 1682]\n",
      "loss: 11707.868164 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13991.933143 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 438.291351 [    0/ 1682]\n",
      "loss: 456.222900 [   10/ 1682]\n",
      "loss: 956.227539 [   20/ 1682]\n",
      "loss: 503.628906 [   30/ 1682]\n",
      "loss: 387.649963 [   40/ 1682]\n",
      "loss: 349.617126 [   50/ 1682]\n",
      "loss: 331.507507 [   60/ 1682]\n",
      "loss: 393.859406 [   70/ 1682]\n",
      "loss: 489.191895 [   80/ 1682]\n",
      "loss: 450.851013 [   90/ 1682]\n",
      "loss: 741.572144 [  100/ 1682]\n",
      "loss: 455.041809 [  110/ 1682]\n",
      "loss: 450.864258 [  120/ 1682]\n",
      "loss: 425.830078 [  130/ 1682]\n",
      "loss: 354.933289 [  140/ 1682]\n",
      "loss: 390.843323 [  150/ 1682]\n",
      "loss: 351.104187 [  160/ 1682]\n",
      "loss: 319.131165 [  170/ 1682]\n",
      "loss: 286.746857 [  180/ 1682]\n",
      "loss: 680.188171 [  190/ 1682]\n",
      "loss: 602.454163 [  200/ 1682]\n",
      "loss: 328.914978 [  210/ 1682]\n",
      "loss: 311.042786 [  220/ 1682]\n",
      "loss: 286.995300 [  230/ 1682]\n",
      "loss: 256.975922 [  240/ 1682]\n",
      "loss: 253.640106 [  250/ 1682]\n",
      "loss: 405.350891 [  260/ 1682]\n",
      "loss: 157.055328 [  270/ 1682]\n",
      "loss: 407.317322 [  280/ 1682]\n",
      "loss: 465.517181 [  290/ 1682]\n",
      "loss: 153.186630 [  300/ 1682]\n",
      "loss: 98.305801 [  310/ 1682]\n",
      "loss: 98.467529 [  320/ 1682]\n",
      "loss: 63.385487 [  330/ 1682]\n",
      "loss: 318.745270 [  340/ 1682]\n",
      "loss: 58.482738 [  350/ 1682]\n",
      "loss: 307.470490 [  360/ 1682]\n",
      "loss: 90.562302 [  370/ 1682]\n",
      "loss: 60.961327 [  380/ 1682]\n",
      "loss: 49.893654 [  390/ 1682]\n",
      "loss: 175.793213 [  400/ 1682]\n",
      "loss: 668.118652 [  410/ 1682]\n",
      "loss: 38.989502 [  420/ 1682]\n",
      "loss: 188.861420 [  430/ 1682]\n",
      "loss: 42.382175 [  440/ 1682]\n",
      "loss: 25.712656 [  450/ 1682]\n",
      "loss: 5.477215 [  460/ 1682]\n",
      "loss: 6.902850 [  470/ 1682]\n",
      "loss: 6.895925 [  480/ 1682]\n",
      "loss: 6.889455 [  490/ 1682]\n",
      "loss: 159.844147 [  500/ 1682]\n",
      "loss: 40.741390 [  510/ 1682]\n",
      "loss: 24.071981 [  520/ 1682]\n",
      "loss: 3.858187 [  530/ 1682]\n",
      "loss: 21.818323 [  540/ 1682]\n",
      "loss: 10.034026 [  550/ 1682]\n",
      "loss: 5.149723 [  560/ 1682]\n",
      "loss: 14.399519 [  570/ 1682]\n",
      "loss: 1.640101 [  580/ 1682]\n",
      "loss: 1.266320 [  590/ 1682]\n",
      "loss: 260.471527 [  600/ 1682]\n",
      "loss: 1.994628 [  610/ 1682]\n",
      "loss: 2.121543 [  620/ 1682]\n",
      "loss: 171.298477 [  630/ 1682]\n",
      "loss: 26.522293 [  640/ 1682]\n",
      "loss: 111.732971 [  650/ 1682]\n",
      "loss: 99.544312 [  660/ 1682]\n",
      "loss: 91.242905 [  670/ 1682]\n",
      "loss: 111.699387 [  680/ 1682]\n",
      "loss: 165.131378 [  690/ 1682]\n",
      "loss: 74.174110 [  700/ 1682]\n",
      "loss: 22.691668 [  710/ 1682]\n",
      "loss: 1.537688 [  720/ 1682]\n",
      "loss: 8.578626 [  730/ 1682]\n",
      "loss: 41.866985 [  740/ 1682]\n",
      "loss: 186.718414 [  750/ 1682]\n",
      "loss: 31.797083 [  760/ 1682]\n",
      "loss: 3.983546 [  770/ 1682]\n",
      "loss: 24.218685 [  780/ 1682]\n",
      "loss: 1.565319 [  790/ 1682]\n",
      "loss: 6.826803 [  800/ 1682]\n",
      "loss: 20.936413 [  810/ 1682]\n",
      "loss: 36.618305 [  820/ 1682]\n",
      "loss: 32.013588 [  830/ 1682]\n",
      "loss: 3.070178 [  840/ 1682]\n",
      "loss: 7.747354 [  850/ 1682]\n",
      "loss: 21.130577 [  860/ 1682]\n",
      "loss: 30.981140 [  870/ 1682]\n",
      "loss: 44.155037 [  880/ 1682]\n",
      "loss: 90.303253 [  890/ 1682]\n",
      "loss: 50.444641 [  900/ 1682]\n",
      "loss: 54.412773 [  910/ 1682]\n",
      "loss: 105.025253 [  920/ 1682]\n",
      "loss: 113.710388 [  930/ 1682]\n",
      "loss: 187.138077 [  940/ 1682]\n",
      "loss: 284.586761 [  950/ 1682]\n",
      "loss: 389.118469 [  960/ 1682]\n",
      "loss: 364.739532 [  970/ 1682]\n",
      "loss: 550.216187 [  980/ 1682]\n",
      "loss: 743.219788 [  990/ 1682]\n",
      "loss: 1027.608276 [ 1000/ 1682]\n",
      "loss: 1144.861328 [ 1010/ 1682]\n",
      "loss: 1247.307861 [ 1020/ 1682]\n",
      "loss: 871.298340 [ 1030/ 1682]\n",
      "loss: 510.004700 [ 1040/ 1682]\n",
      "loss: 285.727936 [ 1050/ 1682]\n",
      "loss: 408.739746 [ 1060/ 1682]\n",
      "loss: 637.011230 [ 1070/ 1682]\n",
      "loss: 941.224426 [ 1080/ 1682]\n",
      "loss: 1149.488770 [ 1090/ 1682]\n",
      "loss: 1427.073364 [ 1100/ 1682]\n",
      "loss: 1885.381836 [ 1110/ 1682]\n",
      "loss: 2065.641113 [ 1120/ 1682]\n",
      "loss: 2591.169434 [ 1130/ 1682]\n",
      "loss: 3137.129395 [ 1140/ 1682]\n",
      "loss: 4729.041016 [ 1150/ 1682]\n",
      "loss: 6058.630371 [ 1160/ 1682]\n",
      "loss: 4229.154297 [ 1170/ 1682]\n",
      "loss: 4584.026367 [ 1180/ 1682]\n",
      "loss: 5286.924805 [ 1190/ 1682]\n",
      "loss: 4433.551758 [ 1200/ 1682]\n",
      "loss: 4780.218750 [ 1210/ 1682]\n",
      "loss: 5247.678711 [ 1220/ 1682]\n",
      "loss: 6055.353027 [ 1230/ 1682]\n",
      "loss: 7104.508789 [ 1240/ 1682]\n",
      "loss: 7088.875000 [ 1250/ 1682]\n",
      "loss: 7734.145996 [ 1260/ 1682]\n",
      "loss: 7870.103027 [ 1270/ 1682]\n",
      "loss: 6218.622070 [ 1280/ 1682]\n",
      "loss: 5801.960938 [ 1290/ 1682]\n",
      "loss: 5858.947754 [ 1300/ 1682]\n",
      "loss: 6366.103027 [ 1310/ 1682]\n",
      "loss: 6887.256836 [ 1320/ 1682]\n",
      "loss: 6932.369629 [ 1330/ 1682]\n",
      "loss: 6494.139160 [ 1340/ 1682]\n",
      "loss: 6460.007812 [ 1350/ 1682]\n",
      "loss: 7256.583496 [ 1360/ 1682]\n",
      "loss: 8241.212891 [ 1370/ 1682]\n",
      "loss: 10088.815430 [ 1380/ 1682]\n",
      "loss: 9606.591797 [ 1390/ 1682]\n",
      "loss: 10468.970703 [ 1400/ 1682]\n",
      "loss: 10963.896484 [ 1410/ 1682]\n",
      "loss: 9783.397461 [ 1420/ 1682]\n",
      "loss: 8643.245117 [ 1430/ 1682]\n",
      "loss: 9153.607422 [ 1440/ 1682]\n",
      "loss: 10740.737305 [ 1450/ 1682]\n",
      "loss: 10945.076172 [ 1460/ 1682]\n",
      "loss: 12561.032227 [ 1470/ 1682]\n",
      "loss: 14314.224609 [ 1480/ 1682]\n",
      "loss: 16097.132812 [ 1490/ 1682]\n",
      "loss: 17106.382812 [ 1500/ 1682]\n",
      "loss: 14677.796875 [ 1510/ 1682]\n",
      "loss: 15746.978516 [ 1520/ 1682]\n",
      "loss: 14085.409180 [ 1530/ 1682]\n",
      "loss: 12708.218750 [ 1540/ 1682]\n",
      "loss: 13729.562500 [ 1550/ 1682]\n",
      "loss: 15148.856445 [ 1560/ 1682]\n",
      "loss: 14677.002930 [ 1570/ 1682]\n",
      "loss: 12314.095703 [ 1580/ 1682]\n",
      "loss: 10088.189453 [ 1590/ 1682]\n",
      "loss: 9886.997070 [ 1600/ 1682]\n",
      "loss: 8245.522461 [ 1610/ 1682]\n",
      "loss: 8885.871094 [ 1620/ 1682]\n",
      "loss: 10207.935547 [ 1630/ 1682]\n",
      "loss: 12676.044922 [ 1640/ 1682]\n",
      "loss: 14675.453125 [ 1650/ 1682]\n",
      "loss: 14637.627930 [ 1660/ 1682]\n",
      "loss: 12341.085938 [ 1670/ 1682]\n",
      "loss: 11524.397461 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 13795.797251 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 474.654846 [    0/ 1682]\n",
      "loss: 493.291199 [   10/ 1682]\n",
      "loss: 498.138489 [   20/ 1682]\n",
      "loss: 468.938324 [   30/ 1682]\n",
      "loss: 1139.024414 [   40/ 1682]\n",
      "loss: 932.236328 [   50/ 1682]\n",
      "loss: 486.746246 [   60/ 1682]\n",
      "loss: 457.492920 [   70/ 1682]\n",
      "loss: 634.933960 [   80/ 1682]\n",
      "loss: 487.639832 [   90/ 1682]\n",
      "loss: 469.785736 [  100/ 1682]\n",
      "loss: 605.581177 [  110/ 1682]\n",
      "loss: 808.601318 [  120/ 1682]\n",
      "loss: 1621.640747 [  130/ 1682]\n",
      "loss: 2045.522217 [  140/ 1682]\n",
      "loss: 461.563538 [  150/ 1682]\n",
      "loss: 1864.998779 [  160/ 1682]\n",
      "loss: 356.105347 [  170/ 1682]\n",
      "loss: 1405.215576 [  180/ 1682]\n",
      "loss: 619.071411 [  190/ 1682]\n",
      "loss: 316.601654 [  200/ 1682]\n",
      "loss: 360.204163 [  210/ 1682]\n",
      "loss: 341.464355 [  220/ 1682]\n",
      "loss: 581.828430 [  230/ 1682]\n",
      "loss: 284.657837 [  240/ 1682]\n",
      "loss: 693.276489 [  250/ 1682]\n",
      "loss: 252.180328 [  260/ 1682]\n",
      "loss: 178.795074 [  270/ 1682]\n",
      "loss: 148.109543 [  280/ 1682]\n",
      "loss: 416.636719 [  290/ 1682]\n",
      "loss: 402.640503 [  300/ 1682]\n",
      "loss: 679.269653 [  310/ 1682]\n",
      "loss: 115.781372 [  320/ 1682]\n",
      "loss: 155.902802 [  330/ 1682]\n",
      "loss: 256.134888 [  340/ 1682]\n",
      "loss: 268.329102 [  350/ 1682]\n",
      "loss: 101.046959 [  360/ 1682]\n",
      "loss: 107.147118 [  370/ 1682]\n",
      "loss: 74.695786 [  380/ 1682]\n",
      "loss: 62.283630 [  390/ 1682]\n",
      "loss: 44.092377 [  400/ 1682]\n",
      "loss: 374.598053 [  410/ 1682]\n",
      "loss: 264.166504 [  420/ 1682]\n",
      "loss: 61.362999 [  430/ 1682]\n",
      "loss: 53.908180 [  440/ 1682]\n",
      "loss: 207.102325 [  450/ 1682]\n",
      "loss: 9.988791 [  460/ 1682]\n",
      "loss: 11.910949 [  470/ 1682]\n",
      "loss: 203.043945 [  480/ 1682]\n",
      "loss: 11.890002 [  490/ 1682]\n",
      "loss: 6.047865 [  500/ 1682]\n",
      "loss: 366.145660 [  510/ 1682]\n",
      "loss: 197.324738 [  520/ 1682]\n",
      "loss: 7.225124 [  530/ 1682]\n",
      "loss: 78.143906 [  540/ 1682]\n",
      "loss: 117.836632 [  550/ 1682]\n",
      "loss: 102.885483 [  560/ 1682]\n",
      "loss: 21.032360 [  570/ 1682]\n",
      "loss: 26.212723 [  580/ 1682]\n",
      "loss: 0.087112 [  590/ 1682]\n",
      "loss: 2.598340 [  600/ 1682]\n",
      "loss: 0.647567 [  610/ 1682]\n",
      "loss: 0.607700 [  620/ 1682]\n",
      "loss: 2.764763 [  630/ 1682]\n",
      "loss: 19.201479 [  640/ 1682]\n",
      "loss: 48.241508 [  650/ 1682]\n",
      "loss: 83.764084 [  660/ 1682]\n",
      "loss: 76.126282 [  670/ 1682]\n",
      "loss: 94.924957 [  680/ 1682]\n",
      "loss: 73.556160 [  690/ 1682]\n",
      "loss: 60.718300 [  700/ 1682]\n",
      "loss: 49150.906250 [  710/ 1682]\n",
      "loss: 3.178310 [  720/ 1682]\n",
      "loss: 13.882932 [  730/ 1682]\n",
      "loss: 53.175850 [  740/ 1682]\n",
      "loss: 194.431381 [  750/ 1682]\n",
      "loss: 173.786591 [  760/ 1682]\n",
      "loss: 7.977607 [  770/ 1682]\n",
      "loss: 5.129510 [  780/ 1682]\n",
      "loss: 77.616287 [  790/ 1682]\n",
      "loss: 3.297302 [  800/ 1682]\n",
      "loss: 14.056585 [  810/ 1682]\n",
      "loss: 128.405930 [  820/ 1682]\n",
      "loss: 23.957352 [  830/ 1682]\n",
      "loss: 245.012360 [  840/ 1682]\n",
      "loss: 5.387958 [  850/ 1682]\n",
      "loss: 14.246565 [  860/ 1682]\n",
      "loss: 22.421619 [  870/ 1682]\n",
      "loss: 33.752689 [  880/ 1682]\n",
      "loss: 28.045475 [  890/ 1682]\n",
      "loss: 39.373394 [  900/ 1682]\n",
      "loss: 42.839520 [  910/ 1682]\n",
      "loss: 84.983116 [  920/ 1682]\n",
      "loss: 386.488373 [  930/ 1682]\n",
      "loss: 206.055695 [  940/ 1682]\n",
      "loss: 227.881012 [  950/ 1682]\n",
      "loss: 372.369385 [  960/ 1682]\n",
      "loss: 458.940979 [  970/ 1682]\n",
      "loss: 396.100250 [  980/ 1682]\n",
      "loss: 490.252106 [  990/ 1682]\n",
      "loss: 974.900391 [ 1000/ 1682]\n",
      "loss: 860.825684 [ 1010/ 1682]\n",
      "loss: 1080.787109 [ 1020/ 1682]\n",
      "loss: 666.031006 [ 1030/ 1682]\n",
      "loss: 461.737976 [ 1040/ 1682]\n",
      "loss: 239.409882 [ 1050/ 1682]\n",
      "loss: 275.842285 [ 1060/ 1682]\n",
      "loss: 541.474426 [ 1070/ 1682]\n",
      "loss: 649.919434 [ 1080/ 1682]\n",
      "loss: 920.589661 [ 1090/ 1682]\n",
      "loss: 928.066040 [ 1100/ 1682]\n",
      "loss: 1304.971069 [ 1110/ 1682]\n",
      "loss: 1775.361694 [ 1120/ 1682]\n",
      "loss: 2396.064941 [ 1130/ 1682]\n",
      "loss: 3261.650879 [ 1140/ 1682]\n",
      "loss: 3216.468018 [ 1150/ 1682]\n",
      "loss: 6374.109863 [ 1160/ 1682]\n",
      "loss: 3703.381348 [ 1170/ 1682]\n",
      "loss: 3535.507812 [ 1180/ 1682]\n",
      "loss: 3876.557129 [ 1190/ 1682]\n",
      "loss: 4146.939453 [ 1200/ 1682]\n",
      "loss: 4694.505859 [ 1210/ 1682]\n",
      "loss: 4800.477051 [ 1220/ 1682]\n",
      "loss: 5557.768555 [ 1230/ 1682]\n",
      "loss: 6593.282227 [ 1240/ 1682]\n",
      "loss: 6532.574707 [ 1250/ 1682]\n",
      "loss: 7998.143066 [ 1260/ 1682]\n",
      "loss: 6962.242188 [ 1270/ 1682]\n",
      "loss: 5416.635742 [ 1280/ 1682]\n",
      "loss: 4363.375977 [ 1290/ 1682]\n",
      "loss: 3392.656738 [ 1300/ 1682]\n",
      "loss: 5118.375977 [ 1310/ 1682]\n",
      "loss: 6410.912109 [ 1320/ 1682]\n",
      "loss: 5847.357910 [ 1330/ 1682]\n",
      "loss: 5418.737305 [ 1340/ 1682]\n",
      "loss: 5851.054199 [ 1350/ 1682]\n",
      "loss: 6566.290039 [ 1360/ 1682]\n",
      "loss: 7828.551758 [ 1370/ 1682]\n",
      "loss: 9623.681641 [ 1380/ 1682]\n",
      "loss: 7871.115723 [ 1390/ 1682]\n",
      "loss: 8279.345703 [ 1400/ 1682]\n",
      "loss: 8438.422852 [ 1410/ 1682]\n",
      "loss: 9150.367188 [ 1420/ 1682]\n",
      "loss: 7244.969727 [ 1430/ 1682]\n",
      "loss: 7691.345215 [ 1440/ 1682]\n",
      "loss: 8773.164062 [ 1450/ 1682]\n",
      "loss: 7854.639160 [ 1460/ 1682]\n",
      "loss: 11192.129883 [ 1470/ 1682]\n",
      "loss: 11681.494141 [ 1480/ 1682]\n",
      "loss: 10921.789062 [ 1490/ 1682]\n",
      "loss: 12772.990234 [ 1500/ 1682]\n",
      "loss: 11709.132812 [ 1510/ 1682]\n",
      "loss: 13861.721680 [ 1520/ 1682]\n",
      "loss: 10566.661133 [ 1530/ 1682]\n",
      "loss: 12245.416992 [ 1540/ 1682]\n",
      "loss: 10613.042969 [ 1550/ 1682]\n",
      "loss: 15117.395508 [ 1560/ 1682]\n",
      "loss: 11687.403320 [ 1570/ 1682]\n",
      "loss: 11697.755859 [ 1580/ 1682]\n",
      "loss: 8365.130859 [ 1590/ 1682]\n",
      "loss: 5327.651855 [ 1600/ 1682]\n",
      "loss: 8010.047852 [ 1610/ 1682]\n",
      "loss: 7483.676758 [ 1620/ 1682]\n",
      "loss: 5739.161621 [ 1630/ 1682]\n",
      "loss: 11479.722656 [ 1640/ 1682]\n",
      "loss: 13734.286133 [ 1650/ 1682]\n",
      "loss: 9093.088867 [ 1660/ 1682]\n",
      "loss: 10932.652344 [ 1670/ 1682]\n",
      "loss: 11367.948242 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11315.159856 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 4400.694824 [    0/ 1682]\n",
      "loss: 2806.352539 [   10/ 1682]\n",
      "loss: 4058.660889 [   20/ 1682]\n",
      "loss: 2111.881836 [   30/ 1682]\n",
      "loss: 2910.971436 [   40/ 1682]\n",
      "loss: 2627.855957 [   50/ 1682]\n",
      "loss: 3125.630859 [   60/ 1682]\n",
      "loss: 4360.639648 [   70/ 1682]\n",
      "loss: 1228.363525 [   80/ 1682]\n",
      "loss: 519.060059 [   90/ 1682]\n",
      "loss: 3698.197998 [  100/ 1682]\n",
      "loss: 1370.143188 [  110/ 1682]\n",
      "loss: 1274.777710 [  120/ 1682]\n",
      "loss: 1559.411865 [  130/ 1682]\n",
      "loss: 912.981628 [  140/ 1682]\n",
      "loss: 1487.320801 [  150/ 1682]\n",
      "loss: 1573.755859 [  160/ 1682]\n",
      "loss: 1803.955322 [  170/ 1682]\n",
      "loss: 1033.337280 [  180/ 1682]\n",
      "loss: 315.508453 [  190/ 1682]\n",
      "loss: 1168.441406 [  200/ 1682]\n",
      "loss: 1246.536865 [  210/ 1682]\n",
      "loss: 678.710327 [  220/ 1682]\n",
      "loss: 517.135132 [  230/ 1682]\n",
      "loss: 355.598328 [  240/ 1682]\n",
      "loss: 1011.233398 [  250/ 1682]\n",
      "loss: 476.757996 [  260/ 1682]\n",
      "loss: 316.250000 [  270/ 1682]\n",
      "loss: 304.192657 [  280/ 1682]\n",
      "loss: 438.820801 [  290/ 1682]\n",
      "loss: 215.964279 [  300/ 1682]\n",
      "loss: 130.348343 [  310/ 1682]\n",
      "loss: 343.259216 [  320/ 1682]\n",
      "loss: 150.420624 [  330/ 1682]\n",
      "loss: 232.086700 [  340/ 1682]\n",
      "loss: 83.513290 [  350/ 1682]\n",
      "loss: 190.572876 [  360/ 1682]\n",
      "loss: 323.742493 [  370/ 1682]\n",
      "loss: 238.835129 [  380/ 1682]\n",
      "loss: 148.966949 [  390/ 1682]\n",
      "loss: 102.781494 [  400/ 1682]\n",
      "loss: 107.994362 [  410/ 1682]\n",
      "loss: 131.703140 [  420/ 1682]\n",
      "loss: 137.969086 [  430/ 1682]\n",
      "loss: 101.179520 [  440/ 1682]\n",
      "loss: 81.651337 [  450/ 1682]\n",
      "loss: 57.675194 [  460/ 1682]\n",
      "loss: 16.825390 [  470/ 1682]\n",
      "loss: 16.810123 [  480/ 1682]\n",
      "loss: 75.009743 [  490/ 1682]\n",
      "loss: 15.856092 [  500/ 1682]\n",
      "loss: 33.671471 [  510/ 1682]\n",
      "loss: 67.812874 [  520/ 1682]\n",
      "loss: 11.358509 [  530/ 1682]\n",
      "loss: 16.974840 [  540/ 1682]\n",
      "loss: 45.027912 [  550/ 1682]\n",
      "loss: 23.503658 [  560/ 1682]\n",
      "loss: 87.052101 [  570/ 1682]\n",
      "loss: 3.259492 [  580/ 1682]\n",
      "loss: 1.775420 [  590/ 1682]\n",
      "loss: 2.349753 [  600/ 1682]\n",
      "loss: 1.558505 [  610/ 1682]\n",
      "loss: 0.955236 [  620/ 1682]\n",
      "loss: 18.615726 [  630/ 1682]\n",
      "loss: 14.403723 [  640/ 1682]\n",
      "loss: 34.910435 [  650/ 1682]\n",
      "loss: 70.456207 [  660/ 1682]\n",
      "loss: 59.395607 [  670/ 1682]\n",
      "loss: 90.720230 [  680/ 1682]\n",
      "loss: 62.534264 [  690/ 1682]\n",
      "loss: 51.584118 [  700/ 1682]\n",
      "loss: 12.514981 [  710/ 1682]\n",
      "loss: 35.227871 [  720/ 1682]\n",
      "loss: 36.683037 [  730/ 1682]\n",
      "loss: 92.512100 [  740/ 1682]\n",
      "loss: 130.550720 [  750/ 1682]\n",
      "loss: 63.208111 [  760/ 1682]\n",
      "loss: 11.017439 [  770/ 1682]\n",
      "loss: 10.112938 [  780/ 1682]\n",
      "loss: 11.149130 [  790/ 1682]\n",
      "loss: 4.288141 [  800/ 1682]\n",
      "loss: 9.728042 [  810/ 1682]\n",
      "loss: 25.452921 [  820/ 1682]\n",
      "loss: 27.106770 [  830/ 1682]\n",
      "loss: 2.435746 [  840/ 1682]\n",
      "loss: 8.853086 [  850/ 1682]\n",
      "loss: 13.411160 [  860/ 1682]\n",
      "loss: 20.692497 [  870/ 1682]\n",
      "loss: 23.002384 [  880/ 1682]\n",
      "loss: 22.497883 [  890/ 1682]\n",
      "loss: 47.249588 [  900/ 1682]\n",
      "loss: 34.950260 [  910/ 1682]\n",
      "loss: 87.119011 [  920/ 1682]\n",
      "loss: 95.423706 [  930/ 1682]\n",
      "loss: 126.913269 [  940/ 1682]\n",
      "loss: 226.246490 [  950/ 1682]\n",
      "loss: 355.053650 [  960/ 1682]\n",
      "loss: 368.033966 [  970/ 1682]\n",
      "loss: 465.684021 [  980/ 1682]\n",
      "loss: 654.128357 [  990/ 1682]\n",
      "loss: 916.128601 [ 1000/ 1682]\n",
      "loss: 1018.396301 [ 1010/ 1682]\n",
      "loss: 1100.592041 [ 1020/ 1682]\n",
      "loss: 712.276062 [ 1030/ 1682]\n",
      "loss: 462.580261 [ 1040/ 1682]\n",
      "loss: 183.200531 [ 1050/ 1682]\n",
      "loss: 386.904572 [ 1060/ 1682]\n",
      "loss: 567.284302 [ 1070/ 1682]\n",
      "loss: 835.667297 [ 1080/ 1682]\n",
      "loss: 1011.958008 [ 1090/ 1682]\n",
      "loss: 1269.567261 [ 1100/ 1682]\n",
      "loss: 1606.735962 [ 1110/ 1682]\n",
      "loss: 1952.025757 [ 1120/ 1682]\n",
      "loss: 2094.283691 [ 1130/ 1682]\n",
      "loss: 3304.188232 [ 1140/ 1682]\n",
      "loss: 4174.931641 [ 1150/ 1682]\n",
      "loss: 5803.877930 [ 1160/ 1682]\n",
      "loss: 4334.825195 [ 1170/ 1682]\n",
      "loss: 4080.387939 [ 1180/ 1682]\n",
      "loss: 4685.913086 [ 1190/ 1682]\n",
      "loss: 4003.779297 [ 1200/ 1682]\n",
      "loss: 4158.038574 [ 1210/ 1682]\n",
      "loss: 4600.616211 [ 1220/ 1682]\n",
      "loss: 5004.045898 [ 1230/ 1682]\n",
      "loss: 6645.229492 [ 1240/ 1682]\n",
      "loss: 6842.979492 [ 1250/ 1682]\n",
      "loss: 7589.179688 [ 1260/ 1682]\n",
      "loss: 6565.274414 [ 1270/ 1682]\n",
      "loss: 5555.004883 [ 1280/ 1682]\n",
      "loss: 5012.232910 [ 1290/ 1682]\n",
      "loss: 5326.744629 [ 1300/ 1682]\n",
      "loss: 6539.167969 [ 1310/ 1682]\n",
      "loss: 6094.506348 [ 1320/ 1682]\n",
      "loss: 5103.559570 [ 1330/ 1682]\n",
      "loss: 4828.873535 [ 1340/ 1682]\n",
      "loss: 6225.742676 [ 1350/ 1682]\n",
      "loss: 5997.813477 [ 1360/ 1682]\n",
      "loss: 7218.291992 [ 1370/ 1682]\n",
      "loss: 9795.810547 [ 1380/ 1682]\n",
      "loss: 9370.739258 [ 1390/ 1682]\n",
      "loss: 9675.668945 [ 1400/ 1682]\n",
      "loss: 9644.095703 [ 1410/ 1682]\n",
      "loss: 10412.916992 [ 1420/ 1682]\n",
      "loss: 8376.601562 [ 1430/ 1682]\n",
      "loss: 8496.882812 [ 1440/ 1682]\n",
      "loss: 9136.596680 [ 1450/ 1682]\n",
      "loss: 10640.233398 [ 1460/ 1682]\n",
      "loss: 11750.368164 [ 1470/ 1682]\n",
      "loss: 13318.966797 [ 1480/ 1682]\n",
      "loss: 15582.013672 [ 1490/ 1682]\n",
      "loss: 14115.924805 [ 1500/ 1682]\n",
      "loss: 13337.870117 [ 1510/ 1682]\n",
      "loss: 14782.921875 [ 1520/ 1682]\n",
      "loss: 12130.463867 [ 1530/ 1682]\n",
      "loss: 11871.267578 [ 1540/ 1682]\n",
      "loss: 13389.018555 [ 1550/ 1682]\n",
      "loss: 16512.453125 [ 1560/ 1682]\n",
      "loss: 13172.841797 [ 1570/ 1682]\n",
      "loss: 11668.318359 [ 1580/ 1682]\n",
      "loss: 9168.224609 [ 1590/ 1682]\n",
      "loss: 9874.990234 [ 1600/ 1682]\n",
      "loss: 6713.454102 [ 1610/ 1682]\n",
      "loss: 8023.318848 [ 1620/ 1682]\n",
      "loss: 8679.744141 [ 1630/ 1682]\n",
      "loss: 10887.853516 [ 1640/ 1682]\n",
      "loss: 11933.880859 [ 1650/ 1682]\n",
      "loss: 13374.625000 [ 1660/ 1682]\n",
      "loss: 10896.493164 [ 1670/ 1682]\n",
      "loss: 11212.828125 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 12163.038443 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 3236.313965 [    0/ 1682]\n",
      "loss: 1102.762207 [   10/ 1682]\n",
      "loss: 2446.858887 [   20/ 1682]\n",
      "loss: 1663.214111 [   30/ 1682]\n",
      "loss: 1484.315063 [   40/ 1682]\n",
      "loss: 1396.226318 [   50/ 1682]\n",
      "loss: 1396.378662 [   60/ 1682]\n",
      "loss: 1647.643311 [   70/ 1682]\n",
      "loss: 1285.213013 [   80/ 1682]\n",
      "loss: 838.692993 [   90/ 1682]\n",
      "loss: 855.932495 [  100/ 1682]\n",
      "loss: 1131.352661 [  110/ 1682]\n",
      "loss: 1338.612671 [  120/ 1682]\n",
      "loss: 1881.149170 [  130/ 1682]\n",
      "loss: 2419.359863 [  140/ 1682]\n",
      "loss: 692.964600 [  150/ 1682]\n",
      "loss: 1974.022461 [  160/ 1682]\n",
      "loss: 1823.070923 [  170/ 1682]\n",
      "loss: 767.839355 [  180/ 1682]\n",
      "loss: 1122.999268 [  190/ 1682]\n",
      "loss: 1802.501221 [  200/ 1682]\n",
      "loss: 1949.838257 [  210/ 1682]\n",
      "loss: 860.276978 [  220/ 1682]\n",
      "loss: 1574.181396 [  230/ 1682]\n",
      "loss: 739.132690 [  240/ 1682]\n",
      "loss: 946.461609 [  250/ 1682]\n",
      "loss: 456.801605 [  260/ 1682]\n",
      "loss: 761.159912 [  270/ 1682]\n",
      "loss: 184.809067 [  280/ 1682]\n",
      "loss: 173.766006 [  290/ 1682]\n",
      "loss: 715.653687 [  300/ 1682]\n",
      "loss: 306.308289 [  310/ 1682]\n",
      "loss: 952.817505 [  320/ 1682]\n",
      "loss: 806.690063 [  330/ 1682]\n",
      "loss: 407.464203 [  340/ 1682]\n",
      "loss: 368.247467 [  350/ 1682]\n",
      "loss: 131.494904 [  360/ 1682]\n",
      "loss: 365.568573 [  370/ 1682]\n",
      "loss: 400.940033 [  380/ 1682]\n",
      "loss: 419.139252 [  390/ 1682]\n",
      "loss: 190.180008 [  400/ 1682]\n",
      "loss: 54.070374 [  410/ 1682]\n",
      "loss: 71.836121 [  420/ 1682]\n",
      "loss: 85.409996 [  430/ 1682]\n",
      "loss: 205.187408 [  440/ 1682]\n",
      "loss: 178.661224 [  450/ 1682]\n",
      "loss: 303.561096 [  460/ 1682]\n",
      "loss: 77.821152 [  470/ 1682]\n",
      "loss: 234.053543 [  480/ 1682]\n",
      "loss: 204.706665 [  490/ 1682]\n",
      "loss: 68.407341 [  500/ 1682]\n",
      "loss: 415.720551 [  510/ 1682]\n",
      "loss: 201.281158 [  520/ 1682]\n",
      "loss: 44.647343 [  530/ 1682]\n",
      "loss: 299.862244 [  540/ 1682]\n",
      "loss: 151.437592 [  550/ 1682]\n",
      "loss: 141.759720 [  560/ 1682]\n",
      "loss: 140.607956 [  570/ 1682]\n",
      "loss: 104.485229 [  580/ 1682]\n",
      "loss: 43.077744 [  590/ 1682]\n",
      "loss: 17.035606 [  600/ 1682]\n",
      "loss: 77.711365 [  610/ 1682]\n",
      "loss: 73.953278 [  620/ 1682]\n",
      "loss: 7.825205 [  630/ 1682]\n",
      "loss: 89.077675 [  640/ 1682]\n",
      "loss: 53.308430 [  650/ 1682]\n",
      "loss: 70.279266 [  660/ 1682]\n",
      "loss: 83.590164 [  670/ 1682]\n",
      "loss: 69.874855 [  680/ 1682]\n",
      "loss: 62.189411 [  690/ 1682]\n",
      "loss: 82.505478 [  700/ 1682]\n",
      "loss: 84.857658 [  710/ 1682]\n",
      "loss: 108.092201 [  720/ 1682]\n",
      "loss: 91.239243 [  730/ 1682]\n",
      "loss: 233.891083 [  740/ 1682]\n",
      "loss: 153.102020 [  750/ 1682]\n",
      "loss: 138.005524 [  760/ 1682]\n",
      "loss: 164.147675 [  770/ 1682]\n",
      "loss: 103.072853 [  780/ 1682]\n",
      "loss: 34.078346 [  790/ 1682]\n",
      "loss: 78.031937 [  800/ 1682]\n",
      "loss: 45.608746 [  810/ 1682]\n",
      "loss: 25.822926 [  820/ 1682]\n",
      "loss: 11.474197 [  830/ 1682]\n",
      "loss: 61.068886 [  840/ 1682]\n",
      "loss: 46.425705 [  850/ 1682]\n",
      "loss: 65.276390 [  860/ 1682]\n",
      "loss: 33.084213 [  870/ 1682]\n",
      "loss: 35.977501 [  880/ 1682]\n",
      "loss: 43.790222 [  890/ 1682]\n",
      "loss: 44.448227 [  900/ 1682]\n",
      "loss: 24.596914 [  910/ 1682]\n",
      "loss: 62.958122 [  920/ 1682]\n",
      "loss: 71.829391 [  930/ 1682]\n",
      "loss: 85.609940 [  940/ 1682]\n",
      "loss: 154.545624 [  950/ 1682]\n",
      "loss: 306.575317 [  960/ 1682]\n",
      "loss: 317.719666 [  970/ 1682]\n",
      "loss: 411.916199 [  980/ 1682]\n",
      "loss: 580.423828 [  990/ 1682]\n",
      "loss: 637.425537 [ 1000/ 1682]\n",
      "loss: 790.957397 [ 1010/ 1682]\n",
      "loss: 958.556641 [ 1020/ 1682]\n",
      "loss: 748.078247 [ 1030/ 1682]\n",
      "loss: 367.975037 [ 1040/ 1682]\n",
      "loss: 132.584198 [ 1050/ 1682]\n",
      "loss: 287.461090 [ 1060/ 1682]\n",
      "loss: 383.795044 [ 1070/ 1682]\n",
      "loss: 624.688965 [ 1080/ 1682]\n",
      "loss: 872.994507 [ 1090/ 1682]\n",
      "loss: 1004.803711 [ 1100/ 1682]\n",
      "loss: 1514.994507 [ 1110/ 1682]\n",
      "loss: 1706.545288 [ 1120/ 1682]\n",
      "loss: 2232.087402 [ 1130/ 1682]\n",
      "loss: 2942.661133 [ 1140/ 1682]\n",
      "loss: 3702.168457 [ 1150/ 1682]\n",
      "loss: 5303.296387 [ 1160/ 1682]\n",
      "loss: 3888.623047 [ 1170/ 1682]\n",
      "loss: 3398.521973 [ 1180/ 1682]\n",
      "loss: 4591.225098 [ 1190/ 1682]\n",
      "loss: 3691.208252 [ 1200/ 1682]\n",
      "loss: 4701.726562 [ 1210/ 1682]\n",
      "loss: 4702.588379 [ 1220/ 1682]\n",
      "loss: 5484.801758 [ 1230/ 1682]\n",
      "loss: 6660.830566 [ 1240/ 1682]\n",
      "loss: 5862.282715 [ 1250/ 1682]\n",
      "loss: 7113.288086 [ 1260/ 1682]\n",
      "loss: 7181.591309 [ 1270/ 1682]\n",
      "loss: 5399.959473 [ 1280/ 1682]\n",
      "loss: 4320.641113 [ 1290/ 1682]\n",
      "loss: 5231.880371 [ 1300/ 1682]\n",
      "loss: 5890.597656 [ 1310/ 1682]\n",
      "loss: 7083.459473 [ 1320/ 1682]\n",
      "loss: 6781.321289 [ 1330/ 1682]\n",
      "loss: 5513.592285 [ 1340/ 1682]\n",
      "loss: 5485.867188 [ 1350/ 1682]\n",
      "loss: 6193.726562 [ 1360/ 1682]\n",
      "loss: 7145.334473 [ 1370/ 1682]\n",
      "loss: 8472.466797 [ 1380/ 1682]\n",
      "loss: 9622.244141 [ 1390/ 1682]\n",
      "loss: 8585.693359 [ 1400/ 1682]\n",
      "loss: 10077.933594 [ 1410/ 1682]\n",
      "loss: 9789.208984 [ 1420/ 1682]\n",
      "loss: 8237.834961 [ 1430/ 1682]\n",
      "loss: 7995.548340 [ 1440/ 1682]\n",
      "loss: 9848.561523 [ 1450/ 1682]\n",
      "loss: 8309.169922 [ 1460/ 1682]\n",
      "loss: 10139.381836 [ 1470/ 1682]\n",
      "loss: 14133.703125 [ 1480/ 1682]\n",
      "loss: 15554.041992 [ 1490/ 1682]\n",
      "loss: 16543.300781 [ 1500/ 1682]\n",
      "loss: 11694.017578 [ 1510/ 1682]\n",
      "loss: 15559.119141 [ 1520/ 1682]\n",
      "loss: 12274.767578 [ 1530/ 1682]\n",
      "loss: 11162.089844 [ 1540/ 1682]\n",
      "loss: 12405.548828 [ 1550/ 1682]\n",
      "loss: 15548.549805 [ 1560/ 1682]\n",
      "loss: 14156.115234 [ 1570/ 1682]\n",
      "loss: 10903.167969 [ 1580/ 1682]\n",
      "loss: 8279.044922 [ 1590/ 1682]\n",
      "loss: 7422.073242 [ 1600/ 1682]\n",
      "loss: 7843.151367 [ 1610/ 1682]\n",
      "loss: 7917.165039 [ 1620/ 1682]\n",
      "loss: 8810.922852 [ 1630/ 1682]\n",
      "loss: 12192.982422 [ 1640/ 1682]\n",
      "loss: 13859.298828 [ 1650/ 1682]\n",
      "loss: 13318.998047 [ 1660/ 1682]\n",
      "loss: 9380.029297 [ 1670/ 1682]\n",
      "loss: 7417.562012 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11492.554274 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1609.206787 [    0/ 1682]\n",
      "loss: 2248.893555 [   10/ 1682]\n",
      "loss: 1921.547241 [   20/ 1682]\n",
      "loss: 1629.896973 [   30/ 1682]\n",
      "loss: 515.318542 [   40/ 1682]\n",
      "loss: 2150.530762 [   50/ 1682]\n",
      "loss: 2120.659668 [   60/ 1682]\n",
      "loss: 2220.229736 [   70/ 1682]\n",
      "loss: 2577.746094 [   80/ 1682]\n",
      "loss: 2366.589600 [   90/ 1682]\n",
      "loss: 1044.548828 [  100/ 1682]\n",
      "loss: 1695.726562 [  110/ 1682]\n",
      "loss: 1183.751465 [  120/ 1682]\n",
      "loss: 1880.815674 [  130/ 1682]\n",
      "loss: 839.438782 [  140/ 1682]\n",
      "loss: 909.234863 [  150/ 1682]\n",
      "loss: 472.010681 [  160/ 1682]\n",
      "loss: 1568.668213 [  170/ 1682]\n",
      "loss: 396.737518 [  180/ 1682]\n",
      "loss: 667.905701 [  190/ 1682]\n",
      "loss: 1443.937256 [  200/ 1682]\n",
      "loss: 445.827545 [  210/ 1682]\n",
      "loss: 820.249329 [  220/ 1682]\n",
      "loss: 2168.887939 [  230/ 1682]\n",
      "loss: 1080.921509 [  240/ 1682]\n",
      "loss: 701.034546 [  250/ 1682]\n",
      "loss: 880.615601 [  260/ 1682]\n",
      "loss: 1270.824585 [  270/ 1682]\n",
      "loss: 204.356979 [  280/ 1682]\n",
      "loss: 901.905396 [  290/ 1682]\n",
      "loss: 352.778381 [  300/ 1682]\n",
      "loss: 517.727051 [  310/ 1682]\n",
      "loss: 901.641785 [  320/ 1682]\n",
      "loss: 570.577087 [  330/ 1682]\n",
      "loss: 570.015503 [  340/ 1682]\n",
      "loss: 615.874146 [  350/ 1682]\n",
      "loss: 686.779785 [  360/ 1682]\n",
      "loss: 604.514343 [  370/ 1682]\n",
      "loss: 378.413269 [  380/ 1682]\n",
      "loss: 427.300690 [  390/ 1682]\n",
      "loss: 496.165436 [  400/ 1682]\n",
      "loss: 439.499908 [  410/ 1682]\n",
      "loss: 84.085457 [  420/ 1682]\n",
      "loss: 530.930298 [  430/ 1682]\n",
      "loss: 276.463257 [  440/ 1682]\n",
      "loss: 155.695999 [  450/ 1682]\n",
      "loss: 219.696365 [  460/ 1682]\n",
      "loss: 125.239334 [  470/ 1682]\n",
      "loss: 178.977448 [  480/ 1682]\n",
      "loss: 202.830353 [  490/ 1682]\n",
      "loss: 187.620514 [  500/ 1682]\n",
      "loss: 217.128586 [  510/ 1682]\n",
      "loss: 331.930450 [  520/ 1682]\n",
      "loss: 22.831047 [  530/ 1682]\n",
      "loss: 179.772430 [  540/ 1682]\n",
      "loss: 36.570568 [  550/ 1682]\n",
      "loss: 186.407562 [  560/ 1682]\n",
      "loss: 204.459991 [  570/ 1682]\n",
      "loss: 99.606216 [  580/ 1682]\n",
      "loss: 152.534637 [  590/ 1682]\n",
      "loss: 16.068914 [  600/ 1682]\n",
      "loss: 91.784653 [  610/ 1682]\n",
      "loss: 187.049850 [  620/ 1682]\n",
      "loss: 81.691704 [  630/ 1682]\n",
      "loss: 16.138378 [  640/ 1682]\n",
      "loss: 121.493713 [  650/ 1682]\n",
      "loss: 47.324669 [  660/ 1682]\n",
      "loss: 63.893688 [  670/ 1682]\n",
      "loss: 53.951984 [  680/ 1682]\n",
      "loss: 58.365051 [  690/ 1682]\n",
      "loss: 40.672272 [  700/ 1682]\n",
      "loss: 49.832672 [  710/ 1682]\n",
      "loss: 138.831573 [  720/ 1682]\n",
      "loss: 146.745468 [  730/ 1682]\n",
      "loss: 173.789276 [  740/ 1682]\n",
      "loss: 270.463135 [  750/ 1682]\n",
      "loss: 95.520493 [  760/ 1682]\n",
      "loss: 135.656113 [  770/ 1682]\n",
      "loss: 98.839874 [  780/ 1682]\n",
      "loss: 42.453697 [  790/ 1682]\n",
      "loss: 21.986294 [  800/ 1682]\n",
      "loss: 58.684887 [  810/ 1682]\n",
      "loss: 40.414211 [  820/ 1682]\n",
      "loss: 18.733891 [  830/ 1682]\n",
      "loss: 97.491119 [  840/ 1682]\n",
      "loss: 49.093723 [  850/ 1682]\n",
      "loss: 36.881889 [  860/ 1682]\n",
      "loss: 35.522942 [  870/ 1682]\n",
      "loss: 17.488340 [  880/ 1682]\n",
      "loss: 14.144444 [  890/ 1682]\n",
      "loss: 42.099686 [  900/ 1682]\n",
      "loss: 24.807426 [  910/ 1682]\n",
      "loss: 50.173939 [  920/ 1682]\n",
      "loss: 54.491505 [  930/ 1682]\n",
      "loss: 100.375290 [  940/ 1682]\n",
      "loss: 158.323227 [  950/ 1682]\n",
      "loss: 229.288422 [  960/ 1682]\n",
      "loss: 211.934082 [  970/ 1682]\n",
      "loss: 388.252045 [  980/ 1682]\n",
      "loss: 517.237183 [  990/ 1682]\n",
      "loss: 740.283813 [ 1000/ 1682]\n",
      "loss: 852.936340 [ 1010/ 1682]\n",
      "loss: 859.600403 [ 1020/ 1682]\n",
      "loss: 539.929321 [ 1030/ 1682]\n",
      "loss: 403.483826 [ 1040/ 1682]\n",
      "loss: 177.819077 [ 1050/ 1682]\n",
      "loss: 340.289978 [ 1060/ 1682]\n",
      "loss: 344.085876 [ 1070/ 1682]\n",
      "loss: 682.508850 [ 1080/ 1682]\n",
      "loss: 835.620789 [ 1090/ 1682]\n",
      "loss: 1070.522583 [ 1100/ 1682]\n",
      "loss: 1405.813843 [ 1110/ 1682]\n",
      "loss: 1794.705322 [ 1120/ 1682]\n",
      "loss: 2083.161621 [ 1130/ 1682]\n",
      "loss: 2999.271240 [ 1140/ 1682]\n",
      "loss: 4257.298340 [ 1150/ 1682]\n",
      "loss: 5792.911133 [ 1160/ 1682]\n",
      "loss: 4016.414551 [ 1170/ 1682]\n",
      "loss: 3733.100830 [ 1180/ 1682]\n",
      "loss: 4241.291992 [ 1190/ 1682]\n",
      "loss: 4043.269531 [ 1200/ 1682]\n",
      "loss: 4424.660156 [ 1210/ 1682]\n",
      "loss: 4387.605957 [ 1220/ 1682]\n",
      "loss: 5180.106445 [ 1230/ 1682]\n",
      "loss: 6455.266113 [ 1240/ 1682]\n",
      "loss: 6470.922852 [ 1250/ 1682]\n",
      "loss: 7635.508789 [ 1260/ 1682]\n",
      "loss: 6788.341309 [ 1270/ 1682]\n",
      "loss: 5542.147949 [ 1280/ 1682]\n",
      "loss: 5184.226074 [ 1290/ 1682]\n",
      "loss: 5173.972168 [ 1300/ 1682]\n",
      "loss: 5374.341797 [ 1310/ 1682]\n",
      "loss: 7069.323242 [ 1320/ 1682]\n",
      "loss: 6400.045898 [ 1330/ 1682]\n",
      "loss: 5269.135254 [ 1340/ 1682]\n",
      "loss: 5558.587891 [ 1350/ 1682]\n",
      "loss: 6514.407715 [ 1360/ 1682]\n",
      "loss: 7814.395996 [ 1370/ 1682]\n",
      "loss: 9522.525391 [ 1380/ 1682]\n",
      "loss: 8870.794922 [ 1390/ 1682]\n",
      "loss: 9112.990234 [ 1400/ 1682]\n",
      "loss: 9008.596680 [ 1410/ 1682]\n",
      "loss: 9797.982422 [ 1420/ 1682]\n",
      "loss: 7645.377930 [ 1430/ 1682]\n",
      "loss: 7843.031250 [ 1440/ 1682]\n",
      "loss: 9397.184570 [ 1450/ 1682]\n",
      "loss: 9525.800781 [ 1460/ 1682]\n",
      "loss: 8966.732422 [ 1470/ 1682]\n",
      "loss: 13770.661133 [ 1480/ 1682]\n",
      "loss: 14713.981445 [ 1490/ 1682]\n",
      "loss: 14378.518555 [ 1500/ 1682]\n",
      "loss: 13153.518555 [ 1510/ 1682]\n",
      "loss: 14402.140625 [ 1520/ 1682]\n",
      "loss: 12120.916016 [ 1530/ 1682]\n",
      "loss: 12666.572266 [ 1540/ 1682]\n",
      "loss: 12784.478516 [ 1550/ 1682]\n",
      "loss: 14006.984375 [ 1560/ 1682]\n",
      "loss: 12795.252930 [ 1570/ 1682]\n",
      "loss: 11902.045898 [ 1580/ 1682]\n",
      "loss: 7984.592773 [ 1590/ 1682]\n",
      "loss: 7711.188965 [ 1600/ 1682]\n",
      "loss: 7427.602539 [ 1610/ 1682]\n",
      "loss: 6823.417969 [ 1620/ 1682]\n",
      "loss: 8768.456055 [ 1630/ 1682]\n",
      "loss: 11246.193359 [ 1640/ 1682]\n",
      "loss: 14711.841797 [ 1650/ 1682]\n",
      "loss: 12379.803711 [ 1660/ 1682]\n",
      "loss: 10249.055664 [ 1670/ 1682]\n",
      "loss: 10919.976562 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11894.854549 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1912.823608 [    0/ 1682]\n",
      "loss: 1948.443604 [   10/ 1682]\n",
      "loss: 1708.694580 [   20/ 1682]\n",
      "loss: 1117.768799 [   30/ 1682]\n",
      "loss: 1423.692627 [   40/ 1682]\n",
      "loss: 1038.148438 [   50/ 1682]\n",
      "loss: 1562.758667 [   60/ 1682]\n",
      "loss: 1167.558350 [   70/ 1682]\n",
      "loss: 1200.535400 [   80/ 1682]\n",
      "loss: 2177.452637 [   90/ 1682]\n",
      "loss: 1803.915771 [  100/ 1682]\n",
      "loss: 1490.173218 [  110/ 1682]\n",
      "loss: 1614.800415 [  120/ 1682]\n",
      "loss: 1239.157471 [  130/ 1682]\n",
      "loss: 507.281799 [  140/ 1682]\n",
      "loss: 823.576660 [  150/ 1682]\n",
      "loss: 1846.835938 [  160/ 1682]\n",
      "loss: 463.882416 [  170/ 1682]\n",
      "loss: 876.355835 [  180/ 1682]\n",
      "loss: 1416.621582 [  190/ 1682]\n",
      "loss: 1241.943481 [  200/ 1682]\n",
      "loss: 1238.306763 [  210/ 1682]\n",
      "loss: 1147.179321 [  220/ 1682]\n",
      "loss: 810.813293 [  230/ 1682]\n",
      "loss: 1090.987549 [  240/ 1682]\n",
      "loss: 820.160767 [  250/ 1682]\n",
      "loss: 777.148254 [  260/ 1682]\n",
      "loss: 762.994812 [  270/ 1682]\n",
      "loss: 666.518433 [  280/ 1682]\n",
      "loss: 795.196045 [  290/ 1682]\n",
      "loss: 448.073639 [  300/ 1682]\n",
      "loss: 546.918640 [  310/ 1682]\n",
      "loss: 478.076721 [  320/ 1682]\n",
      "loss: 421.080170 [  330/ 1682]\n",
      "loss: 421.264954 [  340/ 1682]\n",
      "loss: 127.240456 [  350/ 1682]\n",
      "loss: 777.445435 [  360/ 1682]\n",
      "loss: 173.063568 [  370/ 1682]\n",
      "loss: 424.398590 [  380/ 1682]\n",
      "loss: 222.918991 [  390/ 1682]\n",
      "loss: 439.528961 [  400/ 1682]\n",
      "loss: 433.957947 [  410/ 1682]\n",
      "loss: 340.098969 [  420/ 1682]\n",
      "loss: 113.024681 [  430/ 1682]\n",
      "loss: 102.737656 [  440/ 1682]\n",
      "loss: 194.869278 [  450/ 1682]\n",
      "loss: 118.384354 [  460/ 1682]\n",
      "loss: 212.565918 [  470/ 1682]\n",
      "loss: 38.815594 [  480/ 1682]\n",
      "loss: 245.404694 [  490/ 1682]\n",
      "loss: 182.551834 [  500/ 1682]\n",
      "loss: 185.438232 [  510/ 1682]\n",
      "loss: 226.789017 [  520/ 1682]\n",
      "loss: 193.511810 [  530/ 1682]\n",
      "loss: 183.418900 [  540/ 1682]\n",
      "loss: 45.415504 [  550/ 1682]\n",
      "loss: 351.498505 [  560/ 1682]\n",
      "loss: 241.744598 [  570/ 1682]\n",
      "loss: 121.635963 [  580/ 1682]\n",
      "loss: 125.486572 [  590/ 1682]\n",
      "loss: 40.404591 [  600/ 1682]\n",
      "loss: 151.393616 [  610/ 1682]\n",
      "loss: 70.088806 [  620/ 1682]\n",
      "loss: 123.621201 [  630/ 1682]\n",
      "loss: 93.847816 [  640/ 1682]\n",
      "loss: 63.380848 [  650/ 1682]\n",
      "loss: 77.903389 [  660/ 1682]\n",
      "loss: 47.297932 [  670/ 1682]\n",
      "loss: 108.993752 [  680/ 1682]\n",
      "loss: 36.807224 [  690/ 1682]\n",
      "loss: 50.974464 [  700/ 1682]\n",
      "loss: 46.088032 [  710/ 1682]\n",
      "loss: 124.608475 [  720/ 1682]\n",
      "loss: 109.548340 [  730/ 1682]\n",
      "loss: 148.189789 [  740/ 1682]\n",
      "loss: 110.005821 [  750/ 1682]\n",
      "loss: 333.128021 [  760/ 1682]\n",
      "loss: 257.526154 [  770/ 1682]\n",
      "loss: 24.908501 [  780/ 1682]\n",
      "loss: 72.728874 [  790/ 1682]\n",
      "loss: 72.774033 [  800/ 1682]\n",
      "loss: 1.319791 [  810/ 1682]\n",
      "loss: 106.095276 [  820/ 1682]\n",
      "loss: 7.379777 [  830/ 1682]\n",
      "loss: 164.840698 [  840/ 1682]\n",
      "loss: 73.053833 [  850/ 1682]\n",
      "loss: 73.999985 [  860/ 1682]\n",
      "loss: 35.452843 [  870/ 1682]\n",
      "loss: 9.520271 [  880/ 1682]\n",
      "loss: 43.848930 [  890/ 1682]\n",
      "loss: 26.996510 [  900/ 1682]\n",
      "loss: 39.560394 [  910/ 1682]\n",
      "loss: 44.675911 [  920/ 1682]\n",
      "loss: 45.385174 [  930/ 1682]\n",
      "loss: 108.153404 [  940/ 1682]\n",
      "loss: 139.941086 [  950/ 1682]\n",
      "loss: 209.530930 [  960/ 1682]\n",
      "loss: 216.886795 [  970/ 1682]\n",
      "loss: 314.548523 [  980/ 1682]\n",
      "loss: 509.483582 [  990/ 1682]\n",
      "loss: 647.650024 [ 1000/ 1682]\n",
      "loss: 726.053650 [ 1010/ 1682]\n",
      "loss: 940.577026 [ 1020/ 1682]\n",
      "loss: 599.919312 [ 1030/ 1682]\n",
      "loss: 384.692322 [ 1040/ 1682]\n",
      "loss: 176.935684 [ 1050/ 1682]\n",
      "loss: 222.557648 [ 1060/ 1682]\n",
      "loss: 430.606262 [ 1070/ 1682]\n",
      "loss: 508.131256 [ 1080/ 1682]\n",
      "loss: 920.833374 [ 1090/ 1682]\n",
      "loss: 1022.892761 [ 1100/ 1682]\n",
      "loss: 1382.839600 [ 1110/ 1682]\n",
      "loss: 1461.717041 [ 1120/ 1682]\n",
      "loss: 1708.792725 [ 1130/ 1682]\n",
      "loss: 2874.162109 [ 1140/ 1682]\n",
      "loss: 3851.651123 [ 1150/ 1682]\n",
      "loss: 5199.419434 [ 1160/ 1682]\n",
      "loss: 3913.930176 [ 1170/ 1682]\n",
      "loss: 3554.453613 [ 1180/ 1682]\n",
      "loss: 4783.368652 [ 1190/ 1682]\n",
      "loss: 4017.134033 [ 1200/ 1682]\n",
      "loss: 4681.896484 [ 1210/ 1682]\n",
      "loss: 4189.674805 [ 1220/ 1682]\n",
      "loss: 5041.564453 [ 1230/ 1682]\n",
      "loss: 6205.685547 [ 1240/ 1682]\n",
      "loss: 5980.485352 [ 1250/ 1682]\n",
      "loss: 7344.809570 [ 1260/ 1682]\n",
      "loss: 6856.286133 [ 1270/ 1682]\n",
      "loss: 5402.689941 [ 1280/ 1682]\n",
      "loss: 4745.520020 [ 1290/ 1682]\n",
      "loss: 4102.231445 [ 1300/ 1682]\n",
      "loss: 5838.138184 [ 1310/ 1682]\n",
      "loss: 6609.962402 [ 1320/ 1682]\n",
      "loss: 5982.851074 [ 1330/ 1682]\n",
      "loss: 5935.022461 [ 1340/ 1682]\n",
      "loss: 5271.177246 [ 1350/ 1682]\n",
      "loss: 5641.166504 [ 1360/ 1682]\n",
      "loss: 6533.065430 [ 1370/ 1682]\n",
      "loss: 8224.339844 [ 1380/ 1682]\n",
      "loss: 8813.408203 [ 1390/ 1682]\n",
      "loss: 8512.923828 [ 1400/ 1682]\n",
      "loss: 9769.764648 [ 1410/ 1682]\n",
      "loss: 9616.404297 [ 1420/ 1682]\n",
      "loss: 8170.840820 [ 1430/ 1682]\n",
      "loss: 8354.361328 [ 1440/ 1682]\n",
      "loss: 9212.592773 [ 1450/ 1682]\n",
      "loss: 9257.963867 [ 1460/ 1682]\n",
      "loss: 10079.153320 [ 1470/ 1682]\n",
      "loss: 14140.736328 [ 1480/ 1682]\n",
      "loss: 14683.931641 [ 1490/ 1682]\n",
      "loss: 14878.943359 [ 1500/ 1682]\n",
      "loss: 13181.814453 [ 1510/ 1682]\n",
      "loss: 13162.955078 [ 1520/ 1682]\n",
      "loss: 11812.614258 [ 1530/ 1682]\n",
      "loss: 12968.197266 [ 1540/ 1682]\n",
      "loss: 10338.330078 [ 1550/ 1682]\n",
      "loss: 14618.841797 [ 1560/ 1682]\n",
      "loss: 12526.975586 [ 1570/ 1682]\n",
      "loss: 9546.752930 [ 1580/ 1682]\n",
      "loss: 7826.929688 [ 1590/ 1682]\n",
      "loss: 6746.893555 [ 1600/ 1682]\n",
      "loss: 6737.730469 [ 1610/ 1682]\n",
      "loss: 7099.131836 [ 1620/ 1682]\n",
      "loss: 7239.723633 [ 1630/ 1682]\n",
      "loss: 11260.802734 [ 1640/ 1682]\n",
      "loss: 12124.574219 [ 1650/ 1682]\n",
      "loss: 11892.533203 [ 1660/ 1682]\n",
      "loss: 9310.017578 [ 1670/ 1682]\n",
      "loss: 7177.947266 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11430.947716 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2330.563721 [    0/ 1682]\n",
      "loss: 1751.649780 [   10/ 1682]\n",
      "loss: 3048.142090 [   20/ 1682]\n",
      "loss: 1224.146973 [   30/ 1682]\n",
      "loss: 1212.654663 [   40/ 1682]\n",
      "loss: 1516.747314 [   50/ 1682]\n",
      "loss: 1596.110596 [   60/ 1682]\n",
      "loss: 586.173645 [   70/ 1682]\n",
      "loss: 1757.489258 [   80/ 1682]\n",
      "loss: 996.633484 [   90/ 1682]\n",
      "loss: 1572.704102 [  100/ 1682]\n",
      "loss: 1379.682617 [  110/ 1682]\n",
      "loss: 1319.052856 [  120/ 1682]\n",
      "loss: 624.727661 [  130/ 1682]\n",
      "loss: 824.191284 [  140/ 1682]\n",
      "loss: 2503.520996 [  150/ 1682]\n",
      "loss: 1537.689209 [  160/ 1682]\n",
      "loss: 1334.732056 [  170/ 1682]\n",
      "loss: 949.465515 [  180/ 1682]\n",
      "loss: 802.948608 [  190/ 1682]\n",
      "loss: 1875.021851 [  200/ 1682]\n",
      "loss: 505.254303 [  210/ 1682]\n",
      "loss: 1619.216797 [  220/ 1682]\n",
      "loss: 1044.940063 [  230/ 1682]\n",
      "loss: 713.552979 [  240/ 1682]\n",
      "loss: 1224.532104 [  250/ 1682]\n",
      "loss: 1246.222778 [  260/ 1682]\n",
      "loss: 1113.192139 [  270/ 1682]\n",
      "loss: 757.472961 [  280/ 1682]\n",
      "loss: 965.726562 [  290/ 1682]\n",
      "loss: 537.973999 [  300/ 1682]\n",
      "loss: 714.982483 [  310/ 1682]\n",
      "loss: 737.639099 [  320/ 1682]\n",
      "loss: 828.109985 [  330/ 1682]\n",
      "loss: 133.004288 [  340/ 1682]\n",
      "loss: 296.979095 [  350/ 1682]\n",
      "loss: 677.234253 [  360/ 1682]\n",
      "loss: 248.066574 [  370/ 1682]\n",
      "loss: 630.303772 [  380/ 1682]\n",
      "loss: 685.683533 [  390/ 1682]\n",
      "loss: 336.612854 [  400/ 1682]\n",
      "loss: 761.592285 [  410/ 1682]\n",
      "loss: 306.167389 [  420/ 1682]\n",
      "loss: 403.466492 [  430/ 1682]\n",
      "loss: 471.771820 [  440/ 1682]\n",
      "loss: 413.844330 [  450/ 1682]\n",
      "loss: 354.810242 [  460/ 1682]\n",
      "loss: 354.779510 [  470/ 1682]\n",
      "loss: 244.545563 [  480/ 1682]\n",
      "loss: 336.727631 [  490/ 1682]\n",
      "loss: 244.392899 [  500/ 1682]\n",
      "loss: 58.674492 [  510/ 1682]\n",
      "loss: 163.757324 [  520/ 1682]\n",
      "loss: 156.868347 [  530/ 1682]\n",
      "loss: 460.057800 [  540/ 1682]\n",
      "loss: 227.201294 [  550/ 1682]\n",
      "loss: 127.727951 [  560/ 1682]\n",
      "loss: 161.646027 [  570/ 1682]\n",
      "loss: 131.860641 [  580/ 1682]\n",
      "loss: 202.525803 [  590/ 1682]\n",
      "loss: 46.004791 [  600/ 1682]\n",
      "loss: 135.847656 [  610/ 1682]\n",
      "loss: 75.323921 [  620/ 1682]\n",
      "loss: 68.685509 [  630/ 1682]\n",
      "loss: 190.860062 [  640/ 1682]\n",
      "loss: 70.319290 [  650/ 1682]\n",
      "loss: 49.978321 [  660/ 1682]\n",
      "loss: 55.752735 [  670/ 1682]\n",
      "loss: 114.032936 [  680/ 1682]\n",
      "loss: 143.829559 [  690/ 1682]\n",
      "loss: 51.547150 [  700/ 1682]\n",
      "loss: 136.050354 [  710/ 1682]\n",
      "loss: 153.837128 [  720/ 1682]\n",
      "loss: 212.114594 [  730/ 1682]\n",
      "loss: 286.359955 [  740/ 1682]\n",
      "loss: 301.654358 [  750/ 1682]\n",
      "loss: 254.601440 [  760/ 1682]\n",
      "loss: 148.720139 [  770/ 1682]\n",
      "loss: 247.416946 [  780/ 1682]\n",
      "loss: 174.172089 [  790/ 1682]\n",
      "loss: 113.044228 [  800/ 1682]\n",
      "loss: 80.809250 [  810/ 1682]\n",
      "loss: 41.185867 [  820/ 1682]\n",
      "loss: 101.141396 [  830/ 1682]\n",
      "loss: 204.847809 [  840/ 1682]\n",
      "loss: 63.485207 [  850/ 1682]\n",
      "loss: 66.062210 [  860/ 1682]\n",
      "loss: 2.117896 [  870/ 1682]\n",
      "loss: 16.087992 [  880/ 1682]\n",
      "loss: 93.593430 [  890/ 1682]\n",
      "loss: 85.220451 [  900/ 1682]\n",
      "loss: 23.848282 [  910/ 1682]\n",
      "loss: 36.489697 [  920/ 1682]\n",
      "loss: 52.389771 [  930/ 1682]\n",
      "loss: 82.659088 [  940/ 1682]\n",
      "loss: 160.405304 [  950/ 1682]\n",
      "loss: 239.103226 [  960/ 1682]\n",
      "loss: 146.812424 [  970/ 1682]\n",
      "loss: 197.956436 [  980/ 1682]\n",
      "loss: 531.518066 [  990/ 1682]\n",
      "loss: 526.605347 [ 1000/ 1682]\n",
      "loss: 799.203247 [ 1010/ 1682]\n",
      "loss: 849.760925 [ 1020/ 1682]\n",
      "loss: 608.120056 [ 1030/ 1682]\n",
      "loss: 390.984070 [ 1040/ 1682]\n",
      "loss: 151.354080 [ 1050/ 1682]\n",
      "loss: 254.415237 [ 1060/ 1682]\n",
      "loss: 374.753815 [ 1070/ 1682]\n",
      "loss: 599.333374 [ 1080/ 1682]\n",
      "loss: 746.908813 [ 1090/ 1682]\n",
      "loss: 823.884460 [ 1100/ 1682]\n",
      "loss: 1337.958984 [ 1110/ 1682]\n",
      "loss: 1742.093384 [ 1120/ 1682]\n",
      "loss: 2049.695801 [ 1130/ 1682]\n",
      "loss: 2458.486328 [ 1140/ 1682]\n",
      "loss: 4158.616211 [ 1150/ 1682]\n",
      "loss: 5047.715820 [ 1160/ 1682]\n",
      "loss: 3780.539062 [ 1170/ 1682]\n",
      "loss: 3677.259277 [ 1180/ 1682]\n",
      "loss: 4286.279297 [ 1190/ 1682]\n",
      "loss: 3731.026123 [ 1200/ 1682]\n",
      "loss: 4373.632324 [ 1210/ 1682]\n",
      "loss: 4091.436279 [ 1220/ 1682]\n",
      "loss: 4459.528809 [ 1230/ 1682]\n",
      "loss: 6388.479492 [ 1240/ 1682]\n",
      "loss: 5566.676270 [ 1250/ 1682]\n",
      "loss: 6850.990723 [ 1260/ 1682]\n",
      "loss: 6031.547852 [ 1270/ 1682]\n",
      "loss: 5242.500977 [ 1280/ 1682]\n",
      "loss: 4706.465332 [ 1290/ 1682]\n",
      "loss: 4661.051270 [ 1300/ 1682]\n",
      "loss: 5484.872070 [ 1310/ 1682]\n",
      "loss: 6249.875977 [ 1320/ 1682]\n",
      "loss: 5781.989746 [ 1330/ 1682]\n",
      "loss: 4912.336426 [ 1340/ 1682]\n",
      "loss: 5282.647461 [ 1350/ 1682]\n",
      "loss: 6330.264648 [ 1360/ 1682]\n",
      "loss: 7018.596680 [ 1370/ 1682]\n",
      "loss: 8693.437500 [ 1380/ 1682]\n",
      "loss: 8337.666992 [ 1390/ 1682]\n",
      "loss: 8336.009766 [ 1400/ 1682]\n",
      "loss: 9759.505859 [ 1410/ 1682]\n",
      "loss: 9369.375977 [ 1420/ 1682]\n",
      "loss: 8559.806641 [ 1430/ 1682]\n",
      "loss: 6901.475586 [ 1440/ 1682]\n",
      "loss: 8972.274414 [ 1450/ 1682]\n",
      "loss: 9111.892578 [ 1460/ 1682]\n",
      "loss: 11640.971680 [ 1470/ 1682]\n",
      "loss: 12760.190430 [ 1480/ 1682]\n",
      "loss: 15062.252930 [ 1490/ 1682]\n",
      "loss: 16029.455078 [ 1500/ 1682]\n",
      "loss: 12476.657227 [ 1510/ 1682]\n",
      "loss: 15061.005859 [ 1520/ 1682]\n",
      "loss: 14015.458984 [ 1530/ 1682]\n",
      "loss: 11236.015625 [ 1540/ 1682]\n",
      "loss: 11859.693359 [ 1550/ 1682]\n",
      "loss: 15820.658203 [ 1560/ 1682]\n",
      "loss: 12380.182617 [ 1570/ 1682]\n",
      "loss: 10256.117188 [ 1580/ 1682]\n",
      "loss: 8112.704590 [ 1590/ 1682]\n",
      "loss: 8304.782227 [ 1600/ 1682]\n",
      "loss: 7796.043945 [ 1610/ 1682]\n",
      "loss: 7548.200684 [ 1620/ 1682]\n",
      "loss: 8534.029297 [ 1630/ 1682]\n",
      "loss: 11186.729492 [ 1640/ 1682]\n",
      "loss: 13592.606445 [ 1650/ 1682]\n",
      "loss: 13643.181641 [ 1660/ 1682]\n",
      "loss: 10044.610352 [ 1670/ 1682]\n",
      "loss: 7209.270508 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11785.241042 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1193.963135 [    0/ 1682]\n",
      "loss: 1470.429321 [   10/ 1682]\n",
      "loss: 702.402283 [   20/ 1682]\n",
      "loss: 1099.330566 [   30/ 1682]\n",
      "loss: 862.050415 [   40/ 1682]\n",
      "loss: 1350.267212 [   50/ 1682]\n",
      "loss: 1682.765869 [   60/ 1682]\n",
      "loss: 2789.936035 [   70/ 1682]\n",
      "loss: 2045.159912 [   80/ 1682]\n",
      "loss: 1575.535889 [   90/ 1682]\n",
      "loss: 668.225891 [  100/ 1682]\n",
      "loss: 1152.534180 [  110/ 1682]\n",
      "loss: 1824.060181 [  120/ 1682]\n",
      "loss: 1085.406006 [  130/ 1682]\n",
      "loss: 903.357605 [  140/ 1682]\n",
      "loss: 1175.964722 [  150/ 1682]\n",
      "loss: 1514.478882 [  160/ 1682]\n",
      "loss: 1477.930908 [  170/ 1682]\n",
      "loss: 821.022339 [  180/ 1682]\n",
      "loss: 1540.105835 [  190/ 1682]\n",
      "loss: 954.341431 [  200/ 1682]\n",
      "loss: 1142.255249 [  210/ 1682]\n",
      "loss: 1365.818970 [  220/ 1682]\n",
      "loss: 1045.901367 [  230/ 1682]\n",
      "loss: 442.105804 [  240/ 1682]\n",
      "loss: 597.285339 [  250/ 1682]\n",
      "loss: 844.819824 [  260/ 1682]\n",
      "loss: 306.941254 [  270/ 1682]\n",
      "loss: 509.044495 [  280/ 1682]\n",
      "loss: 438.175629 [  290/ 1682]\n",
      "loss: 802.710083 [  300/ 1682]\n",
      "loss: 628.016907 [  310/ 1682]\n",
      "loss: 442.328552 [  320/ 1682]\n",
      "loss: 678.078186 [  330/ 1682]\n",
      "loss: 545.375977 [  340/ 1682]\n",
      "loss: 310.200012 [  350/ 1682]\n",
      "loss: 699.000183 [  360/ 1682]\n",
      "loss: 285.659241 [  370/ 1682]\n",
      "loss: 259.302124 [  380/ 1682]\n",
      "loss: 763.833130 [  390/ 1682]\n",
      "loss: 370.496338 [  400/ 1682]\n",
      "loss: 182.625000 [  410/ 1682]\n",
      "loss: 170.920990 [  420/ 1682]\n",
      "loss: 298.856995 [  430/ 1682]\n",
      "loss: 433.023621 [  440/ 1682]\n",
      "loss: 234.223541 [  450/ 1682]\n",
      "loss: 269.148712 [  460/ 1682]\n",
      "loss: 57.105450 [  470/ 1682]\n",
      "loss: 57.062813 [  480/ 1682]\n",
      "loss: 136.731049 [  490/ 1682]\n",
      "loss: 209.068329 [  500/ 1682]\n",
      "loss: 470.181061 [  510/ 1682]\n",
      "loss: 176.049103 [  520/ 1682]\n",
      "loss: 102.441856 [  530/ 1682]\n",
      "loss: 36.656593 [  540/ 1682]\n",
      "loss: 64.918098 [  550/ 1682]\n",
      "loss: 232.249756 [  560/ 1682]\n",
      "loss: 479.191559 [  570/ 1682]\n",
      "loss: 99.204697 [  580/ 1682]\n",
      "loss: 123.753395 [  590/ 1682]\n",
      "loss: 122.528824 [  600/ 1682]\n",
      "loss: 167.958969 [  610/ 1682]\n",
      "loss: 240.457932 [  620/ 1682]\n",
      "loss: 131.198257 [  630/ 1682]\n",
      "loss: 62.450096 [  640/ 1682]\n",
      "loss: 43.952133 [  650/ 1682]\n",
      "loss: 34.582191 [  660/ 1682]\n",
      "loss: 81.226463 [  670/ 1682]\n",
      "loss: 58.895744 [  680/ 1682]\n",
      "loss: 53.192738 [  690/ 1682]\n",
      "loss: 45.336876 [  700/ 1682]\n",
      "loss: 84.467400 [  710/ 1682]\n",
      "loss: 261.315613 [  720/ 1682]\n",
      "loss: 60.057861 [  730/ 1682]\n",
      "loss: 202.874603 [  740/ 1682]\n",
      "loss: 293.789856 [  750/ 1682]\n",
      "loss: 182.503189 [  760/ 1682]\n",
      "loss: 215.698517 [  770/ 1682]\n",
      "loss: 39.758175 [  780/ 1682]\n",
      "loss: 84.486191 [  790/ 1682]\n",
      "loss: 46.953918 [  800/ 1682]\n",
      "loss: 56.491875 [  810/ 1682]\n",
      "loss: 31.192682 [  820/ 1682]\n",
      "loss: 66.488205 [  830/ 1682]\n",
      "loss: 96.857079 [  840/ 1682]\n",
      "loss: 29.921375 [  850/ 1682]\n",
      "loss: 32.552898 [  860/ 1682]\n",
      "loss: 14.432920 [  870/ 1682]\n",
      "loss: 19.614731 [  880/ 1682]\n",
      "loss: 32.981606 [  890/ 1682]\n",
      "loss: 15.599691 [  900/ 1682]\n",
      "loss: 32.561653 [  910/ 1682]\n",
      "loss: 29.652370 [  920/ 1682]\n",
      "loss: 35.817196 [  930/ 1682]\n",
      "loss: 82.025948 [  940/ 1682]\n",
      "loss: 119.242477 [  950/ 1682]\n",
      "loss: 201.367508 [  960/ 1682]\n",
      "loss: 210.433914 [  970/ 1682]\n",
      "loss: 320.670898 [  980/ 1682]\n",
      "loss: 501.111237 [  990/ 1682]\n",
      "loss: 693.762939 [ 1000/ 1682]\n",
      "loss: 838.407227 [ 1010/ 1682]\n",
      "loss: 723.524902 [ 1020/ 1682]\n",
      "loss: 480.629883 [ 1030/ 1682]\n",
      "loss: 365.652283 [ 1040/ 1682]\n",
      "loss: 113.119629 [ 1050/ 1682]\n",
      "loss: 220.826172 [ 1060/ 1682]\n",
      "loss: 328.346619 [ 1070/ 1682]\n",
      "loss: 666.562134 [ 1080/ 1682]\n",
      "loss: 657.145569 [ 1090/ 1682]\n",
      "loss: 1025.829468 [ 1100/ 1682]\n",
      "loss: 1339.130859 [ 1110/ 1682]\n",
      "loss: 1731.461914 [ 1120/ 1682]\n",
      "loss: 1929.823242 [ 1130/ 1682]\n",
      "loss: 2441.761230 [ 1140/ 1682]\n",
      "loss: 3916.766846 [ 1150/ 1682]\n",
      "loss: 5143.803711 [ 1160/ 1682]\n",
      "loss: 3223.501709 [ 1170/ 1682]\n",
      "loss: 3791.397217 [ 1180/ 1682]\n",
      "loss: 4445.231934 [ 1190/ 1682]\n",
      "loss: 4061.777344 [ 1200/ 1682]\n",
      "loss: 4309.408203 [ 1210/ 1682]\n",
      "loss: 4422.159180 [ 1220/ 1682]\n",
      "loss: 4206.381836 [ 1230/ 1682]\n",
      "loss: 5973.380371 [ 1240/ 1682]\n",
      "loss: 5868.943848 [ 1250/ 1682]\n",
      "loss: 6992.524414 [ 1260/ 1682]\n",
      "loss: 6714.917969 [ 1270/ 1682]\n",
      "loss: 5405.198730 [ 1280/ 1682]\n",
      "loss: 4190.035645 [ 1290/ 1682]\n",
      "loss: 5137.917969 [ 1300/ 1682]\n",
      "loss: 5255.562500 [ 1310/ 1682]\n",
      "loss: 6678.186035 [ 1320/ 1682]\n",
      "loss: 5759.370117 [ 1330/ 1682]\n",
      "loss: 5163.772461 [ 1340/ 1682]\n",
      "loss: 5701.821777 [ 1350/ 1682]\n",
      "loss: 6451.856445 [ 1360/ 1682]\n",
      "loss: 6769.144531 [ 1370/ 1682]\n",
      "loss: 8697.448242 [ 1380/ 1682]\n",
      "loss: 8440.640625 [ 1390/ 1682]\n",
      "loss: 9184.981445 [ 1400/ 1682]\n",
      "loss: 9043.448242 [ 1410/ 1682]\n",
      "loss: 9777.453125 [ 1420/ 1682]\n",
      "loss: 8448.681641 [ 1430/ 1682]\n",
      "loss: 7356.156250 [ 1440/ 1682]\n",
      "loss: 8957.143555 [ 1450/ 1682]\n",
      "loss: 9097.173828 [ 1460/ 1682]\n",
      "loss: 11032.932617 [ 1470/ 1682]\n",
      "loss: 12258.669922 [ 1480/ 1682]\n",
      "loss: 14113.666992 [ 1490/ 1682]\n",
      "loss: 15518.197266 [ 1500/ 1682]\n",
      "loss: 12880.684570 [ 1510/ 1682]\n",
      "loss: 12831.265625 [ 1520/ 1682]\n",
      "loss: 12723.418945 [ 1530/ 1682]\n",
      "loss: 11389.609375 [ 1540/ 1682]\n",
      "loss: 12615.685547 [ 1550/ 1682]\n",
      "loss: 13517.049805 [ 1560/ 1682]\n",
      "loss: 10931.345703 [ 1570/ 1682]\n",
      "loss: 11340.291992 [ 1580/ 1682]\n",
      "loss: 8131.159180 [ 1590/ 1682]\n",
      "loss: 7848.123535 [ 1600/ 1682]\n",
      "loss: 7139.857910 [ 1610/ 1682]\n",
      "loss: 5410.247070 [ 1620/ 1682]\n",
      "loss: 8677.885742 [ 1630/ 1682]\n",
      "loss: 10256.724609 [ 1640/ 1682]\n",
      "loss: 13310.771484 [ 1650/ 1682]\n",
      "loss: 12119.521484 [ 1660/ 1682]\n",
      "loss: 7063.899902 [ 1670/ 1682]\n",
      "loss: 10504.826172 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11512.753662 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1103.125732 [    0/ 1682]\n",
      "loss: 1495.411865 [   10/ 1682]\n",
      "loss: 1745.519287 [   20/ 1682]\n",
      "loss: 1233.650635 [   30/ 1682]\n",
      "loss: 1987.929321 [   40/ 1682]\n",
      "loss: 1832.462524 [   50/ 1682]\n",
      "loss: 1400.086792 [   60/ 1682]\n",
      "loss: 1885.907471 [   70/ 1682]\n",
      "loss: 1844.705688 [   80/ 1682]\n",
      "loss: 725.091492 [   90/ 1682]\n",
      "loss: 2025.853882 [  100/ 1682]\n",
      "loss: 1151.975220 [  110/ 1682]\n",
      "loss: 992.598938 [  120/ 1682]\n",
      "loss: 1812.863892 [  130/ 1682]\n",
      "loss: 1539.934204 [  140/ 1682]\n",
      "loss: 1655.915771 [  150/ 1682]\n",
      "loss: 1040.588135 [  160/ 1682]\n",
      "loss: 969.869934 [  170/ 1682]\n",
      "loss: 1286.006470 [  180/ 1682]\n",
      "loss: 1860.963623 [  190/ 1682]\n",
      "loss: 1171.241089 [  200/ 1682]\n",
      "loss: 1502.003418 [  210/ 1682]\n",
      "loss: 831.875977 [  220/ 1682]\n",
      "loss: 1072.392578 [  230/ 1682]\n",
      "loss: 1569.041016 [  240/ 1682]\n",
      "loss: 711.752441 [  250/ 1682]\n",
      "loss: 613.448120 [  260/ 1682]\n",
      "loss: 818.741516 [  270/ 1682]\n",
      "loss: 1080.967896 [  280/ 1682]\n",
      "loss: 800.869812 [  290/ 1682]\n",
      "loss: 1176.304443 [  300/ 1682]\n",
      "loss: 517.189331 [  310/ 1682]\n",
      "loss: 470.088562 [  320/ 1682]\n",
      "loss: 914.185364 [  330/ 1682]\n",
      "loss: 549.615295 [  340/ 1682]\n",
      "loss: 442.041412 [  350/ 1682]\n",
      "loss: 220.439255 [  360/ 1682]\n",
      "loss: 229.344482 [  370/ 1682]\n",
      "loss: 520.595642 [  380/ 1682]\n",
      "loss: 417.074951 [  390/ 1682]\n",
      "loss: 450.985748 [  400/ 1682]\n",
      "loss: 302.824951 [  410/ 1682]\n",
      "loss: 444.027283 [  420/ 1682]\n",
      "loss: 460.325928 [  430/ 1682]\n",
      "loss: 314.590485 [  440/ 1682]\n",
      "loss: 416.563904 [  450/ 1682]\n",
      "loss: 151.153427 [  460/ 1682]\n",
      "loss: 226.789139 [  470/ 1682]\n",
      "loss: 243.028290 [  480/ 1682]\n",
      "loss: 222.017487 [  490/ 1682]\n",
      "loss: 293.064636 [  500/ 1682]\n",
      "loss: 153.492584 [  510/ 1682]\n",
      "loss: 452.396332 [  520/ 1682]\n",
      "loss: 153.384781 [  530/ 1682]\n",
      "loss: 206.984894 [  540/ 1682]\n",
      "loss: 186.181885 [  550/ 1682]\n",
      "loss: 248.761017 [  560/ 1682]\n",
      "loss: 195.076447 [  570/ 1682]\n",
      "loss: 149.900772 [  580/ 1682]\n",
      "loss: 82.273819 [  590/ 1682]\n",
      "loss: 144.051758 [  600/ 1682]\n",
      "loss: 70.759743 [  610/ 1682]\n",
      "loss: 45.310219 [  620/ 1682]\n",
      "loss: 169.014679 [  630/ 1682]\n",
      "loss: 29.336697 [  640/ 1682]\n",
      "loss: 31.455866 [  650/ 1682]\n",
      "loss: 29.285303 [  660/ 1682]\n",
      "loss: 16.028515 [  670/ 1682]\n",
      "loss: 33.127968 [  680/ 1682]\n",
      "loss: 45.917114 [  690/ 1682]\n",
      "loss: 10.150802 [  700/ 1682]\n",
      "loss: 55.502270 [  710/ 1682]\n",
      "loss: 129.541443 [  720/ 1682]\n",
      "loss: 419.919861 [  730/ 1682]\n",
      "loss: 402.495789 [  740/ 1682]\n",
      "loss: 334.247223 [  750/ 1682]\n",
      "loss: 198.028061 [  760/ 1682]\n",
      "loss: 152.148956 [  770/ 1682]\n",
      "loss: 48.401299 [  780/ 1682]\n",
      "loss: 158.854523 [  790/ 1682]\n",
      "loss: 210.555618 [  800/ 1682]\n",
      "loss: 135.178360 [  810/ 1682]\n",
      "loss: 17.793768 [  820/ 1682]\n",
      "loss: 100.763840 [  830/ 1682]\n",
      "loss: 181.507492 [  840/ 1682]\n",
      "loss: 98.157707 [  850/ 1682]\n",
      "loss: 127.639305 [  860/ 1682]\n",
      "loss: 40.694286 [  870/ 1682]\n",
      "loss: 63.265999 [  880/ 1682]\n",
      "loss: 80.774643 [  890/ 1682]\n",
      "loss: 51.638466 [  900/ 1682]\n",
      "loss: 3.926764 [  910/ 1682]\n",
      "loss: 52.431599 [  920/ 1682]\n",
      "loss: 54.064129 [  930/ 1682]\n",
      "loss: 63.539604 [  940/ 1682]\n",
      "loss: 123.540504 [  950/ 1682]\n",
      "loss: 178.520554 [  960/ 1682]\n",
      "loss: 168.988052 [  970/ 1682]\n",
      "loss: 145.808044 [  980/ 1682]\n",
      "loss: 389.098053 [  990/ 1682]\n",
      "loss: 656.363770 [ 1000/ 1682]\n",
      "loss: 610.082153 [ 1010/ 1682]\n",
      "loss: 887.415344 [ 1020/ 1682]\n",
      "loss: 382.259094 [ 1030/ 1682]\n",
      "loss: 304.432770 [ 1040/ 1682]\n",
      "loss: 114.022659 [ 1050/ 1682]\n",
      "loss: 196.913086 [ 1060/ 1682]\n",
      "loss: 351.318420 [ 1070/ 1682]\n",
      "loss: 526.940063 [ 1080/ 1682]\n",
      "loss: 576.545044 [ 1090/ 1682]\n",
      "loss: 830.385132 [ 1100/ 1682]\n",
      "loss: 1368.439209 [ 1110/ 1682]\n",
      "loss: 1441.713257 [ 1120/ 1682]\n",
      "loss: 1934.291260 [ 1130/ 1682]\n",
      "loss: 2613.773926 [ 1140/ 1682]\n",
      "loss: 3955.026611 [ 1150/ 1682]\n",
      "loss: 5019.973145 [ 1160/ 1682]\n",
      "loss: 3819.984863 [ 1170/ 1682]\n",
      "loss: 3524.069824 [ 1180/ 1682]\n",
      "loss: 3955.807129 [ 1190/ 1682]\n",
      "loss: 3826.400879 [ 1200/ 1682]\n",
      "loss: 3820.507812 [ 1210/ 1682]\n",
      "loss: 4073.373535 [ 1220/ 1682]\n",
      "loss: 4811.651367 [ 1230/ 1682]\n",
      "loss: 6390.460938 [ 1240/ 1682]\n",
      "loss: 5191.522461 [ 1250/ 1682]\n",
      "loss: 6342.176758 [ 1260/ 1682]\n",
      "loss: 6572.817383 [ 1270/ 1682]\n",
      "loss: 5157.933594 [ 1280/ 1682]\n",
      "loss: 4533.423828 [ 1290/ 1682]\n",
      "loss: 4789.491211 [ 1300/ 1682]\n",
      "loss: 5457.935547 [ 1310/ 1682]\n",
      "loss: 6577.174805 [ 1320/ 1682]\n",
      "loss: 6244.661621 [ 1330/ 1682]\n",
      "loss: 5128.234863 [ 1340/ 1682]\n",
      "loss: 5107.805176 [ 1350/ 1682]\n",
      "loss: 6041.736816 [ 1360/ 1682]\n",
      "loss: 6471.630371 [ 1370/ 1682]\n",
      "loss: 8561.799805 [ 1380/ 1682]\n",
      "loss: 8827.996094 [ 1390/ 1682]\n",
      "loss: 8399.035156 [ 1400/ 1682]\n",
      "loss: 8772.870117 [ 1410/ 1682]\n",
      "loss: 9744.776367 [ 1420/ 1682]\n",
      "loss: 8392.726562 [ 1430/ 1682]\n",
      "loss: 6977.067383 [ 1440/ 1682]\n",
      "loss: 9311.193359 [ 1450/ 1682]\n",
      "loss: 8774.337891 [ 1460/ 1682]\n",
      "loss: 9126.112305 [ 1470/ 1682]\n",
      "loss: 13215.585938 [ 1480/ 1682]\n",
      "loss: 14399.017578 [ 1490/ 1682]\n",
      "loss: 13840.418945 [ 1500/ 1682]\n",
      "loss: 12591.255859 [ 1510/ 1682]\n",
      "loss: 14740.248047 [ 1520/ 1682]\n",
      "loss: 12852.864258 [ 1530/ 1682]\n",
      "loss: 12116.771484 [ 1540/ 1682]\n",
      "loss: 11527.813477 [ 1550/ 1682]\n",
      "loss: 14139.481445 [ 1560/ 1682]\n",
      "loss: 13374.731445 [ 1570/ 1682]\n",
      "loss: 11849.164062 [ 1580/ 1682]\n",
      "loss: 7277.475098 [ 1590/ 1682]\n",
      "loss: 7956.770508 [ 1600/ 1682]\n",
      "loss: 7335.483398 [ 1610/ 1682]\n",
      "loss: 7879.263184 [ 1620/ 1682]\n",
      "loss: 8405.155273 [ 1630/ 1682]\n",
      "loss: 10795.470703 [ 1640/ 1682]\n",
      "loss: 12018.783203 [ 1650/ 1682]\n",
      "loss: 10082.870117 [ 1660/ 1682]\n",
      "loss: 10020.802734 [ 1670/ 1682]\n",
      "loss: 10373.776367 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11299.813477 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1134.175903 [    0/ 1682]\n",
      "loss: 767.765442 [   10/ 1682]\n",
      "loss: 773.836365 [   20/ 1682]\n",
      "loss: 1068.810669 [   30/ 1682]\n",
      "loss: 1531.781006 [   40/ 1682]\n",
      "loss: 1331.648560 [   50/ 1682]\n",
      "loss: 1384.303955 [   60/ 1682]\n",
      "loss: 1132.652710 [   70/ 1682]\n",
      "loss: 1722.597046 [   80/ 1682]\n",
      "loss: 760.304016 [   90/ 1682]\n",
      "loss: 1687.325195 [  100/ 1682]\n",
      "loss: 2334.585449 [  110/ 1682]\n",
      "loss: 1276.270020 [  120/ 1682]\n",
      "loss: 1497.743286 [  130/ 1682]\n",
      "loss: 1006.210449 [  140/ 1682]\n",
      "loss: 824.959351 [  150/ 1682]\n",
      "loss: 1527.083130 [  160/ 1682]\n",
      "loss: 1140.982544 [  170/ 1682]\n",
      "loss: 1553.923584 [  180/ 1682]\n",
      "loss: 1268.405151 [  190/ 1682]\n",
      "loss: 1024.002563 [  200/ 1682]\n",
      "loss: 1236.185791 [  210/ 1682]\n",
      "loss: 894.675415 [  220/ 1682]\n",
      "loss: 1469.749878 [  230/ 1682]\n",
      "loss: 1334.364990 [  240/ 1682]\n",
      "loss: 689.344849 [  250/ 1682]\n",
      "loss: 1231.278076 [  260/ 1682]\n",
      "loss: 786.182007 [  270/ 1682]\n",
      "loss: 744.524536 [  280/ 1682]\n",
      "loss: 898.625183 [  290/ 1682]\n",
      "loss: 1127.058350 [  300/ 1682]\n",
      "loss: 737.673950 [  310/ 1682]\n",
      "loss: 627.322083 [  320/ 1682]\n",
      "loss: 504.309326 [  330/ 1682]\n",
      "loss: 651.969971 [  340/ 1682]\n",
      "loss: 538.815063 [  350/ 1682]\n",
      "loss: 704.006042 [  360/ 1682]\n",
      "loss: 929.429871 [  370/ 1682]\n",
      "loss: 347.522583 [  380/ 1682]\n",
      "loss: 236.837067 [  390/ 1682]\n",
      "loss: 584.867188 [  400/ 1682]\n",
      "loss: 230.189255 [  410/ 1682]\n",
      "loss: 493.563416 [  420/ 1682]\n",
      "loss: 395.631287 [  430/ 1682]\n",
      "loss: 321.174591 [  440/ 1682]\n",
      "loss: 491.156158 [  450/ 1682]\n",
      "loss: 153.066254 [  460/ 1682]\n",
      "loss: 216.086456 [  470/ 1682]\n",
      "loss: 140.903870 [  480/ 1682]\n",
      "loss: 213.299347 [  490/ 1682]\n",
      "loss: 145.549530 [  500/ 1682]\n",
      "loss: 252.511749 [  510/ 1682]\n",
      "loss: 200.634216 [  520/ 1682]\n",
      "loss: 214.926193 [  530/ 1682]\n",
      "loss: 406.116638 [  540/ 1682]\n",
      "loss: 211.641312 [  550/ 1682]\n",
      "loss: 334.511536 [  560/ 1682]\n",
      "loss: 227.338089 [  570/ 1682]\n",
      "loss: 82.737915 [  580/ 1682]\n",
      "loss: 176.895401 [  590/ 1682]\n",
      "loss: 101.748047 [  600/ 1682]\n",
      "loss: 103.960464 [  610/ 1682]\n",
      "loss: 24.410265 [  620/ 1682]\n",
      "loss: 121.552223 [  630/ 1682]\n",
      "loss: 131.709763 [  640/ 1682]\n",
      "loss: 29.872097 [  650/ 1682]\n",
      "loss: 62.434582 [  660/ 1682]\n",
      "loss: 57.557953 [  670/ 1682]\n",
      "loss: 57.168175 [  680/ 1682]\n",
      "loss: 94.696945 [  690/ 1682]\n",
      "loss: 59.340721 [  700/ 1682]\n",
      "loss: 51.495247 [  710/ 1682]\n",
      "loss: 84.178040 [  720/ 1682]\n",
      "loss: 205.525345 [  730/ 1682]\n",
      "loss: 159.634201 [  740/ 1682]\n",
      "loss: 171.582367 [  750/ 1682]\n",
      "loss: 485.672546 [  760/ 1682]\n",
      "loss: 67.009483 [  770/ 1682]\n",
      "loss: 57.687172 [  780/ 1682]\n",
      "loss: 129.771301 [  790/ 1682]\n",
      "loss: 120.795288 [  800/ 1682]\n",
      "loss: 92.419449 [  810/ 1682]\n",
      "loss: 79.366951 [  820/ 1682]\n",
      "loss: 130.863144 [  830/ 1682]\n",
      "loss: 133.013046 [  840/ 1682]\n",
      "loss: 53.756325 [  850/ 1682]\n",
      "loss: 28.254848 [  860/ 1682]\n",
      "loss: 54.490612 [  870/ 1682]\n",
      "loss: 34.469383 [  880/ 1682]\n",
      "loss: 23.136976 [  890/ 1682]\n",
      "loss: 87.234001 [  900/ 1682]\n",
      "loss: 67.986618 [  910/ 1682]\n",
      "loss: 30.992456 [  920/ 1682]\n",
      "loss: 41.518669 [  930/ 1682]\n",
      "loss: 47.221779 [  940/ 1682]\n",
      "loss: 89.062340 [  950/ 1682]\n",
      "loss: 118.403954 [  960/ 1682]\n",
      "loss: 173.864914 [  970/ 1682]\n",
      "loss: 181.114380 [  980/ 1682]\n",
      "loss: 329.759583 [  990/ 1682]\n",
      "loss: 619.818359 [ 1000/ 1682]\n",
      "loss: 521.278687 [ 1010/ 1682]\n",
      "loss: 787.225159 [ 1020/ 1682]\n",
      "loss: 502.747864 [ 1030/ 1682]\n",
      "loss: 166.113800 [ 1040/ 1682]\n",
      "loss: 107.266602 [ 1050/ 1682]\n",
      "loss: 210.373291 [ 1060/ 1682]\n",
      "loss: 268.680511 [ 1070/ 1682]\n",
      "loss: 453.386810 [ 1080/ 1682]\n",
      "loss: 647.484558 [ 1090/ 1682]\n",
      "loss: 868.799133 [ 1100/ 1682]\n",
      "loss: 1389.281494 [ 1110/ 1682]\n",
      "loss: 1544.565063 [ 1120/ 1682]\n",
      "loss: 1794.894897 [ 1130/ 1682]\n",
      "loss: 2399.635498 [ 1140/ 1682]\n",
      "loss: 3636.144531 [ 1150/ 1682]\n",
      "loss: 5437.724121 [ 1160/ 1682]\n",
      "loss: 3304.485107 [ 1170/ 1682]\n",
      "loss: 3790.163574 [ 1180/ 1682]\n",
      "loss: 4250.211914 [ 1190/ 1682]\n",
      "loss: 3730.397705 [ 1200/ 1682]\n",
      "loss: 4334.049316 [ 1210/ 1682]\n",
      "loss: 4220.132324 [ 1220/ 1682]\n",
      "loss: 4752.393066 [ 1230/ 1682]\n",
      "loss: 5709.723633 [ 1240/ 1682]\n",
      "loss: 5479.499512 [ 1250/ 1682]\n",
      "loss: 6834.021973 [ 1260/ 1682]\n",
      "loss: 6808.757812 [ 1270/ 1682]\n",
      "loss: 5213.418945 [ 1280/ 1682]\n",
      "loss: 4509.486328 [ 1290/ 1682]\n",
      "loss: 4754.440430 [ 1300/ 1682]\n",
      "loss: 4959.615234 [ 1310/ 1682]\n",
      "loss: 6750.803223 [ 1320/ 1682]\n",
      "loss: 5771.634277 [ 1330/ 1682]\n",
      "loss: 5296.777344 [ 1340/ 1682]\n",
      "loss: 4774.385742 [ 1350/ 1682]\n",
      "loss: 5282.032715 [ 1360/ 1682]\n",
      "loss: 6779.114746 [ 1370/ 1682]\n",
      "loss: 8076.465820 [ 1380/ 1682]\n",
      "loss: 8251.192383 [ 1390/ 1682]\n",
      "loss: 7651.700195 [ 1400/ 1682]\n",
      "loss: 9142.810547 [ 1410/ 1682]\n",
      "loss: 9303.254883 [ 1420/ 1682]\n",
      "loss: 8223.681641 [ 1430/ 1682]\n",
      "loss: 7301.549805 [ 1440/ 1682]\n",
      "loss: 8499.697266 [ 1450/ 1682]\n",
      "loss: 8583.307617 [ 1460/ 1682]\n",
      "loss: 11223.986328 [ 1470/ 1682]\n",
      "loss: 12456.650391 [ 1480/ 1682]\n",
      "loss: 13679.982422 [ 1490/ 1682]\n",
      "loss: 13949.208008 [ 1500/ 1682]\n",
      "loss: 11743.277344 [ 1510/ 1682]\n",
      "loss: 13627.289062 [ 1520/ 1682]\n",
      "loss: 11817.745117 [ 1530/ 1682]\n",
      "loss: 11978.885742 [ 1540/ 1682]\n",
      "loss: 11342.462891 [ 1550/ 1682]\n",
      "loss: 13525.112305 [ 1560/ 1682]\n",
      "loss: 12150.721680 [ 1570/ 1682]\n",
      "loss: 9535.265625 [ 1580/ 1682]\n",
      "loss: 8116.036133 [ 1590/ 1682]\n",
      "loss: 8548.314453 [ 1600/ 1682]\n",
      "loss: 7456.444336 [ 1610/ 1682]\n",
      "loss: 7361.419434 [ 1620/ 1682]\n",
      "loss: 9315.599609 [ 1630/ 1682]\n",
      "loss: 10783.673828 [ 1640/ 1682]\n",
      "loss: 10775.063477 [ 1650/ 1682]\n",
      "loss: 10633.790039 [ 1660/ 1682]\n",
      "loss: 10444.524414 [ 1670/ 1682]\n",
      "loss: 6864.719727 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11444.399076 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1172.754150 [    0/ 1682]\n",
      "loss: 1961.487061 [   10/ 1682]\n",
      "loss: 1169.105225 [   20/ 1682]\n",
      "loss: 1828.971680 [   30/ 1682]\n",
      "loss: 1628.884033 [   40/ 1682]\n",
      "loss: 947.032227 [   50/ 1682]\n",
      "loss: 1244.224243 [   60/ 1682]\n",
      "loss: 991.459656 [   70/ 1682]\n",
      "loss: 1311.213867 [   80/ 1682]\n",
      "loss: 795.414001 [   90/ 1682]\n",
      "loss: 1447.351440 [  100/ 1682]\n",
      "loss: 1690.474609 [  110/ 1682]\n",
      "loss: 1784.812866 [  120/ 1682]\n",
      "loss: 761.761414 [  130/ 1682]\n",
      "loss: 1083.828857 [  140/ 1682]\n",
      "loss: 1047.220947 [  150/ 1682]\n",
      "loss: 1926.265625 [  160/ 1682]\n",
      "loss: 1030.381104 [  170/ 1682]\n",
      "loss: 1488.372314 [  180/ 1682]\n",
      "loss: 913.383118 [  190/ 1682]\n",
      "loss: 1015.713745 [  200/ 1682]\n",
      "loss: 1008.379089 [  210/ 1682]\n",
      "loss: 947.923218 [  220/ 1682]\n",
      "loss: 1036.885376 [  230/ 1682]\n",
      "loss: 527.650085 [  240/ 1682]\n",
      "loss: 754.880798 [  250/ 1682]\n",
      "loss: 966.145325 [  260/ 1682]\n",
      "loss: 1131.700928 [  270/ 1682]\n",
      "loss: 825.429810 [  280/ 1682]\n",
      "loss: 1110.062988 [  290/ 1682]\n",
      "loss: 686.649780 [  300/ 1682]\n",
      "loss: 874.721375 [  310/ 1682]\n",
      "loss: 699.871033 [  320/ 1682]\n",
      "loss: 775.760742 [  330/ 1682]\n",
      "loss: 406.713928 [  340/ 1682]\n",
      "loss: 211.534210 [  350/ 1682]\n",
      "loss: 663.713989 [  360/ 1682]\n",
      "loss: 365.158264 [  370/ 1682]\n",
      "loss: 525.348083 [  380/ 1682]\n",
      "loss: 668.321899 [  390/ 1682]\n",
      "loss: 347.608673 [  400/ 1682]\n",
      "loss: 144.151550 [  410/ 1682]\n",
      "loss: 309.426605 [  420/ 1682]\n",
      "loss: 725.037354 [  430/ 1682]\n",
      "loss: 406.820709 [  440/ 1682]\n",
      "loss: 284.951477 [  450/ 1682]\n",
      "loss: 84.543839 [  460/ 1682]\n",
      "loss: 90.050011 [  470/ 1682]\n",
      "loss: 89.990211 [  480/ 1682]\n",
      "loss: 452.967468 [  490/ 1682]\n",
      "loss: 469.046051 [  500/ 1682]\n",
      "loss: 319.318176 [  510/ 1682]\n",
      "loss: 364.879669 [  520/ 1682]\n",
      "loss: 828.442566 [  530/ 1682]\n",
      "loss: 324.438995 [  540/ 1682]\n",
      "loss: 228.786499 [  550/ 1682]\n",
      "loss: 161.768021 [  560/ 1682]\n",
      "loss: 165.068268 [  570/ 1682]\n",
      "loss: 367.529358 [  580/ 1682]\n",
      "loss: 151.785431 [  590/ 1682]\n",
      "loss: 159.831650 [  600/ 1682]\n",
      "loss: 32.262753 [  610/ 1682]\n",
      "loss: 139.323639 [  620/ 1682]\n",
      "loss: 167.797562 [  630/ 1682]\n",
      "loss: 198.642776 [  640/ 1682]\n",
      "loss: 81.727112 [  650/ 1682]\n",
      "loss: 97.588348 [  660/ 1682]\n",
      "loss: 56.723133 [  670/ 1682]\n",
      "loss: 108.815079 [  680/ 1682]\n",
      "loss: 31.568939 [  690/ 1682]\n",
      "loss: 79.822189 [  700/ 1682]\n",
      "loss: 107.359825 [  710/ 1682]\n",
      "loss: 300.592926 [  720/ 1682]\n",
      "loss: 235.015945 [  730/ 1682]\n",
      "loss: 323.144165 [  740/ 1682]\n",
      "loss: 431.203796 [  750/ 1682]\n",
      "loss: 564.616638 [  760/ 1682]\n",
      "loss: 206.249146 [  770/ 1682]\n",
      "loss: 124.625023 [  780/ 1682]\n",
      "loss: 92.419312 [  790/ 1682]\n",
      "loss: 67.058533 [  800/ 1682]\n",
      "loss: 92.100914 [  810/ 1682]\n",
      "loss: 84.480370 [  820/ 1682]\n",
      "loss: 91.485802 [  830/ 1682]\n",
      "loss: 157.514236 [  840/ 1682]\n",
      "loss: 151.479309 [  850/ 1682]\n",
      "loss: 5.884112 [  860/ 1682]\n",
      "loss: 98.002342 [  870/ 1682]\n",
      "loss: 148.326935 [  880/ 1682]\n",
      "loss: 99.566948 [  890/ 1682]\n",
      "loss: 35.521538 [  900/ 1682]\n",
      "loss: 23.324776 [  910/ 1682]\n",
      "loss: 82.176369 [  920/ 1682]\n",
      "loss: 21.293640 [  930/ 1682]\n",
      "loss: 44.096458 [  940/ 1682]\n",
      "loss: 82.794510 [  950/ 1682]\n",
      "loss: 154.934204 [  960/ 1682]\n",
      "loss: 183.349747 [  970/ 1682]\n",
      "loss: 244.779663 [  980/ 1682]\n",
      "loss: 307.857330 [  990/ 1682]\n",
      "loss: 521.953979 [ 1000/ 1682]\n",
      "loss: 731.761108 [ 1010/ 1682]\n",
      "loss: 618.263794 [ 1020/ 1682]\n",
      "loss: 454.588959 [ 1030/ 1682]\n",
      "loss: 298.727875 [ 1040/ 1682]\n",
      "loss: 101.091141 [ 1050/ 1682]\n",
      "loss: 167.778336 [ 1060/ 1682]\n",
      "loss: 221.321259 [ 1070/ 1682]\n",
      "loss: 426.292236 [ 1080/ 1682]\n",
      "loss: 495.506500 [ 1090/ 1682]\n",
      "loss: 789.807129 [ 1100/ 1682]\n",
      "loss: 1255.502808 [ 1110/ 1682]\n",
      "loss: 1126.638306 [ 1120/ 1682]\n",
      "loss: 1810.120117 [ 1130/ 1682]\n",
      "loss: 2728.361084 [ 1140/ 1682]\n",
      "loss: 3334.594482 [ 1150/ 1682]\n",
      "loss: 5256.831543 [ 1160/ 1682]\n",
      "loss: 3212.078613 [ 1170/ 1682]\n",
      "loss: 3201.778809 [ 1180/ 1682]\n",
      "loss: 3962.237061 [ 1190/ 1682]\n",
      "loss: 3823.848145 [ 1200/ 1682]\n",
      "loss: 3309.902344 [ 1210/ 1682]\n",
      "loss: 3547.975342 [ 1220/ 1682]\n",
      "loss: 4666.388184 [ 1230/ 1682]\n",
      "loss: 5487.581055 [ 1240/ 1682]\n",
      "loss: 5996.039062 [ 1250/ 1682]\n",
      "loss: 5838.819336 [ 1260/ 1682]\n",
      "loss: 6362.975586 [ 1270/ 1682]\n",
      "loss: 5027.782715 [ 1280/ 1682]\n",
      "loss: 4135.854004 [ 1290/ 1682]\n",
      "loss: 4124.034180 [ 1300/ 1682]\n",
      "loss: 4982.372070 [ 1310/ 1682]\n",
      "loss: 5344.932617 [ 1320/ 1682]\n",
      "loss: 5529.970703 [ 1330/ 1682]\n",
      "loss: 4755.348633 [ 1340/ 1682]\n",
      "loss: 5123.004883 [ 1350/ 1682]\n",
      "loss: 5589.846191 [ 1360/ 1682]\n",
      "loss: 5683.832031 [ 1370/ 1682]\n",
      "loss: 8777.868164 [ 1380/ 1682]\n",
      "loss: 8109.664062 [ 1390/ 1682]\n",
      "loss: 8261.690430 [ 1400/ 1682]\n",
      "loss: 9232.619141 [ 1410/ 1682]\n",
      "loss: 9350.608398 [ 1420/ 1682]\n",
      "loss: 8072.590820 [ 1430/ 1682]\n",
      "loss: 7000.247559 [ 1440/ 1682]\n",
      "loss: 8978.999023 [ 1450/ 1682]\n",
      "loss: 8748.279297 [ 1460/ 1682]\n",
      "loss: 10544.134766 [ 1470/ 1682]\n",
      "loss: 13249.048828 [ 1480/ 1682]\n",
      "loss: 13968.869141 [ 1490/ 1682]\n",
      "loss: 14273.942383 [ 1500/ 1682]\n",
      "loss: 12744.950195 [ 1510/ 1682]\n",
      "loss: 13786.203125 [ 1520/ 1682]\n",
      "loss: 13414.981445 [ 1530/ 1682]\n",
      "loss: 11267.021484 [ 1540/ 1682]\n",
      "loss: 10476.871094 [ 1550/ 1682]\n",
      "loss: 13257.340820 [ 1560/ 1682]\n",
      "loss: 11434.622070 [ 1570/ 1682]\n",
      "loss: 10016.373047 [ 1580/ 1682]\n",
      "loss: 6614.481445 [ 1590/ 1682]\n",
      "loss: 7778.768066 [ 1600/ 1682]\n",
      "loss: 6513.275391 [ 1610/ 1682]\n",
      "loss: 5783.287598 [ 1620/ 1682]\n",
      "loss: 6531.476074 [ 1630/ 1682]\n",
      "loss: 10832.194336 [ 1640/ 1682]\n",
      "loss: 13782.708984 [ 1650/ 1682]\n",
      "loss: 13051.728516 [ 1660/ 1682]\n",
      "loss: 9043.208008 [ 1670/ 1682]\n",
      "loss: 10121.317383 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11189.538199 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 814.191772 [    0/ 1682]\n",
      "loss: 2644.556152 [   10/ 1682]\n",
      "loss: 1309.386475 [   20/ 1682]\n",
      "loss: 2025.996826 [   30/ 1682]\n",
      "loss: 1188.699341 [   40/ 1682]\n",
      "loss: 1096.401245 [   50/ 1682]\n",
      "loss: 1437.831421 [   60/ 1682]\n",
      "loss: 1191.484497 [   70/ 1682]\n",
      "loss: 2217.946777 [   80/ 1682]\n",
      "loss: 1195.894897 [   90/ 1682]\n",
      "loss: 1732.451416 [  100/ 1682]\n",
      "loss: 1258.388550 [  110/ 1682]\n",
      "loss: 1260.998535 [  120/ 1682]\n",
      "loss: 1156.959229 [  130/ 1682]\n",
      "loss: 1291.772583 [  140/ 1682]\n",
      "loss: 896.720581 [  150/ 1682]\n",
      "loss: 982.276733 [  160/ 1682]\n",
      "loss: 1463.468018 [  170/ 1682]\n",
      "loss: 600.309204 [  180/ 1682]\n",
      "loss: 795.695618 [  190/ 1682]\n",
      "loss: 1562.942505 [  200/ 1682]\n",
      "loss: 1883.753174 [  210/ 1682]\n",
      "loss: 1862.481201 [  220/ 1682]\n",
      "loss: 839.317200 [  230/ 1682]\n",
      "loss: 1210.378906 [  240/ 1682]\n",
      "loss: 813.008789 [  250/ 1682]\n",
      "loss: 762.096558 [  260/ 1682]\n",
      "loss: 402.967926 [  270/ 1682]\n",
      "loss: 1126.324585 [  280/ 1682]\n",
      "loss: 1173.500732 [  290/ 1682]\n",
      "loss: 612.170288 [  300/ 1682]\n",
      "loss: 540.614380 [  310/ 1682]\n",
      "loss: 964.854492 [  320/ 1682]\n",
      "loss: 782.351257 [  330/ 1682]\n",
      "loss: 486.264069 [  340/ 1682]\n",
      "loss: 568.953369 [  350/ 1682]\n",
      "loss: 931.418579 [  360/ 1682]\n",
      "loss: 801.726562 [  370/ 1682]\n",
      "loss: 916.854370 [  380/ 1682]\n",
      "loss: 531.641418 [  390/ 1682]\n",
      "loss: 300.846985 [  400/ 1682]\n",
      "loss: 745.476807 [  410/ 1682]\n",
      "loss: 440.053406 [  420/ 1682]\n",
      "loss: 492.713867 [  430/ 1682]\n",
      "loss: 437.759521 [  440/ 1682]\n",
      "loss: 289.496857 [  450/ 1682]\n",
      "loss: 518.008118 [  460/ 1682]\n",
      "loss: 512.594604 [  470/ 1682]\n",
      "loss: 268.170197 [  480/ 1682]\n",
      "loss: 173.956848 [  490/ 1682]\n",
      "loss: 451.921387 [  500/ 1682]\n",
      "loss: 410.692383 [  510/ 1682]\n",
      "loss: 351.423401 [  520/ 1682]\n",
      "loss: 240.980743 [  530/ 1682]\n",
      "loss: 229.153473 [  540/ 1682]\n",
      "loss: 313.484528 [  550/ 1682]\n",
      "loss: 439.546570 [  560/ 1682]\n",
      "loss: 452.885590 [  570/ 1682]\n",
      "loss: 109.820053 [  580/ 1682]\n",
      "loss: 79.623398 [  590/ 1682]\n",
      "loss: 79.999008 [  600/ 1682]\n",
      "loss: 208.523773 [  610/ 1682]\n",
      "loss: 37.986935 [  620/ 1682]\n",
      "loss: 228.779297 [  630/ 1682]\n",
      "loss: 187.528641 [  640/ 1682]\n",
      "loss: 19.939705 [  650/ 1682]\n",
      "loss: 32.194420 [  660/ 1682]\n",
      "loss: 33.195492 [  670/ 1682]\n",
      "loss: 62.871391 [  680/ 1682]\n",
      "loss: 98.974777 [  690/ 1682]\n",
      "loss: 46.408836 [  700/ 1682]\n",
      "loss: 83.763985 [  710/ 1682]\n",
      "loss: 231.866486 [  720/ 1682]\n",
      "loss: 251.987274 [  730/ 1682]\n",
      "loss: 263.043915 [  740/ 1682]\n",
      "loss: 272.469269 [  750/ 1682]\n",
      "loss: 302.215973 [  760/ 1682]\n",
      "loss: 88.564774 [  770/ 1682]\n",
      "loss: 194.208603 [  780/ 1682]\n",
      "loss: 161.064270 [  790/ 1682]\n",
      "loss: 65.814453 [  800/ 1682]\n",
      "loss: 105.781143 [  810/ 1682]\n",
      "loss: 2.240162 [  820/ 1682]\n",
      "loss: 80.965019 [  830/ 1682]\n",
      "loss: 89.121307 [  840/ 1682]\n",
      "loss: 69.386139 [  850/ 1682]\n",
      "loss: 98.858246 [  860/ 1682]\n",
      "loss: 105.462303 [  870/ 1682]\n",
      "loss: 40.810005 [  880/ 1682]\n",
      "loss: 100.687599 [  890/ 1682]\n",
      "loss: 69.388580 [  900/ 1682]\n",
      "loss: 61.188568 [  910/ 1682]\n",
      "loss: 28.751348 [  920/ 1682]\n",
      "loss: 22.444441 [  930/ 1682]\n",
      "loss: 44.503040 [  940/ 1682]\n",
      "loss: 77.335121 [  950/ 1682]\n",
      "loss: 157.230728 [  960/ 1682]\n",
      "loss: 167.487717 [  970/ 1682]\n",
      "loss: 172.425430 [  980/ 1682]\n",
      "loss: 395.429993 [  990/ 1682]\n",
      "loss: 559.313293 [ 1000/ 1682]\n",
      "loss: 543.626160 [ 1010/ 1682]\n",
      "loss: 663.243652 [ 1020/ 1682]\n",
      "loss: 418.522797 [ 1030/ 1682]\n",
      "loss: 279.152924 [ 1040/ 1682]\n",
      "loss: 65.031746 [ 1050/ 1682]\n",
      "loss: 164.077988 [ 1060/ 1682]\n",
      "loss: 255.589386 [ 1070/ 1682]\n",
      "loss: 392.521851 [ 1080/ 1682]\n",
      "loss: 597.319275 [ 1090/ 1682]\n",
      "loss: 844.790222 [ 1100/ 1682]\n",
      "loss: 980.391479 [ 1110/ 1682]\n",
      "loss: 1496.947021 [ 1120/ 1682]\n",
      "loss: 1789.414795 [ 1130/ 1682]\n",
      "loss: 2666.685059 [ 1140/ 1682]\n",
      "loss: 3770.156738 [ 1150/ 1682]\n",
      "loss: 5179.703613 [ 1160/ 1682]\n",
      "loss: 3162.669434 [ 1170/ 1682]\n",
      "loss: 3473.408203 [ 1180/ 1682]\n",
      "loss: 3764.385986 [ 1190/ 1682]\n",
      "loss: 3173.569580 [ 1200/ 1682]\n",
      "loss: 3535.237549 [ 1210/ 1682]\n",
      "loss: 3878.535889 [ 1220/ 1682]\n",
      "loss: 4799.045410 [ 1230/ 1682]\n",
      "loss: 5513.737305 [ 1240/ 1682]\n",
      "loss: 5383.329590 [ 1250/ 1682]\n",
      "loss: 6360.305664 [ 1260/ 1682]\n",
      "loss: 6396.007812 [ 1270/ 1682]\n",
      "loss: 5102.432617 [ 1280/ 1682]\n",
      "loss: 4140.067383 [ 1290/ 1682]\n",
      "loss: 3830.047363 [ 1300/ 1682]\n",
      "loss: 4739.623535 [ 1310/ 1682]\n",
      "loss: 5930.002930 [ 1320/ 1682]\n",
      "loss: 5439.608887 [ 1330/ 1682]\n",
      "loss: 5146.889648 [ 1340/ 1682]\n",
      "loss: 4614.120117 [ 1350/ 1682]\n",
      "loss: 5788.871582 [ 1360/ 1682]\n",
      "loss: 6424.878418 [ 1370/ 1682]\n",
      "loss: 7747.227539 [ 1380/ 1682]\n",
      "loss: 8119.676758 [ 1390/ 1682]\n",
      "loss: 9018.083008 [ 1400/ 1682]\n",
      "loss: 9130.798828 [ 1410/ 1682]\n",
      "loss: 9676.895508 [ 1420/ 1682]\n",
      "loss: 8061.412598 [ 1430/ 1682]\n",
      "loss: 7357.636719 [ 1440/ 1682]\n",
      "loss: 8943.638672 [ 1450/ 1682]\n",
      "loss: 9069.686523 [ 1460/ 1682]\n",
      "loss: 10573.150391 [ 1470/ 1682]\n",
      "loss: 13181.151367 [ 1480/ 1682]\n",
      "loss: 14413.132812 [ 1490/ 1682]\n",
      "loss: 15237.037109 [ 1500/ 1682]\n",
      "loss: 11587.556641 [ 1510/ 1682]\n",
      "loss: 13268.095703 [ 1520/ 1682]\n",
      "loss: 11748.468750 [ 1530/ 1682]\n",
      "loss: 10821.567383 [ 1540/ 1682]\n",
      "loss: 11106.618164 [ 1550/ 1682]\n",
      "loss: 12219.880859 [ 1560/ 1682]\n",
      "loss: 12948.924805 [ 1570/ 1682]\n",
      "loss: 10921.255859 [ 1580/ 1682]\n",
      "loss: 7378.549805 [ 1590/ 1682]\n",
      "loss: 7577.954590 [ 1600/ 1682]\n",
      "loss: 5966.116211 [ 1610/ 1682]\n",
      "loss: 6694.460938 [ 1620/ 1682]\n",
      "loss: 8152.778320 [ 1630/ 1682]\n",
      "loss: 10140.189453 [ 1640/ 1682]\n",
      "loss: 12945.869141 [ 1650/ 1682]\n",
      "loss: 10409.923828 [ 1660/ 1682]\n",
      "loss: 8433.247070 [ 1670/ 1682]\n",
      "loss: 6976.629883 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 11209.997202 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1827.636719 [    0/ 1682]\n",
      "loss: 1545.511475 [   10/ 1682]\n",
      "loss: 1711.247314 [   20/ 1682]\n",
      "loss: 1615.052246 [   30/ 1682]\n",
      "loss: 1875.738892 [   40/ 1682]\n",
      "loss: 1158.425781 [   50/ 1682]\n",
      "loss: 1632.330078 [   60/ 1682]\n",
      "loss: 1475.062256 [   70/ 1682]\n",
      "loss: 1387.751709 [   80/ 1682]\n",
      "loss: 865.549194 [   90/ 1682]\n",
      "loss: 2000.654663 [  100/ 1682]\n",
      "loss: 2108.535645 [  110/ 1682]\n",
      "loss: 2064.318604 [  120/ 1682]\n",
      "loss: 1818.659180 [  130/ 1682]\n",
      "loss: 1378.520874 [  140/ 1682]\n",
      "loss: 989.131042 [  150/ 1682]\n",
      "loss: 724.130737 [  160/ 1682]\n",
      "loss: 823.090820 [  170/ 1682]\n",
      "loss: 898.882446 [  180/ 1682]\n",
      "loss: 863.328308 [  190/ 1682]\n",
      "loss: 948.662598 [  200/ 1682]\n",
      "loss: 1862.800171 [  210/ 1682]\n",
      "loss: 1552.285889 [  220/ 1682]\n",
      "loss: 1237.422485 [  230/ 1682]\n",
      "loss: 1166.010010 [  240/ 1682]\n",
      "loss: 1090.405029 [  250/ 1682]\n",
      "loss: 1024.743896 [  260/ 1682]\n",
      "loss: 1106.785522 [  270/ 1682]\n",
      "loss: 582.714722 [  280/ 1682]\n",
      "loss: 1117.385986 [  290/ 1682]\n",
      "loss: 629.328796 [  300/ 1682]\n",
      "loss: 1101.416016 [  310/ 1682]\n",
      "loss: 512.353821 [  320/ 1682]\n",
      "loss: 776.786438 [  330/ 1682]\n",
      "loss: 235.363678 [  340/ 1682]\n",
      "loss: 500.296326 [  350/ 1682]\n",
      "loss: 603.831787 [  360/ 1682]\n",
      "loss: 1105.548584 [  370/ 1682]\n",
      "loss: 511.925476 [  380/ 1682]\n",
      "loss: 487.402435 [  390/ 1682]\n",
      "loss: 321.788757 [  400/ 1682]\n",
      "loss: 540.113159 [  410/ 1682]\n",
      "loss: 428.819519 [  420/ 1682]\n",
      "loss: 408.704346 [  430/ 1682]\n",
      "loss: 432.095886 [  440/ 1682]\n",
      "loss: 170.498337 [  450/ 1682]\n",
      "loss: 306.473938 [  460/ 1682]\n",
      "loss: 469.280182 [  470/ 1682]\n",
      "loss: 224.088013 [  480/ 1682]\n",
      "loss: 291.332092 [  490/ 1682]\n",
      "loss: 338.063477 [  500/ 1682]\n",
      "loss: 216.518112 [  510/ 1682]\n",
      "loss: 267.769867 [  520/ 1682]\n",
      "loss: 158.814789 [  530/ 1682]\n",
      "loss: 173.481903 [  540/ 1682]\n",
      "loss: 355.191345 [  550/ 1682]\n",
      "loss: 370.460114 [  560/ 1682]\n",
      "loss: 205.054398 [  570/ 1682]\n",
      "loss: 140.772263 [  580/ 1682]\n",
      "loss: 369.001801 [  590/ 1682]\n",
      "loss: 155.353760 [  600/ 1682]\n",
      "loss: 210.327438 [  610/ 1682]\n",
      "loss: 90.176331 [  620/ 1682]\n",
      "loss: 229.364548 [  630/ 1682]\n",
      "loss: 62.589333 [  640/ 1682]\n",
      "loss: 81.992668 [  650/ 1682]\n",
      "loss: 71.127953 [  660/ 1682]\n",
      "loss: 75.691956 [  670/ 1682]\n",
      "loss: 6.989426 [  680/ 1682]\n",
      "loss: 77.147766 [  690/ 1682]\n",
      "loss: 100.629402 [  700/ 1682]\n",
      "loss: 190.791840 [  710/ 1682]\n",
      "loss: 213.857819 [  720/ 1682]\n",
      "loss: 234.117401 [  730/ 1682]\n",
      "loss: 258.869202 [  740/ 1682]\n",
      "loss: 313.081665 [  750/ 1682]\n",
      "loss: 323.992706 [  760/ 1682]\n",
      "loss: 287.614685 [  770/ 1682]\n",
      "loss: 238.913010 [  780/ 1682]\n",
      "loss: 214.476227 [  790/ 1682]\n",
      "loss: 79.697197 [  800/ 1682]\n",
      "loss: 104.717957 [  810/ 1682]\n",
      "loss: 41.618328 [  820/ 1682]\n",
      "loss: 61.422932 [  830/ 1682]\n",
      "loss: 178.565765 [  840/ 1682]\n",
      "loss: 83.364265 [  850/ 1682]\n",
      "loss: 36.940208 [  860/ 1682]\n",
      "loss: 45.529594 [  870/ 1682]\n",
      "loss: 33.873512 [  880/ 1682]\n",
      "loss: 77.422058 [  890/ 1682]\n",
      "loss: 108.506813 [  900/ 1682]\n",
      "loss: 22.001789 [  910/ 1682]\n",
      "loss: 79.050781 [  920/ 1682]\n",
      "loss: 39.348888 [  930/ 1682]\n",
      "loss: 47.463333 [  940/ 1682]\n",
      "loss: 73.823601 [  950/ 1682]\n",
      "loss: 124.282974 [  960/ 1682]\n",
      "loss: 130.815292 [  970/ 1682]\n",
      "loss: 202.832031 [  980/ 1682]\n",
      "loss: 296.936096 [  990/ 1682]\n",
      "loss: 535.121094 [ 1000/ 1682]\n",
      "loss: 530.741821 [ 1010/ 1682]\n",
      "loss: 747.335083 [ 1020/ 1682]\n",
      "loss: 363.282959 [ 1030/ 1682]\n",
      "loss: 240.816238 [ 1040/ 1682]\n",
      "loss: 82.922096 [ 1050/ 1682]\n",
      "loss: 132.302567 [ 1060/ 1682]\n",
      "loss: 161.702133 [ 1070/ 1682]\n",
      "loss: 516.998047 [ 1080/ 1682]\n",
      "loss: 465.031250 [ 1090/ 1682]\n",
      "loss: 695.249146 [ 1100/ 1682]\n",
      "loss: 933.649719 [ 1110/ 1682]\n",
      "loss: 1385.364258 [ 1120/ 1682]\n",
      "loss: 1730.541626 [ 1130/ 1682]\n",
      "loss: 2606.637939 [ 1140/ 1682]\n",
      "loss: 3266.620117 [ 1150/ 1682]\n",
      "loss: 4882.913574 [ 1160/ 1682]\n",
      "loss: 3198.053467 [ 1170/ 1682]\n",
      "loss: 3414.731934 [ 1180/ 1682]\n",
      "loss: 4032.992676 [ 1190/ 1682]\n",
      "loss: 3189.744629 [ 1200/ 1682]\n",
      "loss: 3638.615967 [ 1210/ 1682]\n",
      "loss: 3828.141357 [ 1220/ 1682]\n",
      "loss: 4388.415527 [ 1230/ 1682]\n",
      "loss: 5333.628418 [ 1240/ 1682]\n",
      "loss: 5229.045898 [ 1250/ 1682]\n",
      "loss: 6311.519531 [ 1260/ 1682]\n",
      "loss: 6274.096680 [ 1270/ 1682]\n",
      "loss: 5052.019043 [ 1280/ 1682]\n",
      "loss: 4093.853027 [ 1290/ 1682]\n",
      "loss: 4154.729492 [ 1300/ 1682]\n",
      "loss: 5535.183105 [ 1310/ 1682]\n",
      "loss: 6254.737793 [ 1320/ 1682]\n",
      "loss: 5422.700195 [ 1330/ 1682]\n",
      "loss: 4741.211914 [ 1340/ 1682]\n",
      "loss: 4601.436523 [ 1350/ 1682]\n",
      "loss: 5713.209961 [ 1360/ 1682]\n",
      "loss: 6736.428223 [ 1370/ 1682]\n",
      "loss: 7684.864746 [ 1380/ 1682]\n",
      "loss: 7236.523438 [ 1390/ 1682]\n",
      "loss: 8906.465820 [ 1400/ 1682]\n",
      "loss: 8387.530273 [ 1410/ 1682]\n",
      "loss: 8879.978516 [ 1420/ 1682]\n",
      "loss: 7558.893555 [ 1430/ 1682]\n",
      "loss: 6928.476562 [ 1440/ 1682]\n",
      "loss: 8740.832031 [ 1450/ 1682]\n",
      "loss: 8014.109375 [ 1460/ 1682]\n",
      "loss: 10470.529297 [ 1470/ 1682]\n",
      "loss: 12293.230469 [ 1480/ 1682]\n",
      "loss: 12835.020508 [ 1490/ 1682]\n",
      "loss: 13117.240234 [ 1500/ 1682]\n",
      "loss: 11724.517578 [ 1510/ 1682]\n",
      "loss: 13719.806641 [ 1520/ 1682]\n",
      "loss: 12132.434570 [ 1530/ 1682]\n",
      "loss: 9374.099609 [ 1540/ 1682]\n",
      "loss: 11934.665039 [ 1550/ 1682]\n",
      "loss: 14431.588867 [ 1560/ 1682]\n",
      "loss: 10833.787109 [ 1570/ 1682]\n",
      "loss: 10288.549805 [ 1580/ 1682]\n",
      "loss: 7682.344727 [ 1590/ 1682]\n",
      "loss: 7338.778320 [ 1600/ 1682]\n",
      "loss: 5841.614258 [ 1610/ 1682]\n",
      "loss: 6342.110840 [ 1620/ 1682]\n",
      "loss: 7982.707031 [ 1630/ 1682]\n",
      "loss: 10361.161133 [ 1640/ 1682]\n",
      "loss: 10935.053711 [ 1650/ 1682]\n",
      "loss: 10554.346680 [ 1660/ 1682]\n",
      "loss: 10006.228516 [ 1670/ 1682]\n",
      "loss: 9882.981445 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10917.354924 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1312.306885 [    0/ 1682]\n",
      "loss: 1678.029907 [   10/ 1682]\n",
      "loss: 2078.000488 [   20/ 1682]\n",
      "loss: 999.915161 [   30/ 1682]\n",
      "loss: 1231.519287 [   40/ 1682]\n",
      "loss: 1795.987549 [   50/ 1682]\n",
      "loss: 728.433655 [   60/ 1682]\n",
      "loss: 1253.957764 [   70/ 1682]\n",
      "loss: 954.639526 [   80/ 1682]\n",
      "loss: 900.470093 [   90/ 1682]\n",
      "loss: 1197.278076 [  100/ 1682]\n",
      "loss: 906.322632 [  110/ 1682]\n",
      "loss: 1265.622070 [  120/ 1682]\n",
      "loss: 2117.215332 [  130/ 1682]\n",
      "loss: 1368.713379 [  140/ 1682]\n",
      "loss: 1973.685547 [  150/ 1682]\n",
      "loss: 1112.371460 [  160/ 1682]\n",
      "loss: 1539.644897 [  170/ 1682]\n",
      "loss: 1741.270752 [  180/ 1682]\n",
      "loss: 1843.166260 [  190/ 1682]\n",
      "loss: 938.593445 [  200/ 1682]\n",
      "loss: 1573.538330 [  210/ 1682]\n",
      "loss: 1433.446045 [  220/ 1682]\n",
      "loss: 1019.249390 [  230/ 1682]\n",
      "loss: 878.896179 [  240/ 1682]\n",
      "loss: 1092.264404 [  250/ 1682]\n",
      "loss: 1105.229614 [  260/ 1682]\n",
      "loss: 1075.296631 [  270/ 1682]\n",
      "loss: 954.774780 [  280/ 1682]\n",
      "loss: 694.533569 [  290/ 1682]\n",
      "loss: 700.618042 [  300/ 1682]\n",
      "loss: 347.378632 [  310/ 1682]\n",
      "loss: 905.680359 [  320/ 1682]\n",
      "loss: 469.336182 [  330/ 1682]\n",
      "loss: 358.518555 [  340/ 1682]\n",
      "loss: 489.678070 [  350/ 1682]\n",
      "loss: 854.539734 [  360/ 1682]\n",
      "loss: 537.332642 [  370/ 1682]\n",
      "loss: 979.238159 [  380/ 1682]\n",
      "loss: 1045.266357 [  390/ 1682]\n",
      "loss: 647.349976 [  400/ 1682]\n",
      "loss: 1180.232544 [  410/ 1682]\n",
      "loss: 394.660645 [  420/ 1682]\n",
      "loss: 517.897461 [  430/ 1682]\n",
      "loss: 531.948792 [  440/ 1682]\n",
      "loss: 723.803528 [  450/ 1682]\n",
      "loss: 315.752838 [  460/ 1682]\n",
      "loss: 595.110352 [  470/ 1682]\n",
      "loss: 293.819183 [  480/ 1682]\n",
      "loss: 311.767639 [  490/ 1682]\n",
      "loss: 301.589417 [  500/ 1682]\n",
      "loss: 304.763336 [  510/ 1682]\n",
      "loss: 408.982361 [  520/ 1682]\n",
      "loss: 203.017502 [  530/ 1682]\n",
      "loss: 263.062469 [  540/ 1682]\n",
      "loss: 338.413300 [  550/ 1682]\n",
      "loss: 413.194641 [  560/ 1682]\n",
      "loss: 151.208084 [  570/ 1682]\n",
      "loss: 398.930084 [  580/ 1682]\n",
      "loss: 264.523865 [  590/ 1682]\n",
      "loss: 303.403168 [  600/ 1682]\n",
      "loss: 186.278488 [  610/ 1682]\n",
      "loss: 166.791626 [  620/ 1682]\n",
      "loss: 207.978195 [  630/ 1682]\n",
      "loss: 108.636642 [  640/ 1682]\n",
      "loss: 93.221939 [  650/ 1682]\n",
      "loss: 54.570457 [  660/ 1682]\n",
      "loss: 143.849197 [  670/ 1682]\n",
      "loss: 4.450710 [  680/ 1682]\n",
      "loss: 58.599476 [  690/ 1682]\n",
      "loss: 74.214706 [  700/ 1682]\n",
      "loss: 22.401478 [  710/ 1682]\n",
      "loss: 494.128113 [  720/ 1682]\n",
      "loss: 248.547440 [  730/ 1682]\n",
      "loss: 417.704498 [  740/ 1682]\n",
      "loss: 584.292786 [  750/ 1682]\n",
      "loss: 432.877350 [  760/ 1682]\n",
      "loss: 112.073936 [  770/ 1682]\n",
      "loss: 229.848602 [  780/ 1682]\n",
      "loss: 142.227478 [  790/ 1682]\n",
      "loss: 94.469994 [  800/ 1682]\n",
      "loss: 62.263733 [  810/ 1682]\n",
      "loss: 37.393021 [  820/ 1682]\n",
      "loss: 91.541458 [  830/ 1682]\n",
      "loss: 245.699066 [  840/ 1682]\n",
      "loss: 50.173164 [  850/ 1682]\n",
      "loss: 17.147451 [  860/ 1682]\n",
      "loss: 76.771019 [  870/ 1682]\n",
      "loss: 114.836937 [  880/ 1682]\n",
      "loss: 97.637589 [  890/ 1682]\n",
      "loss: 114.008255 [  900/ 1682]\n",
      "loss: 82.090729 [  910/ 1682]\n",
      "loss: 13.612269 [  920/ 1682]\n",
      "loss: 29.340399 [  930/ 1682]\n",
      "loss: 29.488041 [  940/ 1682]\n",
      "loss: 57.852844 [  950/ 1682]\n",
      "loss: 143.689423 [  960/ 1682]\n",
      "loss: 141.976166 [  970/ 1682]\n",
      "loss: 186.626709 [  980/ 1682]\n",
      "loss: 350.424316 [  990/ 1682]\n",
      "loss: 421.723724 [ 1000/ 1682]\n",
      "loss: 597.082581 [ 1010/ 1682]\n",
      "loss: 436.302826 [ 1020/ 1682]\n",
      "loss: 410.386414 [ 1030/ 1682]\n",
      "loss: 165.134293 [ 1040/ 1682]\n",
      "loss: 69.018723 [ 1050/ 1682]\n",
      "loss: 146.951935 [ 1060/ 1682]\n",
      "loss: 193.003876 [ 1070/ 1682]\n",
      "loss: 419.163879 [ 1080/ 1682]\n",
      "loss: 488.410156 [ 1090/ 1682]\n",
      "loss: 804.193848 [ 1100/ 1682]\n",
      "loss: 1154.355103 [ 1110/ 1682]\n",
      "loss: 1434.599243 [ 1120/ 1682]\n",
      "loss: 1294.465210 [ 1130/ 1682]\n",
      "loss: 2403.750977 [ 1140/ 1682]\n",
      "loss: 3474.743652 [ 1150/ 1682]\n",
      "loss: 4331.995117 [ 1160/ 1682]\n",
      "loss: 3130.975830 [ 1170/ 1682]\n",
      "loss: 2712.938477 [ 1180/ 1682]\n",
      "loss: 4120.210938 [ 1190/ 1682]\n",
      "loss: 2656.120605 [ 1200/ 1682]\n",
      "loss: 3510.011719 [ 1210/ 1682]\n",
      "loss: 3711.205566 [ 1220/ 1682]\n",
      "loss: 4607.330566 [ 1230/ 1682]\n",
      "loss: 5445.885742 [ 1240/ 1682]\n",
      "loss: 5285.828613 [ 1250/ 1682]\n",
      "loss: 6252.899414 [ 1260/ 1682]\n",
      "loss: 5883.044922 [ 1270/ 1682]\n",
      "loss: 4956.356934 [ 1280/ 1682]\n",
      "loss: 3823.614502 [ 1290/ 1682]\n",
      "loss: 4202.927246 [ 1300/ 1682]\n",
      "loss: 4776.980469 [ 1310/ 1682]\n",
      "loss: 5288.507324 [ 1320/ 1682]\n",
      "loss: 5777.157715 [ 1330/ 1682]\n",
      "loss: 4947.708008 [ 1340/ 1682]\n",
      "loss: 4902.966309 [ 1350/ 1682]\n",
      "loss: 5366.554688 [ 1360/ 1682]\n",
      "loss: 6890.338379 [ 1370/ 1682]\n",
      "loss: 7510.186035 [ 1380/ 1682]\n",
      "loss: 8242.654297 [ 1390/ 1682]\n",
      "loss: 8134.301758 [ 1400/ 1682]\n",
      "loss: 8547.452148 [ 1410/ 1682]\n",
      "loss: 8605.277344 [ 1420/ 1682]\n",
      "loss: 7120.708008 [ 1430/ 1682]\n",
      "loss: 7473.104492 [ 1440/ 1682]\n",
      "loss: 8669.107422 [ 1450/ 1682]\n",
      "loss: 9235.949219 [ 1460/ 1682]\n",
      "loss: 10274.191406 [ 1470/ 1682]\n",
      "loss: 12372.575195 [ 1480/ 1682]\n",
      "loss: 13717.698242 [ 1490/ 1682]\n",
      "loss: 14439.596680 [ 1500/ 1682]\n",
      "loss: 10051.585938 [ 1510/ 1682]\n",
      "loss: 13515.775391 [ 1520/ 1682]\n",
      "loss: 11974.501953 [ 1530/ 1682]\n",
      "loss: 11385.363281 [ 1540/ 1682]\n",
      "loss: 9727.728516 [ 1550/ 1682]\n",
      "loss: 14148.158203 [ 1560/ 1682]\n",
      "loss: 11357.294922 [ 1570/ 1682]\n",
      "loss: 10719.829102 [ 1580/ 1682]\n",
      "loss: 7450.333496 [ 1590/ 1682]\n",
      "loss: 8045.127441 [ 1600/ 1682]\n",
      "loss: 5124.889648 [ 1610/ 1682]\n",
      "loss: 7014.437500 [ 1620/ 1682]\n",
      "loss: 7966.294434 [ 1630/ 1682]\n",
      "loss: 9836.691406 [ 1640/ 1682]\n",
      "loss: 12249.448242 [ 1650/ 1682]\n",
      "loss: 11297.511719 [ 1660/ 1682]\n",
      "loss: 8165.474121 [ 1670/ 1682]\n",
      "loss: 9769.035156 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10917.952130 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1292.831787 [    0/ 1682]\n",
      "loss: 1984.829346 [   10/ 1682]\n",
      "loss: 950.540649 [   20/ 1682]\n",
      "loss: 1499.019165 [   30/ 1682]\n",
      "loss: 843.892700 [   40/ 1682]\n",
      "loss: 2137.032959 [   50/ 1682]\n",
      "loss: 1856.027710 [   60/ 1682]\n",
      "loss: 1148.448486 [   70/ 1682]\n",
      "loss: 990.502747 [   80/ 1682]\n",
      "loss: 935.291626 [   90/ 1682]\n",
      "loss: 1986.027588 [  100/ 1682]\n",
      "loss: 1200.416260 [  110/ 1682]\n",
      "loss: 1906.859741 [  120/ 1682]\n",
      "loss: 1648.420288 [  130/ 1682]\n",
      "loss: 928.771362 [  140/ 1682]\n",
      "loss: 1660.678711 [  150/ 1682]\n",
      "loss: 1097.811157 [  160/ 1682]\n",
      "loss: 2022.708252 [  170/ 1682]\n",
      "loss: 1029.058594 [  180/ 1682]\n",
      "loss: 1074.903564 [  190/ 1682]\n",
      "loss: 967.028809 [  200/ 1682]\n",
      "loss: 1202.265381 [  210/ 1682]\n",
      "loss: 1791.650635 [  220/ 1682]\n",
      "loss: 954.423157 [  230/ 1682]\n",
      "loss: 865.923462 [  240/ 1682]\n",
      "loss: 881.695923 [  250/ 1682]\n",
      "loss: 1038.750244 [  260/ 1682]\n",
      "loss: 1171.084717 [  270/ 1682]\n",
      "loss: 668.550903 [  280/ 1682]\n",
      "loss: 989.684937 [  290/ 1682]\n",
      "loss: 1042.710938 [  300/ 1682]\n",
      "loss: 1055.602539 [  310/ 1682]\n",
      "loss: 733.924622 [  320/ 1682]\n",
      "loss: 296.684143 [  330/ 1682]\n",
      "loss: 393.422638 [  340/ 1682]\n",
      "loss: 1288.201904 [  350/ 1682]\n",
      "loss: 601.502502 [  360/ 1682]\n",
      "loss: 644.300049 [  370/ 1682]\n",
      "loss: 681.744751 [  380/ 1682]\n",
      "loss: 414.582184 [  390/ 1682]\n",
      "loss: 732.391907 [  400/ 1682]\n",
      "loss: 328.876801 [  410/ 1682]\n",
      "loss: 484.078552 [  420/ 1682]\n",
      "loss: 334.402405 [  430/ 1682]\n",
      "loss: 611.293579 [  440/ 1682]\n",
      "loss: 517.985352 [  450/ 1682]\n",
      "loss: 215.937057 [  460/ 1682]\n",
      "loss: 538.885376 [  470/ 1682]\n",
      "loss: 252.412674 [  480/ 1682]\n",
      "loss: 336.255676 [  490/ 1682]\n",
      "loss: 612.732483 [  500/ 1682]\n",
      "loss: 470.999939 [  510/ 1682]\n",
      "loss: 195.943359 [  520/ 1682]\n",
      "loss: 388.592499 [  530/ 1682]\n",
      "loss: 464.211365 [  540/ 1682]\n",
      "loss: 242.874466 [  550/ 1682]\n",
      "loss: 513.441040 [  560/ 1682]\n",
      "loss: 165.521652 [  570/ 1682]\n",
      "loss: 148.006012 [  580/ 1682]\n",
      "loss: 202.653015 [  590/ 1682]\n",
      "loss: 83.655472 [  600/ 1682]\n",
      "loss: 195.603760 [  610/ 1682]\n",
      "loss: 265.571625 [  620/ 1682]\n",
      "loss: 116.249405 [  630/ 1682]\n",
      "loss: 94.693497 [  640/ 1682]\n",
      "loss: 121.307945 [  650/ 1682]\n",
      "loss: 25.572754 [  660/ 1682]\n",
      "loss: 89.159142 [  670/ 1682]\n",
      "loss: 48.131302 [  680/ 1682]\n",
      "loss: 91.966148 [  690/ 1682]\n",
      "loss: 89.834396 [  700/ 1682]\n",
      "loss: 27.641052 [  710/ 1682]\n",
      "loss: 306.340942 [  720/ 1682]\n",
      "loss: 399.190704 [  730/ 1682]\n",
      "loss: 524.938599 [  740/ 1682]\n",
      "loss: 509.418274 [  750/ 1682]\n",
      "loss: 456.394714 [  760/ 1682]\n",
      "loss: 247.181839 [  770/ 1682]\n",
      "loss: 232.985397 [  780/ 1682]\n",
      "loss: 151.601410 [  790/ 1682]\n",
      "loss: 97.608910 [  800/ 1682]\n",
      "loss: 271.515350 [  810/ 1682]\n",
      "loss: 115.729065 [  820/ 1682]\n",
      "loss: 55.826496 [  830/ 1682]\n",
      "loss: 173.251892 [  840/ 1682]\n",
      "loss: 244.310913 [  850/ 1682]\n",
      "loss: 104.995934 [  860/ 1682]\n",
      "loss: 42.124687 [  870/ 1682]\n",
      "loss: 28.169418 [  880/ 1682]\n",
      "loss: 61.330231 [  890/ 1682]\n",
      "loss: 85.249298 [  900/ 1682]\n",
      "loss: 71.913628 [  910/ 1682]\n",
      "loss: 1.653525 [  920/ 1682]\n",
      "loss: 33.613396 [  930/ 1682]\n",
      "loss: 19.741426 [  940/ 1682]\n",
      "loss: 52.315727 [  950/ 1682]\n",
      "loss: 119.257347 [  960/ 1682]\n",
      "loss: 139.427902 [  970/ 1682]\n",
      "loss: 180.308411 [  980/ 1682]\n",
      "loss: 305.112610 [  990/ 1682]\n",
      "loss: 526.386230 [ 1000/ 1682]\n",
      "loss: 610.897827 [ 1010/ 1682]\n",
      "loss: 640.793701 [ 1020/ 1682]\n",
      "loss: 351.822601 [ 1030/ 1682]\n",
      "loss: 166.028458 [ 1040/ 1682]\n",
      "loss: 42.980453 [ 1050/ 1682]\n",
      "loss: 98.418304 [ 1060/ 1682]\n",
      "loss: 180.023819 [ 1070/ 1682]\n",
      "loss: 466.551361 [ 1080/ 1682]\n",
      "loss: 565.685364 [ 1090/ 1682]\n",
      "loss: 823.243347 [ 1100/ 1682]\n",
      "loss: 959.033325 [ 1110/ 1682]\n",
      "loss: 1186.911011 [ 1120/ 1682]\n",
      "loss: 1464.674683 [ 1130/ 1682]\n",
      "loss: 2397.230469 [ 1140/ 1682]\n",
      "loss: 3557.746094 [ 1150/ 1682]\n",
      "loss: 4775.868164 [ 1160/ 1682]\n",
      "loss: 3096.443848 [ 1170/ 1682]\n",
      "loss: 2865.560791 [ 1180/ 1682]\n",
      "loss: 3219.929932 [ 1190/ 1682]\n",
      "loss: 3540.968018 [ 1200/ 1682]\n",
      "loss: 3596.164795 [ 1210/ 1682]\n",
      "loss: 3309.801270 [ 1220/ 1682]\n",
      "loss: 4396.417480 [ 1230/ 1682]\n",
      "loss: 5639.151367 [ 1240/ 1682]\n",
      "loss: 5026.487305 [ 1250/ 1682]\n",
      "loss: 6361.029785 [ 1260/ 1682]\n",
      "loss: 6301.628906 [ 1270/ 1682]\n",
      "loss: 4477.791504 [ 1280/ 1682]\n",
      "loss: 4159.243164 [ 1290/ 1682]\n",
      "loss: 4354.971680 [ 1300/ 1682]\n",
      "loss: 4630.722168 [ 1310/ 1682]\n",
      "loss: 6274.663086 [ 1320/ 1682]\n",
      "loss: 4996.722168 [ 1330/ 1682]\n",
      "loss: 4679.036133 [ 1340/ 1682]\n",
      "loss: 4653.156250 [ 1350/ 1682]\n",
      "loss: 4898.262207 [ 1360/ 1682]\n",
      "loss: 6530.729004 [ 1370/ 1682]\n",
      "loss: 7779.350586 [ 1380/ 1682]\n",
      "loss: 8183.256348 [ 1390/ 1682]\n",
      "loss: 8051.825684 [ 1400/ 1682]\n",
      "loss: 8556.852539 [ 1410/ 1682]\n",
      "loss: 9022.853516 [ 1420/ 1682]\n",
      "loss: 7764.217285 [ 1430/ 1682]\n",
      "loss: 7111.143066 [ 1440/ 1682]\n",
      "loss: 8289.467773 [ 1450/ 1682]\n",
      "loss: 8447.607422 [ 1460/ 1682]\n",
      "loss: 10211.853516 [ 1470/ 1682]\n",
      "loss: 12506.567383 [ 1480/ 1682]\n",
      "loss: 14526.705078 [ 1490/ 1682]\n",
      "loss: 13490.958008 [ 1500/ 1682]\n",
      "loss: 11768.632812 [ 1510/ 1682]\n",
      "loss: 12516.648438 [ 1520/ 1682]\n",
      "loss: 11932.958008 [ 1530/ 1682]\n",
      "loss: 10402.062500 [ 1540/ 1682]\n",
      "loss: 10829.598633 [ 1550/ 1682]\n",
      "loss: 12630.385742 [ 1560/ 1682]\n",
      "loss: 12558.206055 [ 1570/ 1682]\n",
      "loss: 9705.589844 [ 1580/ 1682]\n",
      "loss: 7634.213867 [ 1590/ 1682]\n",
      "loss: 7172.201660 [ 1600/ 1682]\n",
      "loss: 6191.983398 [ 1610/ 1682]\n",
      "loss: 6486.152832 [ 1620/ 1682]\n",
      "loss: 6878.268555 [ 1630/ 1682]\n",
      "loss: 9586.126953 [ 1640/ 1682]\n",
      "loss: 11536.907227 [ 1650/ 1682]\n",
      "loss: 11436.150391 [ 1660/ 1682]\n",
      "loss: 8705.473633 [ 1670/ 1682]\n",
      "loss: 9656.538086 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10712.511756 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1221.479492 [    0/ 1682]\n",
      "loss: 1818.316772 [   10/ 1682]\n",
      "loss: 2041.944580 [   20/ 1682]\n",
      "loss: 1390.996582 [   30/ 1682]\n",
      "loss: 877.354126 [   40/ 1682]\n",
      "loss: 1142.692627 [   50/ 1682]\n",
      "loss: 994.117676 [   60/ 1682]\n",
      "loss: 1238.165405 [   70/ 1682]\n",
      "loss: 1412.531128 [   80/ 1682]\n",
      "loss: 2336.806396 [   90/ 1682]\n",
      "loss: 1654.802368 [  100/ 1682]\n",
      "loss: 2009.869141 [  110/ 1682]\n",
      "loss: 1969.029663 [  120/ 1682]\n",
      "loss: 1554.165283 [  130/ 1682]\n",
      "loss: 1960.283203 [  140/ 1682]\n",
      "loss: 1353.994873 [  150/ 1682]\n",
      "loss: 1446.575195 [  160/ 1682]\n",
      "loss: 770.420044 [  170/ 1682]\n",
      "loss: 1269.259033 [  180/ 1682]\n",
      "loss: 958.244263 [  190/ 1682]\n",
      "loss: 1011.385742 [  200/ 1682]\n",
      "loss: 785.462097 [  210/ 1682]\n",
      "loss: 966.804871 [  220/ 1682]\n",
      "loss: 719.463440 [  230/ 1682]\n",
      "loss: 1120.768188 [  240/ 1682]\n",
      "loss: 910.002747 [  250/ 1682]\n",
      "loss: 809.906799 [  260/ 1682]\n",
      "loss: 1250.007324 [  270/ 1682]\n",
      "loss: 876.509094 [  280/ 1682]\n",
      "loss: 1032.329712 [  290/ 1682]\n",
      "loss: 1020.489258 [  300/ 1682]\n",
      "loss: 726.805542 [  310/ 1682]\n",
      "loss: 581.895386 [  320/ 1682]\n",
      "loss: 523.861023 [  330/ 1682]\n",
      "loss: 781.435669 [  340/ 1682]\n",
      "loss: 417.206360 [  350/ 1682]\n",
      "loss: 501.537292 [  360/ 1682]\n",
      "loss: 548.277710 [  370/ 1682]\n",
      "loss: 311.516113 [  380/ 1682]\n",
      "loss: 641.294495 [  390/ 1682]\n",
      "loss: 347.119171 [  400/ 1682]\n",
      "loss: 740.451538 [  410/ 1682]\n",
      "loss: 483.656311 [  420/ 1682]\n",
      "loss: 519.572937 [  430/ 1682]\n",
      "loss: 506.096863 [  440/ 1682]\n",
      "loss: 437.685150 [  450/ 1682]\n",
      "loss: 322.867249 [  460/ 1682]\n",
      "loss: 367.407288 [  470/ 1682]\n",
      "loss: 438.795746 [  480/ 1682]\n",
      "loss: 330.071350 [  490/ 1682]\n",
      "loss: 332.277374 [  500/ 1682]\n",
      "loss: 383.654694 [  510/ 1682]\n",
      "loss: 504.459229 [  520/ 1682]\n",
      "loss: 352.921051 [  530/ 1682]\n",
      "loss: 288.717407 [  540/ 1682]\n",
      "loss: 486.388733 [  550/ 1682]\n",
      "loss: 227.077835 [  560/ 1682]\n",
      "loss: 353.374939 [  570/ 1682]\n",
      "loss: 250.085236 [  580/ 1682]\n",
      "loss: 228.134521 [  590/ 1682]\n",
      "loss: 191.852203 [  600/ 1682]\n",
      "loss: 206.687500 [  610/ 1682]\n",
      "loss: 245.616425 [  620/ 1682]\n",
      "loss: 127.161209 [  630/ 1682]\n",
      "loss: 154.895721 [  640/ 1682]\n",
      "loss: 4.836237 [  650/ 1682]\n",
      "loss: 163.794098 [  660/ 1682]\n",
      "loss: 75.703094 [  670/ 1682]\n",
      "loss: 33.168976 [  680/ 1682]\n",
      "loss: 133.827332 [  690/ 1682]\n",
      "loss: 2.970719 [  700/ 1682]\n",
      "loss: 187.007004 [  710/ 1682]\n",
      "loss: 187.523849 [  720/ 1682]\n",
      "loss: 447.155090 [  730/ 1682]\n",
      "loss: 567.638245 [  740/ 1682]\n",
      "loss: 370.061218 [  750/ 1682]\n",
      "loss: 380.838501 [  760/ 1682]\n",
      "loss: 200.009735 [  770/ 1682]\n",
      "loss: 291.865570 [  780/ 1682]\n",
      "loss: 133.718811 [  790/ 1682]\n",
      "loss: 158.244858 [  800/ 1682]\n",
      "loss: 105.211319 [  810/ 1682]\n",
      "loss: 74.860573 [  820/ 1682]\n",
      "loss: 84.632126 [  830/ 1682]\n",
      "loss: 236.542191 [  840/ 1682]\n",
      "loss: 66.876823 [  850/ 1682]\n",
      "loss: 50.875961 [  860/ 1682]\n",
      "loss: 90.737000 [  870/ 1682]\n",
      "loss: 90.138435 [  880/ 1682]\n",
      "loss: 153.945526 [  890/ 1682]\n",
      "loss: 96.336067 [  900/ 1682]\n",
      "loss: 67.919235 [  910/ 1682]\n",
      "loss: 20.874556 [  920/ 1682]\n",
      "loss: 35.955791 [  930/ 1682]\n",
      "loss: 46.424740 [  940/ 1682]\n",
      "loss: 50.405384 [  950/ 1682]\n",
      "loss: 99.319084 [  960/ 1682]\n",
      "loss: 104.732018 [  970/ 1682]\n",
      "loss: 169.648987 [  980/ 1682]\n",
      "loss: 252.415436 [  990/ 1682]\n",
      "loss: 429.301514 [ 1000/ 1682]\n",
      "loss: 436.436829 [ 1010/ 1682]\n",
      "loss: 605.039917 [ 1020/ 1682]\n",
      "loss: 366.071625 [ 1030/ 1682]\n",
      "loss: 181.850494 [ 1040/ 1682]\n",
      "loss: 57.006653 [ 1050/ 1682]\n",
      "loss: 131.900925 [ 1060/ 1682]\n",
      "loss: 139.079559 [ 1070/ 1682]\n",
      "loss: 412.821381 [ 1080/ 1682]\n",
      "loss: 390.023346 [ 1090/ 1682]\n",
      "loss: 676.716370 [ 1100/ 1682]\n",
      "loss: 1076.314209 [ 1110/ 1682]\n",
      "loss: 1262.720215 [ 1120/ 1682]\n",
      "loss: 1700.608032 [ 1130/ 1682]\n",
      "loss: 2309.872559 [ 1140/ 1682]\n",
      "loss: 3072.658691 [ 1150/ 1682]\n",
      "loss: 4494.083008 [ 1160/ 1682]\n",
      "loss: 2918.784424 [ 1170/ 1682]\n",
      "loss: 3369.856934 [ 1180/ 1682]\n",
      "loss: 3682.915527 [ 1190/ 1682]\n",
      "loss: 3186.582031 [ 1200/ 1682]\n",
      "loss: 3728.137451 [ 1210/ 1682]\n",
      "loss: 3456.872314 [ 1220/ 1682]\n",
      "loss: 4292.768555 [ 1230/ 1682]\n",
      "loss: 5292.640137 [ 1240/ 1682]\n",
      "loss: 5127.107910 [ 1250/ 1682]\n",
      "loss: 5590.845215 [ 1260/ 1682]\n",
      "loss: 6020.050293 [ 1270/ 1682]\n",
      "loss: 4341.549805 [ 1280/ 1682]\n",
      "loss: 4238.113281 [ 1290/ 1682]\n",
      "loss: 3899.817627 [ 1300/ 1682]\n",
      "loss: 4664.278320 [ 1310/ 1682]\n",
      "loss: 5754.063477 [ 1320/ 1682]\n",
      "loss: 5353.447754 [ 1330/ 1682]\n",
      "loss: 4635.975586 [ 1340/ 1682]\n",
      "loss: 4114.668945 [ 1350/ 1682]\n",
      "loss: 5243.550293 [ 1360/ 1682]\n",
      "loss: 6935.007812 [ 1370/ 1682]\n",
      "loss: 7703.014160 [ 1380/ 1682]\n",
      "loss: 7306.092773 [ 1390/ 1682]\n",
      "loss: 7969.176758 [ 1400/ 1682]\n",
      "loss: 9037.021484 [ 1410/ 1682]\n",
      "loss: 7934.873535 [ 1420/ 1682]\n",
      "loss: 7660.027344 [ 1430/ 1682]\n",
      "loss: 7018.985352 [ 1440/ 1682]\n",
      "loss: 8834.482422 [ 1450/ 1682]\n",
      "loss: 9019.841797 [ 1460/ 1682]\n",
      "loss: 9720.419922 [ 1470/ 1682]\n",
      "loss: 12292.685547 [ 1480/ 1682]\n",
      "loss: 12985.490234 [ 1490/ 1682]\n",
      "loss: 13723.158203 [ 1500/ 1682]\n",
      "loss: 12435.796875 [ 1510/ 1682]\n",
      "loss: 12434.624023 [ 1520/ 1682]\n",
      "loss: 12753.000977 [ 1530/ 1682]\n",
      "loss: 10193.521484 [ 1540/ 1682]\n",
      "loss: 10299.807617 [ 1550/ 1682]\n",
      "loss: 14476.843750 [ 1560/ 1682]\n",
      "loss: 11066.042969 [ 1570/ 1682]\n",
      "loss: 10634.604492 [ 1580/ 1682]\n",
      "loss: 7936.657227 [ 1590/ 1682]\n",
      "loss: 6884.256348 [ 1600/ 1682]\n",
      "loss: 6501.047852 [ 1610/ 1682]\n",
      "loss: 6329.047852 [ 1620/ 1682]\n",
      "loss: 8237.197266 [ 1630/ 1682]\n",
      "loss: 10121.051758 [ 1640/ 1682]\n",
      "loss: 12541.439453 [ 1650/ 1682]\n",
      "loss: 11305.642578 [ 1660/ 1682]\n",
      "loss: 9842.624023 [ 1670/ 1682]\n",
      "loss: 9546.253906 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10882.730750 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1250.475098 [    0/ 1682]\n",
      "loss: 1525.564697 [   10/ 1682]\n",
      "loss: 1210.123779 [   20/ 1682]\n",
      "loss: 1855.866211 [   30/ 1682]\n",
      "loss: 1224.030762 [   40/ 1682]\n",
      "loss: 1086.572998 [   50/ 1682]\n",
      "loss: 1770.114624 [   60/ 1682]\n",
      "loss: 1733.836670 [   70/ 1682]\n",
      "loss: 1514.256714 [   80/ 1682]\n",
      "loss: 2077.463623 [   90/ 1682]\n",
      "loss: 1849.234619 [  100/ 1682]\n",
      "loss: 1745.456665 [  110/ 1682]\n",
      "loss: 1870.985107 [  120/ 1682]\n",
      "loss: 1272.040771 [  130/ 1682]\n",
      "loss: 859.275269 [  140/ 1682]\n",
      "loss: 2076.283691 [  150/ 1682]\n",
      "loss: 1082.437866 [  160/ 1682]\n",
      "loss: 960.576782 [  170/ 1682]\n",
      "loss: 750.691040 [  180/ 1682]\n",
      "loss: 1122.754028 [  190/ 1682]\n",
      "loss: 1330.036865 [  200/ 1682]\n",
      "loss: 947.540527 [  210/ 1682]\n",
      "loss: 1233.928467 [  220/ 1682]\n",
      "loss: 1121.319580 [  230/ 1682]\n",
      "loss: 701.068542 [  240/ 1682]\n",
      "loss: 1532.218018 [  250/ 1682]\n",
      "loss: 1325.844971 [  260/ 1682]\n",
      "loss: 1165.436768 [  270/ 1682]\n",
      "loss: 653.875610 [  280/ 1682]\n",
      "loss: 780.306030 [  290/ 1682]\n",
      "loss: 594.426575 [  300/ 1682]\n",
      "loss: 851.709473 [  310/ 1682]\n",
      "loss: 657.764709 [  320/ 1682]\n",
      "loss: 697.463440 [  330/ 1682]\n",
      "loss: 535.906372 [  340/ 1682]\n",
      "loss: 594.044434 [  350/ 1682]\n",
      "loss: 740.220825 [  360/ 1682]\n",
      "loss: 761.262756 [  370/ 1682]\n",
      "loss: 674.388428 [  380/ 1682]\n",
      "loss: 423.476959 [  390/ 1682]\n",
      "loss: 592.710876 [  400/ 1682]\n",
      "loss: 240.288162 [  410/ 1682]\n",
      "loss: 275.895447 [  420/ 1682]\n",
      "loss: 302.415649 [  430/ 1682]\n",
      "loss: 471.474701 [  440/ 1682]\n",
      "loss: 490.951904 [  450/ 1682]\n",
      "loss: 594.900818 [  460/ 1682]\n",
      "loss: 168.461304 [  470/ 1682]\n",
      "loss: 333.501282 [  480/ 1682]\n",
      "loss: 448.570160 [  490/ 1682]\n",
      "loss: 327.298645 [  500/ 1682]\n",
      "loss: 345.469635 [  510/ 1682]\n",
      "loss: 483.584564 [  520/ 1682]\n",
      "loss: 148.276733 [  530/ 1682]\n",
      "loss: 415.779114 [  540/ 1682]\n",
      "loss: 181.232300 [  550/ 1682]\n",
      "loss: 262.234406 [  560/ 1682]\n",
      "loss: 396.371735 [  570/ 1682]\n",
      "loss: 142.368866 [  580/ 1682]\n",
      "loss: 85.155571 [  590/ 1682]\n",
      "loss: 242.128693 [  600/ 1682]\n",
      "loss: 142.761734 [  610/ 1682]\n",
      "loss: 139.864899 [  620/ 1682]\n",
      "loss: 192.749298 [  630/ 1682]\n",
      "loss: 100.697556 [  640/ 1682]\n",
      "loss: 70.634392 [  650/ 1682]\n",
      "loss: 66.692787 [  660/ 1682]\n",
      "loss: 67.876671 [  670/ 1682]\n",
      "loss: 0.878638 [  680/ 1682]\n",
      "loss: 23.380047 [  690/ 1682]\n",
      "loss: 115.145729 [  700/ 1682]\n",
      "loss: 70.862221 [  710/ 1682]\n",
      "loss: 216.259232 [  720/ 1682]\n",
      "loss: 301.534241 [  730/ 1682]\n",
      "loss: 279.881287 [  740/ 1682]\n",
      "loss: 434.247314 [  750/ 1682]\n",
      "loss: 356.035187 [  760/ 1682]\n",
      "loss: 201.789505 [  770/ 1682]\n",
      "loss: 242.061737 [  780/ 1682]\n",
      "loss: 191.980545 [  790/ 1682]\n",
      "loss: 106.607590 [  800/ 1682]\n",
      "loss: 70.000427 [  810/ 1682]\n",
      "loss: 140.079849 [  820/ 1682]\n",
      "loss: 100.514786 [  830/ 1682]\n",
      "loss: 151.401520 [  840/ 1682]\n",
      "loss: 170.413300 [  850/ 1682]\n",
      "loss: 33.876553 [  860/ 1682]\n",
      "loss: 92.739944 [  870/ 1682]\n",
      "loss: 134.019974 [  880/ 1682]\n",
      "loss: 75.185883 [  890/ 1682]\n",
      "loss: 35.238361 [  900/ 1682]\n",
      "loss: 108.055786 [  910/ 1682]\n",
      "loss: 50.094196 [  920/ 1682]\n",
      "loss: 34.772514 [  930/ 1682]\n",
      "loss: 18.054644 [  940/ 1682]\n",
      "loss: 42.835072 [  950/ 1682]\n",
      "loss: 67.258881 [  960/ 1682]\n",
      "loss: 79.383835 [  970/ 1682]\n",
      "loss: 128.714645 [  980/ 1682]\n",
      "loss: 198.150574 [  990/ 1682]\n",
      "loss: 475.943787 [ 1000/ 1682]\n",
      "loss: 344.110840 [ 1010/ 1682]\n",
      "loss: 537.024170 [ 1020/ 1682]\n",
      "loss: 274.867615 [ 1030/ 1682]\n",
      "loss: 195.569794 [ 1040/ 1682]\n",
      "loss: 44.749794 [ 1050/ 1682]\n",
      "loss: 128.757919 [ 1060/ 1682]\n",
      "loss: 179.470001 [ 1070/ 1682]\n",
      "loss: 363.201721 [ 1080/ 1682]\n",
      "loss: 520.593140 [ 1090/ 1682]\n",
      "loss: 667.008240 [ 1100/ 1682]\n",
      "loss: 1042.282837 [ 1110/ 1682]\n",
      "loss: 1321.829102 [ 1120/ 1682]\n",
      "loss: 1401.377686 [ 1130/ 1682]\n",
      "loss: 2080.574951 [ 1140/ 1682]\n",
      "loss: 3425.469482 [ 1150/ 1682]\n",
      "loss: 4826.701660 [ 1160/ 1682]\n",
      "loss: 3022.313477 [ 1170/ 1682]\n",
      "loss: 3067.632324 [ 1180/ 1682]\n",
      "loss: 3356.205078 [ 1190/ 1682]\n",
      "loss: 2915.725342 [ 1200/ 1682]\n",
      "loss: 3671.805908 [ 1210/ 1682]\n",
      "loss: 3592.520996 [ 1220/ 1682]\n",
      "loss: 4271.240723 [ 1230/ 1682]\n",
      "loss: 5113.671387 [ 1240/ 1682]\n",
      "loss: 4609.024414 [ 1250/ 1682]\n",
      "loss: 5877.385254 [ 1260/ 1682]\n",
      "loss: 5790.598145 [ 1270/ 1682]\n",
      "loss: 5114.929688 [ 1280/ 1682]\n",
      "loss: 4178.955078 [ 1290/ 1682]\n",
      "loss: 3885.783691 [ 1300/ 1682]\n",
      "loss: 5042.449219 [ 1310/ 1682]\n",
      "loss: 5470.207031 [ 1320/ 1682]\n",
      "loss: 5137.841309 [ 1330/ 1682]\n",
      "loss: 4345.706543 [ 1340/ 1682]\n",
      "loss: 4544.470703 [ 1350/ 1682]\n",
      "loss: 5406.219727 [ 1360/ 1682]\n",
      "loss: 6625.642090 [ 1370/ 1682]\n",
      "loss: 7939.367188 [ 1380/ 1682]\n",
      "loss: 7716.304688 [ 1390/ 1682]\n",
      "loss: 7862.711914 [ 1400/ 1682]\n",
      "loss: 8618.974609 [ 1410/ 1682]\n",
      "loss: 7421.541992 [ 1420/ 1682]\n",
      "loss: 7583.067383 [ 1430/ 1682]\n",
      "loss: 7251.720215 [ 1440/ 1682]\n",
      "loss: 8153.453125 [ 1450/ 1682]\n",
      "loss: 7968.261719 [ 1460/ 1682]\n",
      "loss: 9222.387695 [ 1470/ 1682]\n",
      "loss: 12249.593750 [ 1480/ 1682]\n",
      "loss: 13798.918945 [ 1490/ 1682]\n",
      "loss: 13212.937500 [ 1500/ 1682]\n",
      "loss: 11414.213867 [ 1510/ 1682]\n",
      "loss: 13619.759766 [ 1520/ 1682]\n",
      "loss: 11804.470703 [ 1530/ 1682]\n",
      "loss: 11494.564453 [ 1540/ 1682]\n",
      "loss: 11445.435547 [ 1550/ 1682]\n",
      "loss: 13904.965820 [ 1560/ 1682]\n",
      "loss: 11638.273438 [ 1570/ 1682]\n",
      "loss: 10846.322266 [ 1580/ 1682]\n",
      "loss: 7805.924805 [ 1590/ 1682]\n",
      "loss: 7452.031250 [ 1600/ 1682]\n",
      "loss: 6415.752930 [ 1610/ 1682]\n",
      "loss: 6000.721191 [ 1620/ 1682]\n",
      "loss: 8121.445312 [ 1630/ 1682]\n",
      "loss: 9458.981445 [ 1640/ 1682]\n",
      "loss: 11874.000000 [ 1650/ 1682]\n",
      "loss: 11778.674805 [ 1660/ 1682]\n",
      "loss: 9229.996094 [ 1670/ 1682]\n",
      "loss: 9436.343750 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10702.227295 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1023.924683 [    0/ 1682]\n",
      "loss: 1308.868774 [   10/ 1682]\n",
      "loss: 1058.173950 [   20/ 1682]\n",
      "loss: 1007.606079 [   30/ 1682]\n",
      "loss: 1334.588379 [   40/ 1682]\n",
      "loss: 1167.740723 [   50/ 1682]\n",
      "loss: 1159.260010 [   60/ 1682]\n",
      "loss: 1573.781006 [   70/ 1682]\n",
      "loss: 1100.446167 [   80/ 1682]\n",
      "loss: 1367.817505 [   90/ 1682]\n",
      "loss: 1530.422607 [  100/ 1682]\n",
      "loss: 1366.726318 [  110/ 1682]\n",
      "loss: 1345.971313 [  120/ 1682]\n",
      "loss: 1431.930298 [  130/ 1682]\n",
      "loss: 1583.798584 [  140/ 1682]\n",
      "loss: 1423.030640 [  150/ 1682]\n",
      "loss: 1860.575195 [  160/ 1682]\n",
      "loss: 1525.632568 [  170/ 1682]\n",
      "loss: 1055.765869 [  180/ 1682]\n",
      "loss: 1179.842529 [  190/ 1682]\n",
      "loss: 934.514832 [  200/ 1682]\n",
      "loss: 1104.375366 [  210/ 1682]\n",
      "loss: 1122.785400 [  220/ 1682]\n",
      "loss: 1240.591553 [  230/ 1682]\n",
      "loss: 909.173462 [  240/ 1682]\n",
      "loss: 1328.042847 [  250/ 1682]\n",
      "loss: 1035.907593 [  260/ 1682]\n",
      "loss: 762.910583 [  270/ 1682]\n",
      "loss: 608.941772 [  280/ 1682]\n",
      "loss: 985.388550 [  290/ 1682]\n",
      "loss: 981.466614 [  300/ 1682]\n",
      "loss: 437.273743 [  310/ 1682]\n",
      "loss: 607.603394 [  320/ 1682]\n",
      "loss: 980.267090 [  330/ 1682]\n",
      "loss: 443.521912 [  340/ 1682]\n",
      "loss: 669.833679 [  350/ 1682]\n",
      "loss: 518.144104 [  360/ 1682]\n",
      "loss: 585.724060 [  370/ 1682]\n",
      "loss: 498.839111 [  380/ 1682]\n",
      "loss: 725.275085 [  390/ 1682]\n",
      "loss: 787.643433 [  400/ 1682]\n",
      "loss: 718.908569 [  410/ 1682]\n",
      "loss: 928.600708 [  420/ 1682]\n",
      "loss: 781.791687 [  430/ 1682]\n",
      "loss: 416.231354 [  440/ 1682]\n",
      "loss: 503.825256 [  450/ 1682]\n",
      "loss: 464.458191 [  460/ 1682]\n",
      "loss: 370.349457 [  470/ 1682]\n",
      "loss: 282.737671 [  480/ 1682]\n",
      "loss: 363.492859 [  490/ 1682]\n",
      "loss: 400.097992 [  500/ 1682]\n",
      "loss: 375.874390 [  510/ 1682]\n",
      "loss: 377.967133 [  520/ 1682]\n",
      "loss: 253.836304 [  530/ 1682]\n",
      "loss: 316.109436 [  540/ 1682]\n",
      "loss: 436.483307 [  550/ 1682]\n",
      "loss: 340.520844 [  560/ 1682]\n",
      "loss: 279.259125 [  570/ 1682]\n",
      "loss: 164.291809 [  580/ 1682]\n",
      "loss: 207.417969 [  590/ 1682]\n",
      "loss: 257.927795 [  600/ 1682]\n",
      "loss: 210.710907 [  610/ 1682]\n",
      "loss: 233.382370 [  620/ 1682]\n",
      "loss: 341.015259 [  630/ 1682]\n",
      "loss: 146.641068 [  640/ 1682]\n",
      "loss: 46.580627 [  650/ 1682]\n",
      "loss: 22.606842 [  660/ 1682]\n",
      "loss: 94.166580 [  670/ 1682]\n",
      "loss: 36.547356 [  680/ 1682]\n",
      "loss: 29.137203 [  690/ 1682]\n",
      "loss: 66.221176 [  700/ 1682]\n",
      "loss: 142.679977 [  710/ 1682]\n",
      "loss: 239.310638 [  720/ 1682]\n",
      "loss: 312.600281 [  730/ 1682]\n",
      "loss: 447.636871 [  740/ 1682]\n",
      "loss: 463.324036 [  750/ 1682]\n",
      "loss: 269.081543 [  760/ 1682]\n",
      "loss: 284.711426 [  770/ 1682]\n",
      "loss: 263.431030 [  780/ 1682]\n",
      "loss: 181.214752 [  790/ 1682]\n",
      "loss: 117.401978 [  800/ 1682]\n",
      "loss: 176.910995 [  810/ 1682]\n",
      "loss: 86.834106 [  820/ 1682]\n",
      "loss: 58.441521 [  830/ 1682]\n",
      "loss: 276.286316 [  840/ 1682]\n",
      "loss: 283.709167 [  850/ 1682]\n",
      "loss: 114.217003 [  860/ 1682]\n",
      "loss: 129.780548 [  870/ 1682]\n",
      "loss: 49.277523 [  880/ 1682]\n",
      "loss: 110.056412 [  890/ 1682]\n",
      "loss: 43.426823 [  900/ 1682]\n",
      "loss: 40.435772 [  910/ 1682]\n",
      "loss: 42.657852 [  920/ 1682]\n",
      "loss: 14.416880 [  930/ 1682]\n",
      "loss: 45.803429 [  940/ 1682]\n",
      "loss: 41.306694 [  950/ 1682]\n",
      "loss: 73.946838 [  960/ 1682]\n",
      "loss: 92.185341 [  970/ 1682]\n",
      "loss: 135.493988 [  980/ 1682]\n",
      "loss: 163.227371 [  990/ 1682]\n",
      "loss: 376.379944 [ 1000/ 1682]\n",
      "loss: 449.859772 [ 1010/ 1682]\n",
      "loss: 600.699097 [ 1020/ 1682]\n",
      "loss: 301.623108 [ 1030/ 1682]\n",
      "loss: 181.260223 [ 1040/ 1682]\n",
      "loss: 41.583630 [ 1050/ 1682]\n",
      "loss: 119.819458 [ 1060/ 1682]\n",
      "loss: 175.414276 [ 1070/ 1682]\n",
      "loss: 370.106873 [ 1080/ 1682]\n",
      "loss: 375.372192 [ 1090/ 1682]\n",
      "loss: 577.526978 [ 1100/ 1682]\n",
      "loss: 940.715942 [ 1110/ 1682]\n",
      "loss: 1212.989868 [ 1120/ 1682]\n",
      "loss: 1541.834473 [ 1130/ 1682]\n",
      "loss: 2040.857422 [ 1140/ 1682]\n",
      "loss: 3269.586914 [ 1150/ 1682]\n",
      "loss: 4576.118164 [ 1160/ 1682]\n",
      "loss: 3085.977539 [ 1170/ 1682]\n",
      "loss: 2993.476807 [ 1180/ 1682]\n",
      "loss: 3836.892090 [ 1190/ 1682]\n",
      "loss: 2956.056152 [ 1200/ 1682]\n",
      "loss: 3342.507812 [ 1210/ 1682]\n",
      "loss: 3387.101562 [ 1220/ 1682]\n",
      "loss: 3726.156738 [ 1230/ 1682]\n",
      "loss: 5576.417969 [ 1240/ 1682]\n",
      "loss: 5215.041992 [ 1250/ 1682]\n",
      "loss: 6007.858398 [ 1260/ 1682]\n",
      "loss: 6085.312988 [ 1270/ 1682]\n",
      "loss: 4874.259277 [ 1280/ 1682]\n",
      "loss: 4132.680664 [ 1290/ 1682]\n",
      "loss: 4179.149902 [ 1300/ 1682]\n",
      "loss: 4382.605469 [ 1310/ 1682]\n",
      "loss: 5223.474121 [ 1320/ 1682]\n",
      "loss: 5066.785156 [ 1330/ 1682]\n",
      "loss: 4699.746094 [ 1340/ 1682]\n",
      "loss: 4685.206055 [ 1350/ 1682]\n",
      "loss: 5350.965820 [ 1360/ 1682]\n",
      "loss: 5463.847168 [ 1370/ 1682]\n",
      "loss: 7338.324219 [ 1380/ 1682]\n",
      "loss: 7406.989746 [ 1390/ 1682]\n",
      "loss: 7869.154785 [ 1400/ 1682]\n",
      "loss: 8277.974609 [ 1410/ 1682]\n",
      "loss: 7905.866211 [ 1420/ 1682]\n",
      "loss: 7246.037598 [ 1430/ 1682]\n",
      "loss: 7157.134277 [ 1440/ 1682]\n",
      "loss: 8308.234375 [ 1450/ 1682]\n",
      "loss: 8212.527344 [ 1460/ 1682]\n",
      "loss: 8976.260742 [ 1470/ 1682]\n",
      "loss: 12552.241211 [ 1480/ 1682]\n",
      "loss: 13697.513672 [ 1490/ 1682]\n",
      "loss: 14407.713867 [ 1500/ 1682]\n",
      "loss: 11437.865234 [ 1510/ 1682]\n",
      "loss: 12676.687500 [ 1520/ 1682]\n",
      "loss: 11776.310547 [ 1530/ 1682]\n",
      "loss: 9790.129883 [ 1540/ 1682]\n",
      "loss: 10462.814453 [ 1550/ 1682]\n",
      "loss: 13739.732422 [ 1560/ 1682]\n",
      "loss: 11304.919922 [ 1570/ 1682]\n",
      "loss: 9884.398438 [ 1580/ 1682]\n",
      "loss: 7329.708008 [ 1590/ 1682]\n",
      "loss: 8114.353027 [ 1600/ 1682]\n",
      "loss: 6059.769531 [ 1610/ 1682]\n",
      "loss: 5208.254883 [ 1620/ 1682]\n",
      "loss: 7193.616211 [ 1630/ 1682]\n",
      "loss: 9475.669922 [ 1640/ 1682]\n",
      "loss: 11909.062500 [ 1650/ 1682]\n",
      "loss: 11640.970703 [ 1660/ 1682]\n",
      "loss: 9098.543945 [ 1670/ 1682]\n",
      "loss: 7177.143555 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10477.498122 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1909.849976 [    0/ 1682]\n",
      "loss: 1582.121460 [   10/ 1682]\n",
      "loss: 1094.184204 [   20/ 1682]\n",
      "loss: 1611.963135 [   30/ 1682]\n",
      "loss: 1210.562622 [   40/ 1682]\n",
      "loss: 1183.836182 [   50/ 1682]\n",
      "loss: 2091.049316 [   60/ 1682]\n",
      "loss: 1471.987061 [   70/ 1682]\n",
      "loss: 2529.622803 [   80/ 1682]\n",
      "loss: 2965.082764 [   90/ 1682]\n",
      "loss: 1884.242920 [  100/ 1682]\n",
      "loss: 1083.962158 [  110/ 1682]\n",
      "loss: 1611.094360 [  120/ 1682]\n",
      "loss: 1343.380249 [  130/ 1682]\n",
      "loss: 1211.286011 [  140/ 1682]\n",
      "loss: 1545.359619 [  150/ 1682]\n",
      "loss: 2251.619629 [  160/ 1682]\n",
      "loss: 1400.989624 [  170/ 1682]\n",
      "loss: 2200.819092 [  180/ 1682]\n",
      "loss: 968.115234 [  190/ 1682]\n",
      "loss: 1443.365845 [  200/ 1682]\n",
      "loss: 1123.050049 [  210/ 1682]\n",
      "loss: 1460.285034 [  220/ 1682]\n",
      "loss: 1415.136963 [  230/ 1682]\n",
      "loss: 760.693054 [  240/ 1682]\n",
      "loss: 1283.877075 [  250/ 1682]\n",
      "loss: 1243.478394 [  260/ 1682]\n",
      "loss: 930.184204 [  270/ 1682]\n",
      "loss: 685.264343 [  280/ 1682]\n",
      "loss: 504.153625 [  290/ 1682]\n",
      "loss: 637.004333 [  300/ 1682]\n",
      "loss: 873.218750 [  310/ 1682]\n",
      "loss: 841.267944 [  320/ 1682]\n",
      "loss: 876.406555 [  330/ 1682]\n",
      "loss: 687.547974 [  340/ 1682]\n",
      "loss: 703.814331 [  350/ 1682]\n",
      "loss: 656.640747 [  360/ 1682]\n",
      "loss: 790.964478 [  370/ 1682]\n",
      "loss: 607.375610 [  380/ 1682]\n",
      "loss: 752.217651 [  390/ 1682]\n",
      "loss: 299.330444 [  400/ 1682]\n",
      "loss: 345.761169 [  410/ 1682]\n",
      "loss: 651.829590 [  420/ 1682]\n",
      "loss: 524.565796 [  430/ 1682]\n",
      "loss: 419.970306 [  440/ 1682]\n",
      "loss: 350.431335 [  450/ 1682]\n",
      "loss: 331.122650 [  460/ 1682]\n",
      "loss: 412.449615 [  470/ 1682]\n",
      "loss: 354.615997 [  480/ 1682]\n",
      "loss: 266.826050 [  490/ 1682]\n",
      "loss: 302.359558 [  500/ 1682]\n",
      "loss: 378.718445 [  510/ 1682]\n",
      "loss: 349.332764 [  520/ 1682]\n",
      "loss: 176.081558 [  530/ 1682]\n",
      "loss: 222.260895 [  540/ 1682]\n",
      "loss: 370.281067 [  550/ 1682]\n",
      "loss: 318.007263 [  560/ 1682]\n",
      "loss: 358.510803 [  570/ 1682]\n",
      "loss: 220.230927 [  580/ 1682]\n",
      "loss: 262.070587 [  590/ 1682]\n",
      "loss: 267.353943 [  600/ 1682]\n",
      "loss: 194.836029 [  610/ 1682]\n",
      "loss: 252.720169 [  620/ 1682]\n",
      "loss: 172.703491 [  630/ 1682]\n",
      "loss: 134.096100 [  640/ 1682]\n",
      "loss: 44.930508 [  650/ 1682]\n",
      "loss: 30.949036 [  660/ 1682]\n",
      "loss: 61.315056 [  670/ 1682]\n",
      "loss: 22.605656 [  680/ 1682]\n",
      "loss: 30.097662 [  690/ 1682]\n",
      "loss: 37.048515 [  700/ 1682]\n",
      "loss: 173.992599 [  710/ 1682]\n",
      "loss: 193.185211 [  720/ 1682]\n",
      "loss: 381.136749 [  730/ 1682]\n",
      "loss: 534.278381 [  740/ 1682]\n",
      "loss: 334.613708 [  750/ 1682]\n",
      "loss: 544.763489 [  760/ 1682]\n",
      "loss: 364.999573 [  770/ 1682]\n",
      "loss: 214.025024 [  780/ 1682]\n",
      "loss: 371.323853 [  790/ 1682]\n",
      "loss: 163.069244 [  800/ 1682]\n",
      "loss: 82.026535 [  810/ 1682]\n",
      "loss: 84.469360 [  820/ 1682]\n",
      "loss: 106.974014 [  830/ 1682]\n",
      "loss: 293.257965 [  840/ 1682]\n",
      "loss: 131.743195 [  850/ 1682]\n",
      "loss: 114.020691 [  860/ 1682]\n",
      "loss: 98.729118 [  870/ 1682]\n",
      "loss: 106.777512 [  880/ 1682]\n",
      "loss: 53.020699 [  890/ 1682]\n",
      "loss: 121.262329 [  900/ 1682]\n",
      "loss: 62.901550 [  910/ 1682]\n",
      "loss: 38.126976 [  920/ 1682]\n",
      "loss: 20.513752 [  930/ 1682]\n",
      "loss: 19.796993 [  940/ 1682]\n",
      "loss: 38.330757 [  950/ 1682]\n",
      "loss: 80.507820 [  960/ 1682]\n",
      "loss: 78.381439 [  970/ 1682]\n",
      "loss: 128.951523 [  980/ 1682]\n",
      "loss: 207.587357 [  990/ 1682]\n",
      "loss: 319.604553 [ 1000/ 1682]\n",
      "loss: 400.114685 [ 1010/ 1682]\n",
      "loss: 450.479736 [ 1020/ 1682]\n",
      "loss: 228.865112 [ 1030/ 1682]\n",
      "loss: 168.220612 [ 1040/ 1682]\n",
      "loss: 51.238728 [ 1050/ 1682]\n",
      "loss: 95.261078 [ 1060/ 1682]\n",
      "loss: 142.041901 [ 1070/ 1682]\n",
      "loss: 282.605225 [ 1080/ 1682]\n",
      "loss: 406.134094 [ 1090/ 1682]\n",
      "loss: 655.552795 [ 1100/ 1682]\n",
      "loss: 914.474121 [ 1110/ 1682]\n",
      "loss: 1092.109985 [ 1120/ 1682]\n",
      "loss: 1487.286133 [ 1130/ 1682]\n",
      "loss: 2022.781616 [ 1140/ 1682]\n",
      "loss: 2940.644043 [ 1150/ 1682]\n",
      "loss: 4377.498535 [ 1160/ 1682]\n",
      "loss: 2646.752197 [ 1170/ 1682]\n",
      "loss: 2950.625244 [ 1180/ 1682]\n",
      "loss: 3345.735840 [ 1190/ 1682]\n",
      "loss: 3033.131836 [ 1200/ 1682]\n",
      "loss: 3426.391846 [ 1210/ 1682]\n",
      "loss: 3485.815674 [ 1220/ 1682]\n",
      "loss: 3961.978516 [ 1230/ 1682]\n",
      "loss: 5327.948242 [ 1240/ 1682]\n",
      "loss: 4829.376465 [ 1250/ 1682]\n",
      "loss: 5911.590332 [ 1260/ 1682]\n",
      "loss: 5451.635742 [ 1270/ 1682]\n",
      "loss: 4777.041504 [ 1280/ 1682]\n",
      "loss: 4209.372070 [ 1290/ 1682]\n",
      "loss: 3787.162109 [ 1300/ 1682]\n",
      "loss: 5049.409668 [ 1310/ 1682]\n",
      "loss: 5761.027344 [ 1320/ 1682]\n",
      "loss: 4824.858887 [ 1330/ 1682]\n",
      "loss: 4301.467285 [ 1340/ 1682]\n",
      "loss: 4589.360352 [ 1350/ 1682]\n",
      "loss: 5088.972168 [ 1360/ 1682]\n",
      "loss: 6446.014160 [ 1370/ 1682]\n",
      "loss: 7460.029785 [ 1380/ 1682]\n",
      "loss: 7864.211914 [ 1390/ 1682]\n",
      "loss: 7533.791992 [ 1400/ 1682]\n",
      "loss: 8195.114258 [ 1410/ 1682]\n",
      "loss: 8387.006836 [ 1420/ 1682]\n",
      "loss: 7113.364258 [ 1430/ 1682]\n",
      "loss: 7089.811523 [ 1440/ 1682]\n",
      "loss: 8528.585938 [ 1450/ 1682]\n",
      "loss: 7588.668945 [ 1460/ 1682]\n",
      "loss: 9223.298828 [ 1470/ 1682]\n",
      "loss: 12100.310547 [ 1480/ 1682]\n",
      "loss: 13611.849609 [ 1490/ 1682]\n",
      "loss: 13871.286133 [ 1500/ 1682]\n",
      "loss: 11019.186523 [ 1510/ 1682]\n",
      "loss: 12955.921875 [ 1520/ 1682]\n",
      "loss: 12006.210938 [ 1530/ 1682]\n",
      "loss: 10874.581055 [ 1540/ 1682]\n",
      "loss: 10856.166992 [ 1550/ 1682]\n",
      "loss: 12447.776367 [ 1560/ 1682]\n",
      "loss: 10848.952148 [ 1570/ 1682]\n",
      "loss: 9025.623047 [ 1580/ 1682]\n",
      "loss: 7627.761719 [ 1590/ 1682]\n",
      "loss: 7351.183594 [ 1600/ 1682]\n",
      "loss: 5659.651855 [ 1610/ 1682]\n",
      "loss: 5537.052246 [ 1620/ 1682]\n",
      "loss: 7564.178223 [ 1630/ 1682]\n",
      "loss: 9879.276367 [ 1640/ 1682]\n",
      "loss: 12264.144531 [ 1650/ 1682]\n",
      "loss: 11552.199219 [ 1660/ 1682]\n",
      "loss: 9500.168945 [ 1670/ 1682]\n",
      "loss: 9228.607422 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10701.764630 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1521.541626 [    0/ 1682]\n",
      "loss: 1775.563477 [   10/ 1682]\n",
      "loss: 1561.451660 [   20/ 1682]\n",
      "loss: 1542.982666 [   30/ 1682]\n",
      "loss: 1287.730103 [   40/ 1682]\n",
      "loss: 1224.625977 [   50/ 1682]\n",
      "loss: 1435.350342 [   60/ 1682]\n",
      "loss: 1920.252319 [   70/ 1682]\n",
      "loss: 2502.269775 [   80/ 1682]\n",
      "loss: 1333.231812 [   90/ 1682]\n",
      "loss: 1636.930908 [  100/ 1682]\n",
      "loss: 2425.452637 [  110/ 1682]\n",
      "loss: 1766.053955 [  120/ 1682]\n",
      "loss: 1970.440674 [  130/ 1682]\n",
      "loss: 1439.169678 [  140/ 1682]\n",
      "loss: 1323.721436 [  150/ 1682]\n",
      "loss: 951.232544 [  160/ 1682]\n",
      "loss: 1549.831543 [  170/ 1682]\n",
      "loss: 2014.055664 [  180/ 1682]\n",
      "loss: 1376.117920 [  190/ 1682]\n",
      "loss: 843.063965 [  200/ 1682]\n",
      "loss: 1683.502686 [  210/ 1682]\n",
      "loss: 1230.833984 [  220/ 1682]\n",
      "loss: 1213.037842 [  230/ 1682]\n",
      "loss: 1111.207275 [  240/ 1682]\n",
      "loss: 953.057617 [  250/ 1682]\n",
      "loss: 1030.729248 [  260/ 1682]\n",
      "loss: 604.994263 [  270/ 1682]\n",
      "loss: 940.879089 [  280/ 1682]\n",
      "loss: 664.174438 [  290/ 1682]\n",
      "loss: 824.793640 [  300/ 1682]\n",
      "loss: 953.847839 [  310/ 1682]\n",
      "loss: 609.368347 [  320/ 1682]\n",
      "loss: 399.253479 [  330/ 1682]\n",
      "loss: 593.551086 [  340/ 1682]\n",
      "loss: 927.379883 [  350/ 1682]\n",
      "loss: 668.140442 [  360/ 1682]\n",
      "loss: 954.740540 [  370/ 1682]\n",
      "loss: 393.587769 [  380/ 1682]\n",
      "loss: 555.850220 [  390/ 1682]\n",
      "loss: 507.827545 [  400/ 1682]\n",
      "loss: 465.900635 [  410/ 1682]\n",
      "loss: 508.926361 [  420/ 1682]\n",
      "loss: 541.773865 [  430/ 1682]\n",
      "loss: 342.816620 [  440/ 1682]\n",
      "loss: 520.900635 [  450/ 1682]\n",
      "loss: 277.858856 [  460/ 1682]\n",
      "loss: 278.505798 [  470/ 1682]\n",
      "loss: 504.848053 [  480/ 1682]\n",
      "loss: 288.158234 [  490/ 1682]\n",
      "loss: 386.332764 [  500/ 1682]\n",
      "loss: 428.761658 [  510/ 1682]\n",
      "loss: 356.372864 [  520/ 1682]\n",
      "loss: 243.560516 [  530/ 1682]\n",
      "loss: 294.379700 [  540/ 1682]\n",
      "loss: 554.026245 [  550/ 1682]\n",
      "loss: 200.934937 [  560/ 1682]\n",
      "loss: 444.157623 [  570/ 1682]\n",
      "loss: 330.631104 [  580/ 1682]\n",
      "loss: 170.372726 [  590/ 1682]\n",
      "loss: 185.271423 [  600/ 1682]\n",
      "loss: 211.087921 [  610/ 1682]\n",
      "loss: 161.093857 [  620/ 1682]\n",
      "loss: 173.483566 [  630/ 1682]\n",
      "loss: 164.375900 [  640/ 1682]\n",
      "loss: 73.093605 [  650/ 1682]\n",
      "loss: 28.464767 [  660/ 1682]\n",
      "loss: 30.808493 [  670/ 1682]\n",
      "loss: 87.625229 [  680/ 1682]\n",
      "loss: 33.028152 [  690/ 1682]\n",
      "loss: 88.309975 [  700/ 1682]\n",
      "loss: 89.058815 [  710/ 1682]\n",
      "loss: 306.799866 [  720/ 1682]\n",
      "loss: 320.179626 [  730/ 1682]\n",
      "loss: 512.974121 [  740/ 1682]\n",
      "loss: 413.254395 [  750/ 1682]\n",
      "loss: 416.430084 [  760/ 1682]\n",
      "loss: 193.354050 [  770/ 1682]\n",
      "loss: 275.841156 [  780/ 1682]\n",
      "loss: 145.476685 [  790/ 1682]\n",
      "loss: 241.223114 [  800/ 1682]\n",
      "loss: 115.762291 [  810/ 1682]\n",
      "loss: 92.990746 [  820/ 1682]\n",
      "loss: 102.245132 [  830/ 1682]\n",
      "loss: 202.843918 [  840/ 1682]\n",
      "loss: 106.040604 [  850/ 1682]\n",
      "loss: 122.521500 [  860/ 1682]\n",
      "loss: 101.894058 [  870/ 1682]\n",
      "loss: 103.447411 [  880/ 1682]\n",
      "loss: 69.026695 [  890/ 1682]\n",
      "loss: 51.827080 [  900/ 1682]\n",
      "loss: 46.159199 [  910/ 1682]\n",
      "loss: 16.675335 [  920/ 1682]\n",
      "loss: 2.448043 [  930/ 1682]\n",
      "loss: 11.166203 [  940/ 1682]\n",
      "loss: 34.133842 [  950/ 1682]\n",
      "loss: 54.192200 [  960/ 1682]\n",
      "loss: 71.929756 [  970/ 1682]\n",
      "loss: 90.934204 [  980/ 1682]\n",
      "loss: 237.888763 [  990/ 1682]\n",
      "loss: 270.895050 [ 1000/ 1682]\n",
      "loss: 375.024170 [ 1010/ 1682]\n",
      "loss: 550.104675 [ 1020/ 1682]\n",
      "loss: 216.796631 [ 1030/ 1682]\n",
      "loss: 144.137939 [ 1040/ 1682]\n",
      "loss: 32.902237 [ 1050/ 1682]\n",
      "loss: 88.788620 [ 1060/ 1682]\n",
      "loss: 124.180664 [ 1070/ 1682]\n",
      "loss: 318.711426 [ 1080/ 1682]\n",
      "loss: 452.159729 [ 1090/ 1682]\n",
      "loss: 624.084961 [ 1100/ 1682]\n",
      "loss: 880.655273 [ 1110/ 1682]\n",
      "loss: 1211.792236 [ 1120/ 1682]\n",
      "loss: 1309.878296 [ 1130/ 1682]\n",
      "loss: 1853.811279 [ 1140/ 1682]\n",
      "loss: 3028.364746 [ 1150/ 1682]\n",
      "loss: 4182.463867 [ 1160/ 1682]\n",
      "loss: 2857.377441 [ 1170/ 1682]\n",
      "loss: 3122.907227 [ 1180/ 1682]\n",
      "loss: 3318.147949 [ 1190/ 1682]\n",
      "loss: 2997.088867 [ 1200/ 1682]\n",
      "loss: 3361.531982 [ 1210/ 1682]\n",
      "loss: 3283.263184 [ 1220/ 1682]\n",
      "loss: 4213.313477 [ 1230/ 1682]\n",
      "loss: 5257.956055 [ 1240/ 1682]\n",
      "loss: 5081.514160 [ 1250/ 1682]\n",
      "loss: 5983.008789 [ 1260/ 1682]\n",
      "loss: 5739.771484 [ 1270/ 1682]\n",
      "loss: 4587.365234 [ 1280/ 1682]\n",
      "loss: 3997.639160 [ 1290/ 1682]\n",
      "loss: 3880.269531 [ 1300/ 1682]\n",
      "loss: 4316.485352 [ 1310/ 1682]\n",
      "loss: 5675.561523 [ 1320/ 1682]\n",
      "loss: 4621.850586 [ 1330/ 1682]\n",
      "loss: 4576.921387 [ 1340/ 1682]\n",
      "loss: 4208.764160 [ 1350/ 1682]\n",
      "loss: 5383.468262 [ 1360/ 1682]\n",
      "loss: 5491.876465 [ 1370/ 1682]\n",
      "loss: 7370.661621 [ 1380/ 1682]\n",
      "loss: 7993.451660 [ 1390/ 1682]\n",
      "loss: 7717.719727 [ 1400/ 1682]\n",
      "loss: 8377.608398 [ 1410/ 1682]\n",
      "loss: 8580.900391 [ 1420/ 1682]\n",
      "loss: 7337.431152 [ 1430/ 1682]\n",
      "loss: 6991.877930 [ 1440/ 1682]\n",
      "loss: 8174.971191 [ 1450/ 1682]\n",
      "loss: 8382.079102 [ 1460/ 1682]\n",
      "loss: 9472.595703 [ 1470/ 1682]\n",
      "loss: 12392.070312 [ 1480/ 1682]\n",
      "loss: 13511.487305 [ 1490/ 1682]\n",
      "loss: 13809.965820 [ 1500/ 1682]\n",
      "loss: 11312.474609 [ 1510/ 1682]\n",
      "loss: 13247.976562 [ 1520/ 1682]\n",
      "loss: 11662.536133 [ 1530/ 1682]\n",
      "loss: 10154.169922 [ 1540/ 1682]\n",
      "loss: 10758.296875 [ 1550/ 1682]\n",
      "loss: 13198.728516 [ 1560/ 1682]\n",
      "loss: 11598.994141 [ 1570/ 1682]\n",
      "loss: 9873.728516 [ 1580/ 1682]\n",
      "loss: 6721.563965 [ 1590/ 1682]\n",
      "loss: 7659.167969 [ 1600/ 1682]\n",
      "loss: 5923.611328 [ 1610/ 1682]\n",
      "loss: 6525.177246 [ 1620/ 1682]\n",
      "loss: 7666.311035 [ 1630/ 1682]\n",
      "loss: 10156.682617 [ 1640/ 1682]\n",
      "loss: 10710.497070 [ 1650/ 1682]\n",
      "loss: 10723.444336 [ 1660/ 1682]\n",
      "loss: 8705.710938 [ 1670/ 1682]\n",
      "loss: 9127.242188 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10673.307429 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1348.639038 [    0/ 1682]\n",
      "loss: 1578.230103 [   10/ 1682]\n",
      "loss: 1165.090576 [   20/ 1682]\n",
      "loss: 1763.000000 [   30/ 1682]\n",
      "loss: 1046.684814 [   40/ 1682]\n",
      "loss: 1195.777954 [   50/ 1682]\n",
      "loss: 1326.774658 [   60/ 1682]\n",
      "loss: 1055.717773 [   70/ 1682]\n",
      "loss: 1619.851318 [   80/ 1682]\n",
      "loss: 1754.463867 [   90/ 1682]\n",
      "loss: 1527.434814 [  100/ 1682]\n",
      "loss: 1580.083618 [  110/ 1682]\n",
      "loss: 1696.583008 [  120/ 1682]\n",
      "loss: 1280.214111 [  130/ 1682]\n",
      "loss: 991.039185 [  140/ 1682]\n",
      "loss: 1345.186768 [  150/ 1682]\n",
      "loss: 1386.303101 [  160/ 1682]\n",
      "loss: 1134.261963 [  170/ 1682]\n",
      "loss: 1224.896118 [  180/ 1682]\n",
      "loss: 1420.153076 [  190/ 1682]\n",
      "loss: 874.364746 [  200/ 1682]\n",
      "loss: 946.070129 [  210/ 1682]\n",
      "loss: 1449.523438 [  220/ 1682]\n",
      "loss: 1546.710449 [  230/ 1682]\n",
      "loss: 820.493042 [  240/ 1682]\n",
      "loss: 969.368469 [  250/ 1682]\n",
      "loss: 1092.304932 [  260/ 1682]\n",
      "loss: 770.486694 [  270/ 1682]\n",
      "loss: 572.570190 [  280/ 1682]\n",
      "loss: 1053.666992 [  290/ 1682]\n",
      "loss: 813.884277 [  300/ 1682]\n",
      "loss: 1007.962585 [  310/ 1682]\n",
      "loss: 624.630371 [  320/ 1682]\n",
      "loss: 654.955078 [  330/ 1682]\n",
      "loss: 508.954742 [  340/ 1682]\n",
      "loss: 509.941345 [  350/ 1682]\n",
      "loss: 1034.677734 [  360/ 1682]\n",
      "loss: 836.241333 [  370/ 1682]\n",
      "loss: 415.196136 [  380/ 1682]\n",
      "loss: 579.090820 [  390/ 1682]\n",
      "loss: 430.762543 [  400/ 1682]\n",
      "loss: 407.171112 [  410/ 1682]\n",
      "loss: 439.520905 [  420/ 1682]\n",
      "loss: 382.325928 [  430/ 1682]\n",
      "loss: 525.731018 [  440/ 1682]\n",
      "loss: 489.446533 [  450/ 1682]\n",
      "loss: 301.469910 [  460/ 1682]\n",
      "loss: 229.232758 [  470/ 1682]\n",
      "loss: 229.109222 [  480/ 1682]\n",
      "loss: 633.399231 [  490/ 1682]\n",
      "loss: 417.952209 [  500/ 1682]\n",
      "loss: 251.632568 [  510/ 1682]\n",
      "loss: 368.144348 [  520/ 1682]\n",
      "loss: 586.211487 [  530/ 1682]\n",
      "loss: 332.292938 [  540/ 1682]\n",
      "loss: 383.199738 [  550/ 1682]\n",
      "loss: 355.416016 [  560/ 1682]\n",
      "loss: 407.008789 [  570/ 1682]\n",
      "loss: 323.417206 [  580/ 1682]\n",
      "loss: 375.179382 [  590/ 1682]\n",
      "loss: 104.063332 [  600/ 1682]\n",
      "loss: 241.626343 [  610/ 1682]\n",
      "loss: 236.796463 [  620/ 1682]\n",
      "loss: 199.516510 [  630/ 1682]\n",
      "loss: 275.688965 [  640/ 1682]\n",
      "loss: 129.281342 [  650/ 1682]\n",
      "loss: 64.934830 [  660/ 1682]\n",
      "loss: 69.283356 [  670/ 1682]\n",
      "loss: 148.975113 [  680/ 1682]\n",
      "loss: 60.816628 [  690/ 1682]\n",
      "loss: 112.621483 [  700/ 1682]\n",
      "loss: 70.396896 [  710/ 1682]\n",
      "loss: 217.580719 [  720/ 1682]\n",
      "loss: 292.406403 [  730/ 1682]\n",
      "loss: 490.605621 [  740/ 1682]\n",
      "loss: 588.172974 [  750/ 1682]\n",
      "loss: 531.603333 [  760/ 1682]\n",
      "loss: 270.683655 [  770/ 1682]\n",
      "loss: 288.598206 [  780/ 1682]\n",
      "loss: 362.761810 [  790/ 1682]\n",
      "loss: 142.903458 [  800/ 1682]\n",
      "loss: 143.814056 [  810/ 1682]\n",
      "loss: 107.675034 [  820/ 1682]\n",
      "loss: 112.817886 [  830/ 1682]\n",
      "loss: 181.322784 [  840/ 1682]\n",
      "loss: 165.752213 [  850/ 1682]\n",
      "loss: 168.837173 [  860/ 1682]\n",
      "loss: 188.241867 [  870/ 1682]\n",
      "loss: 33.954185 [  880/ 1682]\n",
      "loss: 103.565208 [  890/ 1682]\n",
      "loss: 120.976601 [  900/ 1682]\n",
      "loss: 131.652069 [  910/ 1682]\n",
      "loss: 67.502914 [  920/ 1682]\n",
      "loss: 64.053482 [  930/ 1682]\n",
      "loss: 2.425526 [  940/ 1682]\n",
      "loss: 25.749905 [  950/ 1682]\n",
      "loss: 45.964256 [  960/ 1682]\n",
      "loss: 62.374989 [  970/ 1682]\n",
      "loss: 102.671951 [  980/ 1682]\n",
      "loss: 194.102081 [  990/ 1682]\n",
      "loss: 293.003265 [ 1000/ 1682]\n",
      "loss: 360.138611 [ 1010/ 1682]\n",
      "loss: 452.546448 [ 1020/ 1682]\n",
      "loss: 225.982346 [ 1030/ 1682]\n",
      "loss: 126.921730 [ 1040/ 1682]\n",
      "loss: 36.396252 [ 1050/ 1682]\n",
      "loss: 87.799957 [ 1060/ 1682]\n",
      "loss: 129.147156 [ 1070/ 1682]\n",
      "loss: 252.908203 [ 1080/ 1682]\n",
      "loss: 349.193573 [ 1090/ 1682]\n",
      "loss: 550.230042 [ 1100/ 1682]\n",
      "loss: 964.903137 [ 1110/ 1682]\n",
      "loss: 1095.619873 [ 1120/ 1682]\n",
      "loss: 1168.817993 [ 1130/ 1682]\n",
      "loss: 2021.041382 [ 1140/ 1682]\n",
      "loss: 2985.400391 [ 1150/ 1682]\n",
      "loss: 4541.504883 [ 1160/ 1682]\n",
      "loss: 2562.862793 [ 1170/ 1682]\n",
      "loss: 2366.632080 [ 1180/ 1682]\n",
      "loss: 3520.764160 [ 1190/ 1682]\n",
      "loss: 3057.987549 [ 1200/ 1682]\n",
      "loss: 3317.634766 [ 1210/ 1682]\n",
      "loss: 3490.223145 [ 1220/ 1682]\n",
      "loss: 3992.988281 [ 1230/ 1682]\n",
      "loss: 5167.634277 [ 1240/ 1682]\n",
      "loss: 4987.520508 [ 1250/ 1682]\n",
      "loss: 5885.506348 [ 1260/ 1682]\n",
      "loss: 5838.621094 [ 1270/ 1682]\n",
      "loss: 4815.215820 [ 1280/ 1682]\n",
      "loss: 3789.369873 [ 1290/ 1682]\n",
      "loss: 3975.438721 [ 1300/ 1682]\n",
      "loss: 4567.984863 [ 1310/ 1682]\n",
      "loss: 5422.976074 [ 1320/ 1682]\n",
      "loss: 4863.908691 [ 1330/ 1682]\n",
      "loss: 4506.680664 [ 1340/ 1682]\n",
      "loss: 4123.456543 [ 1350/ 1682]\n",
      "loss: 5306.891602 [ 1360/ 1682]\n",
      "loss: 6295.561035 [ 1370/ 1682]\n",
      "loss: 7317.096680 [ 1380/ 1682]\n",
      "loss: 7900.006348 [ 1390/ 1682]\n",
      "loss: 7417.484375 [ 1400/ 1682]\n",
      "loss: 8053.298340 [ 1410/ 1682]\n",
      "loss: 8238.589844 [ 1420/ 1682]\n",
      "loss: 7040.602539 [ 1430/ 1682]\n",
      "loss: 6726.040039 [ 1440/ 1682]\n",
      "loss: 7815.734375 [ 1450/ 1682]\n",
      "loss: 7729.577148 [ 1460/ 1682]\n",
      "loss: 9693.395508 [ 1470/ 1682]\n",
      "loss: 11561.240234 [ 1480/ 1682]\n",
      "loss: 13051.440430 [ 1490/ 1682]\n",
      "loss: 13333.573242 [ 1500/ 1682]\n",
      "loss: 10796.623047 [ 1510/ 1682]\n",
      "loss: 12761.063477 [ 1520/ 1682]\n",
      "loss: 11135.364258 [ 1530/ 1682]\n",
      "loss: 10355.961914 [ 1540/ 1682]\n",
      "loss: 10640.522461 [ 1550/ 1682]\n",
      "loss: 13436.904297 [ 1560/ 1682]\n",
      "loss: 11044.732422 [ 1570/ 1682]\n",
      "loss: 9700.065430 [ 1580/ 1682]\n",
      "loss: 7132.167969 [ 1590/ 1682]\n",
      "loss: 7833.361816 [ 1600/ 1682]\n",
      "loss: 6191.001953 [ 1610/ 1682]\n",
      "loss: 6712.504883 [ 1620/ 1682]\n",
      "loss: 7824.860352 [ 1630/ 1682]\n",
      "loss: 8461.407227 [ 1640/ 1682]\n",
      "loss: 11336.259766 [ 1650/ 1682]\n",
      "loss: 11353.692383 [ 1660/ 1682]\n",
      "loss: 8910.671875 [ 1670/ 1682]\n",
      "loss: 6993.696777 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10498.205360 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1634.066772 [    0/ 1682]\n",
      "loss: 1406.473877 [   10/ 1682]\n",
      "loss: 1811.792969 [   20/ 1682]\n",
      "loss: 1763.554932 [   30/ 1682]\n",
      "loss: 1540.082397 [   40/ 1682]\n",
      "loss: 1667.408203 [   50/ 1682]\n",
      "loss: 1388.847168 [   60/ 1682]\n",
      "loss: 1298.057007 [   70/ 1682]\n",
      "loss: 1245.675049 [   80/ 1682]\n",
      "loss: 1839.156616 [   90/ 1682]\n",
      "loss: 1799.445068 [  100/ 1682]\n",
      "loss: 1856.949463 [  110/ 1682]\n",
      "loss: 1840.315674 [  120/ 1682]\n",
      "loss: 1521.923828 [  130/ 1682]\n",
      "loss: 1254.337646 [  140/ 1682]\n",
      "loss: 1416.803955 [  150/ 1682]\n",
      "loss: 1351.381958 [  160/ 1682]\n",
      "loss: 1742.872803 [  170/ 1682]\n",
      "loss: 1080.718994 [  180/ 1682]\n",
      "loss: 1197.343018 [  190/ 1682]\n",
      "loss: 1586.172974 [  200/ 1682]\n",
      "loss: 1182.345947 [  210/ 1682]\n",
      "loss: 1519.464355 [  220/ 1682]\n",
      "loss: 904.069824 [  230/ 1682]\n",
      "loss: 1135.256470 [  240/ 1682]\n",
      "loss: 978.600098 [  250/ 1682]\n",
      "loss: 1406.549194 [  260/ 1682]\n",
      "loss: 1343.782959 [  270/ 1682]\n",
      "loss: 597.345215 [  280/ 1682]\n",
      "loss: 577.463257 [  290/ 1682]\n",
      "loss: 1311.523560 [  300/ 1682]\n",
      "loss: 1142.612305 [  310/ 1682]\n",
      "loss: 934.609863 [  320/ 1682]\n",
      "loss: 688.325317 [  330/ 1682]\n",
      "loss: 666.904419 [  340/ 1682]\n",
      "loss: 643.624451 [  350/ 1682]\n",
      "loss: 497.202087 [  360/ 1682]\n",
      "loss: 854.208496 [  370/ 1682]\n",
      "loss: 621.214478 [  380/ 1682]\n",
      "loss: 593.573914 [  390/ 1682]\n",
      "loss: 544.527466 [  400/ 1682]\n",
      "loss: 694.284180 [  410/ 1682]\n",
      "loss: 566.953735 [  420/ 1682]\n",
      "loss: 707.651367 [  430/ 1682]\n",
      "loss: 483.574463 [  440/ 1682]\n",
      "loss: 666.708862 [  450/ 1682]\n",
      "loss: 304.992065 [  460/ 1682]\n",
      "loss: 545.572876 [  470/ 1682]\n",
      "loss: 463.999023 [  480/ 1682]\n",
      "loss: 395.518646 [  490/ 1682]\n",
      "loss: 346.834320 [  500/ 1682]\n",
      "loss: 327.095367 [  510/ 1682]\n",
      "loss: 390.037292 [  520/ 1682]\n",
      "loss: 344.757751 [  530/ 1682]\n",
      "loss: 392.231781 [  540/ 1682]\n",
      "loss: 450.331879 [  550/ 1682]\n",
      "loss: 345.965759 [  560/ 1682]\n",
      "loss: 468.304535 [  570/ 1682]\n",
      "loss: 252.122437 [  580/ 1682]\n",
      "loss: 245.085785 [  590/ 1682]\n",
      "loss: 308.478333 [  600/ 1682]\n",
      "loss: 297.364655 [  610/ 1682]\n",
      "loss: 295.136169 [  620/ 1682]\n",
      "loss: 244.408203 [  630/ 1682]\n",
      "loss: 111.174118 [  640/ 1682]\n",
      "loss: 61.421314 [  650/ 1682]\n",
      "loss: 62.399700 [  660/ 1682]\n",
      "loss: 86.984940 [  670/ 1682]\n",
      "loss: 113.463219 [  680/ 1682]\n",
      "loss: 69.445023 [  690/ 1682]\n",
      "loss: 78.846771 [  700/ 1682]\n",
      "loss: 110.085403 [  710/ 1682]\n",
      "loss: 288.597931 [  720/ 1682]\n",
      "loss: 301.836456 [  730/ 1682]\n",
      "loss: 375.486755 [  740/ 1682]\n",
      "loss: 394.128540 [  750/ 1682]\n",
      "loss: 505.108826 [  760/ 1682]\n",
      "loss: 272.645935 [  770/ 1682]\n",
      "loss: 251.899612 [  780/ 1682]\n",
      "loss: 302.110413 [  790/ 1682]\n",
      "loss: 187.496368 [  800/ 1682]\n",
      "loss: 131.360596 [  810/ 1682]\n",
      "loss: 163.334686 [  820/ 1682]\n",
      "loss: 188.587051 [  830/ 1682]\n",
      "loss: 175.721848 [  840/ 1682]\n",
      "loss: 166.164001 [  850/ 1682]\n",
      "loss: 130.337601 [  860/ 1682]\n",
      "loss: 107.589767 [  870/ 1682]\n",
      "loss: 92.242661 [  880/ 1682]\n",
      "loss: 74.034317 [  890/ 1682]\n",
      "loss: 134.501617 [  900/ 1682]\n",
      "loss: 54.536243 [  910/ 1682]\n",
      "loss: 28.739140 [  920/ 1682]\n",
      "loss: 45.400028 [  930/ 1682]\n",
      "loss: 12.135542 [  940/ 1682]\n",
      "loss: 28.449427 [  950/ 1682]\n",
      "loss: 43.531773 [  960/ 1682]\n",
      "loss: 61.262653 [  970/ 1682]\n",
      "loss: 98.022362 [  980/ 1682]\n",
      "loss: 184.514832 [  990/ 1682]\n",
      "loss: 282.739288 [ 1000/ 1682]\n",
      "loss: 373.787598 [ 1010/ 1682]\n",
      "loss: 466.156006 [ 1020/ 1682]\n",
      "loss: 216.736038 [ 1030/ 1682]\n",
      "loss: 120.537819 [ 1040/ 1682]\n",
      "loss: 24.403036 [ 1050/ 1682]\n",
      "loss: 71.146103 [ 1060/ 1682]\n",
      "loss: 130.562637 [ 1070/ 1682]\n",
      "loss: 260.172882 [ 1080/ 1682]\n",
      "loss: 377.963654 [ 1090/ 1682]\n",
      "loss: 550.538818 [ 1100/ 1682]\n",
      "loss: 889.977051 [ 1110/ 1682]\n",
      "loss: 869.148621 [ 1120/ 1682]\n",
      "loss: 1375.707642 [ 1130/ 1682]\n",
      "loss: 1972.703735 [ 1140/ 1682]\n",
      "loss: 2938.414795 [ 1150/ 1682]\n",
      "loss: 4328.753906 [ 1160/ 1682]\n",
      "loss: 2865.790527 [ 1170/ 1682]\n",
      "loss: 2696.804932 [ 1180/ 1682]\n",
      "loss: 3357.746582 [ 1190/ 1682]\n",
      "loss: 2910.382324 [ 1200/ 1682]\n",
      "loss: 3044.476807 [ 1210/ 1682]\n",
      "loss: 3171.839844 [ 1220/ 1682]\n",
      "loss: 3685.429688 [ 1230/ 1682]\n",
      "loss: 4968.728027 [ 1240/ 1682]\n",
      "loss: 4806.302246 [ 1250/ 1682]\n",
      "loss: 5992.917969 [ 1260/ 1682]\n",
      "loss: 5267.436523 [ 1270/ 1682]\n",
      "loss: 4170.035645 [ 1280/ 1682]\n",
      "loss: 3717.857910 [ 1290/ 1682]\n",
      "loss: 3766.003906 [ 1300/ 1682]\n",
      "loss: 4522.034180 [ 1310/ 1682]\n",
      "loss: 5173.096680 [ 1320/ 1682]\n",
      "loss: 4787.905273 [ 1330/ 1682]\n",
      "loss: 4132.881348 [ 1340/ 1682]\n",
      "loss: 4407.107910 [ 1350/ 1682]\n",
      "loss: 5042.709473 [ 1360/ 1682]\n",
      "loss: 6217.005371 [ 1370/ 1682]\n",
      "loss: 6808.503906 [ 1380/ 1682]\n",
      "loss: 7811.594727 [ 1390/ 1682]\n",
      "loss: 7759.588379 [ 1400/ 1682]\n",
      "loss: 7943.851562 [ 1410/ 1682]\n",
      "loss: 7616.615723 [ 1420/ 1682]\n",
      "loss: 7140.993652 [ 1430/ 1682]\n",
      "loss: 6827.482910 [ 1440/ 1682]\n",
      "loss: 8244.991211 [ 1450/ 1682]\n",
      "loss: 7917.601562 [ 1460/ 1682]\n",
      "loss: 9014.993164 [ 1470/ 1682]\n",
      "loss: 12128.180664 [ 1480/ 1682]\n",
      "loss: 13299.831055 [ 1490/ 1682]\n",
      "loss: 13552.494141 [ 1500/ 1682]\n",
      "loss: 11024.898438 [ 1510/ 1682]\n",
      "loss: 12025.961914 [ 1520/ 1682]\n",
      "loss: 10975.491211 [ 1530/ 1682]\n",
      "loss: 9891.509766 [ 1540/ 1682]\n",
      "loss: 9312.900391 [ 1550/ 1682]\n",
      "loss: 12105.165039 [ 1560/ 1682]\n",
      "loss: 11013.572266 [ 1570/ 1682]\n",
      "loss: 9668.086914 [ 1580/ 1682]\n",
      "loss: 6646.303711 [ 1590/ 1682]\n",
      "loss: 6465.570801 [ 1600/ 1682]\n",
      "loss: 5773.352051 [ 1610/ 1682]\n",
      "loss: 6313.161133 [ 1620/ 1682]\n",
      "loss: 7349.407715 [ 1630/ 1682]\n",
      "loss: 7962.199219 [ 1640/ 1682]\n",
      "loss: 11533.962891 [ 1650/ 1682]\n",
      "loss: 10889.330078 [ 1660/ 1682]\n",
      "loss: 8301.320312 [ 1670/ 1682]\n",
      "loss: 8934.201172 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10403.241342 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1667.886475 [    0/ 1682]\n",
      "loss: 2091.850342 [   10/ 1682]\n",
      "loss: 2433.787842 [   20/ 1682]\n",
      "loss: 1380.186157 [   30/ 1682]\n",
      "loss: 1355.881104 [   40/ 1682]\n",
      "loss: 1508.592041 [   50/ 1682]\n",
      "loss: 1546.249390 [   60/ 1682]\n",
      "loss: 2033.348022 [   70/ 1682]\n",
      "loss: 1865.271240 [   80/ 1682]\n",
      "loss: 1472.635376 [   90/ 1682]\n",
      "loss: 1733.825195 [  100/ 1682]\n",
      "loss: 1655.273438 [  110/ 1682]\n",
      "loss: 2090.374268 [  120/ 1682]\n",
      "loss: 2219.443359 [  130/ 1682]\n",
      "loss: 1778.117432 [  140/ 1682]\n",
      "loss: 1393.052490 [  150/ 1682]\n",
      "loss: 1485.649292 [  160/ 1682]\n",
      "loss: 1566.554932 [  170/ 1682]\n",
      "loss: 1392.546875 [  180/ 1682]\n",
      "loss: 1861.299194 [  190/ 1682]\n",
      "loss: 1910.949219 [  200/ 1682]\n",
      "loss: 1395.148071 [  210/ 1682]\n",
      "loss: 1682.699951 [  220/ 1682]\n",
      "loss: 1127.198242 [  230/ 1682]\n",
      "loss: 878.804565 [  240/ 1682]\n",
      "loss: 1225.503662 [  250/ 1682]\n",
      "loss: 1310.921753 [  260/ 1682]\n",
      "loss: 1010.996948 [  270/ 1682]\n",
      "loss: 779.644897 [  280/ 1682]\n",
      "loss: 1150.611084 [  290/ 1682]\n",
      "loss: 828.504272 [  300/ 1682]\n",
      "loss: 687.668823 [  310/ 1682]\n",
      "loss: 810.391968 [  320/ 1682]\n",
      "loss: 677.483276 [  330/ 1682]\n",
      "loss: 541.713806 [  340/ 1682]\n",
      "loss: 1011.875183 [  350/ 1682]\n",
      "loss: 1098.565674 [  360/ 1682]\n",
      "loss: 1073.111938 [  370/ 1682]\n",
      "loss: 1095.503662 [  380/ 1682]\n",
      "loss: 536.346191 [  390/ 1682]\n",
      "loss: 646.603821 [  400/ 1682]\n",
      "loss: 847.519165 [  410/ 1682]\n",
      "loss: 739.827209 [  420/ 1682]\n",
      "loss: 505.998383 [  430/ 1682]\n",
      "loss: 738.531860 [  440/ 1682]\n",
      "loss: 419.944580 [  450/ 1682]\n",
      "loss: 466.555573 [  460/ 1682]\n",
      "loss: 469.596039 [  470/ 1682]\n",
      "loss: 463.723938 [  480/ 1682]\n",
      "loss: 259.763702 [  490/ 1682]\n",
      "loss: 357.038025 [  500/ 1682]\n",
      "loss: 413.474609 [  510/ 1682]\n",
      "loss: 538.843628 [  520/ 1682]\n",
      "loss: 234.566940 [  530/ 1682]\n",
      "loss: 213.194244 [  540/ 1682]\n",
      "loss: 407.430634 [  550/ 1682]\n",
      "loss: 433.402771 [  560/ 1682]\n",
      "loss: 293.226196 [  570/ 1682]\n",
      "loss: 373.115540 [  580/ 1682]\n",
      "loss: 395.977936 [  590/ 1682]\n",
      "loss: 217.608521 [  600/ 1682]\n",
      "loss: 245.419144 [  610/ 1682]\n",
      "loss: 194.620255 [  620/ 1682]\n",
      "loss: 164.779831 [  630/ 1682]\n",
      "loss: 235.787750 [  640/ 1682]\n",
      "loss: 33.673668 [  650/ 1682]\n",
      "loss: 62.350269 [  660/ 1682]\n",
      "loss: 94.098236 [  670/ 1682]\n",
      "loss: 9.475911 [  680/ 1682]\n",
      "loss: 72.235886 [  690/ 1682]\n",
      "loss: 131.828583 [  700/ 1682]\n",
      "loss: 146.441238 [  710/ 1682]\n",
      "loss: 473.758148 [  720/ 1682]\n",
      "loss: 317.507812 [  730/ 1682]\n",
      "loss: 460.672272 [  740/ 1682]\n",
      "loss: 589.926331 [  750/ 1682]\n",
      "loss: 634.031372 [  760/ 1682]\n",
      "loss: 379.774841 [  770/ 1682]\n",
      "loss: 313.199310 [  780/ 1682]\n",
      "loss: 281.271393 [  790/ 1682]\n",
      "loss: 154.020111 [  800/ 1682]\n",
      "loss: 144.402084 [  810/ 1682]\n",
      "loss: 110.777939 [  820/ 1682]\n",
      "loss: 130.466644 [  830/ 1682]\n",
      "loss: 152.940994 [  840/ 1682]\n",
      "loss: 245.217682 [  850/ 1682]\n",
      "loss: 109.009056 [  860/ 1682]\n",
      "loss: 113.833191 [  870/ 1682]\n",
      "loss: 99.328682 [  880/ 1682]\n",
      "loss: 58.120655 [  890/ 1682]\n",
      "loss: 112.279518 [  900/ 1682]\n",
      "loss: 80.962463 [  910/ 1682]\n",
      "loss: 59.836128 [  920/ 1682]\n",
      "loss: 69.235596 [  930/ 1682]\n",
      "loss: 1.410934 [  940/ 1682]\n",
      "loss: 18.418499 [  950/ 1682]\n",
      "loss: 51.735008 [  960/ 1682]\n",
      "loss: 57.342384 [  970/ 1682]\n",
      "loss: 102.540649 [  980/ 1682]\n",
      "loss: 169.472107 [  990/ 1682]\n",
      "loss: 271.879211 [ 1000/ 1682]\n",
      "loss: 301.602600 [ 1010/ 1682]\n",
      "loss: 446.801666 [ 1020/ 1682]\n",
      "loss: 251.855026 [ 1030/ 1682]\n",
      "loss: 107.283081 [ 1040/ 1682]\n",
      "loss: 11.501362 [ 1050/ 1682]\n",
      "loss: 39.099174 [ 1060/ 1682]\n",
      "loss: 113.598167 [ 1070/ 1682]\n",
      "loss: 205.935745 [ 1080/ 1682]\n",
      "loss: 330.775574 [ 1090/ 1682]\n",
      "loss: 512.204529 [ 1100/ 1682]\n",
      "loss: 761.370911 [ 1110/ 1682]\n",
      "loss: 930.003052 [ 1120/ 1682]\n",
      "loss: 1411.408813 [ 1130/ 1682]\n",
      "loss: 1761.497803 [ 1140/ 1682]\n",
      "loss: 3109.061035 [ 1150/ 1682]\n",
      "loss: 4135.294434 [ 1160/ 1682]\n",
      "loss: 2809.044434 [ 1170/ 1682]\n",
      "loss: 2845.613037 [ 1180/ 1682]\n",
      "loss: 3527.581543 [ 1190/ 1682]\n",
      "loss: 2951.841797 [ 1200/ 1682]\n",
      "loss: 3119.938965 [ 1210/ 1682]\n",
      "loss: 3383.490723 [ 1220/ 1682]\n",
      "loss: 4043.063721 [ 1230/ 1682]\n",
      "loss: 4657.692383 [ 1240/ 1682]\n",
      "loss: 4882.760254 [ 1250/ 1682]\n",
      "loss: 5919.896973 [ 1260/ 1682]\n",
      "loss: 5723.217285 [ 1270/ 1682]\n",
      "loss: 4146.924316 [ 1280/ 1682]\n",
      "loss: 3837.249268 [ 1290/ 1682]\n",
      "loss: 3614.666748 [ 1300/ 1682]\n",
      "loss: 4613.831055 [ 1310/ 1682]\n",
      "loss: 4996.864258 [ 1320/ 1682]\n",
      "loss: 4779.232910 [ 1330/ 1682]\n",
      "loss: 4236.702148 [ 1340/ 1682]\n",
      "loss: 4496.771973 [ 1350/ 1682]\n",
      "loss: 4688.499023 [ 1360/ 1682]\n",
      "loss: 5822.882812 [ 1370/ 1682]\n",
      "loss: 7176.726562 [ 1380/ 1682]\n",
      "loss: 7318.281250 [ 1390/ 1682]\n",
      "loss: 7709.911133 [ 1400/ 1682]\n",
      "loss: 8132.922852 [ 1410/ 1682]\n",
      "loss: 7393.407227 [ 1420/ 1682]\n",
      "loss: 6899.666992 [ 1430/ 1682]\n",
      "loss: 5945.721191 [ 1440/ 1682]\n",
      "loss: 7949.828125 [ 1450/ 1682]\n",
      "loss: 8130.502930 [ 1460/ 1682]\n",
      "loss: 9252.137695 [ 1470/ 1682]\n",
      "loss: 11102.357422 [ 1480/ 1682]\n",
      "loss: 11936.715820 [ 1490/ 1682]\n",
      "loss: 13206.168945 [ 1500/ 1682]\n",
      "loss: 11633.026367 [ 1510/ 1682]\n",
      "loss: 11938.721680 [ 1520/ 1682]\n",
      "loss: 11634.593750 [ 1530/ 1682]\n",
      "loss: 10839.682617 [ 1540/ 1682]\n",
      "loss: 9810.824219 [ 1550/ 1682]\n",
      "loss: 12501.682617 [ 1560/ 1682]\n",
      "loss: 11253.510742 [ 1570/ 1682]\n",
      "loss: 9223.568359 [ 1580/ 1682]\n",
      "loss: 7351.861816 [ 1590/ 1682]\n",
      "loss: 6180.001953 [ 1600/ 1682]\n",
      "loss: 5949.237305 [ 1610/ 1682]\n",
      "loss: 6282.159668 [ 1620/ 1682]\n",
      "loss: 7384.501465 [ 1630/ 1682]\n",
      "loss: 9499.488281 [ 1640/ 1682]\n",
      "loss: 11863.845703 [ 1650/ 1682]\n",
      "loss: 11228.375977 [ 1660/ 1682]\n",
      "loss: 8814.358398 [ 1670/ 1682]\n",
      "loss: 7376.590332 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10115.789175 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1641.717529 [    0/ 1682]\n",
      "loss: 1659.083618 [   10/ 1682]\n",
      "loss: 1731.310303 [   20/ 1682]\n",
      "loss: 1213.649658 [   30/ 1682]\n",
      "loss: 1524.385742 [   40/ 1682]\n",
      "loss: 1783.024048 [   50/ 1682]\n",
      "loss: 1047.020752 [   60/ 1682]\n",
      "loss: 1580.086426 [   70/ 1682]\n",
      "loss: 1501.712891 [   80/ 1682]\n",
      "loss: 1913.344360 [   90/ 1682]\n",
      "loss: 2070.426270 [  100/ 1682]\n",
      "loss: 2306.381104 [  110/ 1682]\n",
      "loss: 1868.691772 [  120/ 1682]\n",
      "loss: 1208.817627 [  130/ 1682]\n",
      "loss: 1274.174683 [  140/ 1682]\n",
      "loss: 1027.908569 [  150/ 1682]\n",
      "loss: 1490.401978 [  160/ 1682]\n",
      "loss: 1022.232910 [  170/ 1682]\n",
      "loss: 1142.925049 [  180/ 1682]\n",
      "loss: 1247.741577 [  190/ 1682]\n",
      "loss: 1121.078735 [  200/ 1682]\n",
      "loss: 1348.841187 [  210/ 1682]\n",
      "loss: 1701.388916 [  220/ 1682]\n",
      "loss: 1286.868774 [  230/ 1682]\n",
      "loss: 1525.550049 [  240/ 1682]\n",
      "loss: 1364.314697 [  250/ 1682]\n",
      "loss: 1171.628174 [  260/ 1682]\n",
      "loss: 984.041626 [  270/ 1682]\n",
      "loss: 1054.571655 [  280/ 1682]\n",
      "loss: 624.972717 [  290/ 1682]\n",
      "loss: 849.578308 [  300/ 1682]\n",
      "loss: 701.617920 [  310/ 1682]\n",
      "loss: 675.850281 [  320/ 1682]\n",
      "loss: 680.336914 [  330/ 1682]\n",
      "loss: 562.834473 [  340/ 1682]\n",
      "loss: 813.761169 [  350/ 1682]\n",
      "loss: 541.353699 [  360/ 1682]\n",
      "loss: 655.622681 [  370/ 1682]\n",
      "loss: 681.567444 [  380/ 1682]\n",
      "loss: 857.410645 [  390/ 1682]\n",
      "loss: 497.068787 [  400/ 1682]\n",
      "loss: 697.181335 [  410/ 1682]\n",
      "loss: 885.045898 [  420/ 1682]\n",
      "loss: 706.933594 [  430/ 1682]\n",
      "loss: 522.268005 [  440/ 1682]\n",
      "loss: 712.660278 [  450/ 1682]\n",
      "loss: 399.373352 [  460/ 1682]\n",
      "loss: 353.851990 [  470/ 1682]\n",
      "loss: 346.532349 [  480/ 1682]\n",
      "loss: 497.172516 [  490/ 1682]\n",
      "loss: 313.411255 [  500/ 1682]\n",
      "loss: 448.238525 [  510/ 1682]\n",
      "loss: 649.301086 [  520/ 1682]\n",
      "loss: 457.339355 [  530/ 1682]\n",
      "loss: 354.996521 [  540/ 1682]\n",
      "loss: 783.334961 [  550/ 1682]\n",
      "loss: 528.313477 [  560/ 1682]\n",
      "loss: 591.289673 [  570/ 1682]\n",
      "loss: 234.756958 [  580/ 1682]\n",
      "loss: 324.517395 [  590/ 1682]\n",
      "loss: 182.101639 [  600/ 1682]\n",
      "loss: 263.928101 [  610/ 1682]\n",
      "loss: 160.228851 [  620/ 1682]\n",
      "loss: 133.387405 [  630/ 1682]\n",
      "loss: 86.689720 [  640/ 1682]\n",
      "loss: 101.027527 [  650/ 1682]\n",
      "loss: 87.984543 [  660/ 1682]\n",
      "loss: 49.586349 [  670/ 1682]\n",
      "loss: 61.214958 [  680/ 1682]\n",
      "loss: 84.087982 [  690/ 1682]\n",
      "loss: 56.015705 [  700/ 1682]\n",
      "loss: 96.442970 [  710/ 1682]\n",
      "loss: 408.629639 [  720/ 1682]\n",
      "loss: 520.991577 [  730/ 1682]\n",
      "loss: 542.624756 [  740/ 1682]\n",
      "loss: 492.223328 [  750/ 1682]\n",
      "loss: 448.679504 [  760/ 1682]\n",
      "loss: 252.915283 [  770/ 1682]\n",
      "loss: 343.809143 [  780/ 1682]\n",
      "loss: 197.400787 [  790/ 1682]\n",
      "loss: 259.662231 [  800/ 1682]\n",
      "loss: 156.033844 [  810/ 1682]\n",
      "loss: 191.085526 [  820/ 1682]\n",
      "loss: 125.290726 [  830/ 1682]\n",
      "loss: 295.701111 [  840/ 1682]\n",
      "loss: 263.571533 [  850/ 1682]\n",
      "loss: 125.893600 [  860/ 1682]\n",
      "loss: 70.513756 [  870/ 1682]\n",
      "loss: 80.236679 [  880/ 1682]\n",
      "loss: 181.522125 [  890/ 1682]\n",
      "loss: 115.668091 [  900/ 1682]\n",
      "loss: 124.287354 [  910/ 1682]\n",
      "loss: 52.352856 [  920/ 1682]\n",
      "loss: 64.075127 [  930/ 1682]\n",
      "loss: 25.661362 [  940/ 1682]\n",
      "loss: 17.757235 [  950/ 1682]\n",
      "loss: 40.708504 [  960/ 1682]\n",
      "loss: 48.689552 [  970/ 1682]\n",
      "loss: 55.090973 [  980/ 1682]\n",
      "loss: 180.809784 [  990/ 1682]\n",
      "loss: 297.822083 [ 1000/ 1682]\n",
      "loss: 214.191315 [ 1010/ 1682]\n",
      "loss: 330.387207 [ 1020/ 1682]\n",
      "loss: 224.006790 [ 1030/ 1682]\n",
      "loss: 118.643906 [ 1040/ 1682]\n",
      "loss: 37.532318 [ 1050/ 1682]\n",
      "loss: 65.085037 [ 1060/ 1682]\n",
      "loss: 95.114983 [ 1070/ 1682]\n",
      "loss: 262.883636 [ 1080/ 1682]\n",
      "loss: 375.402466 [ 1090/ 1682]\n",
      "loss: 501.426819 [ 1100/ 1682]\n",
      "loss: 830.354858 [ 1110/ 1682]\n",
      "loss: 1078.999878 [ 1120/ 1682]\n",
      "loss: 1314.101074 [ 1130/ 1682]\n",
      "loss: 1709.581299 [ 1140/ 1682]\n",
      "loss: 3048.698975 [ 1150/ 1682]\n",
      "loss: 4298.418457 [ 1160/ 1682]\n",
      "loss: 2663.516846 [ 1170/ 1682]\n",
      "loss: 2794.561523 [ 1180/ 1682]\n",
      "loss: 3225.610840 [ 1190/ 1682]\n",
      "loss: 3002.969238 [ 1200/ 1682]\n",
      "loss: 3259.020996 [ 1210/ 1682]\n",
      "loss: 3209.550049 [ 1220/ 1682]\n",
      "loss: 3982.079590 [ 1230/ 1682]\n",
      "loss: 4961.212891 [ 1240/ 1682]\n",
      "loss: 4373.912598 [ 1250/ 1682]\n",
      "loss: 5392.606934 [ 1260/ 1682]\n",
      "loss: 5477.782715 [ 1270/ 1682]\n",
      "loss: 4342.290039 [ 1280/ 1682]\n",
      "loss: 3760.367676 [ 1290/ 1682]\n",
      "loss: 3680.781250 [ 1300/ 1682]\n",
      "loss: 4258.415527 [ 1310/ 1682]\n",
      "loss: 5398.563965 [ 1320/ 1682]\n",
      "loss: 4685.842773 [ 1330/ 1682]\n",
      "loss: 3849.498535 [ 1340/ 1682]\n",
      "loss: 4138.540039 [ 1350/ 1682]\n",
      "loss: 4925.836914 [ 1360/ 1682]\n",
      "loss: 5467.145508 [ 1370/ 1682]\n",
      "loss: 6931.235840 [ 1380/ 1682]\n",
      "loss: 7028.895508 [ 1390/ 1682]\n",
      "loss: 7368.753906 [ 1400/ 1682]\n",
      "loss: 8022.963379 [ 1410/ 1682]\n",
      "loss: 8191.125000 [ 1420/ 1682]\n",
      "loss: 7222.988281 [ 1430/ 1682]\n",
      "loss: 6723.708496 [ 1440/ 1682]\n",
      "loss: 7581.339844 [ 1450/ 1682]\n",
      "loss: 7783.080566 [ 1460/ 1682]\n",
      "loss: 9445.179688 [ 1470/ 1682]\n",
      "loss: 11945.087891 [ 1480/ 1682]\n",
      "loss: 12820.666992 [ 1490/ 1682]\n",
      "loss: 13085.471680 [ 1500/ 1682]\n",
      "loss: 11186.083984 [ 1510/ 1682]\n",
      "loss: 12080.395508 [ 1520/ 1682]\n",
      "loss: 11514.847656 [ 1530/ 1682]\n",
      "loss: 10460.500977 [ 1540/ 1682]\n",
      "loss: 10693.853516 [ 1550/ 1682]\n",
      "loss: 12760.020508 [ 1560/ 1682]\n",
      "loss: 10581.642578 [ 1570/ 1682]\n",
      "loss: 9534.604492 [ 1580/ 1682]\n",
      "loss: 6972.254883 [ 1590/ 1682]\n",
      "loss: 7336.006348 [ 1600/ 1682]\n",
      "loss: 5895.506836 [ 1610/ 1682]\n",
      "loss: 5698.139160 [ 1620/ 1682]\n",
      "loss: 6427.893066 [ 1630/ 1682]\n",
      "loss: 9424.516602 [ 1640/ 1682]\n",
      "loss: 11397.222656 [ 1650/ 1682]\n",
      "loss: 11083.871094 [ 1660/ 1682]\n",
      "loss: 9144.580078 [ 1670/ 1682]\n",
      "loss: 8754.166016 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10363.146146 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1430.299072 [    0/ 1682]\n",
      "loss: 1813.354248 [   10/ 1682]\n",
      "loss: 1518.273193 [   20/ 1682]\n",
      "loss: 1430.455444 [   30/ 1682]\n",
      "loss: 2056.186035 [   40/ 1682]\n",
      "loss: 1317.166626 [   50/ 1682]\n",
      "loss: 1238.262329 [   60/ 1682]\n",
      "loss: 1957.789795 [   70/ 1682]\n",
      "loss: 1726.039673 [   80/ 1682]\n",
      "loss: 1489.496338 [   90/ 1682]\n",
      "loss: 2141.119141 [  100/ 1682]\n",
      "loss: 1503.176025 [  110/ 1682]\n",
      "loss: 1640.078125 [  120/ 1682]\n",
      "loss: 1563.504395 [  130/ 1682]\n",
      "loss: 1421.785034 [  140/ 1682]\n",
      "loss: 1391.192749 [  150/ 1682]\n",
      "loss: 1250.752686 [  160/ 1682]\n",
      "loss: 1749.588623 [  170/ 1682]\n",
      "loss: 1291.960938 [  180/ 1682]\n",
      "loss: 1078.028564 [  190/ 1682]\n",
      "loss: 1647.057373 [  200/ 1682]\n",
      "loss: 1368.537598 [  210/ 1682]\n",
      "loss: 1507.138184 [  220/ 1682]\n",
      "loss: 1416.324219 [  230/ 1682]\n",
      "loss: 936.846680 [  240/ 1682]\n",
      "loss: 930.329712 [  250/ 1682]\n",
      "loss: 1140.083618 [  260/ 1682]\n",
      "loss: 861.324829 [  270/ 1682]\n",
      "loss: 793.237732 [  280/ 1682]\n",
      "loss: 1007.991211 [  290/ 1682]\n",
      "loss: 977.954895 [  300/ 1682]\n",
      "loss: 598.744324 [  310/ 1682]\n",
      "loss: 598.683228 [  320/ 1682]\n",
      "loss: 779.356079 [  330/ 1682]\n",
      "loss: 850.535278 [  340/ 1682]\n",
      "loss: 1035.766602 [  350/ 1682]\n",
      "loss: 653.039062 [  360/ 1682]\n",
      "loss: 791.251343 [  370/ 1682]\n",
      "loss: 768.619385 [  380/ 1682]\n",
      "loss: 654.128784 [  390/ 1682]\n",
      "loss: 413.065979 [  400/ 1682]\n",
      "loss: 470.404846 [  410/ 1682]\n",
      "loss: 516.759583 [  420/ 1682]\n",
      "loss: 549.392578 [  430/ 1682]\n",
      "loss: 695.355591 [  440/ 1682]\n",
      "loss: 688.773010 [  450/ 1682]\n",
      "loss: 425.905457 [  460/ 1682]\n",
      "loss: 482.198975 [  470/ 1682]\n",
      "loss: 361.361877 [  480/ 1682]\n",
      "loss: 291.883484 [  490/ 1682]\n",
      "loss: 321.904724 [  500/ 1682]\n",
      "loss: 385.473877 [  510/ 1682]\n",
      "loss: 580.936462 [  520/ 1682]\n",
      "loss: 328.919403 [  530/ 1682]\n",
      "loss: 355.035248 [  540/ 1682]\n",
      "loss: 564.929504 [  550/ 1682]\n",
      "loss: 341.787170 [  560/ 1682]\n",
      "loss: 384.424042 [  570/ 1682]\n",
      "loss: 293.286499 [  580/ 1682]\n",
      "loss: 276.915985 [  590/ 1682]\n",
      "loss: 147.567383 [  600/ 1682]\n",
      "loss: 227.456268 [  610/ 1682]\n",
      "loss: 222.445526 [  620/ 1682]\n",
      "loss: 235.994217 [  630/ 1682]\n",
      "loss: 95.727821 [  640/ 1682]\n",
      "loss: 74.271683 [  650/ 1682]\n",
      "loss: 112.683495 [  660/ 1682]\n",
      "loss: 24.540657 [  670/ 1682]\n",
      "loss: 36.386700 [  680/ 1682]\n",
      "loss: 75.925148 [  690/ 1682]\n",
      "loss: 90.052322 [  700/ 1682]\n",
      "loss: 214.644440 [  710/ 1682]\n",
      "loss: 271.896484 [  720/ 1682]\n",
      "loss: 515.722046 [  730/ 1682]\n",
      "loss: 545.658325 [  740/ 1682]\n",
      "loss: 575.304565 [  750/ 1682]\n",
      "loss: 453.259857 [  760/ 1682]\n",
      "loss: 365.309814 [  770/ 1682]\n",
      "loss: 350.271118 [  780/ 1682]\n",
      "loss: 305.566162 [  790/ 1682]\n",
      "loss: 185.207062 [  800/ 1682]\n",
      "loss: 196.287003 [  810/ 1682]\n",
      "loss: 100.387611 [  820/ 1682]\n",
      "loss: 113.842995 [  830/ 1682]\n",
      "loss: 216.157715 [  840/ 1682]\n",
      "loss: 240.506866 [  850/ 1682]\n",
      "loss: 190.191681 [  860/ 1682]\n",
      "loss: 159.421158 [  870/ 1682]\n",
      "loss: 115.162704 [  880/ 1682]\n",
      "loss: 108.757156 [  890/ 1682]\n",
      "loss: 55.031281 [  900/ 1682]\n",
      "loss: 50.349724 [  910/ 1682]\n",
      "loss: 39.561531 [  920/ 1682]\n",
      "loss: 34.891167 [  930/ 1682]\n",
      "loss: 29.907068 [  940/ 1682]\n",
      "loss: 7.694443 [  950/ 1682]\n",
      "loss: 42.412163 [  960/ 1682]\n",
      "loss: 36.315964 [  970/ 1682]\n",
      "loss: 64.981003 [  980/ 1682]\n",
      "loss: 106.605812 [  990/ 1682]\n",
      "loss: 238.761475 [ 1000/ 1682]\n",
      "loss: 270.004211 [ 1010/ 1682]\n",
      "loss: 406.213715 [ 1020/ 1682]\n",
      "loss: 216.052933 [ 1030/ 1682]\n",
      "loss: 86.462967 [ 1040/ 1682]\n",
      "loss: 32.479923 [ 1050/ 1682]\n",
      "loss: 44.912483 [ 1060/ 1682]\n",
      "loss: 120.455627 [ 1070/ 1682]\n",
      "loss: 239.210159 [ 1080/ 1682]\n",
      "loss: 359.958679 [ 1090/ 1682]\n",
      "loss: 553.076782 [ 1100/ 1682]\n",
      "loss: 740.332642 [ 1110/ 1682]\n",
      "loss: 1050.687866 [ 1120/ 1682]\n",
      "loss: 1151.323120 [ 1130/ 1682]\n",
      "loss: 1848.161133 [ 1140/ 1682]\n",
      "loss: 2863.621582 [ 1150/ 1682]\n",
      "loss: 4405.295898 [ 1160/ 1682]\n",
      "loss: 2530.483154 [ 1170/ 1682]\n",
      "loss: 2662.904541 [ 1180/ 1682]\n",
      "loss: 3075.069580 [ 1190/ 1682]\n",
      "loss: 2844.891602 [ 1200/ 1682]\n",
      "loss: 3213.842285 [ 1210/ 1682]\n",
      "loss: 3148.888184 [ 1220/ 1682]\n",
      "loss: 3798.685547 [ 1230/ 1682]\n",
      "loss: 4648.957031 [ 1240/ 1682]\n",
      "loss: 4474.612305 [ 1250/ 1682]\n",
      "loss: 5611.817383 [ 1260/ 1682]\n",
      "loss: 5727.869629 [ 1270/ 1682]\n",
      "loss: 4415.049316 [ 1280/ 1682]\n",
      "loss: 3576.173340 [ 1290/ 1682]\n",
      "loss: 3753.302002 [ 1300/ 1682]\n",
      "loss: 4229.002441 [ 1310/ 1682]\n",
      "loss: 5336.627930 [ 1320/ 1682]\n",
      "loss: 4640.466309 [ 1330/ 1682]\n",
      "loss: 4262.022461 [ 1340/ 1682]\n",
      "loss: 4085.435059 [ 1350/ 1682]\n",
      "loss: 4565.058594 [ 1360/ 1682]\n",
      "loss: 6037.573242 [ 1370/ 1682]\n",
      "loss: 7062.288086 [ 1380/ 1682]\n",
      "loss: 7182.094727 [ 1390/ 1682]\n",
      "loss: 6984.273438 [ 1400/ 1682]\n",
      "loss: 7983.643555 [ 1410/ 1682]\n",
      "loss: 7942.050781 [ 1420/ 1682]\n",
      "loss: 7141.616211 [ 1430/ 1682]\n",
      "loss: 6460.146973 [ 1440/ 1682]\n",
      "loss: 6493.229492 [ 1450/ 1682]\n",
      "loss: 7492.383789 [ 1460/ 1682]\n",
      "loss: 9568.162109 [ 1470/ 1682]\n",
      "loss: 10689.054688 [ 1480/ 1682]\n",
      "loss: 12721.467773 [ 1490/ 1682]\n",
      "loss: 12584.168945 [ 1500/ 1682]\n",
      "loss: 10525.539062 [ 1510/ 1682]\n",
      "loss: 12325.419922 [ 1520/ 1682]\n",
      "loss: 10437.581055 [ 1530/ 1682]\n",
      "loss: 9388.967773 [ 1540/ 1682]\n",
      "loss: 10343.044922 [ 1550/ 1682]\n",
      "loss: 12631.541992 [ 1560/ 1682]\n",
      "loss: 11070.000977 [ 1570/ 1682]\n",
      "loss: 9094.598633 [ 1580/ 1682]\n",
      "loss: 6320.900391 [ 1590/ 1682]\n",
      "loss: 7215.558594 [ 1600/ 1682]\n",
      "loss: 5867.725586 [ 1610/ 1682]\n",
      "loss: 5880.003906 [ 1620/ 1682]\n",
      "loss: 7498.271973 [ 1630/ 1682]\n",
      "loss: 8932.720703 [ 1640/ 1682]\n",
      "loss: 11617.578125 [ 1650/ 1682]\n",
      "loss: 10110.455078 [ 1660/ 1682]\n",
      "loss: 9055.192383 [ 1670/ 1682]\n",
      "loss: 7087.226074 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10113.130165 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1992.729736 [    0/ 1682]\n",
      "loss: 1696.856445 [   10/ 1682]\n",
      "loss: 1338.193237 [   20/ 1682]\n",
      "loss: 1281.184570 [   30/ 1682]\n",
      "loss: 2596.595215 [   40/ 1682]\n",
      "loss: 1548.371338 [   50/ 1682]\n",
      "loss: 1669.322021 [   60/ 1682]\n",
      "loss: 1460.970337 [   70/ 1682]\n",
      "loss: 2529.341309 [   80/ 1682]\n",
      "loss: 1763.666992 [   90/ 1682]\n",
      "loss: 1721.633789 [  100/ 1682]\n",
      "loss: 1731.417725 [  110/ 1682]\n",
      "loss: 1319.341553 [  120/ 1682]\n",
      "loss: 1711.317017 [  130/ 1682]\n",
      "loss: 1930.044678 [  140/ 1682]\n",
      "loss: 1476.134644 [  150/ 1682]\n",
      "loss: 1143.351440 [  160/ 1682]\n",
      "loss: 1084.012451 [  170/ 1682]\n",
      "loss: 1997.366577 [  180/ 1682]\n",
      "loss: 1874.839844 [  190/ 1682]\n",
      "loss: 1743.954712 [  200/ 1682]\n",
      "loss: 1646.161865 [  210/ 1682]\n",
      "loss: 1229.997192 [  220/ 1682]\n",
      "loss: 1863.409424 [  230/ 1682]\n",
      "loss: 1493.687256 [  240/ 1682]\n",
      "loss: 1130.613159 [  250/ 1682]\n",
      "loss: 1204.832031 [  260/ 1682]\n",
      "loss: 1294.288574 [  270/ 1682]\n",
      "loss: 1098.783447 [  280/ 1682]\n",
      "loss: 804.271851 [  290/ 1682]\n",
      "loss: 775.606812 [  300/ 1682]\n",
      "loss: 1117.106323 [  310/ 1682]\n",
      "loss: 857.223511 [  320/ 1682]\n",
      "loss: 757.732300 [  330/ 1682]\n",
      "loss: 710.373657 [  340/ 1682]\n",
      "loss: 920.250122 [  350/ 1682]\n",
      "loss: 586.158997 [  360/ 1682]\n",
      "loss: 804.873779 [  370/ 1682]\n",
      "loss: 811.381409 [  380/ 1682]\n",
      "loss: 681.254944 [  390/ 1682]\n",
      "loss: 432.088074 [  400/ 1682]\n",
      "loss: 755.800354 [  410/ 1682]\n",
      "loss: 539.178833 [  420/ 1682]\n",
      "loss: 660.471924 [  430/ 1682]\n",
      "loss: 633.179810 [  440/ 1682]\n",
      "loss: 634.099243 [  450/ 1682]\n",
      "loss: 507.578217 [  460/ 1682]\n",
      "loss: 382.316162 [  470/ 1682]\n",
      "loss: 454.808289 [  480/ 1682]\n",
      "loss: 590.218262 [  490/ 1682]\n",
      "loss: 338.919342 [  500/ 1682]\n",
      "loss: 481.600891 [  510/ 1682]\n",
      "loss: 621.645874 [  520/ 1682]\n",
      "loss: 405.057922 [  530/ 1682]\n",
      "loss: 318.676483 [  540/ 1682]\n",
      "loss: 597.120789 [  550/ 1682]\n",
      "loss: 489.397064 [  560/ 1682]\n",
      "loss: 540.431885 [  570/ 1682]\n",
      "loss: 307.739441 [  580/ 1682]\n",
      "loss: 246.581573 [  590/ 1682]\n",
      "loss: 268.601501 [  600/ 1682]\n",
      "loss: 290.318939 [  610/ 1682]\n",
      "loss: 232.730957 [  620/ 1682]\n",
      "loss: 247.024536 [  630/ 1682]\n",
      "loss: 140.613190 [  640/ 1682]\n",
      "loss: 126.309586 [  650/ 1682]\n",
      "loss: 51.845112 [  660/ 1682]\n",
      "loss: 98.111954 [  670/ 1682]\n",
      "loss: 118.423050 [  680/ 1682]\n",
      "loss: 83.474792 [  690/ 1682]\n",
      "loss: 101.255699 [  700/ 1682]\n",
      "loss: 196.899261 [  710/ 1682]\n",
      "loss: 347.005310 [  720/ 1682]\n",
      "loss: 378.676178 [  730/ 1682]\n",
      "loss: 514.818115 [  740/ 1682]\n",
      "loss: 606.407043 [  750/ 1682]\n",
      "loss: 647.694092 [  760/ 1682]\n",
      "loss: 389.864166 [  770/ 1682]\n",
      "loss: 316.693695 [  780/ 1682]\n",
      "loss: 384.368286 [  790/ 1682]\n",
      "loss: 230.535492 [  800/ 1682]\n",
      "loss: 140.825760 [  810/ 1682]\n",
      "loss: 294.260590 [  820/ 1682]\n",
      "loss: 183.717468 [  830/ 1682]\n",
      "loss: 381.128754 [  840/ 1682]\n",
      "loss: 173.689240 [  850/ 1682]\n",
      "loss: 214.820038 [  860/ 1682]\n",
      "loss: 149.039124 [  870/ 1682]\n",
      "loss: 149.360199 [  880/ 1682]\n",
      "loss: 195.864960 [  890/ 1682]\n",
      "loss: 88.594872 [  900/ 1682]\n",
      "loss: 160.535660 [  910/ 1682]\n",
      "loss: 38.820423 [  920/ 1682]\n",
      "loss: 52.465679 [  930/ 1682]\n",
      "loss: 28.331219 [  940/ 1682]\n",
      "loss: 27.306713 [  950/ 1682]\n",
      "loss: 21.795435 [  960/ 1682]\n",
      "loss: 38.022518 [  970/ 1682]\n",
      "loss: 65.553703 [  980/ 1682]\n",
      "loss: 106.520973 [  990/ 1682]\n",
      "loss: 272.801483 [ 1000/ 1682]\n",
      "loss: 328.640259 [ 1010/ 1682]\n",
      "loss: 421.828766 [ 1020/ 1682]\n",
      "loss: 191.855133 [ 1030/ 1682]\n",
      "loss: 81.370956 [ 1040/ 1682]\n",
      "loss: 42.676243 [ 1050/ 1682]\n",
      "loss: 45.977703 [ 1060/ 1682]\n",
      "loss: 58.009716 [ 1070/ 1682]\n",
      "loss: 213.252319 [ 1080/ 1682]\n",
      "loss: 310.461121 [ 1090/ 1682]\n",
      "loss: 425.944397 [ 1100/ 1682]\n",
      "loss: 734.416443 [ 1110/ 1682]\n",
      "loss: 951.822937 [ 1120/ 1682]\n",
      "loss: 1188.825806 [ 1130/ 1682]\n",
      "loss: 1757.221436 [ 1140/ 1682]\n",
      "loss: 2624.269531 [ 1150/ 1682]\n",
      "loss: 3878.550781 [ 1160/ 1682]\n",
      "loss: 2528.145508 [ 1170/ 1682]\n",
      "loss: 2806.715576 [ 1180/ 1682]\n",
      "loss: 3254.269287 [ 1190/ 1682]\n",
      "loss: 2797.911865 [ 1200/ 1682]\n",
      "loss: 3171.916992 [ 1210/ 1682]\n",
      "loss: 2985.979980 [ 1220/ 1682]\n",
      "loss: 3491.256348 [ 1230/ 1682]\n",
      "loss: 5001.405762 [ 1240/ 1682]\n",
      "loss: 4455.293945 [ 1250/ 1682]\n",
      "loss: 5250.804688 [ 1260/ 1682]\n",
      "loss: 5660.704590 [ 1270/ 1682]\n",
      "loss: 4365.461914 [ 1280/ 1682]\n",
      "loss: 3776.155029 [ 1290/ 1682]\n",
      "loss: 3339.134033 [ 1300/ 1682]\n",
      "loss: 4175.814941 [ 1310/ 1682]\n",
      "loss: 5245.112305 [ 1320/ 1682]\n",
      "loss: 4274.072754 [ 1330/ 1682]\n",
      "loss: 3761.728516 [ 1340/ 1682]\n",
      "loss: 3739.352051 [ 1350/ 1682]\n",
      "loss: 4823.641113 [ 1360/ 1682]\n",
      "loss: 5596.543945 [ 1370/ 1682]\n",
      "loss: 6957.446777 [ 1380/ 1682]\n",
      "loss: 7483.141602 [ 1390/ 1682]\n",
      "loss: 7463.284180 [ 1400/ 1682]\n",
      "loss: 7474.089844 [ 1410/ 1682]\n",
      "loss: 7447.332031 [ 1420/ 1682]\n",
      "loss: 6674.716309 [ 1430/ 1682]\n",
      "loss: 6363.588867 [ 1440/ 1682]\n",
      "loss: 7468.997559 [ 1450/ 1682]\n",
      "loss: 7639.389648 [ 1460/ 1682]\n",
      "loss: 8774.291016 [ 1470/ 1682]\n",
      "loss: 11134.154297 [ 1480/ 1682]\n",
      "loss: 12912.786133 [ 1490/ 1682]\n",
      "loss: 13136.718750 [ 1500/ 1682]\n",
      "loss: 9617.939453 [ 1510/ 1682]\n",
      "loss: 12267.992188 [ 1520/ 1682]\n",
      "loss: 11065.824219 [ 1530/ 1682]\n",
      "loss: 9924.021484 [ 1540/ 1682]\n",
      "loss: 9586.424805 [ 1550/ 1682]\n",
      "loss: 12942.321289 [ 1560/ 1682]\n",
      "loss: 11329.617188 [ 1570/ 1682]\n",
      "loss: 9309.882812 [ 1580/ 1682]\n",
      "loss: 7353.812500 [ 1590/ 1682]\n",
      "loss: 7150.211914 [ 1600/ 1682]\n",
      "loss: 6052.973633 [ 1610/ 1682]\n",
      "loss: 5325.179199 [ 1620/ 1682]\n",
      "loss: 7160.252930 [ 1630/ 1682]\n",
      "loss: 8954.603516 [ 1640/ 1682]\n",
      "loss: 11239.009766 [ 1650/ 1682]\n",
      "loss: 10567.739258 [ 1660/ 1682]\n",
      "loss: 8585.203125 [ 1670/ 1682]\n",
      "loss: 8582.935547 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10236.431678 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1331.969971 [    0/ 1682]\n",
      "loss: 2331.576416 [   10/ 1682]\n",
      "loss: 1765.579834 [   20/ 1682]\n",
      "loss: 1683.396851 [   30/ 1682]\n",
      "loss: 1614.589355 [   40/ 1682]\n",
      "loss: 1742.498047 [   50/ 1682]\n",
      "loss: 2038.738647 [   60/ 1682]\n",
      "loss: 2190.598389 [   70/ 1682]\n",
      "loss: 1418.600952 [   80/ 1682]\n",
      "loss: 2472.617676 [   90/ 1682]\n",
      "loss: 1889.497070 [  100/ 1682]\n",
      "loss: 1913.640381 [  110/ 1682]\n",
      "loss: 1732.388672 [  120/ 1682]\n",
      "loss: 1845.591553 [  130/ 1682]\n",
      "loss: 1332.665771 [  140/ 1682]\n",
      "loss: 2451.063965 [  150/ 1682]\n",
      "loss: 1334.281250 [  160/ 1682]\n",
      "loss: 1431.362305 [  170/ 1682]\n",
      "loss: 1516.131592 [  180/ 1682]\n",
      "loss: 1007.810181 [  190/ 1682]\n",
      "loss: 1195.503784 [  200/ 1682]\n",
      "loss: 1582.907959 [  210/ 1682]\n",
      "loss: 1539.303589 [  220/ 1682]\n",
      "loss: 1457.061768 [  230/ 1682]\n",
      "loss: 993.479492 [  240/ 1682]\n",
      "loss: 1111.717285 [  250/ 1682]\n",
      "loss: 1050.872925 [  260/ 1682]\n",
      "loss: 1137.505615 [  270/ 1682]\n",
      "loss: 1068.541626 [  280/ 1682]\n",
      "loss: 1251.062012 [  290/ 1682]\n",
      "loss: 874.730286 [  300/ 1682]\n",
      "loss: 746.176636 [  310/ 1682]\n",
      "loss: 643.991516 [  320/ 1682]\n",
      "loss: 656.218506 [  330/ 1682]\n",
      "loss: 774.904785 [  340/ 1682]\n",
      "loss: 711.165894 [  350/ 1682]\n",
      "loss: 607.883118 [  360/ 1682]\n",
      "loss: 897.692871 [  370/ 1682]\n",
      "loss: 718.579346 [  380/ 1682]\n",
      "loss: 750.382690 [  390/ 1682]\n",
      "loss: 690.251221 [  400/ 1682]\n",
      "loss: 659.076233 [  410/ 1682]\n",
      "loss: 642.786499 [  420/ 1682]\n",
      "loss: 659.206116 [  430/ 1682]\n",
      "loss: 793.506470 [  440/ 1682]\n",
      "loss: 414.040039 [  450/ 1682]\n",
      "loss: 441.744812 [  460/ 1682]\n",
      "loss: 532.764893 [  470/ 1682]\n",
      "loss: 323.811707 [  480/ 1682]\n",
      "loss: 456.256592 [  490/ 1682]\n",
      "loss: 405.349731 [  500/ 1682]\n",
      "loss: 350.197968 [  510/ 1682]\n",
      "loss: 602.059570 [  520/ 1682]\n",
      "loss: 430.390808 [  530/ 1682]\n",
      "loss: 271.349457 [  540/ 1682]\n",
      "loss: 461.688873 [  550/ 1682]\n",
      "loss: 596.923523 [  560/ 1682]\n",
      "loss: 482.515869 [  570/ 1682]\n",
      "loss: 277.987823 [  580/ 1682]\n",
      "loss: 307.032928 [  590/ 1682]\n",
      "loss: 260.345032 [  600/ 1682]\n",
      "loss: 338.752777 [  610/ 1682]\n",
      "loss: 290.690430 [  620/ 1682]\n",
      "loss: 209.182098 [  630/ 1682]\n",
      "loss: 157.081177 [  640/ 1682]\n",
      "loss: 119.401489 [  650/ 1682]\n",
      "loss: 56.953053 [  660/ 1682]\n",
      "loss: 127.121414 [  670/ 1682]\n",
      "loss: 46.838055 [  680/ 1682]\n",
      "loss: 118.342041 [  690/ 1682]\n",
      "loss: 110.253952 [  700/ 1682]\n",
      "loss: 290.723328 [  710/ 1682]\n",
      "loss: 447.279724 [  720/ 1682]\n",
      "loss: 435.443848 [  730/ 1682]\n",
      "loss: 528.645142 [  740/ 1682]\n",
      "loss: 695.019226 [  750/ 1682]\n",
      "loss: 548.249634 [  760/ 1682]\n",
      "loss: 392.105774 [  770/ 1682]\n",
      "loss: 459.337311 [  780/ 1682]\n",
      "loss: 415.870270 [  790/ 1682]\n",
      "loss: 270.956085 [  800/ 1682]\n",
      "loss: 215.557892 [  810/ 1682]\n",
      "loss: 151.152039 [  820/ 1682]\n",
      "loss: 133.287354 [  830/ 1682]\n",
      "loss: 251.226288 [  840/ 1682]\n",
      "loss: 284.085083 [  850/ 1682]\n",
      "loss: 164.572906 [  860/ 1682]\n",
      "loss: 174.835480 [  870/ 1682]\n",
      "loss: 102.871986 [  880/ 1682]\n",
      "loss: 156.180222 [  890/ 1682]\n",
      "loss: 164.434601 [  900/ 1682]\n",
      "loss: 112.804665 [  910/ 1682]\n",
      "loss: 43.042770 [  920/ 1682]\n",
      "loss: 22.299629 [  930/ 1682]\n",
      "loss: 34.803684 [  940/ 1682]\n",
      "loss: 30.718586 [  950/ 1682]\n",
      "loss: 23.490242 [  960/ 1682]\n",
      "loss: 28.384449 [  970/ 1682]\n",
      "loss: 43.211052 [  980/ 1682]\n",
      "loss: 146.008591 [  990/ 1682]\n",
      "loss: 244.476471 [ 1000/ 1682]\n",
      "loss: 290.195374 [ 1010/ 1682]\n",
      "loss: 350.125000 [ 1020/ 1682]\n",
      "loss: 188.476273 [ 1030/ 1682]\n",
      "loss: 85.311874 [ 1040/ 1682]\n",
      "loss: 15.032389 [ 1050/ 1682]\n",
      "loss: 54.313061 [ 1060/ 1682]\n",
      "loss: 86.133202 [ 1070/ 1682]\n",
      "loss: 208.991043 [ 1080/ 1682]\n",
      "loss: 350.025482 [ 1090/ 1682]\n",
      "loss: 352.702087 [ 1100/ 1682]\n",
      "loss: 749.772644 [ 1110/ 1682]\n",
      "loss: 896.822632 [ 1120/ 1682]\n",
      "loss: 1216.017822 [ 1130/ 1682]\n",
      "loss: 1845.094727 [ 1140/ 1682]\n",
      "loss: 2644.471924 [ 1150/ 1682]\n",
      "loss: 4288.223145 [ 1160/ 1682]\n",
      "loss: 2552.353760 [ 1170/ 1682]\n",
      "loss: 2389.583252 [ 1180/ 1682]\n",
      "loss: 3219.065918 [ 1190/ 1682]\n",
      "loss: 2526.586426 [ 1200/ 1682]\n",
      "loss: 2814.454102 [ 1210/ 1682]\n",
      "loss: 2887.086426 [ 1220/ 1682]\n",
      "loss: 3923.880127 [ 1230/ 1682]\n",
      "loss: 4939.024902 [ 1240/ 1682]\n",
      "loss: 4357.756836 [ 1250/ 1682]\n",
      "loss: 5639.554199 [ 1260/ 1682]\n",
      "loss: 5331.607910 [ 1270/ 1682]\n",
      "loss: 4324.999512 [ 1280/ 1682]\n",
      "loss: 3488.927734 [ 1290/ 1682]\n",
      "loss: 3534.092529 [ 1300/ 1682]\n",
      "loss: 4169.313965 [ 1310/ 1682]\n",
      "loss: 5066.270508 [ 1320/ 1682]\n",
      "loss: 4537.758789 [ 1330/ 1682]\n",
      "loss: 4139.294922 [ 1340/ 1682]\n",
      "loss: 3869.629395 [ 1350/ 1682]\n",
      "loss: 4778.248535 [ 1360/ 1682]\n",
      "loss: 5733.027344 [ 1370/ 1682]\n",
      "loss: 7094.823242 [ 1380/ 1682]\n",
      "loss: 7220.363281 [ 1390/ 1682]\n",
      "loss: 7077.376465 [ 1400/ 1682]\n",
      "loss: 7274.459473 [ 1410/ 1682]\n",
      "loss: 7782.865723 [ 1420/ 1682]\n",
      "loss: 6268.175781 [ 1430/ 1682]\n",
      "loss: 6305.499023 [ 1440/ 1682]\n",
      "loss: 7629.438965 [ 1450/ 1682]\n",
      "loss: 7807.395508 [ 1460/ 1682]\n",
      "loss: 8875.221680 [ 1470/ 1682]\n",
      "loss: 10905.664062 [ 1480/ 1682]\n",
      "loss: 12561.602539 [ 1490/ 1682]\n",
      "loss: 13077.799805 [ 1500/ 1682]\n",
      "loss: 10637.056641 [ 1510/ 1682]\n",
      "loss: 12199.806641 [ 1520/ 1682]\n",
      "loss: 10660.427734 [ 1530/ 1682]\n",
      "loss: 9069.158203 [ 1540/ 1682]\n",
      "loss: 9564.900391 [ 1550/ 1682]\n",
      "loss: 12512.695312 [ 1560/ 1682]\n",
      "loss: 9982.685547 [ 1570/ 1682]\n",
      "loss: 8961.088867 [ 1580/ 1682]\n",
      "loss: 6487.502930 [ 1590/ 1682]\n",
      "loss: 6838.384277 [ 1600/ 1682]\n",
      "loss: 4910.014648 [ 1610/ 1682]\n",
      "loss: 5444.513672 [ 1620/ 1682]\n",
      "loss: 7342.786133 [ 1630/ 1682]\n",
      "loss: 8173.938477 [ 1640/ 1682]\n",
      "loss: 10259.401367 [ 1650/ 1682]\n",
      "loss: 10051.103516 [ 1660/ 1682]\n",
      "loss: 8474.689453 [ 1670/ 1682]\n",
      "loss: 8501.625977 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 10110.335656 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 2032.459717 [    0/ 1682]\n",
      "loss: 1395.412231 [   10/ 1682]\n",
      "loss: 1860.117920 [   20/ 1682]\n",
      "loss: 1583.659180 [   30/ 1682]\n",
      "loss: 1725.604492 [   40/ 1682]\n",
      "loss: 1430.966553 [   50/ 1682]\n",
      "loss: 1618.819580 [   60/ 1682]\n",
      "loss: 1524.721802 [   70/ 1682]\n",
      "loss: 2191.017822 [   80/ 1682]\n",
      "loss: 2113.474854 [   90/ 1682]\n",
      "loss: 1354.596436 [  100/ 1682]\n",
      "loss: 1878.407593 [  110/ 1682]\n",
      "loss: 1384.279663 [  120/ 1682]\n",
      "loss: 1575.721924 [  130/ 1682]\n",
      "loss: 1442.931763 [  140/ 1682]\n",
      "loss: 1590.951416 [  150/ 1682]\n",
      "loss: 2090.810791 [  160/ 1682]\n",
      "loss: 1994.042603 [  170/ 1682]\n",
      "loss: 1081.558716 [  180/ 1682]\n",
      "loss: 1643.371460 [  190/ 1682]\n",
      "loss: 1477.575928 [  200/ 1682]\n",
      "loss: 1563.800781 [  210/ 1682]\n",
      "loss: 1712.615479 [  220/ 1682]\n",
      "loss: 1266.650146 [  230/ 1682]\n",
      "loss: 1761.143311 [  240/ 1682]\n",
      "loss: 1748.278320 [  250/ 1682]\n",
      "loss: 1131.886963 [  260/ 1682]\n",
      "loss: 1130.995850 [  270/ 1682]\n",
      "loss: 895.015137 [  280/ 1682]\n",
      "loss: 871.706665 [  290/ 1682]\n",
      "loss: 980.653137 [  300/ 1682]\n",
      "loss: 809.438171 [  310/ 1682]\n",
      "loss: 1082.728271 [  320/ 1682]\n",
      "loss: 699.208862 [  330/ 1682]\n",
      "loss: 908.093262 [  340/ 1682]\n",
      "loss: 788.707947 [  350/ 1682]\n",
      "loss: 759.388794 [  360/ 1682]\n",
      "loss: 1145.164062 [  370/ 1682]\n",
      "loss: 560.683960 [  380/ 1682]\n",
      "loss: 523.986572 [  390/ 1682]\n",
      "loss: 1004.821167 [  400/ 1682]\n",
      "loss: 650.593323 [  410/ 1682]\n",
      "loss: 810.555847 [  420/ 1682]\n",
      "loss: 734.612915 [  430/ 1682]\n",
      "loss: 603.797729 [  440/ 1682]\n",
      "loss: 731.640503 [  450/ 1682]\n",
      "loss: 502.951752 [  460/ 1682]\n",
      "loss: 427.264252 [  470/ 1682]\n",
      "loss: 339.636871 [  480/ 1682]\n",
      "loss: 339.457703 [  490/ 1682]\n",
      "loss: 384.085785 [  500/ 1682]\n",
      "loss: 448.083679 [  510/ 1682]\n",
      "loss: 515.679504 [  520/ 1682]\n",
      "loss: 310.457825 [  530/ 1682]\n",
      "loss: 577.684265 [  540/ 1682]\n",
      "loss: 521.150757 [  550/ 1682]\n",
      "loss: 476.357758 [  560/ 1682]\n",
      "loss: 620.441711 [  570/ 1682]\n",
      "loss: 294.457977 [  580/ 1682]\n",
      "loss: 408.583313 [  590/ 1682]\n",
      "loss: 181.793671 [  600/ 1682]\n",
      "loss: 400.381897 [  610/ 1682]\n",
      "loss: 332.130219 [  620/ 1682]\n",
      "loss: 296.342712 [  630/ 1682]\n",
      "loss: 174.076569 [  640/ 1682]\n",
      "loss: 106.919617 [  650/ 1682]\n",
      "loss: 103.176903 [  660/ 1682]\n",
      "loss: 151.442764 [  670/ 1682]\n",
      "loss: 129.950317 [  680/ 1682]\n",
      "loss: 152.171677 [  690/ 1682]\n",
      "loss: 209.828934 [  700/ 1682]\n",
      "loss: 296.394684 [  710/ 1682]\n",
      "loss: 328.155457 [  720/ 1682]\n",
      "loss: 477.258301 [  730/ 1682]\n",
      "loss: 724.297485 [  740/ 1682]\n",
      "loss: 743.408813 [  750/ 1682]\n",
      "loss: 591.910889 [  760/ 1682]\n",
      "loss: 439.073181 [  770/ 1682]\n",
      "loss: 471.022461 [  780/ 1682]\n",
      "loss: 306.567444 [  790/ 1682]\n",
      "loss: 273.577911 [  800/ 1682]\n",
      "loss: 170.530945 [  810/ 1682]\n",
      "loss: 172.491272 [  820/ 1682]\n",
      "loss: 276.410431 [  830/ 1682]\n",
      "loss: 318.312836 [  840/ 1682]\n",
      "loss: 291.577728 [  850/ 1682]\n",
      "loss: 125.638916 [  860/ 1682]\n",
      "loss: 214.478027 [  870/ 1682]\n",
      "loss: 155.982956 [  880/ 1682]\n",
      "loss: 131.293015 [  890/ 1682]\n",
      "loss: 174.493729 [  900/ 1682]\n",
      "loss: 103.885704 [  910/ 1682]\n",
      "loss: 84.571701 [  920/ 1682]\n",
      "loss: 47.616737 [  930/ 1682]\n",
      "loss: 43.910210 [  940/ 1682]\n",
      "loss: 42.606926 [  950/ 1682]\n",
      "loss: 29.383371 [  960/ 1682]\n",
      "loss: 33.586308 [  970/ 1682]\n",
      "loss: 61.890907 [  980/ 1682]\n",
      "loss: 72.785637 [  990/ 1682]\n",
      "loss: 241.864868 [ 1000/ 1682]\n",
      "loss: 302.688202 [ 1010/ 1682]\n",
      "loss: 288.107239 [ 1020/ 1682]\n",
      "loss: 171.449249 [ 1030/ 1682]\n",
      "loss: 74.042328 [ 1040/ 1682]\n",
      "loss: 7.950881 [ 1050/ 1682]\n",
      "loss: 42.960938 [ 1060/ 1682]\n",
      "loss: 93.554893 [ 1070/ 1682]\n",
      "loss: 207.053665 [ 1080/ 1682]\n",
      "loss: 303.879181 [ 1090/ 1682]\n",
      "loss: 423.023987 [ 1100/ 1682]\n",
      "loss: 720.993958 [ 1110/ 1682]\n",
      "loss: 742.719116 [ 1120/ 1682]\n",
      "loss: 1245.630249 [ 1130/ 1682]\n",
      "loss: 1737.106812 [ 1140/ 1682]\n",
      "loss: 2850.485840 [ 1150/ 1682]\n",
      "loss: 3845.032471 [ 1160/ 1682]\n",
      "loss: 2276.942871 [ 1170/ 1682]\n",
      "loss: 2606.083496 [ 1180/ 1682]\n",
      "loss: 3012.179199 [ 1190/ 1682]\n",
      "loss: 2809.824463 [ 1200/ 1682]\n",
      "loss: 2710.724121 [ 1210/ 1682]\n",
      "loss: 3105.930420 [ 1220/ 1682]\n",
      "loss: 3614.203613 [ 1230/ 1682]\n",
      "loss: 4734.395020 [ 1240/ 1682]\n",
      "loss: 4556.207031 [ 1250/ 1682]\n",
      "loss: 5234.342773 [ 1260/ 1682]\n",
      "loss: 5370.899902 [ 1270/ 1682]\n",
      "loss: 3641.907715 [ 1280/ 1682]\n",
      "loss: 3268.912598 [ 1290/ 1682]\n",
      "loss: 3440.117188 [ 1300/ 1682]\n",
      "loss: 4458.305664 [ 1310/ 1682]\n",
      "loss: 4793.328125 [ 1320/ 1682]\n",
      "loss: 4457.220215 [ 1330/ 1682]\n",
      "loss: 3788.949951 [ 1340/ 1682]\n",
      "loss: 4047.767090 [ 1350/ 1682]\n",
      "loss: 4345.891602 [ 1360/ 1682]\n",
      "loss: 5250.634766 [ 1370/ 1682]\n",
      "loss: 6376.354492 [ 1380/ 1682]\n",
      "loss: 7126.026367 [ 1390/ 1682]\n",
      "loss: 6215.749023 [ 1400/ 1682]\n",
      "loss: 7706.325684 [ 1410/ 1682]\n",
      "loss: 7658.577148 [ 1420/ 1682]\n",
      "loss: 6922.299805 [ 1430/ 1682]\n",
      "loss: 6622.605469 [ 1440/ 1682]\n",
      "loss: 7754.606445 [ 1450/ 1682]\n",
      "loss: 7202.137695 [ 1460/ 1682]\n",
      "loss: 8747.832031 [ 1470/ 1682]\n",
      "loss: 10847.246094 [ 1480/ 1682]\n",
      "loss: 12331.518555 [ 1490/ 1682]\n",
      "loss: 11917.500977 [ 1500/ 1682]\n",
      "loss: 10820.867188 [ 1510/ 1682]\n",
      "loss: 11704.687500 [ 1520/ 1682]\n",
      "loss: 11102.486328 [ 1530/ 1682]\n",
      "loss: 9733.987305 [ 1540/ 1682]\n",
      "loss: 9393.943359 [ 1550/ 1682]\n",
      "loss: 11177.447266 [ 1560/ 1682]\n",
      "loss: 10433.046875 [ 1570/ 1682]\n",
      "loss: 9066.431641 [ 1580/ 1682]\n",
      "loss: 7206.731934 [ 1590/ 1682]\n",
      "loss: 6953.857910 [ 1600/ 1682]\n",
      "loss: 5378.929199 [ 1610/ 1682]\n",
      "loss: 5630.542969 [ 1620/ 1682]\n",
      "loss: 6942.131836 [ 1630/ 1682]\n",
      "loss: 9414.378906 [ 1640/ 1682]\n",
      "loss: 11370.166016 [ 1650/ 1682]\n",
      "loss: 9498.774414 [ 1660/ 1682]\n",
      "loss: 9124.796875 [ 1670/ 1682]\n",
      "loss: 8423.944336 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9968.784058 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1612.305298 [    0/ 1682]\n",
      "loss: 1646.273071 [   10/ 1682]\n",
      "loss: 1898.977173 [   20/ 1682]\n",
      "loss: 2065.655273 [   30/ 1682]\n",
      "loss: 1752.510986 [   40/ 1682]\n",
      "loss: 2313.913330 [   50/ 1682]\n",
      "loss: 1638.205811 [   60/ 1682]\n",
      "loss: 2207.114990 [   70/ 1682]\n",
      "loss: 2167.390381 [   80/ 1682]\n",
      "loss: 2330.447998 [   90/ 1682]\n",
      "loss: 1609.847412 [  100/ 1682]\n",
      "loss: 1652.757446 [  110/ 1682]\n",
      "loss: 2951.875244 [  120/ 1682]\n",
      "loss: 1577.015869 [  130/ 1682]\n",
      "loss: 1640.994873 [  140/ 1682]\n",
      "loss: 1373.396362 [  150/ 1682]\n",
      "loss: 1819.477173 [  160/ 1682]\n",
      "loss: 1356.705078 [  170/ 1682]\n",
      "loss: 1474.492188 [  180/ 1682]\n",
      "loss: 1407.106689 [  190/ 1682]\n",
      "loss: 1634.442139 [  200/ 1682]\n",
      "loss: 1189.918213 [  210/ 1682]\n",
      "loss: 1681.098999 [  220/ 1682]\n",
      "loss: 1108.119995 [  230/ 1682]\n",
      "loss: 1207.619385 [  240/ 1682]\n",
      "loss: 1838.946899 [  250/ 1682]\n",
      "loss: 1137.724243 [  260/ 1682]\n",
      "loss: 1265.950073 [  270/ 1682]\n",
      "loss: 1036.854736 [  280/ 1682]\n",
      "loss: 1008.141418 [  290/ 1682]\n",
      "loss: 836.045410 [  300/ 1682]\n",
      "loss: 1061.140503 [  310/ 1682]\n",
      "loss: 810.365601 [  320/ 1682]\n",
      "loss: 701.036499 [  330/ 1682]\n",
      "loss: 553.332520 [  340/ 1682]\n",
      "loss: 795.202759 [  350/ 1682]\n",
      "loss: 763.100464 [  360/ 1682]\n",
      "loss: 1003.161926 [  370/ 1682]\n",
      "loss: 580.714844 [  380/ 1682]\n",
      "loss: 645.051147 [  390/ 1682]\n",
      "loss: 682.110168 [  400/ 1682]\n",
      "loss: 738.599548 [  410/ 1682]\n",
      "loss: 699.211609 [  420/ 1682]\n",
      "loss: 640.691772 [  430/ 1682]\n",
      "loss: 805.674316 [  440/ 1682]\n",
      "loss: 631.949707 [  450/ 1682]\n",
      "loss: 749.300171 [  460/ 1682]\n",
      "loss: 432.986725 [  470/ 1682]\n",
      "loss: 434.810730 [  480/ 1682]\n",
      "loss: 509.871185 [  490/ 1682]\n",
      "loss: 612.412964 [  500/ 1682]\n",
      "loss: 624.317993 [  510/ 1682]\n",
      "loss: 439.965027 [  520/ 1682]\n",
      "loss: 396.157532 [  530/ 1682]\n",
      "loss: 439.763580 [  540/ 1682]\n",
      "loss: 451.238129 [  550/ 1682]\n",
      "loss: 551.730591 [  560/ 1682]\n",
      "loss: 546.455627 [  570/ 1682]\n",
      "loss: 371.686279 [  580/ 1682]\n",
      "loss: 227.766022 [  590/ 1682]\n",
      "loss: 311.932526 [  600/ 1682]\n",
      "loss: 344.008209 [  610/ 1682]\n",
      "loss: 339.061249 [  620/ 1682]\n",
      "loss: 189.972412 [  630/ 1682]\n",
      "loss: 231.416779 [  640/ 1682]\n",
      "loss: 154.404938 [  650/ 1682]\n",
      "loss: 137.200974 [  660/ 1682]\n",
      "loss: 80.408348 [  670/ 1682]\n",
      "loss: 128.404266 [  680/ 1682]\n",
      "loss: 84.409599 [  690/ 1682]\n",
      "loss: 160.119614 [  700/ 1682]\n",
      "loss: 196.109222 [  710/ 1682]\n",
      "loss: 396.486572 [  720/ 1682]\n",
      "loss: 542.600891 [  730/ 1682]\n",
      "loss: 581.105652 [  740/ 1682]\n",
      "loss: 531.073853 [  750/ 1682]\n",
      "loss: 615.809570 [  760/ 1682]\n",
      "loss: 446.456482 [  770/ 1682]\n",
      "loss: 539.414551 [  780/ 1682]\n",
      "loss: 424.545502 [  790/ 1682]\n",
      "loss: 236.755692 [  800/ 1682]\n",
      "loss: 135.348999 [  810/ 1682]\n",
      "loss: 178.879517 [  820/ 1682]\n",
      "loss: 155.799652 [  830/ 1682]\n",
      "loss: 373.873352 [  840/ 1682]\n",
      "loss: 304.051605 [  850/ 1682]\n",
      "loss: 218.232010 [  860/ 1682]\n",
      "loss: 263.657257 [  870/ 1682]\n",
      "loss: 159.534393 [  880/ 1682]\n",
      "loss: 252.635086 [  890/ 1682]\n",
      "loss: 151.474426 [  900/ 1682]\n",
      "loss: 144.997818 [  910/ 1682]\n",
      "loss: 89.745407 [  920/ 1682]\n",
      "loss: 71.796066 [  930/ 1682]\n",
      "loss: 30.080917 [  940/ 1682]\n",
      "loss: 39.452126 [  950/ 1682]\n",
      "loss: 19.007771 [  960/ 1682]\n",
      "loss: 27.438557 [  970/ 1682]\n",
      "loss: 45.847538 [  980/ 1682]\n",
      "loss: 96.394379 [  990/ 1682]\n",
      "loss: 229.245514 [ 1000/ 1682]\n",
      "loss: 231.450562 [ 1010/ 1682]\n",
      "loss: 282.070435 [ 1020/ 1682]\n",
      "loss: 187.080276 [ 1030/ 1682]\n",
      "loss: 69.006386 [ 1040/ 1682]\n",
      "loss: 37.099709 [ 1050/ 1682]\n",
      "loss: 41.430595 [ 1060/ 1682]\n",
      "loss: 55.963676 [ 1070/ 1682]\n",
      "loss: 179.795624 [ 1080/ 1682]\n",
      "loss: 296.896423 [ 1090/ 1682]\n",
      "loss: 412.263184 [ 1100/ 1682]\n",
      "loss: 568.926453 [ 1110/ 1682]\n",
      "loss: 928.384399 [ 1120/ 1682]\n",
      "loss: 1096.033447 [ 1130/ 1682]\n",
      "loss: 1646.027588 [ 1140/ 1682]\n",
      "loss: 2690.071777 [ 1150/ 1682]\n",
      "loss: 4055.494629 [ 1160/ 1682]\n",
      "loss: 2243.714844 [ 1170/ 1682]\n",
      "loss: 2385.311035 [ 1180/ 1682]\n",
      "loss: 2788.625244 [ 1190/ 1682]\n",
      "loss: 2354.807861 [ 1200/ 1682]\n",
      "loss: 2907.053467 [ 1210/ 1682]\n",
      "loss: 2746.579102 [ 1220/ 1682]\n",
      "loss: 3691.169189 [ 1230/ 1682]\n",
      "loss: 4117.613281 [ 1240/ 1682]\n",
      "loss: 4365.917969 [ 1250/ 1682]\n",
      "loss: 5356.113770 [ 1260/ 1682]\n",
      "loss: 5156.533203 [ 1270/ 1682]\n",
      "loss: 4044.419922 [ 1280/ 1682]\n",
      "loss: 3373.647217 [ 1290/ 1682]\n",
      "loss: 3275.698486 [ 1300/ 1682]\n",
      "loss: 3792.583496 [ 1310/ 1682]\n",
      "loss: 5070.468750 [ 1320/ 1682]\n",
      "loss: 4376.056152 [ 1330/ 1682]\n",
      "loss: 4171.655273 [ 1340/ 1682]\n",
      "loss: 3851.617920 [ 1350/ 1682]\n",
      "loss: 4299.548340 [ 1360/ 1682]\n",
      "loss: 5744.770996 [ 1370/ 1682]\n",
      "loss: 6530.150391 [ 1380/ 1682]\n",
      "loss: 6627.817383 [ 1390/ 1682]\n",
      "loss: 6796.696777 [ 1400/ 1682]\n",
      "loss: 7423.791992 [ 1410/ 1682]\n",
      "loss: 7361.571289 [ 1420/ 1682]\n",
      "loss: 6646.708008 [ 1430/ 1682]\n",
      "loss: 6555.998535 [ 1440/ 1682]\n",
      "loss: 6989.044434 [ 1450/ 1682]\n",
      "loss: 7855.467773 [ 1460/ 1682]\n",
      "loss: 8991.869141 [ 1470/ 1682]\n",
      "loss: 10856.106445 [ 1480/ 1682]\n",
      "loss: 12251.568359 [ 1490/ 1682]\n",
      "loss: 12491.140625 [ 1500/ 1682]\n",
      "loss: 9870.522461 [ 1510/ 1682]\n",
      "loss: 11652.499023 [ 1520/ 1682]\n",
      "loss: 10687.692383 [ 1530/ 1682]\n",
      "loss: 9353.330078 [ 1540/ 1682]\n",
      "loss: 10245.687500 [ 1550/ 1682]\n",
      "loss: 12275.236328 [ 1560/ 1682]\n",
      "loss: 9360.371094 [ 1570/ 1682]\n",
      "loss: 9677.504883 [ 1580/ 1682]\n",
      "loss: 6308.977539 [ 1590/ 1682]\n",
      "loss: 6365.984375 [ 1600/ 1682]\n",
      "loss: 4831.574219 [ 1610/ 1682]\n",
      "loss: 5084.122070 [ 1620/ 1682]\n",
      "loss: 6633.255371 [ 1630/ 1682]\n",
      "loss: 8658.180664 [ 1640/ 1682]\n",
      "loss: 10451.167969 [ 1650/ 1682]\n",
      "loss: 10251.853516 [ 1660/ 1682]\n",
      "loss: 8295.526367 [ 1670/ 1682]\n",
      "loss: 6645.304199 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9904.555777 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1855.401978 [    0/ 1682]\n",
      "loss: 1674.653320 [   10/ 1682]\n",
      "loss: 2362.763916 [   20/ 1682]\n",
      "loss: 2093.836914 [   30/ 1682]\n",
      "loss: 2001.101562 [   40/ 1682]\n",
      "loss: 1704.489502 [   50/ 1682]\n",
      "loss: 1446.926025 [   60/ 1682]\n",
      "loss: 2230.916748 [   70/ 1682]\n",
      "loss: 1991.298828 [   80/ 1682]\n",
      "loss: 1903.079346 [   90/ 1682]\n",
      "loss: 1878.680054 [  100/ 1682]\n",
      "loss: 1676.232056 [  110/ 1682]\n",
      "loss: 2129.880127 [  120/ 1682]\n",
      "loss: 1401.057739 [  130/ 1682]\n",
      "loss: 1269.473633 [  140/ 1682]\n",
      "loss: 2038.570557 [  150/ 1682]\n",
      "loss: 1897.667358 [  160/ 1682]\n",
      "loss: 1617.469727 [  170/ 1682]\n",
      "loss: 1731.304321 [  180/ 1682]\n",
      "loss: 1089.677979 [  190/ 1682]\n",
      "loss: 1136.451416 [  200/ 1682]\n",
      "loss: 2006.458008 [  210/ 1682]\n",
      "loss: 1183.040771 [  220/ 1682]\n",
      "loss: 1508.912354 [  230/ 1682]\n",
      "loss: 1253.169556 [  240/ 1682]\n",
      "loss: 1421.112183 [  250/ 1682]\n",
      "loss: 1187.519043 [  260/ 1682]\n",
      "loss: 1337.627930 [  270/ 1682]\n",
      "loss: 942.278442 [  280/ 1682]\n",
      "loss: 1066.678467 [  290/ 1682]\n",
      "loss: 877.840454 [  300/ 1682]\n",
      "loss: 1283.797607 [  310/ 1682]\n",
      "loss: 1274.482178 [  320/ 1682]\n",
      "loss: 735.789856 [  330/ 1682]\n",
      "loss: 824.239746 [  340/ 1682]\n",
      "loss: 591.896301 [  350/ 1682]\n",
      "loss: 1332.876709 [  360/ 1682]\n",
      "loss: 947.924194 [  370/ 1682]\n",
      "loss: 847.500977 [  380/ 1682]\n",
      "loss: 803.548950 [  390/ 1682]\n",
      "loss: 841.872559 [  400/ 1682]\n",
      "loss: 692.872925 [  410/ 1682]\n",
      "loss: 746.803528 [  420/ 1682]\n",
      "loss: 672.276733 [  430/ 1682]\n",
      "loss: 870.368469 [  440/ 1682]\n",
      "loss: 671.055481 [  450/ 1682]\n",
      "loss: 545.293762 [  460/ 1682]\n",
      "loss: 460.312256 [  470/ 1682]\n",
      "loss: 370.512268 [  480/ 1682]\n",
      "loss: 548.591064 [  490/ 1682]\n",
      "loss: 332.658997 [  500/ 1682]\n",
      "loss: 669.853577 [  510/ 1682]\n",
      "loss: 555.158508 [  520/ 1682]\n",
      "loss: 758.872925 [  530/ 1682]\n",
      "loss: 472.773254 [  540/ 1682]\n",
      "loss: 556.391907 [  550/ 1682]\n",
      "loss: 519.491089 [  560/ 1682]\n",
      "loss: 576.015442 [  570/ 1682]\n",
      "loss: 484.752197 [  580/ 1682]\n",
      "loss: 448.029449 [  590/ 1682]\n",
      "loss: 337.557892 [  600/ 1682]\n",
      "loss: 304.750519 [  610/ 1682]\n",
      "loss: 371.417206 [  620/ 1682]\n",
      "loss: 328.800598 [  630/ 1682]\n",
      "loss: 244.622238 [  640/ 1682]\n",
      "loss: 79.224785 [  650/ 1682]\n",
      "loss: 196.194931 [  660/ 1682]\n",
      "loss: 133.024231 [  670/ 1682]\n",
      "loss: 188.455887 [  680/ 1682]\n",
      "loss: 131.097855 [  690/ 1682]\n",
      "loss: 154.583221 [  700/ 1682]\n",
      "loss: 153.880585 [  710/ 1682]\n",
      "loss: 355.880920 [  720/ 1682]\n",
      "loss: 593.747375 [  730/ 1682]\n",
      "loss: 688.830444 [  740/ 1682]\n",
      "loss: 714.416382 [  750/ 1682]\n",
      "loss: 640.040039 [  760/ 1682]\n",
      "loss: 409.466705 [  770/ 1682]\n",
      "loss: 448.955902 [  780/ 1682]\n",
      "loss: 277.670868 [  790/ 1682]\n",
      "loss: 305.931519 [  800/ 1682]\n",
      "loss: 386.361511 [  810/ 1682]\n",
      "loss: 110.656227 [  820/ 1682]\n",
      "loss: 398.061371 [  830/ 1682]\n",
      "loss: 296.303650 [  840/ 1682]\n",
      "loss: 433.535706 [  850/ 1682]\n",
      "loss: 232.631744 [  860/ 1682]\n",
      "loss: 204.219925 [  870/ 1682]\n",
      "loss: 216.439240 [  880/ 1682]\n",
      "loss: 203.972198 [  890/ 1682]\n",
      "loss: 201.654877 [  900/ 1682]\n",
      "loss: 227.234283 [  910/ 1682]\n",
      "loss: 190.923477 [  920/ 1682]\n",
      "loss: 67.903053 [  930/ 1682]\n",
      "loss: 10.808515 [  940/ 1682]\n",
      "loss: 61.365032 [  950/ 1682]\n",
      "loss: 22.530346 [  960/ 1682]\n",
      "loss: 20.568222 [  970/ 1682]\n",
      "loss: 46.133953 [  980/ 1682]\n",
      "loss: 81.873726 [  990/ 1682]\n",
      "loss: 196.619217 [ 1000/ 1682]\n",
      "loss: 206.620895 [ 1010/ 1682]\n",
      "loss: 323.643738 [ 1020/ 1682]\n",
      "loss: 119.694115 [ 1030/ 1682]\n",
      "loss: 79.924461 [ 1040/ 1682]\n",
      "loss: 17.930981 [ 1050/ 1682]\n",
      "loss: 50.164371 [ 1060/ 1682]\n",
      "loss: 74.162132 [ 1070/ 1682]\n",
      "loss: 140.008926 [ 1080/ 1682]\n",
      "loss: 253.629364 [ 1090/ 1682]\n",
      "loss: 376.373352 [ 1100/ 1682]\n",
      "loss: 686.529968 [ 1110/ 1682]\n",
      "loss: 898.361633 [ 1120/ 1682]\n",
      "loss: 991.922241 [ 1130/ 1682]\n",
      "loss: 1624.626587 [ 1140/ 1682]\n",
      "loss: 2743.618408 [ 1150/ 1682]\n",
      "loss: 3998.462158 [ 1160/ 1682]\n",
      "loss: 2186.621582 [ 1170/ 1682]\n",
      "loss: 2314.687012 [ 1180/ 1682]\n",
      "loss: 3044.354248 [ 1190/ 1682]\n",
      "loss: 2326.218018 [ 1200/ 1682]\n",
      "loss: 2977.337891 [ 1210/ 1682]\n",
      "loss: 2793.625732 [ 1220/ 1682]\n",
      "loss: 3645.817871 [ 1230/ 1682]\n",
      "loss: 4470.480957 [ 1240/ 1682]\n",
      "loss: 4004.876221 [ 1250/ 1682]\n",
      "loss: 5317.041992 [ 1260/ 1682]\n",
      "loss: 4765.209961 [ 1270/ 1682]\n",
      "loss: 3858.833252 [ 1280/ 1682]\n",
      "loss: 3163.322266 [ 1290/ 1682]\n",
      "loss: 3341.558105 [ 1300/ 1682]\n",
      "loss: 4201.420898 [ 1310/ 1682]\n",
      "loss: 5003.887695 [ 1320/ 1682]\n",
      "loss: 4311.485352 [ 1330/ 1682]\n",
      "loss: 4121.666992 [ 1340/ 1682]\n",
      "loss: 3643.496582 [ 1350/ 1682]\n",
      "loss: 4572.238770 [ 1360/ 1682]\n",
      "loss: 5137.202148 [ 1370/ 1682]\n",
      "loss: 6249.891602 [ 1380/ 1682]\n",
      "loss: 6550.596680 [ 1390/ 1682]\n",
      "loss: 7158.291504 [ 1400/ 1682]\n",
      "loss: 7576.075684 [ 1410/ 1682]\n",
      "loss: 7286.742188 [ 1420/ 1682]\n",
      "loss: 6129.394043 [ 1430/ 1682]\n",
      "loss: 6077.781738 [ 1440/ 1682]\n",
      "loss: 6894.147461 [ 1450/ 1682]\n",
      "loss: 7297.731445 [ 1460/ 1682]\n",
      "loss: 8628.273438 [ 1470/ 1682]\n",
      "loss: 11049.333008 [ 1480/ 1682]\n",
      "loss: 12823.653320 [ 1490/ 1682]\n",
      "loss: 12385.974609 [ 1500/ 1682]\n",
      "loss: 10982.401367 [ 1510/ 1682]\n",
      "loss: 11234.396484 [ 1520/ 1682]\n",
      "loss: 10968.053711 [ 1530/ 1682]\n",
      "loss: 9587.017578 [ 1540/ 1682]\n",
      "loss: 9830.205078 [ 1550/ 1682]\n",
      "loss: 12539.470703 [ 1560/ 1682]\n",
      "loss: 9946.117188 [ 1570/ 1682]\n",
      "loss: 9265.612305 [ 1580/ 1682]\n",
      "loss: 6476.480957 [ 1590/ 1682]\n",
      "loss: 6298.617676 [ 1600/ 1682]\n",
      "loss: 5548.878906 [ 1610/ 1682]\n",
      "loss: 5798.196289 [ 1620/ 1682]\n",
      "loss: 6858.436035 [ 1630/ 1682]\n",
      "loss: 8921.115234 [ 1640/ 1682]\n",
      "loss: 10838.978516 [ 1650/ 1682]\n",
      "loss: 10948.064453 [ 1660/ 1682]\n",
      "loss: 8310.535156 [ 1670/ 1682]\n",
      "loss: 8278.083984 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9725.274583 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 2269.374512 [    0/ 1682]\n",
      "loss: 1688.906860 [   10/ 1682]\n",
      "loss: 1496.599731 [   20/ 1682]\n",
      "loss: 1436.267822 [   30/ 1682]\n",
      "loss: 1562.339722 [   40/ 1682]\n",
      "loss: 1684.232666 [   50/ 1682]\n",
      "loss: 1451.846802 [   60/ 1682]\n",
      "loss: 1371.925415 [   70/ 1682]\n",
      "loss: 2176.172607 [   80/ 1682]\n",
      "loss: 1691.836548 [   90/ 1682]\n",
      "loss: 1653.443604 [  100/ 1682]\n",
      "loss: 1904.731201 [  110/ 1682]\n",
      "loss: 1476.714600 [  120/ 1682]\n",
      "loss: 2033.757812 [  130/ 1682]\n",
      "loss: 1690.754639 [  140/ 1682]\n",
      "loss: 1233.370728 [  150/ 1682]\n",
      "loss: 1676.746704 [  160/ 1682]\n",
      "loss: 1787.190063 [  170/ 1682]\n",
      "loss: 1344.645996 [  180/ 1682]\n",
      "loss: 1476.766357 [  190/ 1682]\n",
      "loss: 1518.463135 [  200/ 1682]\n",
      "loss: 1597.183838 [  210/ 1682]\n",
      "loss: 1745.600830 [  220/ 1682]\n",
      "loss: 1676.720459 [  230/ 1682]\n",
      "loss: 1438.970703 [  240/ 1682]\n",
      "loss: 1425.973389 [  250/ 1682]\n",
      "loss: 1509.973877 [  260/ 1682]\n",
      "loss: 1316.106323 [  270/ 1682]\n",
      "loss: 955.048523 [  280/ 1682]\n",
      "loss: 1342.541992 [  290/ 1682]\n",
      "loss: 753.386719 [  300/ 1682]\n",
      "loss: 1255.719604 [  310/ 1682]\n",
      "loss: 864.320618 [  320/ 1682]\n",
      "loss: 867.403809 [  330/ 1682]\n",
      "loss: 1061.137451 [  340/ 1682]\n",
      "loss: 732.207886 [  350/ 1682]\n",
      "loss: 810.194458 [  360/ 1682]\n",
      "loss: 941.914551 [  370/ 1682]\n",
      "loss: 725.800049 [  380/ 1682]\n",
      "loss: 687.951294 [  390/ 1682]\n",
      "loss: 829.368286 [  400/ 1682]\n",
      "loss: 687.672180 [  410/ 1682]\n",
      "loss: 646.211731 [  420/ 1682]\n",
      "loss: 783.269836 [  430/ 1682]\n",
      "loss: 757.112366 [  440/ 1682]\n",
      "loss: 674.614075 [  450/ 1682]\n",
      "loss: 711.503357 [  460/ 1682]\n",
      "loss: 638.042542 [  470/ 1682]\n",
      "loss: 639.205688 [  480/ 1682]\n",
      "loss: 551.438599 [  490/ 1682]\n",
      "loss: 581.059143 [  500/ 1682]\n",
      "loss: 576.901855 [  510/ 1682]\n",
      "loss: 650.495972 [  520/ 1682]\n",
      "loss: 654.802612 [  530/ 1682]\n",
      "loss: 542.482361 [  540/ 1682]\n",
      "loss: 404.764984 [  550/ 1682]\n",
      "loss: 512.739624 [  560/ 1682]\n",
      "loss: 583.044922 [  570/ 1682]\n",
      "loss: 393.075195 [  580/ 1682]\n",
      "loss: 378.140106 [  590/ 1682]\n",
      "loss: 334.111145 [  600/ 1682]\n",
      "loss: 365.362000 [  610/ 1682]\n",
      "loss: 365.132233 [  620/ 1682]\n",
      "loss: 438.671204 [  630/ 1682]\n",
      "loss: 300.305847 [  640/ 1682]\n",
      "loss: 165.074905 [  650/ 1682]\n",
      "loss: 50.746208 [  660/ 1682]\n",
      "loss: 160.942993 [  670/ 1682]\n",
      "loss: 147.186447 [  680/ 1682]\n",
      "loss: 133.719818 [  690/ 1682]\n",
      "loss: 108.841125 [  700/ 1682]\n",
      "loss: 354.579742 [  710/ 1682]\n",
      "loss: 366.373291 [  720/ 1682]\n",
      "loss: 511.893402 [  730/ 1682]\n",
      "loss: 829.196289 [  740/ 1682]\n",
      "loss: 1076.939209 [  750/ 1682]\n",
      "loss: 709.947266 [  760/ 1682]\n",
      "loss: 596.281494 [  770/ 1682]\n",
      "loss: 448.176849 [  780/ 1682]\n",
      "loss: 553.818481 [  790/ 1682]\n",
      "loss: 299.097382 [  800/ 1682]\n",
      "loss: 323.966492 [  810/ 1682]\n",
      "loss: 234.115280 [  820/ 1682]\n",
      "loss: 292.850159 [  830/ 1682]\n",
      "loss: 251.133148 [  840/ 1682]\n",
      "loss: 323.643005 [  850/ 1682]\n",
      "loss: 194.525681 [  860/ 1682]\n",
      "loss: 129.929855 [  870/ 1682]\n",
      "loss: 209.779205 [  880/ 1682]\n",
      "loss: 122.557236 [  890/ 1682]\n",
      "loss: 127.889816 [  900/ 1682]\n",
      "loss: 154.680450 [  910/ 1682]\n",
      "loss: 44.887459 [  920/ 1682]\n",
      "loss: 108.090591 [  930/ 1682]\n",
      "loss: 63.664997 [  940/ 1682]\n",
      "loss: 26.575857 [  950/ 1682]\n",
      "loss: 31.809216 [  960/ 1682]\n",
      "loss: 25.634308 [  970/ 1682]\n",
      "loss: 31.499020 [  980/ 1682]\n",
      "loss: 99.169395 [  990/ 1682]\n",
      "loss: 211.793915 [ 1000/ 1682]\n",
      "loss: 244.370636 [ 1010/ 1682]\n",
      "loss: 286.138611 [ 1020/ 1682]\n",
      "loss: 129.314209 [ 1030/ 1682]\n",
      "loss: 75.054077 [ 1040/ 1682]\n",
      "loss: 42.848682 [ 1050/ 1682]\n",
      "loss: 21.298161 [ 1060/ 1682]\n",
      "loss: 60.855743 [ 1070/ 1682]\n",
      "loss: 182.160660 [ 1080/ 1682]\n",
      "loss: 267.976929 [ 1090/ 1682]\n",
      "loss: 351.455811 [ 1100/ 1682]\n",
      "loss: 674.070679 [ 1110/ 1682]\n",
      "loss: 939.241089 [ 1120/ 1682]\n",
      "loss: 1004.990845 [ 1130/ 1682]\n",
      "loss: 1614.043945 [ 1140/ 1682]\n",
      "loss: 2535.609863 [ 1150/ 1682]\n",
      "loss: 3962.769531 [ 1160/ 1682]\n",
      "loss: 2372.754150 [ 1170/ 1682]\n",
      "loss: 2212.026367 [ 1180/ 1682]\n",
      "loss: 2835.269043 [ 1190/ 1682]\n",
      "loss: 2396.520020 [ 1200/ 1682]\n",
      "loss: 2949.455566 [ 1210/ 1682]\n",
      "loss: 2991.080811 [ 1220/ 1682]\n",
      "loss: 3610.515625 [ 1230/ 1682]\n",
      "loss: 4446.260254 [ 1240/ 1682]\n",
      "loss: 4017.556152 [ 1250/ 1682]\n",
      "loss: 5250.418457 [ 1260/ 1682]\n",
      "loss: 4925.291504 [ 1270/ 1682]\n",
      "loss: 3600.667236 [ 1280/ 1682]\n",
      "loss: 3527.431152 [ 1290/ 1682]\n",
      "loss: 3330.128906 [ 1300/ 1682]\n",
      "loss: 4061.497314 [ 1310/ 1682]\n",
      "loss: 4529.540527 [ 1320/ 1682]\n",
      "loss: 4456.553223 [ 1330/ 1682]\n",
      "loss: 3797.708496 [ 1340/ 1682]\n",
      "loss: 3784.914062 [ 1350/ 1682]\n",
      "loss: 4540.172852 [ 1360/ 1682]\n",
      "loss: 5799.867188 [ 1370/ 1682]\n",
      "loss: 6620.312500 [ 1380/ 1682]\n",
      "loss: 6561.639160 [ 1390/ 1682]\n",
      "loss: 6930.276367 [ 1400/ 1682]\n",
      "loss: 6938.180664 [ 1410/ 1682]\n",
      "loss: 7522.990723 [ 1420/ 1682]\n",
      "loss: 5947.140625 [ 1430/ 1682]\n",
      "loss: 6235.506836 [ 1440/ 1682]\n",
      "loss: 7546.208496 [ 1450/ 1682]\n",
      "loss: 6864.181152 [ 1460/ 1682]\n",
      "loss: 8865.990234 [ 1470/ 1682]\n",
      "loss: 10764.587891 [ 1480/ 1682]\n",
      "loss: 12734.688477 [ 1490/ 1682]\n",
      "loss: 12409.687500 [ 1500/ 1682]\n",
      "loss: 10076.289062 [ 1510/ 1682]\n",
      "loss: 11834.584961 [ 1520/ 1682]\n",
      "loss: 10918.905273 [ 1530/ 1682]\n",
      "loss: 9020.473633 [ 1540/ 1682]\n",
      "loss: 10088.560547 [ 1550/ 1682]\n",
      "loss: 11842.896484 [ 1560/ 1682]\n",
      "loss: 10298.358398 [ 1570/ 1682]\n",
      "loss: 8649.373047 [ 1580/ 1682]\n",
      "loss: 6231.269531 [ 1590/ 1682]\n",
      "loss: 7069.907715 [ 1600/ 1682]\n",
      "loss: 5303.293945 [ 1610/ 1682]\n",
      "loss: 5534.413086 [ 1620/ 1682]\n",
      "loss: 6054.648926 [ 1630/ 1682]\n",
      "loss: 7989.213867 [ 1640/ 1682]\n",
      "loss: 10789.888672 [ 1650/ 1682]\n",
      "loss: 9798.482422 [ 1660/ 1682]\n",
      "loss: 8581.750977 [ 1670/ 1682]\n",
      "loss: 8206.867188 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9694.135010 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1669.569336 [    0/ 1682]\n",
      "loss: 1694.977783 [   10/ 1682]\n",
      "loss: 1527.079834 [   20/ 1682]\n",
      "loss: 2025.887451 [   30/ 1682]\n",
      "loss: 2333.180176 [   40/ 1682]\n",
      "loss: 1692.553467 [   50/ 1682]\n",
      "loss: 2012.071899 [   60/ 1682]\n",
      "loss: 1974.308350 [   70/ 1682]\n",
      "loss: 2384.449219 [   80/ 1682]\n",
      "loss: 1697.679077 [   90/ 1682]\n",
      "loss: 2046.169556 [  100/ 1682]\n",
      "loss: 1709.590820 [  110/ 1682]\n",
      "loss: 1892.176147 [  120/ 1682]\n",
      "loss: 2223.108398 [  130/ 1682]\n",
      "loss: 1676.197510 [  140/ 1682]\n",
      "loss: 1774.807251 [  150/ 1682]\n",
      "loss: 1848.775024 [  160/ 1682]\n",
      "loss: 1593.170532 [  170/ 1682]\n",
      "loss: 1677.803955 [  180/ 1682]\n",
      "loss: 1296.233765 [  190/ 1682]\n",
      "loss: 1667.267212 [  200/ 1682]\n",
      "loss: 1765.589478 [  210/ 1682]\n",
      "loss: 1395.001221 [  220/ 1682]\n",
      "loss: 1499.557861 [  230/ 1682]\n",
      "loss: 1422.830078 [  240/ 1682]\n",
      "loss: 1266.412354 [  250/ 1682]\n",
      "loss: 1341.501831 [  260/ 1682]\n",
      "loss: 1033.438721 [  270/ 1682]\n",
      "loss: 1072.987183 [  280/ 1682]\n",
      "loss: 1167.006470 [  290/ 1682]\n",
      "loss: 774.595642 [  300/ 1682]\n",
      "loss: 866.338562 [  310/ 1682]\n",
      "loss: 869.674988 [  320/ 1682]\n",
      "loss: 752.597351 [  330/ 1682]\n",
      "loss: 925.220520 [  340/ 1682]\n",
      "loss: 937.197571 [  350/ 1682]\n",
      "loss: 815.553101 [  360/ 1682]\n",
      "loss: 1048.784058 [  370/ 1682]\n",
      "loss: 940.889160 [  380/ 1682]\n",
      "loss: 985.403320 [  390/ 1682]\n",
      "loss: 727.255554 [  400/ 1682]\n",
      "loss: 682.033203 [  410/ 1682]\n",
      "loss: 914.081360 [  420/ 1682]\n",
      "loss: 878.616028 [  430/ 1682]\n",
      "loss: 662.833618 [  440/ 1682]\n",
      "loss: 578.113953 [  450/ 1682]\n",
      "loss: 539.213867 [  460/ 1682]\n",
      "loss: 477.457123 [  470/ 1682]\n",
      "loss: 766.158203 [  480/ 1682]\n",
      "loss: 400.941101 [  490/ 1682]\n",
      "loss: 493.671967 [  500/ 1682]\n",
      "loss: 577.541626 [  510/ 1682]\n",
      "loss: 651.963623 [  520/ 1682]\n",
      "loss: 576.451721 [  530/ 1682]\n",
      "loss: 541.277893 [  540/ 1682]\n",
      "loss: 490.460541 [  550/ 1682]\n",
      "loss: 515.321777 [  560/ 1682]\n",
      "loss: 662.682861 [  570/ 1682]\n",
      "loss: 505.913391 [  580/ 1682]\n",
      "loss: 379.546326 [  590/ 1682]\n",
      "loss: 284.091400 [  600/ 1682]\n",
      "loss: 482.140198 [  610/ 1682]\n",
      "loss: 316.192078 [  620/ 1682]\n",
      "loss: 326.092194 [  630/ 1682]\n",
      "loss: 202.515121 [  640/ 1682]\n",
      "loss: 169.134674 [  650/ 1682]\n",
      "loss: 122.301590 [  660/ 1682]\n",
      "loss: 162.154221 [  670/ 1682]\n",
      "loss: 224.373871 [  680/ 1682]\n",
      "loss: 65.414604 [  690/ 1682]\n",
      "loss: 111.518639 [  700/ 1682]\n",
      "loss: 268.216553 [  710/ 1682]\n",
      "loss: 481.212494 [  720/ 1682]\n",
      "loss: 467.250183 [  730/ 1682]\n",
      "loss: 762.009094 [  740/ 1682]\n",
      "loss: 789.280640 [  750/ 1682]\n",
      "loss: 584.647217 [  760/ 1682]\n",
      "loss: 430.042816 [  770/ 1682]\n",
      "loss: 457.076721 [  780/ 1682]\n",
      "loss: 408.216339 [  790/ 1682]\n",
      "loss: 260.475891 [  800/ 1682]\n",
      "loss: 240.727463 [  810/ 1682]\n",
      "loss: 236.280640 [  820/ 1682]\n",
      "loss: 186.409454 [  830/ 1682]\n",
      "loss: 263.424774 [  840/ 1682]\n",
      "loss: 243.772064 [  850/ 1682]\n",
      "loss: 200.373596 [  860/ 1682]\n",
      "loss: 173.646515 [  870/ 1682]\n",
      "loss: 242.534912 [  880/ 1682]\n",
      "loss: 131.163147 [  890/ 1682]\n",
      "loss: 134.890198 [  900/ 1682]\n",
      "loss: 194.678375 [  910/ 1682]\n",
      "loss: 100.448593 [  920/ 1682]\n",
      "loss: 66.636269 [  930/ 1682]\n",
      "loss: 50.444603 [  940/ 1682]\n",
      "loss: 32.930840 [  950/ 1682]\n",
      "loss: 26.804993 [  960/ 1682]\n",
      "loss: 14.287331 [  970/ 1682]\n",
      "loss: 34.353756 [  980/ 1682]\n",
      "loss: 90.135139 [  990/ 1682]\n",
      "loss: 158.766251 [ 1000/ 1682]\n",
      "loss: 205.977829 [ 1010/ 1682]\n",
      "loss: 226.216965 [ 1020/ 1682]\n",
      "loss: 149.879608 [ 1030/ 1682]\n",
      "loss: 60.687408 [ 1040/ 1682]\n",
      "loss: 30.931494 [ 1050/ 1682]\n",
      "loss: 52.585316 [ 1060/ 1682]\n",
      "loss: 58.107410 [ 1070/ 1682]\n",
      "loss: 143.328125 [ 1080/ 1682]\n",
      "loss: 204.961945 [ 1090/ 1682]\n",
      "loss: 367.433594 [ 1100/ 1682]\n",
      "loss: 604.402832 [ 1110/ 1682]\n",
      "loss: 868.976685 [ 1120/ 1682]\n",
      "loss: 1136.700928 [ 1130/ 1682]\n",
      "loss: 1705.859375 [ 1140/ 1682]\n",
      "loss: 2396.195801 [ 1150/ 1682]\n",
      "loss: 3802.376953 [ 1160/ 1682]\n",
      "loss: 2513.969238 [ 1170/ 1682]\n",
      "loss: 2467.482666 [ 1180/ 1682]\n",
      "loss: 2973.516113 [ 1190/ 1682]\n",
      "loss: 2295.991699 [ 1200/ 1682]\n",
      "loss: 2698.978516 [ 1210/ 1682]\n",
      "loss: 2957.913086 [ 1220/ 1682]\n",
      "loss: 3455.610840 [ 1230/ 1682]\n",
      "loss: 4292.495605 [ 1240/ 1682]\n",
      "loss: 4243.197754 [ 1250/ 1682]\n",
      "loss: 5193.253418 [ 1260/ 1682]\n",
      "loss: 5155.749023 [ 1270/ 1682]\n",
      "loss: 4169.666992 [ 1280/ 1682]\n",
      "loss: 3144.746826 [ 1290/ 1682]\n",
      "loss: 3289.041504 [ 1300/ 1682]\n",
      "loss: 3875.044922 [ 1310/ 1682]\n",
      "loss: 4778.296875 [ 1320/ 1682]\n",
      "loss: 4265.829590 [ 1330/ 1682]\n",
      "loss: 3891.205078 [ 1340/ 1682]\n",
      "loss: 3745.319580 [ 1350/ 1682]\n",
      "loss: 4478.805176 [ 1360/ 1682]\n",
      "loss: 5435.300293 [ 1370/ 1682]\n",
      "loss: 6759.146973 [ 1380/ 1682]\n",
      "loss: 6890.300781 [ 1390/ 1682]\n",
      "loss: 6889.850098 [ 1400/ 1682]\n",
      "loss: 7665.752441 [ 1410/ 1682]\n",
      "loss: 7447.790527 [ 1420/ 1682]\n",
      "loss: 6305.626465 [ 1430/ 1682]\n",
      "loss: 6191.640625 [ 1440/ 1682]\n",
      "loss: 7281.950195 [ 1450/ 1682]\n",
      "loss: 7038.087402 [ 1460/ 1682]\n",
      "loss: 8806.494141 [ 1470/ 1682]\n",
      "loss: 10457.624023 [ 1480/ 1682]\n",
      "loss: 12647.756836 [ 1490/ 1682]\n",
      "loss: 12083.239258 [ 1500/ 1682]\n",
      "loss: 10087.958984 [ 1510/ 1682]\n",
      "loss: 11503.431641 [ 1520/ 1682]\n",
      "loss: 10030.447266 [ 1530/ 1682]\n",
      "loss: 9794.099609 [ 1540/ 1682]\n",
      "loss: 9445.015625 [ 1550/ 1682]\n",
      "loss: 12727.436523 [ 1560/ 1682]\n",
      "loss: 9701.283203 [ 1570/ 1682]\n",
      "loss: 9174.542969 [ 1580/ 1682]\n",
      "loss: 6941.941406 [ 1590/ 1682]\n",
      "loss: 6526.505371 [ 1600/ 1682]\n",
      "loss: 4864.612305 [ 1610/ 1682]\n",
      "loss: 5510.819336 [ 1620/ 1682]\n",
      "loss: 7069.454102 [ 1630/ 1682]\n",
      "loss: 8560.755859 [ 1640/ 1682]\n",
      "loss: 10468.712891 [ 1650/ 1682]\n",
      "loss: 9834.511719 [ 1660/ 1682]\n",
      "loss: 7966.416504 [ 1670/ 1682]\n",
      "loss: 8136.954102 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9748.796612 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 2172.292480 [    0/ 1682]\n",
      "loss: 2067.885986 [   10/ 1682]\n",
      "loss: 2235.726807 [   20/ 1682]\n",
      "loss: 1668.460571 [   30/ 1682]\n",
      "loss: 1749.142944 [   40/ 1682]\n",
      "loss: 2020.207275 [   50/ 1682]\n",
      "loss: 1474.185547 [   60/ 1682]\n",
      "loss: 1743.760986 [   70/ 1682]\n",
      "loss: 1952.973022 [   80/ 1682]\n",
      "loss: 2057.857910 [   90/ 1682]\n",
      "loss: 1680.308838 [  100/ 1682]\n",
      "loss: 2388.115234 [  110/ 1682]\n",
      "loss: 2059.480957 [  120/ 1682]\n",
      "loss: 1660.611938 [  130/ 1682]\n",
      "loss: 1661.674072 [  140/ 1682]\n",
      "loss: 1589.430420 [  150/ 1682]\n",
      "loss: 1659.974365 [  160/ 1682]\n",
      "loss: 1281.702393 [  170/ 1682]\n",
      "loss: 1355.760376 [  180/ 1682]\n",
      "loss: 1311.919922 [  190/ 1682]\n",
      "loss: 1638.315430 [  200/ 1682]\n",
      "loss: 1446.198975 [  210/ 1682]\n",
      "loss: 1839.642334 [  220/ 1682]\n",
      "loss: 1638.644531 [  230/ 1682]\n",
      "loss: 1550.333008 [  240/ 1682]\n",
      "loss: 1405.912598 [  250/ 1682]\n",
      "loss: 1210.136841 [  260/ 1682]\n",
      "loss: 1165.067749 [  270/ 1682]\n",
      "loss: 1071.389038 [  280/ 1682]\n",
      "loss: 936.390930 [  290/ 1682]\n",
      "loss: 796.304199 [  300/ 1682]\n",
      "loss: 983.865051 [  310/ 1682]\n",
      "loss: 982.283691 [  320/ 1682]\n",
      "loss: 959.202332 [  330/ 1682]\n",
      "loss: 913.998840 [  340/ 1682]\n",
      "loss: 748.370667 [  350/ 1682]\n",
      "loss: 826.061646 [  360/ 1682]\n",
      "loss: 1031.663696 [  370/ 1682]\n",
      "loss: 746.819458 [  380/ 1682]\n",
      "loss: 713.784912 [  390/ 1682]\n",
      "loss: 811.536621 [  400/ 1682]\n",
      "loss: 851.648926 [  410/ 1682]\n",
      "loss: 823.151855 [  420/ 1682]\n",
      "loss: 951.704285 [  430/ 1682]\n",
      "loss: 752.010803 [  440/ 1682]\n",
      "loss: 518.310181 [  450/ 1682]\n",
      "loss: 546.094116 [  460/ 1682]\n",
      "loss: 556.787292 [  470/ 1682]\n",
      "loss: 482.582672 [  480/ 1682]\n",
      "loss: 548.421021 [  490/ 1682]\n",
      "loss: 497.482513 [  500/ 1682]\n",
      "loss: 517.051392 [  510/ 1682]\n",
      "loss: 724.716675 [  520/ 1682]\n",
      "loss: 690.458130 [  530/ 1682]\n",
      "loss: 535.513306 [  540/ 1682]\n",
      "loss: 571.807007 [  550/ 1682]\n",
      "loss: 523.549805 [  560/ 1682]\n",
      "loss: 580.336670 [  570/ 1682]\n",
      "loss: 509.250427 [  580/ 1682]\n",
      "loss: 436.670654 [  590/ 1682]\n",
      "loss: 285.191345 [  600/ 1682]\n",
      "loss: 420.966217 [  610/ 1682]\n",
      "loss: 367.782501 [  620/ 1682]\n",
      "loss: 283.373352 [  630/ 1682]\n",
      "loss: 283.423126 [  640/ 1682]\n",
      "loss: 233.208694 [  650/ 1682]\n",
      "loss: 121.531387 [  660/ 1682]\n",
      "loss: 184.484467 [  670/ 1682]\n",
      "loss: 137.534088 [  680/ 1682]\n",
      "loss: 160.874420 [  690/ 1682]\n",
      "loss: 144.300858 [  700/ 1682]\n",
      "loss: 273.701019 [  710/ 1682]\n",
      "loss: 433.245850 [  720/ 1682]\n",
      "loss: 522.268311 [  730/ 1682]\n",
      "loss: 816.566101 [  740/ 1682]\n",
      "loss: 665.038391 [  750/ 1682]\n",
      "loss: 706.462036 [  760/ 1682]\n",
      "loss: 534.926880 [  770/ 1682]\n",
      "loss: 410.618408 [  780/ 1682]\n",
      "loss: 507.307281 [  790/ 1682]\n",
      "loss: 230.694382 [  800/ 1682]\n",
      "loss: 281.275574 [  810/ 1682]\n",
      "loss: 168.744568 [  820/ 1682]\n",
      "loss: 218.076828 [  830/ 1682]\n",
      "loss: 393.632172 [  840/ 1682]\n",
      "loss: 295.905762 [  850/ 1682]\n",
      "loss: 206.646683 [  860/ 1682]\n",
      "loss: 240.957764 [  870/ 1682]\n",
      "loss: 152.818024 [  880/ 1682]\n",
      "loss: 228.223480 [  890/ 1682]\n",
      "loss: 170.107422 [  900/ 1682]\n",
      "loss: 182.016937 [  910/ 1682]\n",
      "loss: 77.787460 [  920/ 1682]\n",
      "loss: 88.834167 [  930/ 1682]\n",
      "loss: 64.386703 [  940/ 1682]\n",
      "loss: 23.160385 [  950/ 1682]\n",
      "loss: 14.498227 [  960/ 1682]\n",
      "loss: 15.083574 [  970/ 1682]\n",
      "loss: 35.261227 [  980/ 1682]\n",
      "loss: 81.639664 [  990/ 1682]\n",
      "loss: 188.410126 [ 1000/ 1682]\n",
      "loss: 242.439545 [ 1010/ 1682]\n",
      "loss: 291.258240 [ 1020/ 1682]\n",
      "loss: 132.955322 [ 1030/ 1682]\n",
      "loss: 45.146278 [ 1040/ 1682]\n",
      "loss: 7.671204 [ 1050/ 1682]\n",
      "loss: 25.454275 [ 1060/ 1682]\n",
      "loss: 50.236427 [ 1070/ 1682]\n",
      "loss: 174.133957 [ 1080/ 1682]\n",
      "loss: 266.438293 [ 1090/ 1682]\n",
      "loss: 391.410797 [ 1100/ 1682]\n",
      "loss: 593.605347 [ 1110/ 1682]\n",
      "loss: 847.926941 [ 1120/ 1682]\n",
      "loss: 1059.231689 [ 1130/ 1682]\n",
      "loss: 1681.563110 [ 1140/ 1682]\n",
      "loss: 2506.750488 [ 1150/ 1682]\n",
      "loss: 3785.822754 [ 1160/ 1682]\n",
      "loss: 2334.681641 [ 1170/ 1682]\n",
      "loss: 2515.239746 [ 1180/ 1682]\n",
      "loss: 2965.382324 [ 1190/ 1682]\n",
      "loss: 2433.049316 [ 1200/ 1682]\n",
      "loss: 2880.652344 [ 1210/ 1682]\n",
      "loss: 2766.109375 [ 1220/ 1682]\n",
      "loss: 3350.544434 [ 1230/ 1682]\n",
      "loss: 4396.430664 [ 1240/ 1682]\n",
      "loss: 4328.900879 [ 1250/ 1682]\n",
      "loss: 5070.379883 [ 1260/ 1682]\n",
      "loss: 4868.403320 [ 1270/ 1682]\n",
      "loss: 3895.525391 [ 1280/ 1682]\n",
      "loss: 3232.510254 [ 1290/ 1682]\n",
      "loss: 3274.692139 [ 1300/ 1682]\n",
      "loss: 4004.262939 [ 1310/ 1682]\n",
      "loss: 4636.154785 [ 1320/ 1682]\n",
      "loss: 3998.009033 [ 1330/ 1682]\n",
      "loss: 3860.387207 [ 1340/ 1682]\n",
      "loss: 3726.948730 [ 1350/ 1682]\n",
      "loss: 4202.362305 [ 1360/ 1682]\n",
      "loss: 5553.630371 [ 1370/ 1682]\n",
      "loss: 6234.066406 [ 1380/ 1682]\n",
      "loss: 6682.403809 [ 1390/ 1682]\n",
      "loss: 7187.506348 [ 1400/ 1682]\n",
      "loss: 7428.133789 [ 1410/ 1682]\n",
      "loss: 7600.770508 [ 1420/ 1682]\n",
      "loss: 6103.119629 [ 1430/ 1682]\n",
      "loss: 5986.343750 [ 1440/ 1682]\n",
      "loss: 7043.687500 [ 1450/ 1682]\n",
      "loss: 7208.118652 [ 1460/ 1682]\n",
      "loss: 8344.453125 [ 1470/ 1682]\n",
      "loss: 10704.503906 [ 1480/ 1682]\n",
      "loss: 11801.802734 [ 1490/ 1682]\n",
      "loss: 12062.774414 [ 1500/ 1682]\n",
      "loss: 10740.585938 [ 1510/ 1682]\n",
      "loss: 10936.081055 [ 1520/ 1682]\n",
      "loss: 10269.783203 [ 1530/ 1682]\n",
      "loss: 8991.341797 [ 1540/ 1682]\n",
      "loss: 9935.327148 [ 1550/ 1682]\n",
      "loss: 11793.528320 [ 1560/ 1682]\n",
      "loss: 10196.748047 [ 1570/ 1682]\n",
      "loss: 9375.388672 [ 1580/ 1682]\n",
      "loss: 6247.654297 [ 1590/ 1682]\n",
      "loss: 6941.571289 [ 1600/ 1682]\n",
      "loss: 5414.494141 [ 1610/ 1682]\n",
      "loss: 5479.899414 [ 1620/ 1682]\n",
      "loss: 7002.635254 [ 1630/ 1682]\n",
      "loss: 8768.195312 [ 1640/ 1682]\n",
      "loss: 11055.100586 [ 1650/ 1682]\n",
      "loss: 10076.517578 [ 1660/ 1682]\n",
      "loss: 8217.840820 [ 1670/ 1682]\n",
      "loss: 8068.256836 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9666.974365 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 2015.266968 [    0/ 1682]\n",
      "loss: 2068.676514 [   10/ 1682]\n",
      "loss: 2073.931152 [   20/ 1682]\n",
      "loss: 1673.220093 [   30/ 1682]\n",
      "loss: 1919.825928 [   40/ 1682]\n",
      "loss: 1839.515625 [   50/ 1682]\n",
      "loss: 1496.218506 [   60/ 1682]\n",
      "loss: 1627.427734 [   70/ 1682]\n",
      "loss: 2139.703369 [   80/ 1682]\n",
      "loss: 1729.307617 [   90/ 1682]\n",
      "loss: 1861.213623 [  100/ 1682]\n",
      "loss: 2048.533691 [  110/ 1682]\n",
      "loss: 1567.032837 [  120/ 1682]\n",
      "loss: 1675.973389 [  130/ 1682]\n",
      "loss: 1976.636475 [  140/ 1682]\n",
      "loss: 1602.355713 [  150/ 1682]\n",
      "loss: 1374.569336 [  160/ 1682]\n",
      "loss: 1743.941772 [  170/ 1682]\n",
      "loss: 1784.714844 [  180/ 1682]\n",
      "loss: 1332.273682 [  190/ 1682]\n",
      "loss: 1382.599243 [  200/ 1682]\n",
      "loss: 1741.810913 [  210/ 1682]\n",
      "loss: 1687.809814 [  220/ 1682]\n",
      "loss: 1502.204956 [  230/ 1682]\n",
      "loss: 1433.373535 [  240/ 1682]\n",
      "loss: 1422.684937 [  250/ 1682]\n",
      "loss: 1343.736084 [  260/ 1682]\n",
      "loss: 1062.114624 [  270/ 1682]\n",
      "loss: 983.717468 [  280/ 1682]\n",
      "loss: 1063.863281 [  290/ 1682]\n",
      "loss: 1017.062317 [  300/ 1682]\n",
      "loss: 990.250671 [  310/ 1682]\n",
      "loss: 992.784973 [  320/ 1682]\n",
      "loss: 1053.397583 [  330/ 1682]\n",
      "loss: 831.208801 [  340/ 1682]\n",
      "loss: 846.816040 [  350/ 1682]\n",
      "loss: 937.842468 [  360/ 1682]\n",
      "loss: 863.001343 [  370/ 1682]\n",
      "loss: 852.225952 [  380/ 1682]\n",
      "loss: 723.683289 [  390/ 1682]\n",
      "loss: 734.908386 [  400/ 1682]\n",
      "loss: 851.017761 [  410/ 1682]\n",
      "loss: 676.276001 [  420/ 1682]\n",
      "loss: 948.127808 [  430/ 1682]\n",
      "loss: 686.874939 [  440/ 1682]\n",
      "loss: 610.937744 [  450/ 1682]\n",
      "loss: 616.740845 [  460/ 1682]\n",
      "loss: 628.566772 [  470/ 1682]\n",
      "loss: 496.026428 [  480/ 1682]\n",
      "loss: 623.978394 [  490/ 1682]\n",
      "loss: 512.453003 [  500/ 1682]\n",
      "loss: 646.655212 [  510/ 1682]\n",
      "loss: 662.140259 [  520/ 1682]\n",
      "loss: 458.970520 [  530/ 1682]\n",
      "loss: 426.643707 [  540/ 1682]\n",
      "loss: 699.556580 [  550/ 1682]\n",
      "loss: 474.358398 [  560/ 1682]\n",
      "loss: 535.915649 [  570/ 1682]\n",
      "loss: 365.505432 [  580/ 1682]\n",
      "loss: 290.284668 [  590/ 1682]\n",
      "loss: 297.641357 [  600/ 1682]\n",
      "loss: 480.425781 [  610/ 1682]\n",
      "loss: 424.954529 [  620/ 1682]\n",
      "loss: 337.869720 [  630/ 1682]\n",
      "loss: 301.412354 [  640/ 1682]\n",
      "loss: 142.728745 [  650/ 1682]\n",
      "loss: 123.724854 [  660/ 1682]\n",
      "loss: 132.973175 [  670/ 1682]\n",
      "loss: 137.946289 [  680/ 1682]\n",
      "loss: 138.211411 [  690/ 1682]\n",
      "loss: 124.257912 [  700/ 1682]\n",
      "loss: 234.549591 [  710/ 1682]\n",
      "loss: 396.828827 [  720/ 1682]\n",
      "loss: 486.410980 [  730/ 1682]\n",
      "loss: 657.502747 [  740/ 1682]\n",
      "loss: 745.046204 [  750/ 1682]\n",
      "loss: 672.660767 [  760/ 1682]\n",
      "loss: 500.528870 [  770/ 1682]\n",
      "loss: 473.460358 [  780/ 1682]\n",
      "loss: 371.762115 [  790/ 1682]\n",
      "loss: 280.065674 [  800/ 1682]\n",
      "loss: 252.211426 [  810/ 1682]\n",
      "loss: 177.388397 [  820/ 1682]\n",
      "loss: 169.716354 [  830/ 1682]\n",
      "loss: 326.005768 [  840/ 1682]\n",
      "loss: 390.028351 [  850/ 1682]\n",
      "loss: 183.852798 [  860/ 1682]\n",
      "loss: 251.213959 [  870/ 1682]\n",
      "loss: 192.096405 [  880/ 1682]\n",
      "loss: 179.530533 [  890/ 1682]\n",
      "loss: 152.456757 [  900/ 1682]\n",
      "loss: 174.119171 [  910/ 1682]\n",
      "loss: 129.116196 [  920/ 1682]\n",
      "loss: 100.513100 [  930/ 1682]\n",
      "loss: 73.115372 [  940/ 1682]\n",
      "loss: 46.748890 [  950/ 1682]\n",
      "loss: 19.688641 [  960/ 1682]\n",
      "loss: 17.803572 [  970/ 1682]\n",
      "loss: 31.545034 [  980/ 1682]\n",
      "loss: 63.647694 [  990/ 1682]\n",
      "loss: 198.778625 [ 1000/ 1682]\n",
      "loss: 230.999756 [ 1010/ 1682]\n",
      "loss: 277.770111 [ 1020/ 1682]\n",
      "loss: 121.752670 [ 1030/ 1682]\n",
      "loss: 58.969269 [ 1040/ 1682]\n",
      "loss: 43.308540 [ 1050/ 1682]\n",
      "loss: 27.609964 [ 1060/ 1682]\n",
      "loss: 36.256535 [ 1070/ 1682]\n",
      "loss: 164.341400 [ 1080/ 1682]\n",
      "loss: 217.087555 [ 1090/ 1682]\n",
      "loss: 342.832428 [ 1100/ 1682]\n",
      "loss: 533.836060 [ 1110/ 1682]\n",
      "loss: 742.756958 [ 1120/ 1682]\n",
      "loss: 1034.690796 [ 1130/ 1682]\n",
      "loss: 1579.213623 [ 1140/ 1682]\n",
      "loss: 2630.732910 [ 1150/ 1682]\n",
      "loss: 3439.934326 [ 1160/ 1682]\n",
      "loss: 2355.971436 [ 1170/ 1682]\n",
      "loss: 2392.968262 [ 1180/ 1682]\n",
      "loss: 2912.967529 [ 1190/ 1682]\n",
      "loss: 2483.604004 [ 1200/ 1682]\n",
      "loss: 2827.600342 [ 1210/ 1682]\n",
      "loss: 2793.230957 [ 1220/ 1682]\n",
      "loss: 3288.462158 [ 1230/ 1682]\n",
      "loss: 4119.250977 [ 1240/ 1682]\n",
      "loss: 3836.236328 [ 1250/ 1682]\n",
      "loss: 4970.476562 [ 1260/ 1682]\n",
      "loss: 4939.727539 [ 1270/ 1682]\n",
      "loss: 3735.691895 [ 1280/ 1682]\n",
      "loss: 3192.886719 [ 1290/ 1682]\n",
      "loss: 3124.563232 [ 1300/ 1682]\n",
      "loss: 3903.064941 [ 1310/ 1682]\n",
      "loss: 4305.489258 [ 1320/ 1682]\n",
      "loss: 4184.655762 [ 1330/ 1682]\n",
      "loss: 3454.768066 [ 1340/ 1682]\n",
      "loss: 3782.488281 [ 1350/ 1682]\n",
      "loss: 4261.787598 [ 1360/ 1682]\n",
      "loss: 5173.642090 [ 1370/ 1682]\n",
      "loss: 6479.122070 [ 1380/ 1682]\n",
      "loss: 6260.068848 [ 1390/ 1682]\n",
      "loss: 7123.721680 [ 1400/ 1682]\n",
      "loss: 7347.459961 [ 1410/ 1682]\n",
      "loss: 7514.848633 [ 1420/ 1682]\n",
      "loss: 6187.766602 [ 1430/ 1682]\n",
      "loss: 6081.829102 [ 1440/ 1682]\n",
      "loss: 6971.768555 [ 1450/ 1682]\n",
      "loss: 7517.835938 [ 1460/ 1682]\n",
      "loss: 8645.668945 [ 1470/ 1682]\n",
      "loss: 10568.635742 [ 1480/ 1682]\n",
      "loss: 12477.772461 [ 1490/ 1682]\n",
      "loss: 12191.176758 [ 1500/ 1682]\n",
      "loss: 9918.335938 [ 1510/ 1682]\n",
      "loss: 11347.181641 [ 1520/ 1682]\n",
      "loss: 10159.537109 [ 1530/ 1682]\n",
      "loss: 9437.147461 [ 1540/ 1682]\n",
      "loss: 9119.206055 [ 1550/ 1682]\n",
      "loss: 12261.753906 [ 1560/ 1682]\n",
      "loss: 10392.849609 [ 1570/ 1682]\n",
      "loss: 8790.521484 [ 1580/ 1682]\n",
      "loss: 6383.528809 [ 1590/ 1682]\n",
      "loss: 6659.472656 [ 1600/ 1682]\n",
      "loss: 5386.444336 [ 1610/ 1682]\n",
      "loss: 5622.178711 [ 1620/ 1682]\n",
      "loss: 6948.290039 [ 1630/ 1682]\n",
      "loss: 8146.193848 [ 1640/ 1682]\n",
      "loss: 10072.291016 [ 1650/ 1682]\n",
      "loss: 10072.498047 [ 1660/ 1682]\n",
      "loss: 8379.798828 [ 1670/ 1682]\n",
      "loss: 8000.792969 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9563.903687 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 2390.265869 [    0/ 1682]\n",
      "loss: 1778.418213 [   10/ 1682]\n",
      "loss: 1617.854370 [   20/ 1682]\n",
      "loss: 1555.081177 [   30/ 1682]\n",
      "loss: 1806.882202 [   40/ 1682]\n",
      "loss: 2050.968262 [   50/ 1682]\n",
      "loss: 1516.061768 [   60/ 1682]\n",
      "loss: 1966.235107 [   70/ 1682]\n",
      "loss: 2188.806152 [   80/ 1682]\n",
      "loss: 1935.035156 [   90/ 1682]\n",
      "loss: 2043.873657 [  100/ 1682]\n",
      "loss: 2107.707275 [  110/ 1682]\n",
      "loss: 1920.847900 [  120/ 1682]\n",
      "loss: 1879.245117 [  130/ 1682]\n",
      "loss: 1704.114624 [  140/ 1682]\n",
      "loss: 1638.489990 [  150/ 1682]\n",
      "loss: 1703.326172 [  160/ 1682]\n",
      "loss: 1640.402954 [  170/ 1682]\n",
      "loss: 1550.114014 [  180/ 1682]\n",
      "loss: 1491.228760 [  190/ 1682]\n",
      "loss: 1554.110596 [  200/ 1682]\n",
      "loss: 1498.137451 [  210/ 1682]\n",
      "loss: 1459.586914 [  220/ 1682]\n",
      "loss: 1395.245361 [  230/ 1682]\n",
      "loss: 1466.833862 [  240/ 1682]\n",
      "loss: 1838.287476 [  250/ 1682]\n",
      "loss: 1634.337280 [  260/ 1682]\n",
      "loss: 1090.095093 [  270/ 1682]\n",
      "loss: 1338.340454 [  280/ 1682]\n",
      "loss: 1087.577148 [  290/ 1682]\n",
      "loss: 1154.581299 [  300/ 1682]\n",
      "loss: 1120.721558 [  310/ 1682]\n",
      "loss: 1012.891907 [  320/ 1682]\n",
      "loss: 897.536316 [  330/ 1682]\n",
      "loss: 757.182739 [  340/ 1682]\n",
      "loss: 1050.421631 [  350/ 1682]\n",
      "loss: 866.646179 [  360/ 1682]\n",
      "loss: 1061.581909 [  370/ 1682]\n",
      "loss: 957.905945 [  380/ 1682]\n",
      "loss: 824.988098 [  390/ 1682]\n",
      "loss: 752.565186 [  400/ 1682]\n",
      "loss: 717.860962 [  410/ 1682]\n",
      "loss: 848.546753 [  420/ 1682]\n",
      "loss: 733.174744 [  430/ 1682]\n",
      "loss: 858.818054 [  440/ 1682]\n",
      "loss: 689.781128 [  450/ 1682]\n",
      "loss: 498.733398 [  460/ 1682]\n",
      "loss: 513.076294 [  470/ 1682]\n",
      "loss: 577.060181 [  480/ 1682]\n",
      "loss: 635.384277 [  490/ 1682]\n",
      "loss: 524.782227 [  500/ 1682]\n",
      "loss: 602.675415 [  510/ 1682]\n",
      "loss: 675.855713 [  520/ 1682]\n",
      "loss: 528.443115 [  530/ 1682]\n",
      "loss: 609.977722 [  540/ 1682]\n",
      "loss: 590.922729 [  550/ 1682]\n",
      "loss: 544.364136 [  560/ 1682]\n",
      "loss: 604.412903 [  570/ 1682]\n",
      "loss: 470.230225 [  580/ 1682]\n",
      "loss: 400.141785 [  590/ 1682]\n",
      "loss: 534.097229 [  600/ 1682]\n",
      "loss: 490.407166 [  610/ 1682]\n",
      "loss: 342.189484 [  620/ 1682]\n",
      "loss: 386.284607 [  630/ 1682]\n",
      "loss: 226.033371 [  640/ 1682]\n",
      "loss: 180.500977 [  650/ 1682]\n",
      "loss: 125.209633 [  660/ 1682]\n",
      "loss: 135.778427 [  670/ 1682]\n",
      "loss: 115.763489 [  680/ 1682]\n",
      "loss: 220.314865 [  690/ 1682]\n",
      "loss: 160.973007 [  700/ 1682]\n",
      "loss: 234.968628 [  710/ 1682]\n",
      "loss: 545.095032 [  720/ 1682]\n",
      "loss: 550.585327 [  730/ 1682]\n",
      "loss: 839.438599 [  740/ 1682]\n",
      "loss: 809.691895 [  750/ 1682]\n",
      "loss: 786.349609 [  760/ 1682]\n",
      "loss: 465.113037 [  770/ 1682]\n",
      "loss: 480.344727 [  780/ 1682]\n",
      "loss: 511.806885 [  790/ 1682]\n",
      "loss: 290.978851 [  800/ 1682]\n",
      "loss: 226.949509 [  810/ 1682]\n",
      "loss: 211.571320 [  820/ 1682]\n",
      "loss: 298.709137 [  830/ 1682]\n",
      "loss: 414.165466 [  840/ 1682]\n",
      "loss: 458.035339 [  850/ 1682]\n",
      "loss: 314.421234 [  860/ 1682]\n",
      "loss: 166.720688 [  870/ 1682]\n",
      "loss: 215.389648 [  880/ 1682]\n",
      "loss: 240.107742 [  890/ 1682]\n",
      "loss: 232.340408 [  900/ 1682]\n",
      "loss: 171.043137 [  910/ 1682]\n",
      "loss: 103.116844 [  920/ 1682]\n",
      "loss: 99.512474 [  930/ 1682]\n",
      "loss: 38.710625 [  940/ 1682]\n",
      "loss: 24.290691 [  950/ 1682]\n",
      "loss: 17.485300 [  960/ 1682]\n",
      "loss: 10.378687 [  970/ 1682]\n",
      "loss: 25.822403 [  980/ 1682]\n",
      "loss: 80.655457 [  990/ 1682]\n",
      "loss: 162.821487 [ 1000/ 1682]\n",
      "loss: 224.888214 [ 1010/ 1682]\n",
      "loss: 267.876068 [ 1020/ 1682]\n",
      "loss: 116.496239 [ 1030/ 1682]\n",
      "loss: 42.082165 [ 1040/ 1682]\n",
      "loss: 32.616879 [ 1050/ 1682]\n",
      "loss: 31.486313 [ 1060/ 1682]\n",
      "loss: 41.565868 [ 1070/ 1682]\n",
      "loss: 133.223434 [ 1080/ 1682]\n",
      "loss: 160.210648 [ 1090/ 1682]\n",
      "loss: 342.235748 [ 1100/ 1682]\n",
      "loss: 565.198547 [ 1110/ 1682]\n",
      "loss: 778.886597 [ 1120/ 1682]\n",
      "loss: 885.790527 [ 1130/ 1682]\n",
      "loss: 1542.342285 [ 1140/ 1682]\n",
      "loss: 2519.638184 [ 1150/ 1682]\n",
      "loss: 3804.800293 [ 1160/ 1682]\n",
      "loss: 2272.941650 [ 1170/ 1682]\n",
      "loss: 2374.192871 [ 1180/ 1682]\n",
      "loss: 2713.964355 [ 1190/ 1682]\n",
      "loss: 2386.471924 [ 1200/ 1682]\n",
      "loss: 2419.920410 [ 1210/ 1682]\n",
      "loss: 2708.309814 [ 1220/ 1682]\n",
      "loss: 3455.193848 [ 1230/ 1682]\n",
      "loss: 4115.365234 [ 1240/ 1682]\n",
      "loss: 4247.991211 [ 1250/ 1682]\n",
      "loss: 4885.005859 [ 1260/ 1682]\n",
      "loss: 5020.686035 [ 1270/ 1682]\n",
      "loss: 3925.122559 [ 1280/ 1682]\n",
      "loss: 3172.141113 [ 1290/ 1682]\n",
      "loss: 3206.883301 [ 1300/ 1682]\n",
      "loss: 4007.016113 [ 1310/ 1682]\n",
      "loss: 4446.034668 [ 1320/ 1682]\n",
      "loss: 4281.456055 [ 1330/ 1682]\n",
      "loss: 3579.257812 [ 1340/ 1682]\n",
      "loss: 3542.207764 [ 1350/ 1682]\n",
      "loss: 4476.979492 [ 1360/ 1682]\n",
      "loss: 5202.251465 [ 1370/ 1682]\n",
      "loss: 6462.672363 [ 1380/ 1682]\n",
      "loss: 6437.510742 [ 1390/ 1682]\n",
      "loss: 6610.670410 [ 1400/ 1682]\n",
      "loss: 7310.450195 [ 1410/ 1682]\n",
      "loss: 7316.109375 [ 1420/ 1682]\n",
      "loss: 6481.026855 [ 1430/ 1682]\n",
      "loss: 5741.299316 [ 1440/ 1682]\n",
      "loss: 7123.536133 [ 1450/ 1682]\n",
      "loss: 7113.827148 [ 1460/ 1682]\n",
      "loss: 8443.755859 [ 1470/ 1682]\n",
      "loss: 10612.450195 [ 1480/ 1682]\n",
      "loss: 12397.559570 [ 1490/ 1682]\n",
      "loss: 11946.679688 [ 1500/ 1682]\n",
      "loss: 10363.060547 [ 1510/ 1682]\n",
      "loss: 11338.862305 [ 1520/ 1682]\n",
      "loss: 10448.166016 [ 1530/ 1682]\n",
      "loss: 9383.082031 [ 1540/ 1682]\n",
      "loss: 9358.498047 [ 1550/ 1682]\n",
      "loss: 11710.303711 [ 1560/ 1682]\n",
      "loss: 10346.029297 [ 1570/ 1682]\n",
      "loss: 8559.962891 [ 1580/ 1682]\n",
      "loss: 6181.384277 [ 1590/ 1682]\n",
      "loss: 5826.069336 [ 1600/ 1682]\n",
      "loss: 5027.551270 [ 1610/ 1682]\n",
      "loss: 5591.276367 [ 1620/ 1682]\n",
      "loss: 6280.774414 [ 1630/ 1682]\n",
      "loss: 7683.387695 [ 1640/ 1682]\n",
      "loss: 10644.473633 [ 1650/ 1682]\n",
      "loss: 10034.951172 [ 1660/ 1682]\n",
      "loss: 8617.610352 [ 1670/ 1682]\n",
      "loss: 7936.819336 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9710.205200 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1750.639893 [    0/ 1682]\n",
      "loss: 1941.982422 [   10/ 1682]\n",
      "loss: 1646.808960 [   20/ 1682]\n",
      "loss: 1740.818359 [   30/ 1682]\n",
      "loss: 2405.594727 [   40/ 1682]\n",
      "loss: 1579.117798 [   50/ 1682]\n",
      "loss: 1539.269165 [   60/ 1682]\n",
      "loss: 1824.192993 [   70/ 1682]\n",
      "loss: 2017.010986 [   80/ 1682]\n",
      "loss: 1941.505249 [   90/ 1682]\n",
      "loss: 1749.047852 [  100/ 1682]\n",
      "loss: 1792.239990 [  110/ 1682]\n",
      "loss: 1625.660889 [  120/ 1682]\n",
      "loss: 1731.761475 [  130/ 1682]\n",
      "loss: 1585.022705 [  140/ 1682]\n",
      "loss: 1656.745361 [  150/ 1682]\n",
      "loss: 1723.098389 [  160/ 1682]\n",
      "loss: 1505.083496 [  170/ 1682]\n",
      "loss: 1712.154541 [  180/ 1682]\n",
      "loss: 1653.267578 [  190/ 1682]\n",
      "loss: 1570.059814 [  200/ 1682]\n",
      "loss: 1660.483765 [  210/ 1682]\n",
      "loss: 1619.365967 [  220/ 1682]\n",
      "loss: 1695.619507 [  230/ 1682]\n",
      "loss: 1229.809814 [  240/ 1682]\n",
      "loss: 1349.196533 [  250/ 1682]\n",
      "loss: 1534.573486 [  260/ 1682]\n",
      "loss: 1578.469971 [  270/ 1682]\n",
      "loss: 1032.627319 [  280/ 1682]\n",
      "loss: 1003.337585 [  290/ 1682]\n",
      "loss: 963.597473 [  300/ 1682]\n",
      "loss: 1353.015259 [  310/ 1682]\n",
      "loss: 937.182007 [  320/ 1682]\n",
      "loss: 1196.796631 [  330/ 1682]\n",
      "loss: 1051.384521 [  340/ 1682]\n",
      "loss: 985.482605 [  350/ 1682]\n",
      "loss: 888.457214 [  360/ 1682]\n",
      "loss: 905.973999 [  370/ 1682]\n",
      "loss: 892.934875 [  380/ 1682]\n",
      "loss: 841.203796 [  390/ 1682]\n",
      "loss: 694.023743 [  400/ 1682]\n",
      "loss: 733.352844 [  410/ 1682]\n",
      "loss: 712.999451 [  420/ 1682]\n",
      "loss: 833.986145 [  430/ 1682]\n",
      "loss: 962.817688 [  440/ 1682]\n",
      "loss: 726.202209 [  450/ 1682]\n",
      "loss: 584.154419 [  460/ 1682]\n",
      "loss: 730.098083 [  470/ 1682]\n",
      "loss: 595.611694 [  480/ 1682]\n",
      "loss: 595.649048 [  490/ 1682]\n",
      "loss: 483.290588 [  500/ 1682]\n",
      "loss: 749.495605 [  510/ 1682]\n",
      "loss: 770.967773 [  520/ 1682]\n",
      "loss: 609.832886 [  530/ 1682]\n",
      "loss: 516.636353 [  540/ 1682]\n",
      "loss: 483.858093 [  550/ 1682]\n",
      "loss: 444.732727 [  560/ 1682]\n",
      "loss: 626.700378 [  570/ 1682]\n",
      "loss: 488.089539 [  580/ 1682]\n",
      "loss: 315.563477 [  590/ 1682]\n",
      "loss: 368.882507 [  600/ 1682]\n",
      "loss: 360.218414 [  610/ 1682]\n",
      "loss: 455.067139 [  620/ 1682]\n",
      "loss: 405.072327 [  630/ 1682]\n",
      "loss: 284.765381 [  640/ 1682]\n",
      "loss: 224.507645 [  650/ 1682]\n",
      "loss: 163.481903 [  660/ 1682]\n",
      "loss: 117.618629 [  670/ 1682]\n",
      "loss: 98.014061 [  680/ 1682]\n",
      "loss: 91.614761 [  690/ 1682]\n",
      "loss: 173.589417 [  700/ 1682]\n",
      "loss: 247.814331 [  710/ 1682]\n",
      "loss: 629.437195 [  720/ 1682]\n",
      "loss: 575.041565 [  730/ 1682]\n",
      "loss: 812.325195 [  740/ 1682]\n",
      "loss: 718.255066 [  750/ 1682]\n",
      "loss: 649.472839 [  760/ 1682]\n",
      "loss: 529.962463 [  770/ 1682]\n",
      "loss: 454.331451 [  780/ 1682]\n",
      "loss: 483.510559 [  790/ 1682]\n",
      "loss: 265.482239 [  800/ 1682]\n",
      "loss: 240.742950 [  810/ 1682]\n",
      "loss: 292.057068 [  820/ 1682]\n",
      "loss: 254.462280 [  830/ 1682]\n",
      "loss: 391.717957 [  840/ 1682]\n",
      "loss: 336.190613 [  850/ 1682]\n",
      "loss: 273.309387 [  860/ 1682]\n",
      "loss: 270.666656 [  870/ 1682]\n",
      "loss: 176.673706 [  880/ 1682]\n",
      "loss: 260.191559 [  890/ 1682]\n",
      "loss: 169.086182 [  900/ 1682]\n",
      "loss: 188.142715 [  910/ 1682]\n",
      "loss: 97.764938 [  920/ 1682]\n",
      "loss: 150.547623 [  930/ 1682]\n",
      "loss: 48.827534 [  940/ 1682]\n",
      "loss: 17.292519 [  950/ 1682]\n",
      "loss: 7.255640 [  960/ 1682]\n",
      "loss: 13.457100 [  970/ 1682]\n",
      "loss: 22.331171 [  980/ 1682]\n",
      "loss: 62.087868 [  990/ 1682]\n",
      "loss: 160.343903 [ 1000/ 1682]\n",
      "loss: 176.306870 [ 1010/ 1682]\n",
      "loss: 234.925858 [ 1020/ 1682]\n",
      "loss: 119.567421 [ 1030/ 1682]\n",
      "loss: 50.497406 [ 1040/ 1682]\n",
      "loss: 36.262108 [ 1050/ 1682]\n",
      "loss: 28.880783 [ 1060/ 1682]\n",
      "loss: 37.343258 [ 1070/ 1682]\n",
      "loss: 98.808617 [ 1080/ 1682]\n",
      "loss: 191.270798 [ 1090/ 1682]\n",
      "loss: 310.634094 [ 1100/ 1682]\n",
      "loss: 579.838989 [ 1110/ 1682]\n",
      "loss: 750.209351 [ 1120/ 1682]\n",
      "loss: 997.471069 [ 1130/ 1682]\n",
      "loss: 1638.825928 [ 1140/ 1682]\n",
      "loss: 2557.051514 [ 1150/ 1682]\n",
      "loss: 3736.776123 [ 1160/ 1682]\n",
      "loss: 2061.039795 [ 1170/ 1682]\n",
      "loss: 2256.289795 [ 1180/ 1682]\n",
      "loss: 2836.428223 [ 1190/ 1682]\n",
      "loss: 2428.572266 [ 1200/ 1682]\n",
      "loss: 2686.926514 [ 1210/ 1682]\n",
      "loss: 2630.870850 [ 1220/ 1682]\n",
      "loss: 3409.415283 [ 1230/ 1682]\n",
      "loss: 4037.423096 [ 1240/ 1682]\n",
      "loss: 4091.660889 [ 1250/ 1682]\n",
      "loss: 5130.200195 [ 1260/ 1682]\n",
      "loss: 4966.333496 [ 1270/ 1682]\n",
      "loss: 3670.839844 [ 1280/ 1682]\n",
      "loss: 3215.175049 [ 1290/ 1682]\n",
      "loss: 3165.425781 [ 1300/ 1682]\n",
      "loss: 3861.407715 [ 1310/ 1682]\n",
      "loss: 4495.010742 [ 1320/ 1682]\n",
      "loss: 3900.991455 [ 1330/ 1682]\n",
      "loss: 3401.762207 [ 1340/ 1682]\n",
      "loss: 3594.477295 [ 1350/ 1682]\n",
      "loss: 4318.215820 [ 1360/ 1682]\n",
      "loss: 5376.666504 [ 1370/ 1682]\n",
      "loss: 6544.235840 [ 1380/ 1682]\n",
      "loss: 6352.467773 [ 1390/ 1682]\n",
      "loss: 6697.335938 [ 1400/ 1682]\n",
      "loss: 7066.213379 [ 1410/ 1682]\n",
      "loss: 6897.956055 [ 1420/ 1682]\n",
      "loss: 6109.064453 [ 1430/ 1682]\n",
      "loss: 5655.389648 [ 1440/ 1682]\n",
      "loss: 7052.871094 [ 1450/ 1682]\n",
      "loss: 7041.982910 [ 1460/ 1682]\n",
      "loss: 8531.278320 [ 1470/ 1682]\n",
      "loss: 10528.700195 [ 1480/ 1682]\n",
      "loss: 11819.404297 [ 1490/ 1682]\n",
      "loss: 12097.597656 [ 1500/ 1682]\n",
      "loss: 9819.600586 [ 1510/ 1682]\n",
      "loss: 10731.207031 [ 1520/ 1682]\n",
      "loss: 9594.514648 [ 1530/ 1682]\n",
      "loss: 9761.518555 [ 1540/ 1682]\n",
      "loss: 9485.870117 [ 1550/ 1682]\n",
      "loss: 12397.404297 [ 1560/ 1682]\n",
      "loss: 9729.895508 [ 1570/ 1682]\n",
      "loss: 8405.360352 [ 1580/ 1682]\n",
      "loss: 6308.804688 [ 1590/ 1682]\n",
      "loss: 6550.127930 [ 1600/ 1682]\n",
      "loss: 4682.705078 [ 1610/ 1682]\n",
      "loss: 4912.293945 [ 1620/ 1682]\n",
      "loss: 6596.635254 [ 1630/ 1682]\n",
      "loss: 8299.519531 [ 1640/ 1682]\n",
      "loss: 10224.240234 [ 1650/ 1682]\n",
      "loss: 9920.421875 [ 1660/ 1682]\n",
      "loss: 8005.877930 [ 1670/ 1682]\n",
      "loss: 6502.216797 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9401.172683 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 2130.920410 [    0/ 1682]\n",
      "loss: 1991.009521 [   10/ 1682]\n",
      "loss: 2020.957642 [   20/ 1682]\n",
      "loss: 2295.159668 [   30/ 1682]\n",
      "loss: 1853.274658 [   40/ 1682]\n",
      "loss: 1966.207031 [   50/ 1682]\n",
      "loss: 1418.552979 [   60/ 1682]\n",
      "loss: 2064.204346 [   70/ 1682]\n",
      "loss: 1727.992920 [   80/ 1682]\n",
      "loss: 1829.847412 [   90/ 1682]\n",
      "loss: 1800.120728 [  100/ 1682]\n",
      "loss: 1840.457275 [  110/ 1682]\n",
      "loss: 2000.952759 [  120/ 1682]\n",
      "loss: 2098.186035 [  130/ 1682]\n",
      "loss: 1786.909912 [  140/ 1682]\n",
      "loss: 1551.378784 [  150/ 1682]\n",
      "loss: 1619.405518 [  160/ 1682]\n",
      "loss: 1879.941772 [  170/ 1682]\n",
      "loss: 1620.457764 [  180/ 1682]\n",
      "loss: 1270.661499 [  190/ 1682]\n",
      "loss: 1630.630615 [  200/ 1682]\n",
      "loss: 1408.900391 [  210/ 1682]\n",
      "loss: 1679.014038 [  220/ 1682]\n",
      "loss: 1605.209717 [  230/ 1682]\n",
      "loss: 1400.033936 [  240/ 1682]\n",
      "loss: 1666.333740 [  250/ 1682]\n",
      "loss: 1319.073975 [  260/ 1682]\n",
      "loss: 1145.605713 [  270/ 1682]\n",
      "loss: 1190.383911 [  280/ 1682]\n",
      "loss: 1159.590942 [  290/ 1682]\n",
      "loss: 1475.659302 [  300/ 1682]\n",
      "loss: 1089.893799 [  310/ 1682]\n",
      "loss: 972.406860 [  320/ 1682]\n",
      "loss: 964.865723 [  330/ 1682]\n",
      "loss: 912.859070 [  340/ 1682]\n",
      "loss: 832.257202 [  350/ 1682]\n",
      "loss: 1136.431274 [  360/ 1682]\n",
      "loss: 1148.613403 [  370/ 1682]\n",
      "loss: 1038.034912 [  380/ 1682]\n",
      "loss: 787.409546 [  390/ 1682]\n",
      "loss: 721.891235 [  400/ 1682]\n",
      "loss: 1041.917603 [  410/ 1682]\n",
      "loss: 741.974609 [  420/ 1682]\n",
      "loss: 1146.348633 [  430/ 1682]\n",
      "loss: 841.742371 [  440/ 1682]\n",
      "loss: 841.721680 [  450/ 1682]\n",
      "loss: 543.999878 [  460/ 1682]\n",
      "loss: 700.251526 [  470/ 1682]\n",
      "loss: 626.166626 [  480/ 1682]\n",
      "loss: 699.146606 [  490/ 1682]\n",
      "loss: 572.581238 [  500/ 1682]\n",
      "loss: 579.097046 [  510/ 1682]\n",
      "loss: 728.028503 [  520/ 1682]\n",
      "loss: 640.100342 [  530/ 1682]\n",
      "loss: 479.647705 [  540/ 1682]\n",
      "loss: 638.647339 [  550/ 1682]\n",
      "loss: 526.862915 [  560/ 1682]\n",
      "loss: 591.447693 [  570/ 1682]\n",
      "loss: 465.096527 [  580/ 1682]\n",
      "loss: 384.000946 [  590/ 1682]\n",
      "loss: 394.354736 [  600/ 1682]\n",
      "loss: 546.089355 [  610/ 1682]\n",
      "loss: 371.424164 [  620/ 1682]\n",
      "loss: 385.009766 [  630/ 1682]\n",
      "loss: 261.637482 [  640/ 1682]\n",
      "loss: 288.901611 [  650/ 1682]\n",
      "loss: 157.149994 [  660/ 1682]\n",
      "loss: 199.096878 [  670/ 1682]\n",
      "loss: 111.138550 [  680/ 1682]\n",
      "loss: 134.110992 [  690/ 1682]\n",
      "loss: 115.112793 [  700/ 1682]\n",
      "loss: 224.729416 [  710/ 1682]\n",
      "loss: 608.855286 [  720/ 1682]\n",
      "loss: 600.455688 [  730/ 1682]\n",
      "loss: 847.620239 [  740/ 1682]\n",
      "loss: 879.689331 [  750/ 1682]\n",
      "loss: 608.961792 [  760/ 1682]\n",
      "loss: 614.435059 [  770/ 1682]\n",
      "loss: 421.792542 [  780/ 1682]\n",
      "loss: 473.436096 [  790/ 1682]\n",
      "loss: 363.473572 [  800/ 1682]\n",
      "loss: 294.326355 [  810/ 1682]\n",
      "loss: 278.415558 [  820/ 1682]\n",
      "loss: 320.923889 [  830/ 1682]\n",
      "loss: 456.062347 [  840/ 1682]\n",
      "loss: 392.459045 [  850/ 1682]\n",
      "loss: 251.395874 [  860/ 1682]\n",
      "loss: 368.087189 [  870/ 1682]\n",
      "loss: 156.285217 [  880/ 1682]\n",
      "loss: 216.972778 [  890/ 1682]\n",
      "loss: 182.311996 [  900/ 1682]\n",
      "loss: 204.423187 [  910/ 1682]\n",
      "loss: 104.628761 [  920/ 1682]\n",
      "loss: 123.187378 [  930/ 1682]\n",
      "loss: 93.667725 [  940/ 1682]\n",
      "loss: 15.643007 [  950/ 1682]\n",
      "loss: 2.899663 [  960/ 1682]\n",
      "loss: 13.448621 [  970/ 1682]\n",
      "loss: 22.952219 [  980/ 1682]\n",
      "loss: 54.432808 [  990/ 1682]\n",
      "loss: 150.952454 [ 1000/ 1682]\n",
      "loss: 185.252808 [ 1010/ 1682]\n",
      "loss: 221.008453 [ 1020/ 1682]\n",
      "loss: 112.271790 [ 1030/ 1682]\n",
      "loss: 41.502506 [ 1040/ 1682]\n",
      "loss: 31.454874 [ 1050/ 1682]\n",
      "loss: 26.226898 [ 1060/ 1682]\n",
      "loss: 40.078197 [ 1070/ 1682]\n",
      "loss: 111.574753 [ 1080/ 1682]\n",
      "loss: 202.964142 [ 1090/ 1682]\n",
      "loss: 270.485046 [ 1100/ 1682]\n",
      "loss: 451.270020 [ 1110/ 1682]\n",
      "loss: 718.118774 [ 1120/ 1682]\n",
      "loss: 966.110718 [ 1130/ 1682]\n",
      "loss: 1492.611694 [ 1140/ 1682]\n",
      "loss: 2525.335205 [ 1150/ 1682]\n",
      "loss: 3599.911377 [ 1160/ 1682]\n",
      "loss: 2257.983154 [ 1170/ 1682]\n",
      "loss: 2286.648438 [ 1180/ 1682]\n",
      "loss: 2437.187744 [ 1190/ 1682]\n",
      "loss: 2304.630615 [ 1200/ 1682]\n",
      "loss: 2531.108154 [ 1210/ 1682]\n",
      "loss: 2670.707764 [ 1220/ 1682]\n",
      "loss: 3062.734863 [ 1230/ 1682]\n",
      "loss: 3962.134766 [ 1240/ 1682]\n",
      "loss: 4133.981934 [ 1250/ 1682]\n",
      "loss: 4800.207520 [ 1260/ 1682]\n",
      "loss: 4517.315430 [ 1270/ 1682]\n",
      "loss: 3825.577637 [ 1280/ 1682]\n",
      "loss: 3271.028320 [ 1290/ 1682]\n",
      "loss: 2898.138184 [ 1300/ 1682]\n",
      "loss: 4017.776611 [ 1310/ 1682]\n",
      "loss: 4544.380371 [ 1320/ 1682]\n",
      "loss: 3934.218750 [ 1330/ 1682]\n",
      "loss: 3432.121826 [ 1340/ 1682]\n",
      "loss: 3534.039795 [ 1350/ 1682]\n",
      "loss: 4012.610840 [ 1360/ 1682]\n",
      "loss: 5192.274414 [ 1370/ 1682]\n",
      "loss: 5973.611816 [ 1380/ 1682]\n",
      "loss: 6600.097656 [ 1390/ 1682]\n",
      "loss: 6774.269531 [ 1400/ 1682]\n",
      "loss: 6806.885742 [ 1410/ 1682]\n",
      "loss: 7140.772461 [ 1420/ 1682]\n",
      "loss: 5849.033691 [ 1430/ 1682]\n",
      "loss: 5567.342773 [ 1440/ 1682]\n",
      "loss: 6976.492188 [ 1450/ 1682]\n",
      "loss: 6350.729004 [ 1460/ 1682]\n",
      "loss: 8236.431641 [ 1470/ 1682]\n",
      "loss: 10611.240234 [ 1480/ 1682]\n",
      "loss: 12244.219727 [ 1490/ 1682]\n",
      "loss: 11672.295898 [ 1500/ 1682]\n",
      "loss: 9940.318359 [ 1510/ 1682]\n",
      "loss: 11086.015625 [ 1520/ 1682]\n",
      "loss: 10249.779297 [ 1530/ 1682]\n",
      "loss: 9428.931641 [ 1540/ 1682]\n",
      "loss: 9386.134766 [ 1550/ 1682]\n",
      "loss: 11714.657227 [ 1560/ 1682]\n",
      "loss: 9328.656250 [ 1570/ 1682]\n",
      "loss: 8845.076172 [ 1580/ 1682]\n",
      "loss: 6234.190430 [ 1590/ 1682]\n",
      "loss: 6490.050293 [ 1600/ 1682]\n",
      "loss: 4541.181641 [ 1610/ 1682]\n",
      "loss: 4379.852539 [ 1620/ 1682]\n",
      "loss: 6500.741211 [ 1630/ 1682]\n",
      "loss: 8237.690430 [ 1640/ 1682]\n",
      "loss: 10096.337891 [ 1650/ 1682]\n",
      "loss: 9815.593750 [ 1660/ 1682]\n",
      "loss: 8192.125000 [ 1670/ 1682]\n",
      "loss: 5001.519531 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9485.989098 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 2027.626953 [    0/ 1682]\n",
      "loss: 1865.554443 [   10/ 1682]\n",
      "loss: 2069.440430 [   20/ 1682]\n",
      "loss: 1832.261719 [   30/ 1682]\n",
      "loss: 1923.286743 [   40/ 1682]\n",
      "loss: 1662.291992 [   50/ 1682]\n",
      "loss: 1992.124634 [   60/ 1682]\n",
      "loss: 2144.086426 [   70/ 1682]\n",
      "loss: 1755.819580 [   80/ 1682]\n",
      "loss: 2088.686768 [   90/ 1682]\n",
      "loss: 1844.635010 [  100/ 1682]\n",
      "loss: 1885.384766 [  110/ 1682]\n",
      "loss: 2246.957520 [  120/ 1682]\n",
      "loss: 2008.673462 [  130/ 1682]\n",
      "loss: 1866.820068 [  140/ 1682]\n",
      "loss: 1995.359985 [  150/ 1682]\n",
      "loss: 1481.534790 [  160/ 1682]\n",
      "loss: 2135.395752 [  170/ 1682]\n",
      "loss: 1885.190186 [  180/ 1682]\n",
      "loss: 1472.618774 [  190/ 1682]\n",
      "loss: 1529.873291 [  200/ 1682]\n",
      "loss: 1786.197998 [  210/ 1682]\n",
      "loss: 1731.652710 [  220/ 1682]\n",
      "loss: 1693.494141 [  230/ 1682]\n",
      "loss: 1446.787231 [  240/ 1682]\n",
      "loss: 1603.609619 [  250/ 1682]\n",
      "loss: 1529.228760 [  260/ 1682]\n",
      "loss: 1181.052612 [  270/ 1682]\n",
      "loss: 1105.244751 [  280/ 1682]\n",
      "loss: 1351.879150 [  290/ 1682]\n",
      "loss: 1298.765381 [  300/ 1682]\n",
      "loss: 1390.683350 [  310/ 1682]\n",
      "loss: 876.017944 [  320/ 1682]\n",
      "loss: 1364.078125 [  330/ 1682]\n",
      "loss: 722.800415 [  340/ 1682]\n",
      "loss: 863.229309 [  350/ 1682]\n",
      "loss: 955.262329 [  360/ 1682]\n",
      "loss: 1202.065674 [  370/ 1682]\n",
      "loss: 753.890564 [  380/ 1682]\n",
      "loss: 1023.235352 [  390/ 1682]\n",
      "loss: 750.364136 [  400/ 1682]\n",
      "loss: 717.343201 [  410/ 1682]\n",
      "loss: 873.017395 [  420/ 1682]\n",
      "loss: 1123.832886 [  430/ 1682]\n",
      "loss: 990.061035 [  440/ 1682]\n",
      "loss: 994.006165 [  450/ 1682]\n",
      "loss: 566.599792 [  460/ 1682]\n",
      "loss: 668.156616 [  470/ 1682]\n",
      "loss: 662.671326 [  480/ 1682]\n",
      "loss: 663.405273 [  490/ 1682]\n",
      "loss: 694.865173 [  500/ 1682]\n",
      "loss: 524.827393 [  510/ 1682]\n",
      "loss: 848.399719 [  520/ 1682]\n",
      "loss: 615.686890 [  530/ 1682]\n",
      "loss: 580.607605 [  540/ 1682]\n",
      "loss: 597.581177 [  550/ 1682]\n",
      "loss: 553.552246 [  560/ 1682]\n",
      "loss: 617.118530 [  570/ 1682]\n",
      "loss: 641.860901 [  580/ 1682]\n",
      "loss: 473.500488 [  590/ 1682]\n",
      "loss: 358.230072 [  600/ 1682]\n",
      "loss: 401.925964 [  610/ 1682]\n",
      "loss: 523.334534 [  620/ 1682]\n",
      "loss: 533.436218 [  630/ 1682]\n",
      "loss: 270.511871 [  640/ 1682]\n",
      "loss: 183.342697 [  650/ 1682]\n",
      "loss: 206.365112 [  660/ 1682]\n",
      "loss: 223.832916 [  670/ 1682]\n",
      "loss: 311.627991 [  680/ 1682]\n",
      "loss: 227.734344 [  690/ 1682]\n",
      "loss: 199.816406 [  700/ 1682]\n",
      "loss: 336.593597 [  710/ 1682]\n",
      "loss: 531.817139 [  720/ 1682]\n",
      "loss: 621.581177 [  730/ 1682]\n",
      "loss: 745.178101 [  740/ 1682]\n",
      "loss: 771.644653 [  750/ 1682]\n",
      "loss: 835.478882 [  760/ 1682]\n",
      "loss: 587.422974 [  770/ 1682]\n",
      "loss: 555.939758 [  780/ 1682]\n",
      "loss: 498.482605 [  790/ 1682]\n",
      "loss: 390.544250 [  800/ 1682]\n",
      "loss: 359.471130 [  810/ 1682]\n",
      "loss: 310.176331 [  820/ 1682]\n",
      "loss: 246.084930 [  830/ 1682]\n",
      "loss: 387.278381 [  840/ 1682]\n",
      "loss: 409.070740 [  850/ 1682]\n",
      "loss: 312.443787 [  860/ 1682]\n",
      "loss: 315.656311 [  870/ 1682]\n",
      "loss: 242.927582 [  880/ 1682]\n",
      "loss: 301.964844 [  890/ 1682]\n",
      "loss: 349.740784 [  900/ 1682]\n",
      "loss: 184.456940 [  910/ 1682]\n",
      "loss: 233.260132 [  920/ 1682]\n",
      "loss: 192.653427 [  930/ 1682]\n",
      "loss: 127.293991 [  940/ 1682]\n",
      "loss: 72.592339 [  950/ 1682]\n",
      "loss: 2.086622 [  960/ 1682]\n",
      "loss: 25.065435 [  970/ 1682]\n",
      "loss: 22.341877 [  980/ 1682]\n",
      "loss: 63.707298 [  990/ 1682]\n",
      "loss: 150.658112 [ 1000/ 1682]\n",
      "loss: 209.762039 [ 1010/ 1682]\n",
      "loss: 254.276901 [ 1020/ 1682]\n",
      "loss: 99.820381 [ 1030/ 1682]\n",
      "loss: 45.752369 [ 1040/ 1682]\n",
      "loss: 61.571785 [ 1050/ 1682]\n",
      "loss: 27.778137 [ 1060/ 1682]\n",
      "loss: 25.065228 [ 1070/ 1682]\n",
      "loss: 106.885788 [ 1080/ 1682]\n",
      "loss: 170.372299 [ 1090/ 1682]\n",
      "loss: 299.639343 [ 1100/ 1682]\n",
      "loss: 518.214233 [ 1110/ 1682]\n",
      "loss: 750.698364 [ 1120/ 1682]\n",
      "loss: 954.225464 [ 1130/ 1682]\n",
      "loss: 1344.042603 [ 1140/ 1682]\n",
      "loss: 2045.202881 [ 1150/ 1682]\n",
      "loss: 3645.431152 [ 1160/ 1682]\n",
      "loss: 2061.452393 [ 1170/ 1682]\n",
      "loss: 2183.479736 [ 1180/ 1682]\n",
      "loss: 2766.587158 [ 1190/ 1682]\n",
      "loss: 2337.029785 [ 1200/ 1682]\n",
      "loss: 2594.461426 [ 1210/ 1682]\n",
      "loss: 2730.889160 [ 1220/ 1682]\n",
      "loss: 3314.197266 [ 1230/ 1682]\n",
      "loss: 4134.375977 [ 1240/ 1682]\n",
      "loss: 3841.286377 [ 1250/ 1682]\n",
      "loss: 4920.729492 [ 1260/ 1682]\n",
      "loss: 4721.666992 [ 1270/ 1682]\n",
      "loss: 3672.219482 [ 1280/ 1682]\n",
      "loss: 3122.718018 [ 1290/ 1682]\n",
      "loss: 3061.045410 [ 1300/ 1682]\n",
      "loss: 3866.144531 [ 1310/ 1682]\n",
      "loss: 4224.596191 [ 1320/ 1682]\n",
      "loss: 3734.385254 [ 1330/ 1682]\n",
      "loss: 3391.758301 [ 1340/ 1682]\n",
      "loss: 3610.125732 [ 1350/ 1682]\n",
      "loss: 4210.308105 [ 1360/ 1682]\n",
      "loss: 5119.372070 [ 1370/ 1682]\n",
      "loss: 6418.106445 [ 1380/ 1682]\n",
      "loss: 6020.807617 [ 1390/ 1682]\n",
      "loss: 6719.154785 [ 1400/ 1682]\n",
      "loss: 6925.846191 [ 1410/ 1682]\n",
      "loss: 7098.404785 [ 1420/ 1682]\n",
      "loss: 5264.357422 [ 1430/ 1682]\n",
      "loss: 5685.932129 [ 1440/ 1682]\n",
      "loss: 6923.438965 [ 1450/ 1682]\n",
      "loss: 6690.178223 [ 1460/ 1682]\n",
      "loss: 7985.097656 [ 1470/ 1682]\n",
      "loss: 9997.080078 [ 1480/ 1682]\n",
      "loss: 11626.399414 [ 1490/ 1682]\n",
      "loss: 11900.568359 [ 1500/ 1682]\n",
      "loss: 9836.337891 [ 1510/ 1682]\n",
      "loss: 10726.268555 [ 1520/ 1682]\n",
      "loss: 10398.393555 [ 1530/ 1682]\n",
      "loss: 9371.984375 [ 1540/ 1682]\n",
      "loss: 8291.418945 [ 1550/ 1682]\n",
      "loss: 11335.115234 [ 1560/ 1682]\n",
      "loss: 10089.133789 [ 1570/ 1682]\n",
      "loss: 8521.164062 [ 1580/ 1682]\n",
      "loss: 5910.043457 [ 1590/ 1682]\n",
      "loss: 5699.739746 [ 1600/ 1682]\n",
      "loss: 4730.079102 [ 1610/ 1682]\n",
      "loss: 5184.064941 [ 1620/ 1682]\n",
      "loss: 6443.486328 [ 1630/ 1682]\n",
      "loss: 8149.127441 [ 1640/ 1682]\n",
      "loss: 10020.422852 [ 1650/ 1682]\n",
      "loss: 10015.246094 [ 1660/ 1682]\n",
      "loss: 7242.398438 [ 1670/ 1682]\n",
      "loss: 7759.479492 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9306.928542 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1868.953369 [    0/ 1682]\n",
      "loss: 2108.328125 [   10/ 1682]\n",
      "loss: 1928.166382 [   20/ 1682]\n",
      "loss: 2048.143066 [   30/ 1682]\n",
      "loss: 1962.664795 [   40/ 1682]\n",
      "loss: 2258.822021 [   50/ 1682]\n",
      "loss: 1843.061768 [   60/ 1682]\n",
      "loss: 1788.242432 [   70/ 1682]\n",
      "loss: 2999.731934 [   80/ 1682]\n",
      "loss: 2702.636963 [   90/ 1682]\n",
      "loss: 2264.749512 [  100/ 1682]\n",
      "loss: 2299.329590 [  110/ 1682]\n",
      "loss: 2090.667236 [  120/ 1682]\n",
      "loss: 1839.921265 [  130/ 1682]\n",
      "loss: 1871.610962 [  140/ 1682]\n",
      "loss: 1801.132446 [  150/ 1682]\n",
      "loss: 1855.714478 [  160/ 1682]\n",
      "loss: 2117.475830 [  170/ 1682]\n",
      "loss: 1698.307373 [  180/ 1682]\n",
      "loss: 1638.861084 [  190/ 1682]\n",
      "loss: 1697.121704 [  200/ 1682]\n",
      "loss: 1622.928467 [  210/ 1682]\n",
      "loss: 1582.510498 [  220/ 1682]\n",
      "loss: 1679.964600 [  230/ 1682]\n",
      "loss: 2207.481934 [  240/ 1682]\n",
      "loss: 1438.133301 [  250/ 1682]\n",
      "loss: 1810.034790 [  260/ 1682]\n",
      "loss: 1462.837524 [  270/ 1682]\n",
      "loss: 981.799133 [  280/ 1682]\n",
      "loss: 1200.931396 [  290/ 1682]\n",
      "loss: 1161.785278 [  300/ 1682]\n",
      "loss: 1010.576782 [  310/ 1682]\n",
      "loss: 1244.037231 [  320/ 1682]\n",
      "loss: 1107.265259 [  330/ 1682]\n",
      "loss: 949.162231 [  340/ 1682]\n",
      "loss: 1073.606445 [  350/ 1682]\n",
      "loss: 957.394714 [  360/ 1682]\n",
      "loss: 1405.820068 [  370/ 1682]\n",
      "loss: 770.898376 [  380/ 1682]\n",
      "loss: 918.735168 [  390/ 1682]\n",
      "loss: 942.789429 [  400/ 1682]\n",
      "loss: 809.250549 [  410/ 1682]\n",
      "loss: 958.202820 [  420/ 1682]\n",
      "loss: 910.279663 [  430/ 1682]\n",
      "loss: 698.549255 [  440/ 1682]\n",
      "loss: 873.091919 [  450/ 1682]\n",
      "loss: 645.320435 [  460/ 1682]\n",
      "loss: 659.301758 [  470/ 1682]\n",
      "loss: 734.096069 [  480/ 1682]\n",
      "loss: 733.502075 [  490/ 1682]\n",
      "loss: 462.034912 [  500/ 1682]\n",
      "loss: 838.044128 [  510/ 1682]\n",
      "loss: 836.474792 [  520/ 1682]\n",
      "loss: 537.915649 [  530/ 1682]\n",
      "loss: 440.013275 [  540/ 1682]\n",
      "loss: 596.318970 [  550/ 1682]\n",
      "loss: 692.433228 [  560/ 1682]\n",
      "loss: 697.931885 [  570/ 1682]\n",
      "loss: 487.817627 [  580/ 1682]\n",
      "loss: 409.903076 [  590/ 1682]\n",
      "loss: 307.891266 [  600/ 1682]\n",
      "loss: 402.873260 [  610/ 1682]\n",
      "loss: 515.479614 [  620/ 1682]\n",
      "loss: 407.604492 [  630/ 1682]\n",
      "loss: 282.576111 [  640/ 1682]\n",
      "loss: 312.816223 [  650/ 1682]\n",
      "loss: 133.877777 [  660/ 1682]\n",
      "loss: 143.382263 [  670/ 1682]\n",
      "loss: 190.726730 [  680/ 1682]\n",
      "loss: 217.200104 [  690/ 1682]\n",
      "loss: 205.308716 [  700/ 1682]\n",
      "loss: 286.000061 [  710/ 1682]\n",
      "loss: 590.161682 [  720/ 1682]\n",
      "loss: 751.623474 [  730/ 1682]\n",
      "loss: 753.517822 [  740/ 1682]\n",
      "loss: 917.901550 [  750/ 1682]\n",
      "loss: 768.126404 [  760/ 1682]\n",
      "loss: 587.693726 [  770/ 1682]\n",
      "loss: 669.585571 [  780/ 1682]\n",
      "loss: 548.762268 [  790/ 1682]\n",
      "loss: 437.036377 [  800/ 1682]\n",
      "loss: 273.445129 [  810/ 1682]\n",
      "loss: 417.518555 [  820/ 1682]\n",
      "loss: 216.075165 [  830/ 1682]\n",
      "loss: 349.061340 [  840/ 1682]\n",
      "loss: 419.619781 [  850/ 1682]\n",
      "loss: 309.596863 [  860/ 1682]\n",
      "loss: 202.726044 [  870/ 1682]\n",
      "loss: 242.356689 [  880/ 1682]\n",
      "loss: 264.445190 [  890/ 1682]\n",
      "loss: 162.665634 [  900/ 1682]\n",
      "loss: 287.701752 [  910/ 1682]\n",
      "loss: 117.899574 [  920/ 1682]\n",
      "loss: 106.978958 [  930/ 1682]\n",
      "loss: 77.945984 [  940/ 1682]\n",
      "loss: 19.846228 [  950/ 1682]\n",
      "loss: 15.343348 [  960/ 1682]\n",
      "loss: 8.027201 [  970/ 1682]\n",
      "loss: 18.181540 [  980/ 1682]\n",
      "loss: 43.410801 [  990/ 1682]\n",
      "loss: 136.388580 [ 1000/ 1682]\n",
      "loss: 168.598709 [ 1010/ 1682]\n",
      "loss: 224.748825 [ 1020/ 1682]\n",
      "loss: 94.841515 [ 1030/ 1682]\n",
      "loss: 27.971603 [ 1040/ 1682]\n",
      "loss: 15.712047 [ 1050/ 1682]\n",
      "loss: 22.370787 [ 1060/ 1682]\n",
      "loss: 31.306051 [ 1070/ 1682]\n",
      "loss: 119.488121 [ 1080/ 1682]\n",
      "loss: 171.362259 [ 1090/ 1682]\n",
      "loss: 267.543701 [ 1100/ 1682]\n",
      "loss: 532.382019 [ 1110/ 1682]\n",
      "loss: 697.340942 [ 1120/ 1682]\n",
      "loss: 879.613464 [ 1130/ 1682]\n",
      "loss: 1339.364258 [ 1140/ 1682]\n",
      "loss: 2535.695801 [ 1150/ 1682]\n",
      "loss: 3331.211670 [ 1160/ 1682]\n",
      "loss: 2035.392212 [ 1170/ 1682]\n",
      "loss: 2231.738281 [ 1180/ 1682]\n",
      "loss: 2820.248291 [ 1190/ 1682]\n",
      "loss: 2174.548340 [ 1200/ 1682]\n",
      "loss: 2584.832520 [ 1210/ 1682]\n",
      "loss: 2790.595947 [ 1220/ 1682]\n",
      "loss: 3286.177002 [ 1230/ 1682]\n",
      "loss: 4113.798828 [ 1240/ 1682]\n",
      "loss: 4056.762451 [ 1250/ 1682]\n",
      "loss: 4728.836426 [ 1260/ 1682]\n",
      "loss: 4693.729492 [ 1270/ 1682]\n",
      "loss: 3646.885254 [ 1280/ 1682]\n",
      "loss: 2902.307129 [ 1290/ 1682]\n",
      "loss: 3045.712158 [ 1300/ 1682]\n",
      "loss: 3489.104248 [ 1310/ 1682]\n",
      "loss: 4464.307129 [ 1320/ 1682]\n",
      "loss: 3978.330566 [ 1330/ 1682]\n",
      "loss: 3492.037598 [ 1340/ 1682]\n",
      "loss: 3245.298096 [ 1350/ 1682]\n",
      "loss: 3940.843018 [ 1360/ 1682]\n",
      "loss: 4950.696289 [ 1370/ 1682]\n",
      "loss: 5895.916992 [ 1380/ 1682]\n",
      "loss: 6662.934570 [ 1390/ 1682]\n",
      "loss: 6503.428223 [ 1400/ 1682]\n",
      "loss: 6900.585938 [ 1410/ 1682]\n",
      "loss: 6695.990723 [ 1420/ 1682]\n",
      "loss: 5773.024902 [ 1430/ 1682]\n",
      "loss: 5157.067871 [ 1440/ 1682]\n",
      "loss: 6697.807617 [ 1450/ 1682]\n",
      "loss: 7230.312500 [ 1460/ 1682]\n",
      "loss: 8341.789062 [ 1470/ 1682]\n",
      "loss: 10011.346680 [ 1480/ 1682]\n",
      "loss: 11848.299805 [ 1490/ 1682]\n",
      "loss: 11577.130859 [ 1500/ 1682]\n",
      "loss: 9853.487305 [ 1510/ 1682]\n",
      "loss: 10770.375000 [ 1520/ 1682]\n",
      "loss: 9856.766602 [ 1530/ 1682]\n",
      "loss: 9086.305664 [ 1540/ 1682]\n",
      "loss: 9018.408203 [ 1550/ 1682]\n",
      "loss: 12185.177734 [ 1560/ 1682]\n",
      "loss: 9497.121094 [ 1570/ 1682]\n",
      "loss: 8488.606445 [ 1580/ 1682]\n",
      "loss: 6350.381348 [ 1590/ 1682]\n",
      "loss: 5736.229004 [ 1600/ 1682]\n",
      "loss: 4546.301270 [ 1610/ 1682]\n",
      "loss: 5150.973633 [ 1620/ 1682]\n",
      "loss: 6671.893555 [ 1630/ 1682]\n",
      "loss: 8653.942383 [ 1640/ 1682]\n",
      "loss: 10286.834961 [ 1650/ 1682]\n",
      "loss: 10288.157227 [ 1660/ 1682]\n",
      "loss: 8104.653809 [ 1670/ 1682]\n",
      "loss: 7705.063965 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9180.074810 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 2220.124512 [    0/ 1682]\n",
      "loss: 2604.058594 [   10/ 1682]\n",
      "loss: 2446.499023 [   20/ 1682]\n",
      "loss: 1861.873047 [   30/ 1682]\n",
      "loss: 2282.590088 [   40/ 1682]\n",
      "loss: 2511.284668 [   50/ 1682]\n",
      "loss: 1652.490845 [   60/ 1682]\n",
      "loss: 1952.162476 [   70/ 1682]\n",
      "loss: 2497.461914 [   80/ 1682]\n",
      "loss: 2404.562500 [   90/ 1682]\n",
      "loss: 1864.007080 [  100/ 1682]\n",
      "loss: 1904.005127 [  110/ 1682]\n",
      "loss: 1898.077881 [  120/ 1682]\n",
      "loss: 1840.200562 [  130/ 1682]\n",
      "loss: 1996.540649 [  140/ 1682]\n",
      "loss: 1764.392944 [  150/ 1682]\n",
      "loss: 1679.890015 [  160/ 1682]\n",
      "loss: 1757.308350 [  170/ 1682]\n",
      "loss: 1673.007202 [  180/ 1682]\n",
      "loss: 1751.125610 [  190/ 1682]\n",
      "loss: 1391.224121 [  200/ 1682]\n",
      "loss: 2041.720947 [  210/ 1682]\n",
      "loss: 1718.064697 [  220/ 1682]\n",
      "loss: 1655.868774 [  230/ 1682]\n",
      "loss: 1579.702393 [  240/ 1682]\n",
      "loss: 1694.953369 [  250/ 1682]\n",
      "loss: 1495.237427 [  260/ 1682]\n",
      "loss: 1079.232788 [  270/ 1682]\n",
      "loss: 1109.206299 [  280/ 1682]\n",
      "loss: 1403.414673 [  290/ 1682]\n",
      "loss: 1143.231445 [  300/ 1682]\n",
      "loss: 1014.975708 [  310/ 1682]\n",
      "loss: 1211.511475 [  320/ 1682]\n",
      "loss: 1073.307617 [  330/ 1682]\n",
      "loss: 936.396973 [  340/ 1682]\n",
      "loss: 1056.255615 [  350/ 1682]\n",
      "loss: 1246.979980 [  360/ 1682]\n",
      "loss: 1167.894287 [  370/ 1682]\n",
      "loss: 964.956909 [  380/ 1682]\n",
      "loss: 830.728638 [  390/ 1682]\n",
      "loss: 838.586914 [  400/ 1682]\n",
      "loss: 875.339966 [  410/ 1682]\n",
      "loss: 939.236145 [  420/ 1682]\n",
      "loss: 902.813843 [  430/ 1682]\n",
      "loss: 1106.072510 [  440/ 1682]\n",
      "loss: 849.037476 [  450/ 1682]\n",
      "loss: 573.092529 [  460/ 1682]\n",
      "loss: 587.892151 [  470/ 1682]\n",
      "loss: 779.602478 [  480/ 1682]\n",
      "loss: 711.526245 [  490/ 1682]\n",
      "loss: 661.704102 [  500/ 1682]\n",
      "loss: 688.947144 [  510/ 1682]\n",
      "loss: 753.894775 [  520/ 1682]\n",
      "loss: 659.336792 [  530/ 1682]\n",
      "loss: 624.044617 [  540/ 1682]\n",
      "loss: 542.237732 [  550/ 1682]\n",
      "loss: 558.540405 [  560/ 1682]\n",
      "loss: 753.461487 [  570/ 1682]\n",
      "loss: 534.538696 [  580/ 1682]\n",
      "loss: 561.878662 [  590/ 1682]\n",
      "loss: 365.446930 [  600/ 1682]\n",
      "loss: 551.900574 [  610/ 1682]\n",
      "loss: 545.234863 [  620/ 1682]\n",
      "loss: 405.078735 [  630/ 1682]\n",
      "loss: 356.728821 [  640/ 1682]\n",
      "loss: 226.285187 [  650/ 1682]\n",
      "loss: 167.772003 [  660/ 1682]\n",
      "loss: 140.834763 [  670/ 1682]\n",
      "loss: 174.660660 [  680/ 1682]\n",
      "loss: 146.397491 [  690/ 1682]\n",
      "loss: 196.461212 [  700/ 1682]\n",
      "loss: 337.309692 [  710/ 1682]\n",
      "loss: 524.399597 [  720/ 1682]\n",
      "loss: 625.713928 [  730/ 1682]\n",
      "loss: 872.987183 [  740/ 1682]\n",
      "loss: 838.404297 [  750/ 1682]\n",
      "loss: 761.493530 [  760/ 1682]\n",
      "loss: 581.756958 [  770/ 1682]\n",
      "loss: 551.823120 [  780/ 1682]\n",
      "loss: 489.647278 [  790/ 1682]\n",
      "loss: 386.199036 [  800/ 1682]\n",
      "loss: 275.703186 [  810/ 1682]\n",
      "loss: 290.971375 [  820/ 1682]\n",
      "loss: 318.742981 [  830/ 1682]\n",
      "loss: 594.710327 [  840/ 1682]\n",
      "loss: 471.364838 [  850/ 1682]\n",
      "loss: 306.576111 [  860/ 1682]\n",
      "loss: 300.326416 [  870/ 1682]\n",
      "loss: 236.891754 [  880/ 1682]\n",
      "loss: 292.977295 [  890/ 1682]\n",
      "loss: 223.146072 [  900/ 1682]\n",
      "loss: 213.831207 [  910/ 1682]\n",
      "loss: 116.802612 [  920/ 1682]\n",
      "loss: 155.677765 [  930/ 1682]\n",
      "loss: 89.841431 [  940/ 1682]\n",
      "loss: 36.517403 [  950/ 1682]\n",
      "loss: 6.469855 [  960/ 1682]\n",
      "loss: 17.517714 [  970/ 1682]\n",
      "loss: 16.875896 [  980/ 1682]\n",
      "loss: 47.988255 [  990/ 1682]\n",
      "loss: 98.944504 [ 1000/ 1682]\n",
      "loss: 146.937332 [ 1010/ 1682]\n",
      "loss: 204.877716 [ 1020/ 1682]\n",
      "loss: 103.169777 [ 1030/ 1682]\n",
      "loss: 27.101221 [ 1040/ 1682]\n",
      "loss: 55.200417 [ 1050/ 1682]\n",
      "loss: 27.764750 [ 1060/ 1682]\n",
      "loss: 22.288803 [ 1070/ 1682]\n",
      "loss: 102.179733 [ 1080/ 1682]\n",
      "loss: 167.784348 [ 1090/ 1682]\n",
      "loss: 295.472137 [ 1100/ 1682]\n",
      "loss: 467.594055 [ 1110/ 1682]\n",
      "loss: 683.233887 [ 1120/ 1682]\n",
      "loss: 800.980042 [ 1130/ 1682]\n",
      "loss: 1539.296143 [ 1140/ 1682]\n",
      "loss: 2123.215088 [ 1150/ 1682]\n",
      "loss: 3526.698486 [ 1160/ 1682]\n",
      "loss: 2003.327515 [ 1170/ 1682]\n",
      "loss: 2103.260742 [ 1180/ 1682]\n",
      "loss: 2578.195557 [ 1190/ 1682]\n",
      "loss: 2300.902344 [ 1200/ 1682]\n",
      "loss: 2638.687256 [ 1210/ 1682]\n",
      "loss: 2531.347656 [ 1220/ 1682]\n",
      "loss: 3351.389160 [ 1230/ 1682]\n",
      "loss: 4294.468750 [ 1240/ 1682]\n",
      "loss: 3942.514404 [ 1250/ 1682]\n",
      "loss: 4852.893066 [ 1260/ 1682]\n",
      "loss: 4800.750977 [ 1270/ 1682]\n",
      "loss: 3635.977051 [ 1280/ 1682]\n",
      "loss: 2994.896973 [ 1290/ 1682]\n",
      "loss: 3207.316895 [ 1300/ 1682]\n",
      "loss: 3615.038574 [ 1310/ 1682]\n",
      "loss: 4257.114258 [ 1320/ 1682]\n",
      "loss: 4070.311768 [ 1330/ 1682]\n",
      "loss: 3399.803955 [ 1340/ 1682]\n",
      "loss: 3558.033691 [ 1350/ 1682]\n",
      "loss: 4050.211426 [ 1360/ 1682]\n",
      "loss: 5333.710938 [ 1370/ 1682]\n",
      "loss: 6094.711426 [ 1380/ 1682]\n",
      "loss: 6202.031250 [ 1390/ 1682]\n",
      "loss: 6646.729492 [ 1400/ 1682]\n",
      "loss: 6906.864746 [ 1410/ 1682]\n",
      "loss: 6925.889160 [ 1420/ 1682]\n",
      "loss: 6080.413086 [ 1430/ 1682]\n",
      "loss: 5938.522461 [ 1440/ 1682]\n",
      "loss: 6712.301758 [ 1450/ 1682]\n",
      "loss: 7022.768066 [ 1460/ 1682]\n",
      "loss: 8338.228516 [ 1470/ 1682]\n",
      "loss: 10512.299805 [ 1480/ 1682]\n",
      "loss: 11618.411133 [ 1490/ 1682]\n",
      "loss: 11450.648438 [ 1500/ 1682]\n",
      "loss: 9835.187500 [ 1510/ 1682]\n",
      "loss: 11013.354492 [ 1520/ 1682]\n",
      "loss: 10545.476562 [ 1530/ 1682]\n",
      "loss: 9120.036133 [ 1540/ 1682]\n",
      "loss: 9274.530273 [ 1550/ 1682]\n",
      "loss: 11879.139648 [ 1560/ 1682]\n",
      "loss: 9814.780273 [ 1570/ 1682]\n",
      "loss: 8311.453125 [ 1580/ 1682]\n",
      "loss: 6332.129883 [ 1590/ 1682]\n",
      "loss: 6190.056152 [ 1600/ 1682]\n",
      "loss: 4976.151367 [ 1610/ 1682]\n",
      "loss: 5033.610840 [ 1620/ 1682]\n",
      "loss: 6850.638184 [ 1630/ 1682]\n",
      "loss: 8182.203125 [ 1640/ 1682]\n",
      "loss: 10143.210938 [ 1650/ 1682]\n",
      "loss: 9293.302734 [ 1660/ 1682]\n",
      "loss: 6774.807617 [ 1670/ 1682]\n",
      "loss: 7650.661133 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9514.582989 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1866.183350 [    0/ 1682]\n",
      "loss: 1771.715210 [   10/ 1682]\n",
      "loss: 2053.251465 [   20/ 1682]\n",
      "loss: 1984.458618 [   30/ 1682]\n",
      "loss: 2170.325439 [   40/ 1682]\n",
      "loss: 1554.383789 [   50/ 1682]\n",
      "loss: 1775.575928 [   60/ 1682]\n",
      "loss: 1784.759033 [   70/ 1682]\n",
      "loss: 2124.399658 [   80/ 1682]\n",
      "loss: 1759.291626 [   90/ 1682]\n",
      "loss: 2007.926758 [  100/ 1682]\n",
      "loss: 1910.332031 [  110/ 1682]\n",
      "loss: 1898.843018 [  120/ 1682]\n",
      "loss: 1985.454468 [  130/ 1682]\n",
      "loss: 2099.223389 [  140/ 1682]\n",
      "loss: 1882.784180 [  150/ 1682]\n",
      "loss: 1687.787842 [  160/ 1682]\n",
      "loss: 1736.248413 [  170/ 1682]\n",
      "loss: 1790.965088 [  180/ 1682]\n",
      "loss: 1727.925049 [  190/ 1682]\n",
      "loss: 1665.543335 [  200/ 1682]\n",
      "loss: 1756.457397 [  210/ 1682]\n",
      "loss: 1714.110596 [  220/ 1682]\n",
      "loss: 1774.548096 [  230/ 1682]\n",
      "loss: 1578.651611 [  240/ 1682]\n",
      "loss: 1796.529541 [  250/ 1682]\n",
      "loss: 1714.713135 [  260/ 1682]\n",
      "loss: 1203.232178 [  270/ 1682]\n",
      "loss: 1021.370972 [  280/ 1682]\n",
      "loss: 995.306641 [  290/ 1682]\n",
      "loss: 1053.583252 [  300/ 1682]\n",
      "loss: 1302.097046 [  310/ 1682]\n",
      "loss: 1204.827881 [  320/ 1682]\n",
      "loss: 990.152710 [  330/ 1682]\n",
      "loss: 1018.550659 [  340/ 1682]\n",
      "loss: 959.461609 [  350/ 1682]\n",
      "loss: 1060.362915 [  360/ 1682]\n",
      "loss: 1073.563721 [  370/ 1682]\n",
      "loss: 1046.451660 [  380/ 1682]\n",
      "loss: 838.827515 [  390/ 1682]\n",
      "loss: 844.054688 [  400/ 1682]\n",
      "loss: 730.150696 [  410/ 1682]\n",
      "loss: 1005.664734 [  420/ 1682]\n",
      "loss: 905.917969 [  430/ 1682]\n",
      "loss: 804.962280 [  440/ 1682]\n",
      "loss: 717.176147 [  450/ 1682]\n",
      "loss: 764.426880 [  460/ 1682]\n",
      "loss: 595.247620 [  470/ 1682]\n",
      "loss: 535.109497 [  480/ 1682]\n",
      "loss: 593.672485 [  490/ 1682]\n",
      "loss: 603.810669 [  500/ 1682]\n",
      "loss: 688.011841 [  510/ 1682]\n",
      "loss: 825.933960 [  520/ 1682]\n",
      "loss: 497.981384 [  530/ 1682]\n",
      "loss: 574.365723 [  540/ 1682]\n",
      "loss: 674.389771 [  550/ 1682]\n",
      "loss: 568.873230 [  560/ 1682]\n",
      "loss: 754.459534 [  570/ 1682]\n",
      "loss: 490.774567 [  580/ 1682]\n",
      "loss: 564.350403 [  590/ 1682]\n",
      "loss: 374.443176 [  600/ 1682]\n",
      "loss: 465.898529 [  610/ 1682]\n",
      "loss: 596.649902 [  620/ 1682]\n",
      "loss: 412.843750 [  630/ 1682]\n",
      "loss: 358.262909 [  640/ 1682]\n",
      "loss: 258.354919 [  650/ 1682]\n",
      "loss: 165.468170 [  660/ 1682]\n",
      "loss: 173.509720 [  670/ 1682]\n",
      "loss: 153.352402 [  680/ 1682]\n",
      "loss: 207.728271 [  690/ 1682]\n",
      "loss: 197.772339 [  700/ 1682]\n",
      "loss: 366.754486 [  710/ 1682]\n",
      "loss: 533.121948 [  720/ 1682]\n",
      "loss: 539.450989 [  730/ 1682]\n",
      "loss: 773.866333 [  740/ 1682]\n",
      "loss: 910.532410 [  750/ 1682]\n",
      "loss: 776.407104 [  760/ 1682]\n",
      "loss: 683.716187 [  770/ 1682]\n",
      "loss: 474.873779 [  780/ 1682]\n",
      "loss: 502.938782 [  790/ 1682]\n",
      "loss: 320.025879 [  800/ 1682]\n",
      "loss: 417.100403 [  810/ 1682]\n",
      "loss: 236.782272 [  820/ 1682]\n",
      "loss: 301.441498 [  830/ 1682]\n",
      "loss: 447.035980 [  840/ 1682]\n",
      "loss: 415.693115 [  850/ 1682]\n",
      "loss: 312.291656 [  860/ 1682]\n",
      "loss: 250.349884 [  870/ 1682]\n",
      "loss: 218.332199 [  880/ 1682]\n",
      "loss: 236.480301 [  890/ 1682]\n",
      "loss: 203.777054 [  900/ 1682]\n",
      "loss: 248.222626 [  910/ 1682]\n",
      "loss: 169.060760 [  920/ 1682]\n",
      "loss: 135.168625 [  930/ 1682]\n",
      "loss: 93.050781 [  940/ 1682]\n",
      "loss: 50.255775 [  950/ 1682]\n",
      "loss: 17.177198 [  960/ 1682]\n",
      "loss: 21.850874 [  970/ 1682]\n",
      "loss: 15.565595 [  980/ 1682]\n",
      "loss: 37.655220 [  990/ 1682]\n",
      "loss: 118.536827 [ 1000/ 1682]\n",
      "loss: 154.031158 [ 1010/ 1682]\n",
      "loss: 162.002655 [ 1020/ 1682]\n",
      "loss: 71.858696 [ 1030/ 1682]\n",
      "loss: 31.476772 [ 1040/ 1682]\n",
      "loss: 51.497784 [ 1050/ 1682]\n",
      "loss: 35.726341 [ 1060/ 1682]\n",
      "loss: 21.881588 [ 1070/ 1682]\n",
      "loss: 100.928146 [ 1080/ 1682]\n",
      "loss: 157.828461 [ 1090/ 1682]\n",
      "loss: 271.525726 [ 1100/ 1682]\n",
      "loss: 478.894470 [ 1110/ 1682]\n",
      "loss: 603.320618 [ 1120/ 1682]\n",
      "loss: 827.168274 [ 1130/ 1682]\n",
      "loss: 1420.656250 [ 1140/ 1682]\n",
      "loss: 2212.307861 [ 1150/ 1682]\n",
      "loss: 3498.846924 [ 1160/ 1682]\n",
      "loss: 2090.390137 [ 1170/ 1682]\n",
      "loss: 2254.051270 [ 1180/ 1682]\n",
      "loss: 2540.911865 [ 1190/ 1682]\n",
      "loss: 2281.810303 [ 1200/ 1682]\n",
      "loss: 2527.815430 [ 1210/ 1682]\n",
      "loss: 2725.634277 [ 1220/ 1682]\n",
      "loss: 3315.283936 [ 1230/ 1682]\n",
      "loss: 3886.501465 [ 1240/ 1682]\n",
      "loss: 3629.980957 [ 1250/ 1682]\n",
      "loss: 4581.086914 [ 1260/ 1682]\n",
      "loss: 4755.635742 [ 1270/ 1682]\n",
      "loss: 3696.960205 [ 1280/ 1682]\n",
      "loss: 2886.771973 [ 1290/ 1682]\n",
      "loss: 2825.646973 [ 1300/ 1682]\n",
      "loss: 3777.809326 [ 1310/ 1682]\n",
      "loss: 4320.442383 [ 1320/ 1682]\n",
      "loss: 4031.548096 [ 1330/ 1682]\n",
      "loss: 3545.519043 [ 1340/ 1682]\n",
      "loss: 3423.044189 [ 1350/ 1682]\n",
      "loss: 3896.805420 [ 1360/ 1682]\n",
      "loss: 5288.387695 [ 1370/ 1682]\n",
      "loss: 6307.306152 [ 1380/ 1682]\n",
      "loss: 6280.421875 [ 1390/ 1682]\n",
      "loss: 6451.030762 [ 1400/ 1682]\n",
      "loss: 7140.543945 [ 1410/ 1682]\n",
      "loss: 7010.821289 [ 1420/ 1682]\n",
      "loss: 5757.957031 [ 1430/ 1682]\n",
      "loss: 5622.524902 [ 1440/ 1682]\n",
      "loss: 6649.089844 [ 1450/ 1682]\n",
      "loss: 6818.684570 [ 1460/ 1682]\n",
      "loss: 7906.755371 [ 1470/ 1682]\n",
      "loss: 10843.122070 [ 1480/ 1682]\n",
      "loss: 11542.978516 [ 1490/ 1682]\n",
      "loss: 12010.284180 [ 1500/ 1682]\n",
      "loss: 9427.915039 [ 1510/ 1682]\n",
      "loss: 10757.041992 [ 1520/ 1682]\n",
      "loss: 9841.679688 [ 1530/ 1682]\n",
      "loss: 9249.064453 [ 1540/ 1682]\n",
      "loss: 8790.528320 [ 1550/ 1682]\n",
      "loss: 11346.881836 [ 1560/ 1682]\n",
      "loss: 9979.622070 [ 1570/ 1682]\n",
      "loss: 8240.599609 [ 1580/ 1682]\n",
      "loss: 5918.185547 [ 1590/ 1682]\n",
      "loss: 6334.086914 [ 1600/ 1682]\n",
      "loss: 5053.610352 [ 1610/ 1682]\n",
      "loss: 4778.948730 [ 1620/ 1682]\n",
      "loss: 6799.567871 [ 1630/ 1682]\n",
      "loss: 8539.169922 [ 1640/ 1682]\n",
      "loss: 10291.999023 [ 1650/ 1682]\n",
      "loss: 9904.808594 [ 1660/ 1682]\n",
      "loss: 7329.888184 [ 1670/ 1682]\n",
      "loss: 6506.789062 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9258.018160 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1898.655518 [    0/ 1682]\n",
      "loss: 2083.488770 [   10/ 1682]\n",
      "loss: 2383.980225 [   20/ 1682]\n",
      "loss: 2173.364746 [   30/ 1682]\n",
      "loss: 1801.327148 [   40/ 1682]\n",
      "loss: 1999.217163 [   50/ 1682]\n",
      "loss: 1677.715820 [   60/ 1682]\n",
      "loss: 1669.462158 [   70/ 1682]\n",
      "loss: 2159.887695 [   80/ 1682]\n",
      "loss: 2235.030762 [   90/ 1682]\n",
      "loss: 2330.312988 [  100/ 1682]\n",
      "loss: 1793.279541 [  110/ 1682]\n",
      "loss: 2077.239014 [  120/ 1682]\n",
      "loss: 2307.594238 [  130/ 1682]\n",
      "loss: 1726.169312 [  140/ 1682]\n",
      "loss: 1650.398682 [  150/ 1682]\n",
      "loss: 2107.787354 [  160/ 1682]\n",
      "loss: 1643.790283 [  170/ 1682]\n",
      "loss: 1566.874878 [  180/ 1682]\n",
      "loss: 1881.865967 [  190/ 1682]\n",
      "loss: 1688.027588 [  200/ 1682]\n",
      "loss: 1781.910522 [  210/ 1682]\n",
      "loss: 1741.536133 [  220/ 1682]\n",
      "loss: 1558.780640 [  230/ 1682]\n",
      "loss: 1486.504028 [  240/ 1682]\n",
      "loss: 1477.012695 [  250/ 1682]\n",
      "loss: 1518.628296 [  260/ 1682]\n",
      "loss: 1326.711670 [  270/ 1682]\n",
      "loss: 1343.526978 [  280/ 1682]\n",
      "loss: 1310.784424 [  290/ 1682]\n",
      "loss: 1266.341675 [  300/ 1682]\n",
      "loss: 1140.213379 [  310/ 1682]\n",
      "loss: 1322.119263 [  320/ 1682]\n",
      "loss: 921.407715 [  330/ 1682]\n",
      "loss: 874.577454 [  340/ 1682]\n",
      "loss: 1150.233032 [  350/ 1682]\n",
      "loss: 906.684204 [  360/ 1682]\n",
      "loss: 924.471985 [  370/ 1682]\n",
      "loss: 1232.763428 [  380/ 1682]\n",
      "loss: 1013.558960 [  390/ 1682]\n",
      "loss: 787.401367 [  400/ 1682]\n",
      "loss: 748.240112 [  410/ 1682]\n",
      "loss: 807.044617 [  420/ 1682]\n",
      "loss: 850.519714 [  430/ 1682]\n",
      "loss: 893.238098 [  440/ 1682]\n",
      "loss: 664.893494 [  450/ 1682]\n",
      "loss: 600.207397 [  460/ 1682]\n",
      "loss: 672.696411 [  470/ 1682]\n",
      "loss: 675.653625 [  480/ 1682]\n",
      "loss: 731.814514 [  490/ 1682]\n",
      "loss: 680.459961 [  500/ 1682]\n",
      "loss: 583.011108 [  510/ 1682]\n",
      "loss: 785.404907 [  520/ 1682]\n",
      "loss: 623.228210 [  530/ 1682]\n",
      "loss: 533.609009 [  540/ 1682]\n",
      "loss: 627.851440 [  550/ 1682]\n",
      "loss: 528.880615 [  560/ 1682]\n",
      "loss: 830.114075 [  570/ 1682]\n",
      "loss: 510.334625 [  580/ 1682]\n",
      "loss: 534.259644 [  590/ 1682]\n",
      "loss: 432.749207 [  600/ 1682]\n",
      "loss: 579.760315 [  610/ 1682]\n",
      "loss: 426.871918 [  620/ 1682]\n",
      "loss: 474.345947 [  630/ 1682]\n",
      "loss: 333.917664 [  640/ 1682]\n",
      "loss: 308.501526 [  650/ 1682]\n",
      "loss: 233.227570 [  660/ 1682]\n",
      "loss: 153.719696 [  670/ 1682]\n",
      "loss: 189.542618 [  680/ 1682]\n",
      "loss: 131.508499 [  690/ 1682]\n",
      "loss: 245.899200 [  700/ 1682]\n",
      "loss: 392.002716 [  710/ 1682]\n",
      "loss: 600.267578 [  720/ 1682]\n",
      "loss: 757.789185 [  730/ 1682]\n",
      "loss: 908.872070 [  740/ 1682]\n",
      "loss: 875.252319 [  750/ 1682]\n",
      "loss: 688.160645 [  760/ 1682]\n",
      "loss: 654.286499 [  770/ 1682]\n",
      "loss: 670.973083 [  780/ 1682]\n",
      "loss: 554.859924 [  790/ 1682]\n",
      "loss: 446.272369 [  800/ 1682]\n",
      "loss: 262.067963 [  810/ 1682]\n",
      "loss: 279.293365 [  820/ 1682]\n",
      "loss: 380.462830 [  830/ 1682]\n",
      "loss: 464.275879 [  840/ 1682]\n",
      "loss: 428.307068 [  850/ 1682]\n",
      "loss: 392.727600 [  860/ 1682]\n",
      "loss: 350.707703 [  870/ 1682]\n",
      "loss: 225.516602 [  880/ 1682]\n",
      "loss: 337.178558 [  890/ 1682]\n",
      "loss: 212.806107 [  900/ 1682]\n",
      "loss: 231.266388 [  910/ 1682]\n",
      "loss: 196.701080 [  920/ 1682]\n",
      "loss: 165.813889 [  930/ 1682]\n",
      "loss: 87.585800 [  940/ 1682]\n",
      "loss: 36.698845 [  950/ 1682]\n",
      "loss: 10.409418 [  960/ 1682]\n",
      "loss: 9.897658 [  970/ 1682]\n",
      "loss: 11.386911 [  980/ 1682]\n",
      "loss: 44.753834 [  990/ 1682]\n",
      "loss: 108.267136 [ 1000/ 1682]\n",
      "loss: 161.737839 [ 1010/ 1682]\n",
      "loss: 201.005905 [ 1020/ 1682]\n",
      "loss: 92.212868 [ 1030/ 1682]\n",
      "loss: 30.440796 [ 1040/ 1682]\n",
      "loss: 75.147507 [ 1050/ 1682]\n",
      "loss: 26.474756 [ 1060/ 1682]\n",
      "loss: 20.730433 [ 1070/ 1682]\n",
      "loss: 94.206985 [ 1080/ 1682]\n",
      "loss: 164.124786 [ 1090/ 1682]\n",
      "loss: 232.207230 [ 1100/ 1682]\n",
      "loss: 440.072998 [ 1110/ 1682]\n",
      "loss: 615.218933 [ 1120/ 1682]\n",
      "loss: 770.617065 [ 1130/ 1682]\n",
      "loss: 1442.696533 [ 1140/ 1682]\n",
      "loss: 2381.670898 [ 1150/ 1682]\n",
      "loss: 3535.734863 [ 1160/ 1682]\n",
      "loss: 2063.037109 [ 1170/ 1682]\n",
      "loss: 2111.452148 [ 1180/ 1682]\n",
      "loss: 2588.113770 [ 1190/ 1682]\n",
      "loss: 2311.185303 [ 1200/ 1682]\n",
      "loss: 2518.201416 [ 1210/ 1682]\n",
      "loss: 2622.056885 [ 1220/ 1682]\n",
      "loss: 3280.705078 [ 1230/ 1682]\n",
      "loss: 4128.325684 [ 1240/ 1682]\n",
      "loss: 4052.600098 [ 1250/ 1682]\n",
      "loss: 4549.118652 [ 1260/ 1682]\n",
      "loss: 4727.207031 [ 1270/ 1682]\n",
      "loss: 3404.735107 [ 1280/ 1682]\n",
      "loss: 2866.038086 [ 1290/ 1682]\n",
      "loss: 3062.922363 [ 1300/ 1682]\n",
      "loss: 3492.650879 [ 1310/ 1682]\n",
      "loss: 4593.788086 [ 1320/ 1682]\n",
      "loss: 3917.159424 [ 1330/ 1682]\n",
      "loss: 3255.945801 [ 1340/ 1682]\n",
      "loss: 3403.175049 [ 1350/ 1682]\n",
      "loss: 4086.555176 [ 1360/ 1682]\n",
      "loss: 5244.661621 [ 1370/ 1682]\n",
      "loss: 6014.770508 [ 1380/ 1682]\n",
      "loss: 6130.334961 [ 1390/ 1682]\n",
      "loss: 6559.411621 [ 1400/ 1682]\n",
      "loss: 6806.408691 [ 1410/ 1682]\n",
      "loss: 6982.080566 [ 1420/ 1682]\n",
      "loss: 5990.938965 [ 1430/ 1682]\n",
      "loss: 5592.936523 [ 1440/ 1682]\n",
      "loss: 6632.206055 [ 1450/ 1682]\n",
      "loss: 6639.185059 [ 1460/ 1682]\n",
      "loss: 8050.751465 [ 1470/ 1682]\n",
      "loss: 10405.932617 [ 1480/ 1682]\n",
      "loss: 11707.716797 [ 1490/ 1682]\n",
      "loss: 11759.884766 [ 1500/ 1682]\n",
      "loss: 9754.346680 [ 1510/ 1682]\n",
      "loss: 10707.315430 [ 1520/ 1682]\n",
      "loss: 9447.045898 [ 1530/ 1682]\n",
      "loss: 8832.239258 [ 1540/ 1682]\n",
      "loss: 8583.673828 [ 1550/ 1682]\n",
      "loss: 11542.602539 [ 1560/ 1682]\n",
      "loss: 9922.099609 [ 1570/ 1682]\n",
      "loss: 8610.556641 [ 1580/ 1682]\n",
      "loss: 6073.400391 [ 1590/ 1682]\n",
      "loss: 6113.831055 [ 1600/ 1682]\n",
      "loss: 5043.312500 [ 1610/ 1682]\n",
      "loss: 4956.403320 [ 1620/ 1682]\n",
      "loss: 6565.962402 [ 1630/ 1682]\n",
      "loss: 7819.496094 [ 1640/ 1682]\n",
      "loss: 10260.234375 [ 1650/ 1682]\n",
      "loss: 9388.194336 [ 1660/ 1682]\n",
      "loss: 7981.590820 [ 1670/ 1682]\n",
      "loss: 7544.389648 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9249.568134 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 2183.978271 [    0/ 1682]\n",
      "loss: 2229.952881 [   10/ 1682]\n",
      "loss: 2379.087402 [   20/ 1682]\n",
      "loss: 1902.137695 [   30/ 1682]\n",
      "loss: 1816.026001 [   40/ 1682]\n",
      "loss: 1602.800537 [   50/ 1682]\n",
      "loss: 2219.735596 [   60/ 1682]\n",
      "loss: 2096.116699 [   70/ 1682]\n",
      "loss: 2169.348633 [   80/ 1682]\n",
      "loss: 2231.718994 [   90/ 1682]\n",
      "loss: 2320.714355 [  100/ 1682]\n",
      "loss: 2089.718262 [  110/ 1682]\n",
      "loss: 2079.100098 [  120/ 1682]\n",
      "loss: 2021.820679 [  130/ 1682]\n",
      "loss: 1868.016846 [  140/ 1682]\n",
      "loss: 1783.538818 [  150/ 1682]\n",
      "loss: 1724.476562 [  160/ 1682]\n",
      "loss: 1649.854492 [  170/ 1682]\n",
      "loss: 1928.155518 [  180/ 1682]\n",
      "loss: 1862.531860 [  190/ 1682]\n",
      "loss: 1807.222412 [  200/ 1682]\n",
      "loss: 1901.742432 [  210/ 1682]\n",
      "loss: 1624.963013 [  220/ 1682]\n",
      "loss: 1896.390625 [  230/ 1682]\n",
      "loss: 1600.841919 [  240/ 1682]\n",
      "loss: 1902.903564 [  250/ 1682]\n",
      "loss: 1417.464722 [  260/ 1682]\n",
      "loss: 1422.553467 [  270/ 1682]\n",
      "loss: 1329.719727 [  280/ 1682]\n",
      "loss: 1033.810181 [  290/ 1682]\n",
      "loss: 1078.694214 [  300/ 1682]\n",
      "loss: 969.656860 [  310/ 1682]\n",
      "loss: 1212.623413 [  320/ 1682]\n",
      "loss: 1003.053833 [  330/ 1682]\n",
      "loss: 807.860535 [  340/ 1682]\n",
      "loss: 975.072632 [  350/ 1682]\n",
      "loss: 1075.860352 [  360/ 1682]\n",
      "loss: 1246.953369 [  370/ 1682]\n",
      "loss: 1054.228882 [  380/ 1682]\n",
      "loss: 931.931335 [  390/ 1682]\n",
      "loss: 990.733276 [  400/ 1682]\n",
      "loss: 752.939575 [  410/ 1682]\n",
      "loss: 814.549988 [  420/ 1682]\n",
      "loss: 989.887024 [  430/ 1682]\n",
      "loss: 890.556274 [  440/ 1682]\n",
      "loss: 738.686951 [  450/ 1682]\n",
      "loss: 603.646729 [  460/ 1682]\n",
      "loss: 671.441528 [  470/ 1682]\n",
      "loss: 721.808777 [  480/ 1682]\n",
      "loss: 614.899048 [  490/ 1682]\n",
      "loss: 766.632874 [  500/ 1682]\n",
      "loss: 751.053101 [  510/ 1682]\n",
      "loss: 784.782593 [  520/ 1682]\n",
      "loss: 525.187012 [  530/ 1682]\n",
      "loss: 539.673828 [  540/ 1682]\n",
      "loss: 636.049377 [  550/ 1682]\n",
      "loss: 642.571594 [  560/ 1682]\n",
      "loss: 764.036743 [  570/ 1682]\n",
      "loss: 504.364899 [  580/ 1682]\n",
      "loss: 440.375183 [  590/ 1682]\n",
      "loss: 391.718933 [  600/ 1682]\n",
      "loss: 515.705017 [  610/ 1682]\n",
      "loss: 430.980072 [  620/ 1682]\n",
      "loss: 348.009918 [  630/ 1682]\n",
      "loss: 330.588684 [  640/ 1682]\n",
      "loss: 207.814667 [  650/ 1682]\n",
      "loss: 147.773575 [  660/ 1682]\n",
      "loss: 207.793213 [  670/ 1682]\n",
      "loss: 111.659775 [  680/ 1682]\n",
      "loss: 237.678680 [  690/ 1682]\n",
      "loss: 183.472855 [  700/ 1682]\n",
      "loss: 317.627625 [  710/ 1682]\n",
      "loss: 551.322876 [  720/ 1682]\n",
      "loss: 655.039673 [  730/ 1682]\n",
      "loss: 850.116577 [  740/ 1682]\n",
      "loss: 976.905151 [  750/ 1682]\n",
      "loss: 797.237915 [  760/ 1682]\n",
      "loss: 570.676941 [  770/ 1682]\n",
      "loss: 580.268982 [  780/ 1682]\n",
      "loss: 483.460297 [  790/ 1682]\n",
      "loss: 410.469147 [  800/ 1682]\n",
      "loss: 302.346680 [  810/ 1682]\n",
      "loss: 279.057953 [  820/ 1682]\n",
      "loss: 367.854950 [  830/ 1682]\n",
      "loss: 573.012695 [  840/ 1682]\n",
      "loss: 400.546509 [  850/ 1682]\n",
      "loss: 390.783539 [  860/ 1682]\n",
      "loss: 266.272278 [  870/ 1682]\n",
      "loss: 306.895660 [  880/ 1682]\n",
      "loss: 311.817780 [  890/ 1682]\n",
      "loss: 217.697906 [  900/ 1682]\n",
      "loss: 212.556274 [  910/ 1682]\n",
      "loss: 135.212677 [  920/ 1682]\n",
      "loss: 146.378326 [  930/ 1682]\n",
      "loss: 121.482361 [  940/ 1682]\n",
      "loss: 43.608715 [  950/ 1682]\n",
      "loss: 1.011895 [  960/ 1682]\n",
      "loss: 7.641065 [  970/ 1682]\n",
      "loss: 9.882364 [  980/ 1682]\n",
      "loss: 30.636389 [  990/ 1682]\n",
      "loss: 112.750137 [ 1000/ 1682]\n",
      "loss: 167.937073 [ 1010/ 1682]\n",
      "loss: 179.640839 [ 1020/ 1682]\n",
      "loss: 70.553574 [ 1030/ 1682]\n",
      "loss: 17.364071 [ 1040/ 1682]\n",
      "loss: 53.219128 [ 1050/ 1682]\n",
      "loss: 21.729673 [ 1060/ 1682]\n",
      "loss: 16.957092 [ 1070/ 1682]\n",
      "loss: 99.568733 [ 1080/ 1682]\n",
      "loss: 170.204742 [ 1090/ 1682]\n",
      "loss: 239.837646 [ 1100/ 1682]\n",
      "loss: 465.503815 [ 1110/ 1682]\n",
      "loss: 619.912903 [ 1120/ 1682]\n",
      "loss: 867.913391 [ 1130/ 1682]\n",
      "loss: 1402.774292 [ 1140/ 1682]\n",
      "loss: 2298.223145 [ 1150/ 1682]\n",
      "loss: 3579.255127 [ 1160/ 1682]\n",
      "loss: 2105.857666 [ 1170/ 1682]\n",
      "loss: 2137.781250 [ 1180/ 1682]\n",
      "loss: 2571.948975 [ 1190/ 1682]\n",
      "loss: 2224.521973 [ 1200/ 1682]\n",
      "loss: 2427.910156 [ 1210/ 1682]\n",
      "loss: 2413.843262 [ 1220/ 1682]\n",
      "loss: 3107.618408 [ 1230/ 1682]\n",
      "loss: 3865.470703 [ 1240/ 1682]\n",
      "loss: 3932.574707 [ 1250/ 1682]\n",
      "loss: 4734.219238 [ 1260/ 1682]\n",
      "loss: 4603.383789 [ 1270/ 1682]\n",
      "loss: 3629.906982 [ 1280/ 1682]\n",
      "loss: 2987.255615 [ 1290/ 1682]\n",
      "loss: 2958.849365 [ 1300/ 1682]\n",
      "loss: 3532.412598 [ 1310/ 1682]\n",
      "loss: 4374.309570 [ 1320/ 1682]\n",
      "loss: 3969.214355 [ 1330/ 1682]\n",
      "loss: 3408.206299 [ 1340/ 1682]\n",
      "loss: 3382.244629 [ 1350/ 1682]\n",
      "loss: 4144.506348 [ 1360/ 1682]\n",
      "loss: 4906.326172 [ 1370/ 1682]\n",
      "loss: 6227.243652 [ 1380/ 1682]\n",
      "loss: 5991.233887 [ 1390/ 1682]\n",
      "loss: 6524.924805 [ 1400/ 1682]\n",
      "loss: 6656.344727 [ 1410/ 1682]\n",
      "loss: 6688.708496 [ 1420/ 1682]\n",
      "loss: 5845.478516 [ 1430/ 1682]\n",
      "loss: 5563.729492 [ 1440/ 1682]\n",
      "loss: 6466.679688 [ 1450/ 1682]\n",
      "loss: 6756.264160 [ 1460/ 1682]\n",
      "loss: 7740.251465 [ 1470/ 1682]\n",
      "loss: 9848.369141 [ 1480/ 1682]\n",
      "loss: 11840.686523 [ 1490/ 1682]\n",
      "loss: 11902.225586 [ 1500/ 1682]\n",
      "loss: 9740.697266 [ 1510/ 1682]\n",
      "loss: 11095.426758 [ 1520/ 1682]\n",
      "loss: 9245.094727 [ 1530/ 1682]\n",
      "loss: 9161.818359 [ 1540/ 1682]\n",
      "loss: 9296.615234 [ 1550/ 1682]\n",
      "loss: 11701.748047 [ 1560/ 1682]\n",
      "loss: 9464.989258 [ 1570/ 1682]\n",
      "loss: 8203.634766 [ 1580/ 1682]\n",
      "loss: 5693.917969 [ 1590/ 1682]\n",
      "loss: 6408.739258 [ 1600/ 1682]\n",
      "loss: 5001.161133 [ 1610/ 1682]\n",
      "loss: 5080.630371 [ 1620/ 1682]\n",
      "loss: 6701.428711 [ 1630/ 1682]\n",
      "loss: 8216.542969 [ 1640/ 1682]\n",
      "loss: 10459.152344 [ 1650/ 1682]\n",
      "loss: 9572.181641 [ 1660/ 1682]\n",
      "loss: 8154.899902 [ 1670/ 1682]\n",
      "loss: 6481.899902 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9191.489183 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1939.258423 [    0/ 1682]\n",
      "loss: 2227.951416 [   10/ 1682]\n",
      "loss: 1985.779297 [   20/ 1682]\n",
      "loss: 1915.816650 [   30/ 1682]\n",
      "loss: 1833.627197 [   40/ 1682]\n",
      "loss: 1750.925049 [   50/ 1682]\n",
      "loss: 1831.230835 [   60/ 1682]\n",
      "loss: 1842.315063 [   70/ 1682]\n",
      "loss: 2179.733643 [   80/ 1682]\n",
      "loss: 2099.862305 [   90/ 1682]\n",
      "loss: 1932.024780 [  100/ 1682]\n",
      "loss: 2237.873047 [  110/ 1682]\n",
      "loss: 2097.766846 [  120/ 1682]\n",
      "loss: 2039.000732 [  130/ 1682]\n",
      "loss: 1884.339233 [  140/ 1682]\n",
      "loss: 1921.591797 [  150/ 1682]\n",
      "loss: 1626.603516 [  160/ 1682]\n",
      "loss: 1555.412476 [  170/ 1682]\n",
      "loss: 1483.809570 [  180/ 1682]\n",
      "loss: 1544.232178 [  190/ 1682]\n",
      "loss: 1819.239258 [  200/ 1682]\n",
      "loss: 1576.614014 [  210/ 1682]\n",
      "loss: 1760.349854 [  220/ 1682]\n",
      "loss: 1704.747070 [  230/ 1682]\n",
      "loss: 1630.626709 [  240/ 1682]\n",
      "loss: 1618.544434 [  250/ 1682]\n",
      "loss: 1545.062744 [  260/ 1682]\n",
      "loss: 1452.609619 [  270/ 1682]\n",
      "loss: 1172.913574 [  280/ 1682]\n",
      "loss: 1419.938721 [  290/ 1682]\n",
      "loss: 1191.082031 [  300/ 1682]\n",
      "loss: 1249.038208 [  310/ 1682]\n",
      "loss: 1247.956299 [  320/ 1682]\n",
      "loss: 1108.334229 [  330/ 1682]\n",
      "loss: 825.039062 [  340/ 1682]\n",
      "loss: 1076.965942 [  350/ 1682]\n",
      "loss: 1186.060059 [  360/ 1682]\n",
      "loss: 961.234985 [  370/ 1682]\n",
      "loss: 1009.496460 [  380/ 1682]\n",
      "loss: 883.443237 [  390/ 1682]\n",
      "loss: 815.781067 [  400/ 1682]\n",
      "loss: 840.288269 [  410/ 1682]\n",
      "loss: 902.930847 [  420/ 1682]\n",
      "loss: 880.301636 [  430/ 1682]\n",
      "loss: 781.725220 [  440/ 1682]\n",
      "loss: 886.516724 [  450/ 1682]\n",
      "loss: 737.169617 [  460/ 1682]\n",
      "loss: 697.279358 [  470/ 1682]\n",
      "loss: 755.735962 [  480/ 1682]\n",
      "loss: 694.062683 [  490/ 1682]\n",
      "loss: 639.622620 [  500/ 1682]\n",
      "loss: 732.364136 [  510/ 1682]\n",
      "loss: 802.005676 [  520/ 1682]\n",
      "loss: 698.953003 [  530/ 1682]\n",
      "loss: 660.480591 [  540/ 1682]\n",
      "loss: 708.729492 [  550/ 1682]\n",
      "loss: 823.371887 [  560/ 1682]\n",
      "loss: 676.917175 [  570/ 1682]\n",
      "loss: 478.665466 [  580/ 1682]\n",
      "loss: 636.380859 [  590/ 1682]\n",
      "loss: 491.958496 [  600/ 1682]\n",
      "loss: 452.605560 [  610/ 1682]\n",
      "loss: 491.351501 [  620/ 1682]\n",
      "loss: 400.154694 [  630/ 1682]\n",
      "loss: 310.433777 [  640/ 1682]\n",
      "loss: 246.205078 [  650/ 1682]\n",
      "loss: 183.371506 [  660/ 1682]\n",
      "loss: 249.672150 [  670/ 1682]\n",
      "loss: 193.513458 [  680/ 1682]\n",
      "loss: 222.217697 [  690/ 1682]\n",
      "loss: 198.507339 [  700/ 1682]\n",
      "loss: 364.854004 [  710/ 1682]\n",
      "loss: 522.936340 [  720/ 1682]\n",
      "loss: 674.880554 [  730/ 1682]\n",
      "loss: 921.115356 [  740/ 1682]\n",
      "loss: 898.589844 [  750/ 1682]\n",
      "loss: 869.502563 [  760/ 1682]\n",
      "loss: 542.400513 [  770/ 1682]\n",
      "loss: 556.028381 [  780/ 1682]\n",
      "loss: 498.417419 [  790/ 1682]\n",
      "loss: 387.834167 [  800/ 1682]\n",
      "loss: 314.114716 [  810/ 1682]\n",
      "loss: 290.690796 [  820/ 1682]\n",
      "loss: 317.844971 [  830/ 1682]\n",
      "loss: 512.438354 [  840/ 1682]\n",
      "loss: 452.136230 [  850/ 1682]\n",
      "loss: 338.245056 [  860/ 1682]\n",
      "loss: 248.301437 [  870/ 1682]\n",
      "loss: 240.911377 [  880/ 1682]\n",
      "loss: 343.234589 [  890/ 1682]\n",
      "loss: 279.317810 [  900/ 1682]\n",
      "loss: 218.066116 [  910/ 1682]\n",
      "loss: 143.342438 [  920/ 1682]\n",
      "loss: 158.206772 [  930/ 1682]\n",
      "loss: 112.112991 [  940/ 1682]\n",
      "loss: 28.614182 [  950/ 1682]\n",
      "loss: 1.344231 [  960/ 1682]\n",
      "loss: 12.896042 [  970/ 1682]\n",
      "loss: 10.169543 [  980/ 1682]\n",
      "loss: 32.398380 [  990/ 1682]\n",
      "loss: 62.944344 [ 1000/ 1682]\n",
      "loss: 120.854630 [ 1010/ 1682]\n",
      "loss: 169.243820 [ 1020/ 1682]\n",
      "loss: 59.059162 [ 1030/ 1682]\n",
      "loss: 36.900890 [ 1040/ 1682]\n",
      "loss: 37.738457 [ 1050/ 1682]\n",
      "loss: 17.896147 [ 1060/ 1682]\n",
      "loss: 16.537418 [ 1070/ 1682]\n",
      "loss: 93.931702 [ 1080/ 1682]\n",
      "loss: 138.948837 [ 1090/ 1682]\n",
      "loss: 231.983566 [ 1100/ 1682]\n",
      "loss: 444.632385 [ 1110/ 1682]\n",
      "loss: 606.228760 [ 1120/ 1682]\n",
      "loss: 812.714661 [ 1130/ 1682]\n",
      "loss: 1194.086182 [ 1140/ 1682]\n",
      "loss: 2325.836426 [ 1150/ 1682]\n",
      "loss: 3471.845215 [ 1160/ 1682]\n",
      "loss: 1848.390869 [ 1170/ 1682]\n",
      "loss: 1884.315063 [ 1180/ 1682]\n",
      "loss: 2533.390869 [ 1190/ 1682]\n",
      "loss: 2134.655762 [ 1200/ 1682]\n",
      "loss: 2530.319824 [ 1210/ 1682]\n",
      "loss: 2365.764160 [ 1220/ 1682]\n",
      "loss: 2913.320801 [ 1230/ 1682]\n",
      "loss: 3964.077637 [ 1240/ 1682]\n",
      "loss: 3897.087402 [ 1250/ 1682]\n",
      "loss: 4783.066895 [ 1260/ 1682]\n",
      "loss: 4448.751465 [ 1270/ 1682]\n",
      "loss: 3515.507324 [ 1280/ 1682]\n",
      "loss: 2959.496338 [ 1290/ 1682]\n",
      "loss: 2992.798096 [ 1300/ 1682]\n",
      "loss: 3658.703125 [ 1310/ 1682]\n",
      "loss: 4415.145508 [ 1320/ 1682]\n",
      "loss: 3730.092285 [ 1330/ 1682]\n",
      "loss: 3444.004639 [ 1340/ 1682]\n",
      "loss: 3426.166504 [ 1350/ 1682]\n",
      "loss: 4007.036621 [ 1360/ 1682]\n",
      "loss: 4836.230469 [ 1370/ 1682]\n",
      "loss: 6027.105469 [ 1380/ 1682]\n",
      "loss: 6286.048828 [ 1390/ 1682]\n",
      "loss: 6188.408691 [ 1400/ 1682]\n",
      "loss: 6291.003418 [ 1410/ 1682]\n",
      "loss: 6874.268555 [ 1420/ 1682]\n",
      "loss: 5768.562012 [ 1430/ 1682]\n",
      "loss: 5493.390625 [ 1440/ 1682]\n",
      "loss: 6813.065430 [ 1450/ 1682]\n",
      "loss: 6670.803223 [ 1460/ 1682]\n",
      "loss: 8280.542969 [ 1470/ 1682]\n",
      "loss: 10096.900391 [ 1480/ 1682]\n",
      "loss: 10942.923828 [ 1490/ 1682]\n",
      "loss: 11596.664062 [ 1500/ 1682]\n",
      "loss: 9461.332031 [ 1510/ 1682]\n",
      "loss: 10798.628906 [ 1520/ 1682]\n",
      "loss: 9665.088867 [ 1530/ 1682]\n",
      "loss: 9081.226562 [ 1540/ 1682]\n",
      "loss: 8842.899414 [ 1550/ 1682]\n",
      "loss: 11371.641602 [ 1560/ 1682]\n",
      "loss: 9358.443359 [ 1570/ 1682]\n",
      "loss: 8301.067383 [ 1580/ 1682]\n",
      "loss: 6302.731445 [ 1590/ 1682]\n",
      "loss: 5815.003906 [ 1600/ 1682]\n",
      "loss: 5104.541992 [ 1610/ 1682]\n",
      "loss: 4832.245117 [ 1620/ 1682]\n",
      "loss: 6468.198242 [ 1630/ 1682]\n",
      "loss: 7489.877441 [ 1640/ 1682]\n",
      "loss: 9565.276367 [ 1650/ 1682]\n",
      "loss: 9203.174805 [ 1660/ 1682]\n",
      "loss: 7856.266602 [ 1670/ 1682]\n",
      "loss: 6301.982422 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9012.939115 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 2133.362305 [    0/ 1682]\n",
      "loss: 2031.979492 [   10/ 1682]\n",
      "loss: 1883.362305 [   20/ 1682]\n",
      "loss: 2270.637207 [   30/ 1682]\n",
      "loss: 2346.868896 [   40/ 1682]\n",
      "loss: 2705.189697 [   50/ 1682]\n",
      "loss: 1610.131714 [   60/ 1682]\n",
      "loss: 1892.433960 [   70/ 1682]\n",
      "loss: 2109.751953 [   80/ 1682]\n",
      "loss: 2016.492798 [   90/ 1682]\n",
      "loss: 1985.514404 [  100/ 1682]\n",
      "loss: 2181.895996 [  110/ 1682]\n",
      "loss: 2329.042480 [  120/ 1682]\n",
      "loss: 1808.655518 [  130/ 1682]\n",
      "loss: 1962.680420 [  140/ 1682]\n",
      "loss: 1892.549561 [  150/ 1682]\n",
      "loss: 1794.074951 [  160/ 1682]\n",
      "loss: 1859.253174 [  170/ 1682]\n",
      "loss: 1788.831665 [  180/ 1682]\n",
      "loss: 1593.302368 [  190/ 1682]\n",
      "loss: 2054.112305 [  200/ 1682]\n",
      "loss: 1740.976929 [  210/ 1682]\n",
      "loss: 1824.005249 [  220/ 1682]\n",
      "loss: 1637.248657 [  230/ 1682]\n",
      "loss: 1810.549561 [  240/ 1682]\n",
      "loss: 1927.924561 [  250/ 1682]\n",
      "loss: 1487.719604 [  260/ 1682]\n",
      "loss: 1180.196167 [  270/ 1682]\n",
      "loss: 1210.476807 [  280/ 1682]\n",
      "loss: 1496.157715 [  290/ 1682]\n",
      "loss: 1032.565796 [  300/ 1682]\n",
      "loss: 1113.809326 [  310/ 1682]\n",
      "loss: 1304.955933 [  320/ 1682]\n",
      "loss: 1168.486572 [  330/ 1682]\n",
      "loss: 1023.929504 [  340/ 1682]\n",
      "loss: 1042.676758 [  350/ 1682]\n",
      "loss: 1153.211060 [  360/ 1682]\n",
      "loss: 1071.744507 [  370/ 1682]\n",
      "loss: 1046.537354 [  380/ 1682]\n",
      "loss: 991.081726 [  390/ 1682]\n",
      "loss: 759.766907 [  400/ 1682]\n",
      "loss: 1117.241821 [  410/ 1682]\n",
      "loss: 1024.173584 [  420/ 1682]\n",
      "loss: 906.157532 [  430/ 1682]\n",
      "loss: 878.442078 [  440/ 1682]\n",
      "loss: 1008.238281 [  450/ 1682]\n",
      "loss: 713.718140 [  460/ 1682]\n",
      "loss: 656.668091 [  470/ 1682]\n",
      "loss: 722.215454 [  480/ 1682]\n",
      "loss: 855.236816 [  490/ 1682]\n",
      "loss: 670.056152 [  500/ 1682]\n",
      "loss: 756.148865 [  510/ 1682]\n",
      "loss: 911.126465 [  520/ 1682]\n",
      "loss: 799.381653 [  530/ 1682]\n",
      "loss: 638.164062 [  540/ 1682]\n",
      "loss: 678.665344 [  550/ 1682]\n",
      "loss: 748.786499 [  560/ 1682]\n",
      "loss: 695.364319 [  570/ 1682]\n",
      "loss: 606.252991 [  580/ 1682]\n",
      "loss: 577.680420 [  590/ 1682]\n",
      "loss: 565.098450 [  600/ 1682]\n",
      "loss: 516.804993 [  610/ 1682]\n",
      "loss: 511.206482 [  620/ 1682]\n",
      "loss: 556.026733 [  630/ 1682]\n",
      "loss: 400.633636 [  640/ 1682]\n",
      "loss: 266.595917 [  650/ 1682]\n",
      "loss: 170.473969 [  660/ 1682]\n",
      "loss: 237.121170 [  670/ 1682]\n",
      "loss: 207.894043 [  680/ 1682]\n",
      "loss: 279.357117 [  690/ 1682]\n",
      "loss: 204.939697 [  700/ 1682]\n",
      "loss: 302.896088 [  710/ 1682]\n",
      "loss: 586.703369 [  720/ 1682]\n",
      "loss: 643.915405 [  730/ 1682]\n",
      "loss: 1011.991516 [  740/ 1682]\n",
      "loss: 867.397339 [  750/ 1682]\n",
      "loss: 949.106750 [  760/ 1682]\n",
      "loss: 555.350220 [  770/ 1682]\n",
      "loss: 620.006897 [  780/ 1682]\n",
      "loss: 557.243713 [  790/ 1682]\n",
      "loss: 440.052582 [  800/ 1682]\n",
      "loss: 393.318665 [  810/ 1682]\n",
      "loss: 306.803467 [  820/ 1682]\n",
      "loss: 338.974701 [  830/ 1682]\n",
      "loss: 455.031250 [  840/ 1682]\n",
      "loss: 513.311646 [  850/ 1682]\n",
      "loss: 391.332672 [  860/ 1682]\n",
      "loss: 320.354309 [  870/ 1682]\n",
      "loss: 222.687454 [  880/ 1682]\n",
      "loss: 432.529785 [  890/ 1682]\n",
      "loss: 242.881500 [  900/ 1682]\n",
      "loss: 313.978058 [  910/ 1682]\n",
      "loss: 196.291443 [  920/ 1682]\n",
      "loss: 210.957489 [  930/ 1682]\n",
      "loss: 66.865379 [  940/ 1682]\n",
      "loss: 45.849041 [  950/ 1682]\n",
      "loss: 7.633286 [  960/ 1682]\n",
      "loss: 8.089031 [  970/ 1682]\n",
      "loss: 10.222499 [  980/ 1682]\n",
      "loss: 35.261345 [  990/ 1682]\n",
      "loss: 94.831970 [ 1000/ 1682]\n",
      "loss: 127.761086 [ 1010/ 1682]\n",
      "loss: 164.285599 [ 1020/ 1682]\n",
      "loss: 72.119614 [ 1030/ 1682]\n",
      "loss: 17.380194 [ 1040/ 1682]\n",
      "loss: 53.007008 [ 1050/ 1682]\n",
      "loss: 27.972321 [ 1060/ 1682]\n",
      "loss: 14.448774 [ 1070/ 1682]\n",
      "loss: 73.179863 [ 1080/ 1682]\n",
      "loss: 155.771057 [ 1090/ 1682]\n",
      "loss: 270.593262 [ 1100/ 1682]\n",
      "loss: 455.585449 [ 1110/ 1682]\n",
      "loss: 617.710693 [ 1120/ 1682]\n",
      "loss: 794.859497 [ 1130/ 1682]\n",
      "loss: 1427.509888 [ 1140/ 1682]\n",
      "loss: 2241.039551 [ 1150/ 1682]\n",
      "loss: 3282.609863 [ 1160/ 1682]\n",
      "loss: 2043.447266 [ 1170/ 1682]\n",
      "loss: 1966.589111 [ 1180/ 1682]\n",
      "loss: 2503.320557 [ 1190/ 1682]\n",
      "loss: 1981.929932 [ 1200/ 1682]\n",
      "loss: 2422.111572 [ 1210/ 1682]\n",
      "loss: 2464.621094 [ 1220/ 1682]\n",
      "loss: 2885.598389 [ 1230/ 1682]\n",
      "loss: 3931.774658 [ 1240/ 1682]\n",
      "loss: 3768.478516 [ 1250/ 1682]\n",
      "loss: 4437.651855 [ 1260/ 1682]\n",
      "loss: 4606.799805 [ 1270/ 1682]\n",
      "loss: 3558.972656 [ 1280/ 1682]\n",
      "loss: 2924.574219 [ 1290/ 1682]\n",
      "loss: 2801.522217 [ 1300/ 1682]\n",
      "loss: 3434.872559 [ 1310/ 1682]\n",
      "loss: 4380.778809 [ 1320/ 1682]\n",
      "loss: 3896.730469 [ 1330/ 1682]\n",
      "loss: 3326.646484 [ 1340/ 1682]\n",
      "loss: 3210.544434 [ 1350/ 1682]\n",
      "loss: 3971.306152 [ 1360/ 1682]\n",
      "loss: 4772.128906 [ 1370/ 1682]\n",
      "loss: 6000.087891 [ 1380/ 1682]\n",
      "loss: 6375.404297 [ 1390/ 1682]\n",
      "loss: 6419.008789 [ 1400/ 1682]\n",
      "loss: 6668.322754 [ 1410/ 1682]\n",
      "loss: 6979.319336 [ 1420/ 1682]\n",
      "loss: 5727.540527 [ 1430/ 1682]\n",
      "loss: 5713.651367 [ 1440/ 1682]\n",
      "loss: 6198.782715 [ 1450/ 1682]\n",
      "loss: 6196.238281 [ 1460/ 1682]\n",
      "loss: 7894.606445 [ 1470/ 1682]\n",
      "loss: 10227.501953 [ 1480/ 1682]\n",
      "loss: 11332.820312 [ 1490/ 1682]\n",
      "loss: 11132.886719 [ 1500/ 1682]\n",
      "loss: 9390.093750 [ 1510/ 1682]\n",
      "loss: 10726.694336 [ 1520/ 1682]\n",
      "loss: 9846.714844 [ 1530/ 1682]\n",
      "loss: 8242.354492 [ 1540/ 1682]\n",
      "loss: 8803.383789 [ 1550/ 1682]\n",
      "loss: 11557.384766 [ 1560/ 1682]\n",
      "loss: 9530.411133 [ 1570/ 1682]\n",
      "loss: 7829.523438 [ 1580/ 1682]\n",
      "loss: 5558.925781 [ 1590/ 1682]\n",
      "loss: 5955.438965 [ 1600/ 1682]\n",
      "loss: 4733.060059 [ 1610/ 1682]\n",
      "loss: 5142.792969 [ 1620/ 1682]\n",
      "loss: 6420.451172 [ 1630/ 1682]\n",
      "loss: 8114.002930 [ 1640/ 1682]\n",
      "loss: 9556.154297 [ 1650/ 1682]\n",
      "loss: 9409.794922 [ 1660/ 1682]\n",
      "loss: 7358.212402 [ 1670/ 1682]\n",
      "loss: 6259.447266 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8997.416335 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 2150.069580 [    0/ 1682]\n",
      "loss: 2941.421143 [   10/ 1682]\n",
      "loss: 1906.851562 [   20/ 1682]\n",
      "loss: 2292.063965 [   30/ 1682]\n",
      "loss: 1902.644775 [   40/ 1682]\n",
      "loss: 2259.035400 [   50/ 1682]\n",
      "loss: 2062.985596 [   60/ 1682]\n",
      "loss: 1915.059204 [   70/ 1682]\n",
      "loss: 2120.924561 [   80/ 1682]\n",
      "loss: 2039.340576 [   90/ 1682]\n",
      "loss: 2001.622803 [  100/ 1682]\n",
      "loss: 2513.789062 [  110/ 1682]\n",
      "loss: 2035.106812 [  120/ 1682]\n",
      "loss: 2438.477295 [  130/ 1682]\n",
      "loss: 1968.399170 [  140/ 1682]\n",
      "loss: 1890.038696 [  150/ 1682]\n",
      "loss: 1671.620361 [  160/ 1682]\n",
      "loss: 2026.955078 [  170/ 1682]\n",
      "loss: 1797.069702 [  180/ 1682]\n",
      "loss: 1731.567017 [  190/ 1682]\n",
      "loss: 1526.517822 [  200/ 1682]\n",
      "loss: 1753.752319 [  210/ 1682]\n",
      "loss: 1711.595947 [  220/ 1682]\n",
      "loss: 1785.770264 [  230/ 1682]\n",
      "loss: 1578.791992 [  240/ 1682]\n",
      "loss: 1693.287476 [  250/ 1682]\n",
      "loss: 1496.957764 [  260/ 1682]\n",
      "loss: 1312.915405 [  270/ 1682]\n",
      "loss: 1223.168213 [  280/ 1682]\n",
      "loss: 1406.849609 [  290/ 1682]\n",
      "loss: 1361.841431 [  300/ 1682]\n",
      "loss: 1124.541260 [  310/ 1682]\n",
      "loss: 1527.893799 [  320/ 1682]\n",
      "loss: 1368.218628 [  330/ 1682]\n",
      "loss: 1038.114990 [  340/ 1682]\n",
      "loss: 1245.040771 [  350/ 1682]\n",
      "loss: 977.151001 [  360/ 1682]\n",
      "loss: 1179.322021 [  370/ 1682]\n",
      "loss: 1154.884521 [  380/ 1682]\n",
      "loss: 1015.415894 [  390/ 1682]\n",
      "loss: 774.532288 [  400/ 1682]\n",
      "loss: 813.655457 [  410/ 1682]\n",
      "loss: 1194.856445 [  420/ 1682]\n",
      "loss: 1164.884277 [  430/ 1682]\n",
      "loss: 892.374146 [  440/ 1682]\n",
      "loss: 801.370789 [  450/ 1682]\n",
      "loss: 791.039734 [  460/ 1682]\n",
      "loss: 740.629272 [  470/ 1682]\n",
      "loss: 735.997009 [  480/ 1682]\n",
      "loss: 670.422729 [  490/ 1682]\n",
      "loss: 743.497192 [  500/ 1682]\n",
      "loss: 702.338318 [  510/ 1682]\n",
      "loss: 712.545593 [  520/ 1682]\n",
      "loss: 871.033325 [  530/ 1682]\n",
      "loss: 646.145447 [  540/ 1682]\n",
      "loss: 815.372620 [  550/ 1682]\n",
      "loss: 762.357605 [  560/ 1682]\n",
      "loss: 774.727173 [  570/ 1682]\n",
      "loss: 510.027344 [  580/ 1682]\n",
      "loss: 484.583893 [  590/ 1682]\n",
      "loss: 480.267090 [  600/ 1682]\n",
      "loss: 479.771088 [  610/ 1682]\n",
      "loss: 520.669861 [  620/ 1682]\n",
      "loss: 471.903137 [  630/ 1682]\n",
      "loss: 381.232056 [  640/ 1682]\n",
      "loss: 240.048340 [  650/ 1682]\n",
      "loss: 240.398438 [  660/ 1682]\n",
      "loss: 153.518341 [  670/ 1682]\n",
      "loss: 248.674759 [  680/ 1682]\n",
      "loss: 193.061935 [  690/ 1682]\n",
      "loss: 213.894684 [  700/ 1682]\n",
      "loss: 351.385406 [  710/ 1682]\n",
      "loss: 553.688416 [  720/ 1682]\n",
      "loss: 712.973022 [  730/ 1682]\n",
      "loss: 798.959106 [  740/ 1682]\n",
      "loss: 886.027161 [  750/ 1682]\n",
      "loss: 909.580750 [  760/ 1682]\n",
      "loss: 718.148071 [  770/ 1682]\n",
      "loss: 682.244019 [  780/ 1682]\n",
      "loss: 710.899780 [  790/ 1682]\n",
      "loss: 455.584473 [  800/ 1682]\n",
      "loss: 451.745514 [  810/ 1682]\n",
      "loss: 284.553192 [  820/ 1682]\n",
      "loss: 461.048889 [  830/ 1682]\n",
      "loss: 471.918152 [  840/ 1682]\n",
      "loss: 442.103363 [  850/ 1682]\n",
      "loss: 332.876160 [  860/ 1682]\n",
      "loss: 299.579163 [  870/ 1682]\n",
      "loss: 290.591064 [  880/ 1682]\n",
      "loss: 357.043060 [  890/ 1682]\n",
      "loss: 219.291534 [  900/ 1682]\n",
      "loss: 324.025391 [  910/ 1682]\n",
      "loss: 160.354584 [  920/ 1682]\n",
      "loss: 149.736908 [  930/ 1682]\n",
      "loss: 130.268280 [  940/ 1682]\n",
      "loss: 40.805428 [  950/ 1682]\n",
      "loss: 2.400521 [  960/ 1682]\n",
      "loss: 1.276058 [  970/ 1682]\n",
      "loss: 8.703387 [  980/ 1682]\n",
      "loss: 25.991104 [  990/ 1682]\n",
      "loss: 89.639618 [ 1000/ 1682]\n",
      "loss: 124.318214 [ 1010/ 1682]\n",
      "loss: 184.547241 [ 1020/ 1682]\n",
      "loss: 37.465981 [ 1030/ 1682]\n",
      "loss: 33.460560 [ 1040/ 1682]\n",
      "loss: 32.175343 [ 1050/ 1682]\n",
      "loss: 36.737263 [ 1060/ 1682]\n",
      "loss: 15.035747 [ 1070/ 1682]\n",
      "loss: 38.269226 [ 1080/ 1682]\n",
      "loss: 149.246857 [ 1090/ 1682]\n",
      "loss: 217.601410 [ 1100/ 1682]\n",
      "loss: 422.393494 [ 1110/ 1682]\n",
      "loss: 631.559204 [ 1120/ 1682]\n",
      "loss: 816.620972 [ 1130/ 1682]\n",
      "loss: 1359.908203 [ 1140/ 1682]\n",
      "loss: 2191.879395 [ 1150/ 1682]\n",
      "loss: 3321.528076 [ 1160/ 1682]\n",
      "loss: 2018.041016 [ 1170/ 1682]\n",
      "loss: 2058.348633 [ 1180/ 1682]\n",
      "loss: 2536.189453 [ 1190/ 1682]\n",
      "loss: 2016.024658 [ 1200/ 1682]\n",
      "loss: 2393.980957 [ 1210/ 1682]\n",
      "loss: 2578.507812 [ 1220/ 1682]\n",
      "loss: 2925.610352 [ 1230/ 1682]\n",
      "loss: 3887.155518 [ 1240/ 1682]\n",
      "loss: 3648.666748 [ 1250/ 1682]\n",
      "loss: 4416.999023 [ 1260/ 1682]\n",
      "loss: 4362.562012 [ 1270/ 1682]\n",
      "loss: 3528.340332 [ 1280/ 1682]\n",
      "loss: 2818.722656 [ 1290/ 1682]\n",
      "loss: 2849.074707 [ 1300/ 1682]\n",
      "loss: 3687.263184 [ 1310/ 1682]\n",
      "loss: 4442.371094 [ 1320/ 1682]\n",
      "loss: 3851.053223 [ 1330/ 1682]\n",
      "loss: 3191.532715 [ 1340/ 1682]\n",
      "loss: 3355.868652 [ 1350/ 1682]\n",
      "loss: 3630.032471 [ 1360/ 1682]\n",
      "loss: 4725.172852 [ 1370/ 1682]\n",
      "loss: 5822.198242 [ 1380/ 1682]\n",
      "loss: 5797.318359 [ 1390/ 1682]\n",
      "loss: 6236.977051 [ 1400/ 1682]\n",
      "loss: 6613.871094 [ 1410/ 1682]\n",
      "loss: 6918.836914 [ 1420/ 1682]\n",
      "loss: 5281.651855 [ 1430/ 1682]\n",
      "loss: 5260.073242 [ 1440/ 1682]\n",
      "loss: 6427.138184 [ 1450/ 1682]\n",
      "loss: 6121.270996 [ 1460/ 1682]\n",
      "loss: 7495.919434 [ 1470/ 1682]\n",
      "loss: 9735.264648 [ 1480/ 1682]\n",
      "loss: 11023.197266 [ 1490/ 1682]\n",
      "loss: 11245.666016 [ 1500/ 1682]\n",
      "loss: 9297.551758 [ 1510/ 1682]\n",
      "loss: 10644.012695 [ 1520/ 1682]\n",
      "loss: 9558.440430 [ 1530/ 1682]\n",
      "loss: 8960.230469 [ 1540/ 1682]\n",
      "loss: 8054.538086 [ 1550/ 1682]\n",
      "loss: 11227.232422 [ 1560/ 1682]\n",
      "loss: 9433.234375 [ 1570/ 1682]\n",
      "loss: 8385.120117 [ 1580/ 1682]\n",
      "loss: 5458.387695 [ 1590/ 1682]\n",
      "loss: 6066.381836 [ 1600/ 1682]\n",
      "loss: 4667.694336 [ 1610/ 1682]\n",
      "loss: 4915.748047 [ 1620/ 1682]\n",
      "loss: 5956.824219 [ 1630/ 1682]\n",
      "loss: 8049.829102 [ 1640/ 1682]\n",
      "loss: 9962.629883 [ 1650/ 1682]\n",
      "loss: 9045.751953 [ 1660/ 1682]\n",
      "loss: 7506.317383 [ 1670/ 1682]\n",
      "loss: 7351.539062 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8914.664100 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1883.573486 [    0/ 1682]\n",
      "loss: 2078.402588 [   10/ 1682]\n",
      "loss: 2270.945312 [   20/ 1682]\n",
      "loss: 2196.495605 [   30/ 1682]\n",
      "loss: 2278.394043 [   40/ 1682]\n",
      "loss: 2339.072021 [   50/ 1682]\n",
      "loss: 2137.632080 [   60/ 1682]\n",
      "loss: 1960.900635 [   70/ 1682]\n",
      "loss: 2866.165527 [   80/ 1682]\n",
      "loss: 2082.661621 [   90/ 1682]\n",
      "loss: 2900.046631 [  100/ 1682]\n",
      "loss: 3100.067383 [  110/ 1682]\n",
      "loss: 2234.741699 [  120/ 1682]\n",
      "loss: 2018.412109 [  130/ 1682]\n",
      "loss: 2318.942871 [  140/ 1682]\n",
      "loss: 1627.895264 [  150/ 1682]\n",
      "loss: 1995.145142 [  160/ 1682]\n",
      "loss: 1908.954834 [  170/ 1682]\n",
      "loss: 2110.245605 [  180/ 1682]\n",
      "loss: 1894.903687 [  190/ 1682]\n",
      "loss: 1814.363525 [  200/ 1682]\n",
      "loss: 1778.712524 [  210/ 1682]\n",
      "loss: 1998.973633 [  220/ 1682]\n",
      "loss: 2193.056641 [  230/ 1682]\n",
      "loss: 1474.298096 [  240/ 1682]\n",
      "loss: 1589.342529 [  250/ 1682]\n",
      "loss: 1757.979248 [  260/ 1682]\n",
      "loss: 1327.498169 [  270/ 1682]\n",
      "loss: 1454.460205 [  280/ 1682]\n",
      "loss: 1209.726807 [  290/ 1682]\n",
      "loss: 1265.898193 [  300/ 1682]\n",
      "loss: 1334.281616 [  310/ 1682]\n",
      "loss: 1136.427734 [  320/ 1682]\n",
      "loss: 1184.946411 [  330/ 1682]\n",
      "loss: 959.599792 [  340/ 1682]\n",
      "loss: 1072.441650 [  350/ 1682]\n",
      "loss: 1265.396240 [  360/ 1682]\n",
      "loss: 1191.386719 [  370/ 1682]\n",
      "loss: 1075.616455 [  380/ 1682]\n",
      "loss: 858.808960 [  390/ 1682]\n",
      "loss: 1102.914551 [  400/ 1682]\n",
      "loss: 825.564819 [  410/ 1682]\n",
      "loss: 1041.985596 [  420/ 1682]\n",
      "loss: 934.512085 [  430/ 1682]\n",
      "loss: 905.007324 [  440/ 1682]\n",
      "loss: 959.869629 [  450/ 1682]\n",
      "loss: 602.656860 [  460/ 1682]\n",
      "loss: 811.219482 [  470/ 1682]\n",
      "loss: 682.312683 [  480/ 1682]\n",
      "loss: 744.087219 [  490/ 1682]\n",
      "loss: 629.376282 [  500/ 1682]\n",
      "loss: 652.386353 [  510/ 1682]\n",
      "loss: 862.782043 [  520/ 1682]\n",
      "loss: 697.760864 [  530/ 1682]\n",
      "loss: 598.617126 [  540/ 1682]\n",
      "loss: 765.575073 [  550/ 1682]\n",
      "loss: 771.976440 [  560/ 1682]\n",
      "loss: 728.658203 [  570/ 1682]\n",
      "loss: 574.630005 [  580/ 1682]\n",
      "loss: 547.203674 [  590/ 1682]\n",
      "loss: 541.649780 [  600/ 1682]\n",
      "loss: 593.827759 [  610/ 1682]\n",
      "loss: 483.047211 [  620/ 1682]\n",
      "loss: 483.856689 [  630/ 1682]\n",
      "loss: 303.944641 [  640/ 1682]\n",
      "loss: 284.140472 [  650/ 1682]\n",
      "loss: 246.787476 [  660/ 1682]\n",
      "loss: 292.254089 [  670/ 1682]\n",
      "loss: 165.199188 [  680/ 1682]\n",
      "loss: 291.733734 [  690/ 1682]\n",
      "loss: 317.993805 [  700/ 1682]\n",
      "loss: 410.172058 [  710/ 1682]\n",
      "loss: 516.781616 [  720/ 1682]\n",
      "loss: 671.754761 [  730/ 1682]\n",
      "loss: 989.461304 [  740/ 1682]\n",
      "loss: 900.436340 [  750/ 1682]\n",
      "loss: 935.561646 [  760/ 1682]\n",
      "loss: 729.239563 [  770/ 1682]\n",
      "loss: 646.641846 [  780/ 1682]\n",
      "loss: 534.595459 [  790/ 1682]\n",
      "loss: 383.405853 [  800/ 1682]\n",
      "loss: 496.245697 [  810/ 1682]\n",
      "loss: 392.372284 [  820/ 1682]\n",
      "loss: 288.795990 [  830/ 1682]\n",
      "loss: 646.686340 [  840/ 1682]\n",
      "loss: 450.850983 [  850/ 1682]\n",
      "loss: 416.620026 [  860/ 1682]\n",
      "loss: 341.695099 [  870/ 1682]\n",
      "loss: 301.413666 [  880/ 1682]\n",
      "loss: 292.699310 [  890/ 1682]\n",
      "loss: 258.563202 [  900/ 1682]\n",
      "loss: 247.348785 [  910/ 1682]\n",
      "loss: 190.954071 [  920/ 1682]\n",
      "loss: 175.562119 [  930/ 1682]\n",
      "loss: 130.040268 [  940/ 1682]\n",
      "loss: 54.064262 [  950/ 1682]\n",
      "loss: 10.390352 [  960/ 1682]\n",
      "loss: 8.695005 [  970/ 1682]\n",
      "loss: 3.782084 [  980/ 1682]\n",
      "loss: 24.090908 [  990/ 1682]\n",
      "loss: 77.199394 [ 1000/ 1682]\n",
      "loss: 115.983177 [ 1010/ 1682]\n",
      "loss: 135.915039 [ 1020/ 1682]\n",
      "loss: 65.616043 [ 1030/ 1682]\n",
      "loss: 14.511783 [ 1040/ 1682]\n",
      "loss: 65.983620 [ 1050/ 1682]\n",
      "loss: 41.283558 [ 1060/ 1682]\n",
      "loss: 12.289694 [ 1070/ 1682]\n",
      "loss: 76.048073 [ 1080/ 1682]\n",
      "loss: 110.431541 [ 1090/ 1682]\n",
      "loss: 235.781082 [ 1100/ 1682]\n",
      "loss: 412.115417 [ 1110/ 1682]\n",
      "loss: 614.476562 [ 1120/ 1682]\n",
      "loss: 761.869263 [ 1130/ 1682]\n",
      "loss: 1319.856201 [ 1140/ 1682]\n",
      "loss: 2125.988770 [ 1150/ 1682]\n",
      "loss: 3374.512207 [ 1160/ 1682]\n",
      "loss: 1882.611572 [ 1170/ 1682]\n",
      "loss: 2030.378174 [ 1180/ 1682]\n",
      "loss: 2451.365967 [ 1190/ 1682]\n",
      "loss: 2058.549072 [ 1200/ 1682]\n",
      "loss: 2290.706543 [ 1210/ 1682]\n",
      "loss: 2346.187012 [ 1220/ 1682]\n",
      "loss: 2894.424561 [ 1230/ 1682]\n",
      "loss: 4037.627441 [ 1240/ 1682]\n",
      "loss: 3702.952393 [ 1250/ 1682]\n",
      "loss: 4294.782715 [ 1260/ 1682]\n",
      "loss: 4238.069336 [ 1270/ 1682]\n",
      "loss: 3494.783936 [ 1280/ 1682]\n",
      "loss: 2710.011963 [ 1290/ 1682]\n",
      "loss: 2906.771240 [ 1300/ 1682]\n",
      "loss: 3559.187012 [ 1310/ 1682]\n",
      "loss: 4102.354492 [ 1320/ 1682]\n",
      "loss: 3628.935547 [ 1330/ 1682]\n",
      "loss: 3260.613525 [ 1340/ 1682]\n",
      "loss: 3236.545898 [ 1350/ 1682]\n",
      "loss: 3804.520264 [ 1360/ 1682]\n",
      "loss: 4934.959961 [ 1370/ 1682]\n",
      "loss: 5776.922852 [ 1380/ 1682]\n",
      "loss: 5897.075195 [ 1390/ 1682]\n",
      "loss: 6195.657715 [ 1400/ 1682]\n",
      "loss: 6573.032227 [ 1410/ 1682]\n",
      "loss: 7030.877930 [ 1420/ 1682]\n",
      "loss: 5506.928223 [ 1430/ 1682]\n",
      "loss: 5505.211914 [ 1440/ 1682]\n",
      "loss: 6389.113281 [ 1450/ 1682]\n",
      "loss: 6541.470703 [ 1460/ 1682]\n",
      "loss: 7621.234375 [ 1470/ 1682]\n",
      "loss: 9922.723633 [ 1480/ 1682]\n",
      "loss: 11608.109375 [ 1490/ 1682]\n",
      "loss: 11438.273438 [ 1500/ 1682]\n",
      "loss: 9677.541016 [ 1510/ 1682]\n",
      "loss: 10644.658203 [ 1520/ 1682]\n",
      "loss: 9945.793945 [ 1530/ 1682]\n",
      "loss: 9130.456055 [ 1540/ 1682]\n",
      "loss: 9091.203125 [ 1550/ 1682]\n",
      "loss: 11226.154297 [ 1560/ 1682]\n",
      "loss: 9641.809570 [ 1570/ 1682]\n",
      "loss: 7976.227539 [ 1580/ 1682]\n",
      "loss: 6001.489258 [ 1590/ 1682]\n",
      "loss: 5913.073242 [ 1600/ 1682]\n",
      "loss: 4845.147949 [ 1610/ 1682]\n",
      "loss: 4920.038086 [ 1620/ 1682]\n",
      "loss: 6338.793945 [ 1630/ 1682]\n",
      "loss: 7614.867188 [ 1640/ 1682]\n",
      "loss: 10223.731445 [ 1650/ 1682]\n",
      "loss: 8677.644531 [ 1660/ 1682]\n",
      "loss: 7341.545898 [ 1670/ 1682]\n",
      "loss: 7308.347656 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9036.674241 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 2300.710938 [    0/ 1682]\n",
      "loss: 2078.294434 [   10/ 1682]\n",
      "loss: 2224.095459 [   20/ 1682]\n",
      "loss: 2290.332764 [   30/ 1682]\n",
      "loss: 1931.362305 [   40/ 1682]\n",
      "loss: 1843.175049 [   50/ 1682]\n",
      "loss: 2064.774658 [   60/ 1682]\n",
      "loss: 1808.616577 [   70/ 1682]\n",
      "loss: 2429.318115 [   80/ 1682]\n",
      "loss: 2339.363281 [   90/ 1682]\n",
      "loss: 2165.255859 [  100/ 1682]\n",
      "loss: 2352.050537 [  110/ 1682]\n",
      "loss: 2202.900391 [  120/ 1682]\n",
      "loss: 2010.467773 [  130/ 1682]\n",
      "loss: 2366.479980 [  140/ 1682]\n",
      "loss: 1773.074585 [  150/ 1682]\n",
      "loss: 2086.878418 [  160/ 1682]\n",
      "loss: 2131.366455 [  170/ 1682]\n",
      "loss: 1686.218018 [  180/ 1682]\n",
      "loss: 1740.801758 [  190/ 1682]\n",
      "loss: 1906.183594 [  200/ 1682]\n",
      "loss: 2133.542969 [  210/ 1682]\n",
      "loss: 1736.602295 [  220/ 1682]\n",
      "loss: 1790.218994 [  230/ 1682]\n",
      "loss: 1601.457886 [  240/ 1682]\n",
      "loss: 1591.812744 [  250/ 1682]\n",
      "loss: 1520.440186 [  260/ 1682]\n",
      "loss: 1331.114624 [  270/ 1682]\n",
      "loss: 1333.130615 [  280/ 1682]\n",
      "loss: 1213.408325 [  290/ 1682]\n",
      "loss: 1257.725098 [  300/ 1682]\n",
      "loss: 1228.234131 [  310/ 1682]\n",
      "loss: 1225.549561 [  320/ 1682]\n",
      "loss: 930.364075 [  330/ 1682]\n",
      "loss: 966.844727 [  340/ 1682]\n",
      "loss: 1142.068726 [  350/ 1682]\n",
      "loss: 1170.658203 [  360/ 1682]\n",
      "loss: 1269.585205 [  370/ 1682]\n",
      "loss: 997.276184 [  380/ 1682]\n",
      "loss: 1245.077393 [  390/ 1682]\n",
      "loss: 1014.421021 [  400/ 1682]\n",
      "loss: 965.024719 [  410/ 1682]\n",
      "loss: 1032.225830 [  420/ 1682]\n",
      "loss: 871.590637 [  430/ 1682]\n",
      "loss: 911.478394 [  440/ 1682]\n",
      "loss: 940.857056 [  450/ 1682]\n",
      "loss: 672.703918 [  460/ 1682]\n",
      "loss: 745.361450 [  470/ 1682]\n",
      "loss: 803.373413 [  480/ 1682]\n",
      "loss: 744.149170 [  490/ 1682]\n",
      "loss: 633.772827 [  500/ 1682]\n",
      "loss: 842.328613 [  510/ 1682]\n",
      "loss: 925.801392 [  520/ 1682]\n",
      "loss: 641.369141 [  530/ 1682]\n",
      "loss: 657.698853 [  540/ 1682]\n",
      "loss: 815.360962 [  550/ 1682]\n",
      "loss: 607.620239 [  560/ 1682]\n",
      "loss: 898.256226 [  570/ 1682]\n",
      "loss: 622.921509 [  580/ 1682]\n",
      "loss: 588.826050 [  590/ 1682]\n",
      "loss: 445.882507 [  600/ 1682]\n",
      "loss: 536.255981 [  610/ 1682]\n",
      "loss: 489.649261 [  620/ 1682]\n",
      "loss: 400.333710 [  630/ 1682]\n",
      "loss: 347.684021 [  640/ 1682]\n",
      "loss: 277.909485 [  650/ 1682]\n",
      "loss: 184.409058 [  660/ 1682]\n",
      "loss: 279.972076 [  670/ 1682]\n",
      "loss: 220.164581 [  680/ 1682]\n",
      "loss: 201.959030 [  690/ 1682]\n",
      "loss: 256.185608 [  700/ 1682]\n",
      "loss: 431.797913 [  710/ 1682]\n",
      "loss: 528.490112 [  720/ 1682]\n",
      "loss: 729.570435 [  730/ 1682]\n",
      "loss: 928.932129 [  740/ 1682]\n",
      "loss: 956.527161 [  750/ 1682]\n",
      "loss: 878.076782 [  760/ 1682]\n",
      "loss: 635.505859 [  770/ 1682]\n",
      "loss: 645.845886 [  780/ 1682]\n",
      "loss: 545.794067 [  790/ 1682]\n",
      "loss: 429.486816 [  800/ 1682]\n",
      "loss: 382.001068 [  810/ 1682]\n",
      "loss: 327.038635 [  820/ 1682]\n",
      "loss: 333.374939 [  830/ 1682]\n",
      "loss: 451.568542 [  840/ 1682]\n",
      "loss: 565.073364 [  850/ 1682]\n",
      "loss: 350.679504 [  860/ 1682]\n",
      "loss: 310.944153 [  870/ 1682]\n",
      "loss: 331.183868 [  880/ 1682]\n",
      "loss: 270.657867 [  890/ 1682]\n",
      "loss: 259.648743 [  900/ 1682]\n",
      "loss: 303.002686 [  910/ 1682]\n",
      "loss: 209.516266 [  920/ 1682]\n",
      "loss: 198.619293 [  930/ 1682]\n",
      "loss: 150.539215 [  940/ 1682]\n",
      "loss: 85.894394 [  950/ 1682]\n",
      "loss: 26.834997 [  960/ 1682]\n",
      "loss: 8.743383 [  970/ 1682]\n",
      "loss: 7.244679 [  980/ 1682]\n",
      "loss: 16.244175 [  990/ 1682]\n",
      "loss: 77.505157 [ 1000/ 1682]\n",
      "loss: 98.392517 [ 1010/ 1682]\n",
      "loss: 144.164658 [ 1020/ 1682]\n",
      "loss: 40.130920 [ 1030/ 1682]\n",
      "loss: 22.537384 [ 1040/ 1682]\n",
      "loss: 73.688110 [ 1050/ 1682]\n",
      "loss: 17.157810 [ 1060/ 1682]\n",
      "loss: 17.415955 [ 1070/ 1682]\n",
      "loss: 66.734062 [ 1080/ 1682]\n",
      "loss: 127.229820 [ 1090/ 1682]\n",
      "loss: 195.497940 [ 1100/ 1682]\n",
      "loss: 429.807678 [ 1110/ 1682]\n",
      "loss: 578.322998 [ 1120/ 1682]\n",
      "loss: 756.893372 [ 1130/ 1682]\n",
      "loss: 1337.488770 [ 1140/ 1682]\n",
      "loss: 2223.386230 [ 1150/ 1682]\n",
      "loss: 3351.417480 [ 1160/ 1682]\n",
      "loss: 2038.279541 [ 1170/ 1682]\n",
      "loss: 1908.565430 [ 1180/ 1682]\n",
      "loss: 2186.735107 [ 1190/ 1682]\n",
      "loss: 2099.483887 [ 1200/ 1682]\n",
      "loss: 2354.277588 [ 1210/ 1682]\n",
      "loss: 2336.724609 [ 1220/ 1682]\n",
      "loss: 2888.750244 [ 1230/ 1682]\n",
      "loss: 3846.359375 [ 1240/ 1682]\n",
      "loss: 3764.248535 [ 1250/ 1682]\n",
      "loss: 4464.170898 [ 1260/ 1682]\n",
      "loss: 4148.153320 [ 1270/ 1682]\n",
      "loss: 3234.479004 [ 1280/ 1682]\n",
      "loss: 2846.330566 [ 1290/ 1682]\n",
      "loss: 2886.941895 [ 1300/ 1682]\n",
      "loss: 3394.027344 [ 1310/ 1682]\n",
      "loss: 4094.899902 [ 1320/ 1682]\n",
      "loss: 3713.934082 [ 1330/ 1682]\n",
      "loss: 3248.386719 [ 1340/ 1682]\n",
      "loss: 3303.432861 [ 1350/ 1682]\n",
      "loss: 3789.979736 [ 1360/ 1682]\n",
      "loss: 5011.460449 [ 1370/ 1682]\n",
      "loss: 6006.609375 [ 1380/ 1682]\n",
      "loss: 6131.044922 [ 1390/ 1682]\n",
      "loss: 6185.452148 [ 1400/ 1682]\n",
      "loss: 6817.972656 [ 1410/ 1682]\n",
      "loss: 6721.914062 [ 1420/ 1682]\n",
      "loss: 5504.928223 [ 1430/ 1682]\n",
      "loss: 5124.388672 [ 1440/ 1682]\n",
      "loss: 6642.481445 [ 1450/ 1682]\n",
      "loss: 6528.487305 [ 1460/ 1682]\n",
      "loss: 7800.010254 [ 1470/ 1682]\n",
      "loss: 10267.379883 [ 1480/ 1682]\n",
      "loss: 11172.804688 [ 1490/ 1682]\n",
      "loss: 11428.139648 [ 1500/ 1682]\n",
      "loss: 9452.726562 [ 1510/ 1682]\n",
      "loss: 10784.620117 [ 1520/ 1682]\n",
      "loss: 9725.353516 [ 1530/ 1682]\n",
      "loss: 8541.880859 [ 1540/ 1682]\n",
      "loss: 8494.821289 [ 1550/ 1682]\n",
      "loss: 11429.173828 [ 1560/ 1682]\n",
      "loss: 9413.912109 [ 1570/ 1682]\n",
      "loss: 7969.856934 [ 1580/ 1682]\n",
      "loss: 5806.383301 [ 1590/ 1682]\n",
      "loss: 5720.559082 [ 1600/ 1682]\n",
      "loss: 4791.823242 [ 1610/ 1682]\n",
      "loss: 4621.202148 [ 1620/ 1682]\n",
      "loss: 6485.554199 [ 1630/ 1682]\n",
      "loss: 8187.088379 [ 1640/ 1682]\n",
      "loss: 9736.247070 [ 1650/ 1682]\n",
      "loss: 9125.072266 [ 1660/ 1682]\n",
      "loss: 7314.286133 [ 1670/ 1682]\n",
      "loss: 7264.659180 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8983.437303 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 2425.616943 [    0/ 1682]\n",
      "loss: 2347.489502 [   10/ 1682]\n",
      "loss: 2486.712402 [   20/ 1682]\n",
      "loss: 2032.353516 [   30/ 1682]\n",
      "loss: 2196.819824 [   40/ 1682]\n",
      "loss: 1981.382202 [   50/ 1682]\n",
      "loss: 1818.961182 [   60/ 1682]\n",
      "loss: 2083.703369 [   70/ 1682]\n",
      "loss: 2163.087646 [   80/ 1682]\n",
      "loss: 1951.494507 [   90/ 1682]\n",
      "loss: 2043.235107 [  100/ 1682]\n",
      "loss: 2215.194336 [  110/ 1682]\n",
      "loss: 2335.428223 [  120/ 1682]\n",
      "loss: 2148.458252 [  130/ 1682]\n",
      "loss: 2105.446777 [  140/ 1682]\n",
      "loss: 1904.401123 [  150/ 1682]\n",
      "loss: 2208.852783 [  160/ 1682]\n",
      "loss: 2007.565063 [  170/ 1682]\n",
      "loss: 1811.149414 [  180/ 1682]\n",
      "loss: 1858.141357 [  190/ 1682]\n",
      "loss: 1805.862061 [  200/ 1682]\n",
      "loss: 2013.703125 [  210/ 1682]\n",
      "loss: 1750.731689 [  220/ 1682]\n",
      "loss: 1688.263672 [  230/ 1682]\n",
      "loss: 1717.558228 [  240/ 1682]\n",
      "loss: 1505.424805 [  250/ 1682]\n",
      "loss: 1630.883423 [  260/ 1682]\n",
      "loss: 1252.405518 [  270/ 1682]\n",
      "loss: 1432.173096 [  280/ 1682]\n",
      "loss: 1311.479492 [  290/ 1682]\n",
      "loss: 1264.645264 [  300/ 1682]\n",
      "loss: 1317.622437 [  310/ 1682]\n",
      "loss: 1236.303833 [  320/ 1682]\n",
      "loss: 1096.936890 [  330/ 1682]\n",
      "loss: 975.143860 [  340/ 1682]\n",
      "loss: 1003.026062 [  350/ 1682]\n",
      "loss: 1025.687866 [  360/ 1682]\n",
      "loss: 1422.482300 [  370/ 1682]\n",
      "loss: 937.027954 [  380/ 1682]\n",
      "loss: 955.969360 [  390/ 1682]\n",
      "loss: 884.055298 [  400/ 1682]\n",
      "loss: 777.974426 [  410/ 1682]\n",
      "loss: 1174.055054 [  420/ 1682]\n",
      "loss: 1021.483582 [  430/ 1682]\n",
      "loss: 1183.064209 [  440/ 1682]\n",
      "loss: 832.372437 [  450/ 1682]\n",
      "loss: 684.895203 [  460/ 1682]\n",
      "loss: 809.570435 [  470/ 1682]\n",
      "loss: 642.534729 [  480/ 1682]\n",
      "loss: 697.274414 [  490/ 1682]\n",
      "loss: 592.253052 [  500/ 1682]\n",
      "loss: 844.639038 [  510/ 1682]\n",
      "loss: 810.162781 [  520/ 1682]\n",
      "loss: 753.830139 [  530/ 1682]\n",
      "loss: 567.197693 [  540/ 1682]\n",
      "loss: 878.633972 [  550/ 1682]\n",
      "loss: 670.058472 [  560/ 1682]\n",
      "loss: 745.851868 [  570/ 1682]\n",
      "loss: 576.982300 [  580/ 1682]\n",
      "loss: 596.255249 [  590/ 1682]\n",
      "loss: 579.880981 [  600/ 1682]\n",
      "loss: 593.157776 [  610/ 1682]\n",
      "loss: 668.923889 [  620/ 1682]\n",
      "loss: 449.310730 [  630/ 1682]\n",
      "loss: 389.968842 [  640/ 1682]\n",
      "loss: 253.948242 [  650/ 1682]\n",
      "loss: 239.750122 [  660/ 1682]\n",
      "loss: 254.850220 [  670/ 1682]\n",
      "loss: 175.493317 [  680/ 1682]\n",
      "loss: 255.817307 [  690/ 1682]\n",
      "loss: 286.367615 [  700/ 1682]\n",
      "loss: 511.372009 [  710/ 1682]\n",
      "loss: 663.253296 [  720/ 1682]\n",
      "loss: 731.742188 [  730/ 1682]\n",
      "loss: 983.624207 [  740/ 1682]\n",
      "loss: 921.154419 [  750/ 1682]\n",
      "loss: 881.619507 [  760/ 1682]\n",
      "loss: 645.983093 [  770/ 1682]\n",
      "loss: 655.560059 [  780/ 1682]\n",
      "loss: 556.387939 [  790/ 1682]\n",
      "loss: 437.436920 [  800/ 1682]\n",
      "loss: 386.454651 [  810/ 1682]\n",
      "loss: 302.847839 [  820/ 1682]\n",
      "loss: 382.884125 [  830/ 1682]\n",
      "loss: 595.187195 [  840/ 1682]\n",
      "loss: 563.196472 [  850/ 1682]\n",
      "loss: 414.418213 [  860/ 1682]\n",
      "loss: 345.841949 [  870/ 1682]\n",
      "loss: 306.001678 [  880/ 1682]\n",
      "loss: 303.769073 [  890/ 1682]\n",
      "loss: 265.730164 [  900/ 1682]\n",
      "loss: 277.672485 [  910/ 1682]\n",
      "loss: 192.989883 [  920/ 1682]\n",
      "loss: 163.149078 [  930/ 1682]\n",
      "loss: 100.710930 [  940/ 1682]\n",
      "loss: 56.426453 [  950/ 1682]\n",
      "loss: 13.196791 [  960/ 1682]\n",
      "loss: 3.144257 [  970/ 1682]\n",
      "loss: 4.517514 [  980/ 1682]\n",
      "loss: 22.057613 [  990/ 1682]\n",
      "loss: 79.326622 [ 1000/ 1682]\n",
      "loss: 107.943565 [ 1010/ 1682]\n",
      "loss: 130.131256 [ 1020/ 1682]\n",
      "loss: 60.124054 [ 1030/ 1682]\n",
      "loss: 20.311016 [ 1040/ 1682]\n",
      "loss: 67.204704 [ 1050/ 1682]\n",
      "loss: 27.511618 [ 1060/ 1682]\n",
      "loss: 11.556927 [ 1070/ 1682]\n",
      "loss: 58.927502 [ 1080/ 1682]\n",
      "loss: 101.085037 [ 1090/ 1682]\n",
      "loss: 238.360687 [ 1100/ 1682]\n",
      "loss: 440.802338 [ 1110/ 1682]\n",
      "loss: 572.315796 [ 1120/ 1682]\n",
      "loss: 750.713013 [ 1130/ 1682]\n",
      "loss: 1278.088379 [ 1140/ 1682]\n",
      "loss: 2261.330566 [ 1150/ 1682]\n",
      "loss: 3327.698486 [ 1160/ 1682]\n",
      "loss: 1966.377197 [ 1170/ 1682]\n",
      "loss: 1951.567993 [ 1180/ 1682]\n",
      "loss: 2404.632080 [ 1190/ 1682]\n",
      "loss: 2030.659424 [ 1200/ 1682]\n",
      "loss: 2233.496582 [ 1210/ 1682]\n",
      "loss: 2449.140137 [ 1220/ 1682]\n",
      "loss: 3068.138184 [ 1230/ 1682]\n",
      "loss: 3830.888184 [ 1240/ 1682]\n",
      "loss: 3457.458496 [ 1250/ 1682]\n",
      "loss: 4442.489258 [ 1260/ 1682]\n",
      "loss: 4562.584961 [ 1270/ 1682]\n",
      "loss: 3166.526611 [ 1280/ 1682]\n",
      "loss: 2693.473145 [ 1290/ 1682]\n",
      "loss: 2930.652832 [ 1300/ 1682]\n",
      "loss: 3526.161377 [ 1310/ 1682]\n",
      "loss: 4090.710205 [ 1320/ 1682]\n",
      "loss: 3781.048340 [ 1330/ 1682]\n",
      "loss: 3309.018799 [ 1340/ 1682]\n",
      "loss: 3360.673828 [ 1350/ 1682]\n",
      "loss: 3619.442871 [ 1360/ 1682]\n",
      "loss: 4878.075684 [ 1370/ 1682]\n",
      "loss: 5775.268555 [ 1380/ 1682]\n",
      "loss: 6100.055664 [ 1390/ 1682]\n",
      "loss: 6388.026855 [ 1400/ 1682]\n",
      "loss: 6427.522461 [ 1410/ 1682]\n",
      "loss: 6696.184570 [ 1420/ 1682]\n",
      "loss: 5497.490723 [ 1430/ 1682]\n",
      "loss: 5342.555664 [ 1440/ 1682]\n",
      "loss: 6115.270020 [ 1450/ 1682]\n",
      "loss: 6258.609375 [ 1460/ 1682]\n",
      "loss: 7909.250977 [ 1470/ 1682]\n",
      "loss: 9902.286133 [ 1480/ 1682]\n",
      "loss: 11316.804688 [ 1490/ 1682]\n",
      "loss: 11038.018555 [ 1500/ 1682]\n",
      "loss: 9760.211914 [ 1510/ 1682]\n",
      "loss: 10572.852539 [ 1520/ 1682]\n",
      "loss: 9692.564453 [ 1530/ 1682]\n",
      "loss: 9033.485352 [ 1540/ 1682]\n",
      "loss: 8646.837891 [ 1550/ 1682]\n",
      "loss: 11380.837891 [ 1560/ 1682]\n",
      "loss: 9027.126953 [ 1570/ 1682]\n",
      "loss: 8460.614258 [ 1580/ 1682]\n",
      "loss: 6099.867676 [ 1590/ 1682]\n",
      "loss: 6158.120117 [ 1600/ 1682]\n",
      "loss: 4637.596191 [ 1610/ 1682]\n",
      "loss: 4893.670898 [ 1620/ 1682]\n",
      "loss: 6444.982422 [ 1630/ 1682]\n",
      "loss: 7942.383789 [ 1640/ 1682]\n",
      "loss: 10159.857422 [ 1650/ 1682]\n",
      "loss: 9336.864258 [ 1660/ 1682]\n",
      "loss: 7127.627930 [ 1670/ 1682]\n",
      "loss: 7221.626953 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 9010.502695 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 2405.532959 [    0/ 1682]\n",
      "loss: 2214.507812 [   10/ 1682]\n",
      "loss: 2228.492676 [   20/ 1682]\n",
      "loss: 2389.087646 [   30/ 1682]\n",
      "loss: 1954.042603 [   40/ 1682]\n",
      "loss: 1867.575562 [   50/ 1682]\n",
      "loss: 1715.470703 [   60/ 1682]\n",
      "loss: 2080.999268 [   70/ 1682]\n",
      "loss: 2289.704590 [   80/ 1682]\n",
      "loss: 2323.231201 [   90/ 1682]\n",
      "loss: 2165.031250 [  100/ 1682]\n",
      "loss: 2327.997314 [  110/ 1682]\n",
      "loss: 1973.344482 [  120/ 1682]\n",
      "loss: 2142.913574 [  130/ 1682]\n",
      "loss: 1872.699219 [  140/ 1682]\n",
      "loss: 1690.096924 [  150/ 1682]\n",
      "loss: 1859.839111 [  160/ 1682]\n",
      "loss: 1986.868896 [  170/ 1682]\n",
      "loss: 1910.555298 [  180/ 1682]\n",
      "loss: 1749.533569 [  190/ 1682]\n",
      "loss: 1709.886108 [  200/ 1682]\n",
      "loss: 1904.745728 [  210/ 1682]\n",
      "loss: 2155.028809 [  220/ 1682]\n",
      "loss: 1700.956299 [  230/ 1682]\n",
      "loss: 1626.338745 [  240/ 1682]\n",
      "loss: 1891.261353 [  250/ 1682]\n",
      "loss: 1725.320679 [  260/ 1682]\n",
      "loss: 1433.946777 [  270/ 1682]\n",
      "loss: 1265.589478 [  280/ 1682]\n",
      "loss: 1235.045166 [  290/ 1682]\n",
      "loss: 1194.481689 [  300/ 1682]\n",
      "loss: 1238.826050 [  310/ 1682]\n",
      "loss: 1165.115967 [  320/ 1682]\n",
      "loss: 1099.086182 [  330/ 1682]\n",
      "loss: 985.113464 [  340/ 1682]\n",
      "loss: 1008.774231 [  350/ 1682]\n",
      "loss: 1245.027222 [  360/ 1682]\n",
      "loss: 1269.463257 [  370/ 1682]\n",
      "loss: 1274.622314 [  380/ 1682]\n",
      "loss: 1028.912964 [  390/ 1682]\n",
      "loss: 950.888794 [  400/ 1682]\n",
      "loss: 965.097168 [  410/ 1682]\n",
      "loss: 974.422546 [  420/ 1682]\n",
      "loss: 1020.369141 [  430/ 1682]\n",
      "loss: 1044.798950 [  440/ 1682]\n",
      "loss: 942.159973 [  450/ 1682]\n",
      "loss: 789.776428 [  460/ 1682]\n",
      "loss: 802.379639 [  470/ 1682]\n",
      "loss: 752.836304 [  480/ 1682]\n",
      "loss: 701.526123 [  490/ 1682]\n",
      "loss: 648.875610 [  500/ 1682]\n",
      "loss: 784.852295 [  510/ 1682]\n",
      "loss: 917.143433 [  520/ 1682]\n",
      "loss: 660.006958 [  530/ 1682]\n",
      "loss: 665.385193 [  540/ 1682]\n",
      "loss: 679.438660 [  550/ 1682]\n",
      "loss: 675.719727 [  560/ 1682]\n",
      "loss: 841.742188 [  570/ 1682]\n",
      "loss: 537.793518 [  580/ 1682]\n",
      "loss: 589.060852 [  590/ 1682]\n",
      "loss: 425.830750 [  600/ 1682]\n",
      "loss: 507.556244 [  610/ 1682]\n",
      "loss: 538.937439 [  620/ 1682]\n",
      "loss: 455.553955 [  630/ 1682]\n",
      "loss: 365.391724 [  640/ 1682]\n",
      "loss: 231.706085 [  650/ 1682]\n",
      "loss: 239.518677 [  660/ 1682]\n",
      "loss: 250.097778 [  670/ 1682]\n",
      "loss: 175.404053 [  680/ 1682]\n",
      "loss: 210.549026 [  690/ 1682]\n",
      "loss: 208.180054 [  700/ 1682]\n",
      "loss: 472.599670 [  710/ 1682]\n",
      "loss: 619.704407 [  720/ 1682]\n",
      "loss: 774.577881 [  730/ 1682]\n",
      "loss: 943.028931 [  740/ 1682]\n",
      "loss: 929.210815 [  750/ 1682]\n",
      "loss: 847.706055 [  760/ 1682]\n",
      "loss: 728.103882 [  770/ 1682]\n",
      "loss: 624.315491 [  780/ 1682]\n",
      "loss: 593.112915 [  790/ 1682]\n",
      "loss: 505.454041 [  800/ 1682]\n",
      "loss: 363.810242 [  810/ 1682]\n",
      "loss: 333.028351 [  820/ 1682]\n",
      "loss: 341.228455 [  830/ 1682]\n",
      "loss: 534.925476 [  840/ 1682]\n",
      "loss: 504.496277 [  850/ 1682]\n",
      "loss: 412.012848 [  860/ 1682]\n",
      "loss: 348.673889 [  870/ 1682]\n",
      "loss: 354.255768 [  880/ 1682]\n",
      "loss: 331.083069 [  890/ 1682]\n",
      "loss: 316.606812 [  900/ 1682]\n",
      "loss: 349.028839 [  910/ 1682]\n",
      "loss: 212.050095 [  920/ 1682]\n",
      "loss: 166.819443 [  930/ 1682]\n",
      "loss: 133.327698 [  940/ 1682]\n",
      "loss: 44.684540 [  950/ 1682]\n",
      "loss: 33.792850 [  960/ 1682]\n",
      "loss: 29.125280 [  970/ 1682]\n",
      "loss: 2.733051 [  980/ 1682]\n",
      "loss: 17.207470 [  990/ 1682]\n",
      "loss: 62.407784 [ 1000/ 1682]\n",
      "loss: 114.357033 [ 1010/ 1682]\n",
      "loss: 124.384621 [ 1020/ 1682]\n",
      "loss: 47.633469 [ 1030/ 1682]\n",
      "loss: 20.574814 [ 1040/ 1682]\n",
      "loss: 76.749077 [ 1050/ 1682]\n",
      "loss: 23.478268 [ 1060/ 1682]\n",
      "loss: 12.468677 [ 1070/ 1682]\n",
      "loss: 47.796089 [ 1080/ 1682]\n",
      "loss: 118.462791 [ 1090/ 1682]\n",
      "loss: 202.502411 [ 1100/ 1682]\n",
      "loss: 347.467041 [ 1110/ 1682]\n",
      "loss: 563.089355 [ 1120/ 1682]\n",
      "loss: 685.135864 [ 1130/ 1682]\n",
      "loss: 1172.052490 [ 1140/ 1682]\n",
      "loss: 2096.442627 [ 1150/ 1682]\n",
      "loss: 3229.541992 [ 1160/ 1682]\n",
      "loss: 1820.516235 [ 1170/ 1682]\n",
      "loss: 1988.629150 [ 1180/ 1682]\n",
      "loss: 2306.157959 [ 1190/ 1682]\n",
      "loss: 2010.394531 [ 1200/ 1682]\n",
      "loss: 2431.643799 [ 1210/ 1682]\n",
      "loss: 2477.083252 [ 1220/ 1682]\n",
      "loss: 2797.633789 [ 1230/ 1682]\n",
      "loss: 3877.827393 [ 1240/ 1682]\n",
      "loss: 3651.757080 [ 1250/ 1682]\n",
      "loss: 4494.387695 [ 1260/ 1682]\n",
      "loss: 4223.241699 [ 1270/ 1682]\n",
      "loss: 3421.070801 [ 1280/ 1682]\n",
      "loss: 2679.913574 [ 1290/ 1682]\n",
      "loss: 2653.226074 [ 1300/ 1682]\n",
      "loss: 3492.921143 [ 1310/ 1682]\n",
      "loss: 4152.548828 [ 1320/ 1682]\n",
      "loss: 3757.593018 [ 1330/ 1682]\n",
      "loss: 3356.079590 [ 1340/ 1682]\n",
      "loss: 3124.008301 [ 1350/ 1682]\n",
      "loss: 3757.018066 [ 1360/ 1682]\n",
      "loss: 4857.522949 [ 1370/ 1682]\n",
      "loss: 6052.385742 [ 1380/ 1682]\n",
      "loss: 5869.604004 [ 1390/ 1682]\n",
      "loss: 6033.572754 [ 1400/ 1682]\n",
      "loss: 6400.509766 [ 1410/ 1682]\n",
      "loss: 6785.261719 [ 1420/ 1682]\n",
      "loss: 5486.747070 [ 1430/ 1682]\n",
      "loss: 5425.229980 [ 1440/ 1682]\n",
      "loss: 6560.930664 [ 1450/ 1682]\n",
      "loss: 6371.104980 [ 1460/ 1682]\n",
      "loss: 7739.361816 [ 1470/ 1682]\n",
      "loss: 10184.056641 [ 1480/ 1682]\n",
      "loss: 10969.513672 [ 1490/ 1682]\n",
      "loss: 11369.076172 [ 1500/ 1682]\n",
      "loss: 9711.043945 [ 1510/ 1682]\n",
      "loss: 10715.600586 [ 1520/ 1682]\n",
      "loss: 9823.156250 [ 1530/ 1682]\n",
      "loss: 8828.234375 [ 1540/ 1682]\n",
      "loss: 8947.499023 [ 1550/ 1682]\n",
      "loss: 11342.250000 [ 1560/ 1682]\n",
      "loss: 9541.287109 [ 1570/ 1682]\n",
      "loss: 8254.661133 [ 1580/ 1682]\n",
      "loss: 5810.724121 [ 1590/ 1682]\n",
      "loss: 5435.662598 [ 1600/ 1682]\n",
      "loss: 4438.149414 [ 1610/ 1682]\n",
      "loss: 4625.399414 [ 1620/ 1682]\n",
      "loss: 5985.464844 [ 1630/ 1682]\n",
      "loss: 7933.500000 [ 1640/ 1682]\n",
      "loss: 9737.251953 [ 1650/ 1682]\n",
      "loss: 9313.380859 [ 1660/ 1682]\n",
      "loss: 7494.776367 [ 1670/ 1682]\n",
      "loss: 7179.390625 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8860.603581 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 2077.787354 [    0/ 1682]\n",
      "loss: 2118.423340 [   10/ 1682]\n",
      "loss: 2343.543457 [   20/ 1682]\n",
      "loss: 2057.936035 [   30/ 1682]\n",
      "loss: 1969.745850 [   40/ 1682]\n",
      "loss: 1777.605713 [   50/ 1682]\n",
      "loss: 1736.149658 [   60/ 1682]\n",
      "loss: 2093.531982 [   70/ 1682]\n",
      "loss: 2305.854004 [   80/ 1682]\n",
      "loss: 2223.876221 [   90/ 1682]\n",
      "loss: 2182.668945 [  100/ 1682]\n",
      "loss: 2118.490234 [  110/ 1682]\n",
      "loss: 2220.000488 [  120/ 1682]\n",
      "loss: 2050.369629 [  130/ 1682]\n",
      "loss: 1892.173096 [  140/ 1682]\n",
      "loss: 1814.174072 [  150/ 1682]\n",
      "loss: 1882.053101 [  160/ 1682]\n",
      "loss: 2007.613647 [  170/ 1682]\n",
      "loss: 1627.826294 [  180/ 1682]\n",
      "loss: 1862.147705 [  190/ 1682]\n",
      "loss: 1627.548340 [  200/ 1682]\n",
      "loss: 1826.570679 [  210/ 1682]\n",
      "loss: 1683.138916 [  220/ 1682]\n",
      "loss: 1722.118774 [  230/ 1682]\n",
      "loss: 1646.861694 [  240/ 1682]\n",
      "loss: 2009.792969 [  250/ 1682]\n",
      "loss: 1567.816650 [  260/ 1682]\n",
      "loss: 1542.940796 [  270/ 1682]\n",
      "loss: 1363.785156 [  280/ 1682]\n",
      "loss: 1414.256226 [  290/ 1682]\n",
      "loss: 1445.360840 [  300/ 1682]\n",
      "loss: 1333.482056 [  310/ 1682]\n",
      "loss: 1106.081787 [  320/ 1682]\n",
      "loss: 1187.304443 [  330/ 1682]\n",
      "loss: 1000.158569 [  340/ 1682]\n",
      "loss: 1099.909302 [  350/ 1682]\n",
      "loss: 1058.198608 [  360/ 1682]\n",
      "loss: 1431.867065 [  370/ 1682]\n",
      "loss: 1102.584106 [  380/ 1682]\n",
      "loss: 1108.997559 [  390/ 1682]\n",
      "loss: 908.046692 [  400/ 1682]\n",
      "loss: 984.837891 [  410/ 1682]\n",
      "loss: 990.637329 [  420/ 1682]\n",
      "loss: 916.902344 [  430/ 1682]\n",
      "loss: 946.454102 [  440/ 1682]\n",
      "loss: 963.240845 [  450/ 1682]\n",
      "loss: 754.529053 [  460/ 1682]\n",
      "loss: 720.682251 [  470/ 1682]\n",
      "loss: 668.250366 [  480/ 1682]\n",
      "loss: 718.151062 [  490/ 1682]\n",
      "loss: 856.624634 [  500/ 1682]\n",
      "loss: 852.976257 [  510/ 1682]\n",
      "loss: 781.800293 [  520/ 1682]\n",
      "loss: 626.492065 [  530/ 1682]\n",
      "loss: 682.225891 [  540/ 1682]\n",
      "loss: 789.942871 [  550/ 1682]\n",
      "loss: 736.288452 [  560/ 1682]\n",
      "loss: 819.183472 [  570/ 1682]\n",
      "loss: 511.420746 [  580/ 1682]\n",
      "loss: 568.469116 [  590/ 1682]\n",
      "loss: 436.157959 [  600/ 1682]\n",
      "loss: 560.288879 [  610/ 1682]\n",
      "loss: 595.852356 [  620/ 1682]\n",
      "loss: 540.909607 [  630/ 1682]\n",
      "loss: 407.251862 [  640/ 1682]\n",
      "loss: 297.768616 [  650/ 1682]\n",
      "loss: 254.810089 [  660/ 1682]\n",
      "loss: 236.012543 [  670/ 1682]\n",
      "loss: 231.429077 [  680/ 1682]\n",
      "loss: 216.301178 [  690/ 1682]\n",
      "loss: 266.432190 [  700/ 1682]\n",
      "loss: 392.050812 [  710/ 1682]\n",
      "loss: 639.929565 [  720/ 1682]\n",
      "loss: 755.757751 [  730/ 1682]\n",
      "loss: 921.934082 [  740/ 1682]\n",
      "loss: 1083.152588 [  750/ 1682]\n",
      "loss: 906.412476 [  760/ 1682]\n",
      "loss: 746.121399 [  770/ 1682]\n",
      "loss: 636.517700 [  780/ 1682]\n",
      "loss: 683.014465 [  790/ 1682]\n",
      "loss: 486.033508 [  800/ 1682]\n",
      "loss: 345.489563 [  810/ 1682]\n",
      "loss: 344.265991 [  820/ 1682]\n",
      "loss: 351.011383 [  830/ 1682]\n",
      "loss: 581.119751 [  840/ 1682]\n",
      "loss: 484.169861 [  850/ 1682]\n",
      "loss: 371.612213 [  860/ 1682]\n",
      "loss: 359.513855 [  870/ 1682]\n",
      "loss: 270.626526 [  880/ 1682]\n",
      "loss: 342.565735 [  890/ 1682]\n",
      "loss: 303.860901 [  900/ 1682]\n",
      "loss: 316.252014 [  910/ 1682]\n",
      "loss: 182.982178 [  920/ 1682]\n",
      "loss: 155.158478 [  930/ 1682]\n",
      "loss: 93.920456 [  940/ 1682]\n",
      "loss: 62.759544 [  950/ 1682]\n",
      "loss: 14.041492 [  960/ 1682]\n",
      "loss: 10.675655 [  970/ 1682]\n",
      "loss: 2.926937 [  980/ 1682]\n",
      "loss: 21.140375 [  990/ 1682]\n",
      "loss: 71.601692 [ 1000/ 1682]\n",
      "loss: 94.447128 [ 1010/ 1682]\n",
      "loss: 142.456619 [ 1020/ 1682]\n",
      "loss: 58.689190 [ 1030/ 1682]\n",
      "loss: 36.730625 [ 1040/ 1682]\n",
      "loss: 79.170135 [ 1050/ 1682]\n",
      "loss: 19.455631 [ 1060/ 1682]\n",
      "loss: 10.343925 [ 1070/ 1682]\n",
      "loss: 51.077908 [ 1080/ 1682]\n",
      "loss: 109.512650 [ 1090/ 1682]\n",
      "loss: 223.424347 [ 1100/ 1682]\n",
      "loss: 404.014221 [ 1110/ 1682]\n",
      "loss: 554.484802 [ 1120/ 1682]\n",
      "loss: 752.551697 [ 1130/ 1682]\n",
      "loss: 1086.369873 [ 1140/ 1682]\n",
      "loss: 2170.797607 [ 1150/ 1682]\n",
      "loss: 3148.738525 [ 1160/ 1682]\n",
      "loss: 1877.336548 [ 1170/ 1682]\n",
      "loss: 1860.412354 [ 1180/ 1682]\n",
      "loss: 2369.746826 [ 1190/ 1682]\n",
      "loss: 2044.135498 [ 1200/ 1682]\n",
      "loss: 2304.259033 [ 1210/ 1682]\n",
      "loss: 2339.311523 [ 1220/ 1682]\n",
      "loss: 2946.455078 [ 1230/ 1682]\n",
      "loss: 3763.877686 [ 1240/ 1682]\n",
      "loss: 3462.260254 [ 1250/ 1682]\n",
      "loss: 4361.650879 [ 1260/ 1682]\n",
      "loss: 4173.681641 [ 1270/ 1682]\n",
      "loss: 3244.357422 [ 1280/ 1682]\n",
      "loss: 2768.564941 [ 1290/ 1682]\n",
      "loss: 2809.334717 [ 1300/ 1682]\n",
      "loss: 3470.142578 [ 1310/ 1682]\n",
      "loss: 4020.618652 [ 1320/ 1682]\n",
      "loss: 3713.178955 [ 1330/ 1682]\n",
      "loss: 3085.818359 [ 1340/ 1682]\n",
      "loss: 3083.612793 [ 1350/ 1682]\n",
      "loss: 3710.668457 [ 1360/ 1682]\n",
      "loss: 4714.657227 [ 1370/ 1682]\n",
      "loss: 6014.185059 [ 1380/ 1682]\n",
      "loss: 6008.008789 [ 1390/ 1682]\n",
      "loss: 6309.108398 [ 1400/ 1682]\n",
      "loss: 6175.375488 [ 1410/ 1682]\n",
      "loss: 6626.387695 [ 1420/ 1682]\n",
      "loss: 5510.705078 [ 1430/ 1682]\n",
      "loss: 5353.642090 [ 1440/ 1682]\n",
      "loss: 5963.360352 [ 1450/ 1682]\n",
      "loss: 6680.664062 [ 1460/ 1682]\n",
      "loss: 7958.834473 [ 1470/ 1682]\n",
      "loss: 9492.112305 [ 1480/ 1682]\n",
      "loss: 11213.028320 [ 1490/ 1682]\n",
      "loss: 11430.368164 [ 1500/ 1682]\n",
      "loss: 9318.742188 [ 1510/ 1682]\n",
      "loss: 10502.905273 [ 1520/ 1682]\n",
      "loss: 9560.576172 [ 1530/ 1682]\n",
      "loss: 7919.376465 [ 1540/ 1682]\n",
      "loss: 8694.523438 [ 1550/ 1682]\n",
      "loss: 11012.802734 [ 1560/ 1682]\n",
      "loss: 8473.598633 [ 1570/ 1682]\n",
      "loss: 7953.606934 [ 1580/ 1682]\n",
      "loss: 5141.793945 [ 1590/ 1682]\n",
      "loss: 5467.107910 [ 1600/ 1682]\n",
      "loss: 4711.495605 [ 1610/ 1682]\n",
      "loss: 3983.019531 [ 1620/ 1682]\n",
      "loss: 6366.249023 [ 1630/ 1682]\n",
      "loss: 7415.036621 [ 1640/ 1682]\n",
      "loss: 9867.366211 [ 1650/ 1682]\n",
      "loss: 8984.987305 [ 1660/ 1682]\n",
      "loss: 7785.090820 [ 1670/ 1682]\n",
      "loss: 5332.586426 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8541.899170 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 2439.569824 [    0/ 1682]\n",
      "loss: 2554.360840 [   10/ 1682]\n",
      "loss: 2374.343018 [   20/ 1682]\n",
      "loss: 2411.501465 [   30/ 1682]\n",
      "loss: 3223.427490 [   40/ 1682]\n",
      "loss: 2459.666504 [   50/ 1682]\n",
      "loss: 2234.130859 [   60/ 1682]\n",
      "loss: 2056.486816 [   70/ 1682]\n",
      "loss: 2214.328613 [   80/ 1682]\n",
      "loss: 2446.026367 [   90/ 1682]\n",
      "loss: 2600.009033 [  100/ 1682]\n",
      "loss: 2025.748657 [  110/ 1682]\n",
      "loss: 2475.942139 [  120/ 1682]\n",
      "loss: 2401.225098 [  130/ 1682]\n",
      "loss: 2217.448730 [  140/ 1682]\n",
      "loss: 2062.152100 [  150/ 1682]\n",
      "loss: 2049.560547 [  160/ 1682]\n",
      "loss: 1961.783447 [  170/ 1682]\n",
      "loss: 1646.561279 [  180/ 1682]\n",
      "loss: 2239.833252 [  190/ 1682]\n",
      "loss: 1646.214233 [  200/ 1682]\n",
      "loss: 2375.437256 [  210/ 1682]\n",
      "loss: 1823.772461 [  220/ 1682]\n",
      "loss: 2151.653076 [  230/ 1682]\n",
      "loss: 1933.412720 [  240/ 1682]\n",
      "loss: 1910.663696 [  250/ 1682]\n",
      "loss: 1596.202393 [  260/ 1682]\n",
      "loss: 1452.000488 [  270/ 1682]\n",
      "loss: 1589.678955 [  280/ 1682]\n",
      "loss: 1678.411743 [  290/ 1682]\n",
      "loss: 1359.153442 [  300/ 1682]\n",
      "loss: 1401.526733 [  310/ 1682]\n",
      "loss: 1121.168823 [  320/ 1682]\n",
      "loss: 1201.597534 [  330/ 1682]\n",
      "loss: 1144.511230 [  340/ 1682]\n",
      "loss: 1040.554199 [  350/ 1682]\n",
      "loss: 1167.910522 [  360/ 1682]\n",
      "loss: 1245.968384 [  370/ 1682]\n",
      "loss: 1067.953979 [  380/ 1682]\n",
      "loss: 999.069946 [  390/ 1682]\n",
      "loss: 1148.441772 [  400/ 1682]\n",
      "loss: 948.055481 [  410/ 1682]\n",
      "loss: 988.332642 [  420/ 1682]\n",
      "loss: 1261.680664 [  430/ 1682]\n",
      "loss: 1141.996094 [  440/ 1682]\n",
      "loss: 1028.021851 [  450/ 1682]\n",
      "loss: 867.982544 [  460/ 1682]\n",
      "loss: 858.735657 [  470/ 1682]\n",
      "loss: 679.823120 [  480/ 1682]\n",
      "loss: 749.592163 [  490/ 1682]\n",
      "loss: 751.405701 [  500/ 1682]\n",
      "loss: 876.945129 [  510/ 1682]\n",
      "loss: 794.251709 [  520/ 1682]\n",
      "loss: 729.746094 [  530/ 1682]\n",
      "loss: 656.144653 [  540/ 1682]\n",
      "loss: 746.628784 [  550/ 1682]\n",
      "loss: 743.620483 [  560/ 1682]\n",
      "loss: 809.611450 [  570/ 1682]\n",
      "loss: 521.463501 [  580/ 1682]\n",
      "loss: 497.399353 [  590/ 1682]\n",
      "loss: 515.234985 [  600/ 1682]\n",
      "loss: 531.695496 [  610/ 1682]\n",
      "loss: 531.188782 [  620/ 1682]\n",
      "loss: 444.781067 [  630/ 1682]\n",
      "loss: 349.862732 [  640/ 1682]\n",
      "loss: 246.112869 [  650/ 1682]\n",
      "loss: 221.854446 [  660/ 1682]\n",
      "loss: 192.558060 [  670/ 1682]\n",
      "loss: 184.251770 [  680/ 1682]\n",
      "loss: 198.413971 [  690/ 1682]\n",
      "loss: 221.892700 [  700/ 1682]\n",
      "loss: 366.618347 [  710/ 1682]\n",
      "loss: 561.143311 [  720/ 1682]\n",
      "loss: 692.178589 [  730/ 1682]\n",
      "loss: 821.684204 [  740/ 1682]\n",
      "loss: 890.619507 [  750/ 1682]\n",
      "loss: 832.563110 [  760/ 1682]\n",
      "loss: 639.954956 [  770/ 1682]\n",
      "loss: 537.083008 [  780/ 1682]\n",
      "loss: 524.439514 [  790/ 1682]\n",
      "loss: 433.223328 [  800/ 1682]\n",
      "loss: 369.656555 [  810/ 1682]\n",
      "loss: 312.385895 [  820/ 1682]\n",
      "loss: 277.546814 [  830/ 1682]\n",
      "loss: 433.686462 [  840/ 1682]\n",
      "loss: 468.977051 [  850/ 1682]\n",
      "loss: 353.203491 [  860/ 1682]\n",
      "loss: 267.187073 [  870/ 1682]\n",
      "loss: 286.664246 [  880/ 1682]\n",
      "loss: 245.642853 [  890/ 1682]\n",
      "loss: 265.480255 [  900/ 1682]\n",
      "loss: 228.723709 [  910/ 1682]\n",
      "loss: 176.517166 [  920/ 1682]\n",
      "loss: 113.965500 [  930/ 1682]\n",
      "loss: 109.405495 [  940/ 1682]\n",
      "loss: 33.824173 [  950/ 1682]\n",
      "loss: 8.339614 [  960/ 1682]\n",
      "loss: 6.015478 [  970/ 1682]\n",
      "loss: 15.454702 [  980/ 1682]\n",
      "loss: 45.353783 [  990/ 1682]\n",
      "loss: 165.691147 [ 1000/ 1682]\n",
      "loss: 251.043411 [ 1010/ 1682]\n",
      "loss: 237.417526 [ 1020/ 1682]\n",
      "loss: 53.986073 [ 1030/ 1682]\n",
      "loss: 94.250038 [ 1040/ 1682]\n",
      "loss: 49.598000 [ 1050/ 1682]\n",
      "loss: 17.008173 [ 1060/ 1682]\n",
      "loss: 44.254768 [ 1070/ 1682]\n",
      "loss: 81.332726 [ 1080/ 1682]\n",
      "loss: 208.201050 [ 1090/ 1682]\n",
      "loss: 216.613358 [ 1100/ 1682]\n",
      "loss: 463.688568 [ 1110/ 1682]\n",
      "loss: 649.710266 [ 1120/ 1682]\n",
      "loss: 814.750916 [ 1130/ 1682]\n",
      "loss: 1549.328491 [ 1140/ 1682]\n",
      "loss: 2344.914307 [ 1150/ 1682]\n",
      "loss: 3376.579590 [ 1160/ 1682]\n",
      "loss: 1956.972656 [ 1170/ 1682]\n",
      "loss: 2119.873779 [ 1180/ 1682]\n",
      "loss: 2461.250000 [ 1190/ 1682]\n",
      "loss: 2159.940674 [ 1200/ 1682]\n",
      "loss: 2455.109863 [ 1210/ 1682]\n",
      "loss: 2343.774658 [ 1220/ 1682]\n",
      "loss: 2987.105469 [ 1230/ 1682]\n",
      "loss: 3881.021973 [ 1240/ 1682]\n",
      "loss: 3656.191895 [ 1250/ 1682]\n",
      "loss: 4457.991211 [ 1260/ 1682]\n",
      "loss: 4463.505371 [ 1270/ 1682]\n",
      "loss: 3238.758301 [ 1280/ 1682]\n",
      "loss: 2737.289795 [ 1290/ 1682]\n",
      "loss: 2851.322510 [ 1300/ 1682]\n",
      "loss: 3417.635986 [ 1310/ 1682]\n",
      "loss: 4245.122070 [ 1320/ 1682]\n",
      "loss: 3768.565918 [ 1330/ 1682]\n",
      "loss: 3107.976074 [ 1340/ 1682]\n",
      "loss: 3161.283203 [ 1350/ 1682]\n",
      "loss: 3850.768066 [ 1360/ 1682]\n",
      "loss: 4629.988281 [ 1370/ 1682]\n",
      "loss: 5882.677734 [ 1380/ 1682]\n",
      "loss: 5685.242188 [ 1390/ 1682]\n",
      "loss: 6059.324707 [ 1400/ 1682]\n",
      "loss: 6335.475098 [ 1410/ 1682]\n",
      "loss: 6594.290527 [ 1420/ 1682]\n",
      "loss: 5413.583984 [ 1430/ 1682]\n",
      "loss: 5214.975098 [ 1440/ 1682]\n",
      "loss: 6481.909668 [ 1450/ 1682]\n",
      "loss: 6032.291992 [ 1460/ 1682]\n",
      "loss: 7915.446289 [ 1470/ 1682]\n",
      "loss: 10244.260742 [ 1480/ 1682]\n",
      "loss: 11163.044922 [ 1490/ 1682]\n",
      "loss: 10777.515625 [ 1500/ 1682]\n",
      "loss: 9443.728516 [ 1510/ 1682]\n",
      "loss: 10592.212891 [ 1520/ 1682]\n",
      "loss: 9502.400391 [ 1530/ 1682]\n",
      "loss: 8508.289062 [ 1540/ 1682]\n",
      "loss: 8668.896484 [ 1550/ 1682]\n",
      "loss: 10717.223633 [ 1560/ 1682]\n",
      "loss: 8583.486328 [ 1570/ 1682]\n",
      "loss: 7693.922852 [ 1580/ 1682]\n",
      "loss: 5985.434570 [ 1590/ 1682]\n",
      "loss: 5833.627930 [ 1600/ 1682]\n",
      "loss: 4654.552734 [ 1610/ 1682]\n",
      "loss: 4729.235840 [ 1620/ 1682]\n",
      "loss: 6327.239258 [ 1630/ 1682]\n",
      "loss: 7540.088379 [ 1640/ 1682]\n",
      "loss: 10209.977539 [ 1650/ 1682]\n",
      "loss: 9295.958984 [ 1660/ 1682]\n",
      "loss: 6540.668945 [ 1670/ 1682]\n",
      "loss: 7097.085938 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8547.103825 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 2538.063721 [    0/ 1682]\n",
      "loss: 2464.192383 [   10/ 1682]\n",
      "loss: 2178.911133 [   20/ 1682]\n",
      "loss: 2324.332520 [   30/ 1682]\n",
      "loss: 2113.116699 [   40/ 1682]\n",
      "loss: 2582.696045 [   50/ 1682]\n",
      "loss: 2180.163574 [   60/ 1682]\n",
      "loss: 2116.749512 [   70/ 1682]\n",
      "loss: 2459.136475 [   80/ 1682]\n",
      "loss: 2671.819580 [   90/ 1682]\n",
      "loss: 2714.838867 [  100/ 1682]\n",
      "loss: 2168.049561 [  110/ 1682]\n",
      "loss: 2225.732910 [  120/ 1682]\n",
      "loss: 2279.958008 [  130/ 1682]\n",
      "loss: 1937.978760 [  140/ 1682]\n",
      "loss: 2042.134521 [  150/ 1682]\n",
      "loss: 1818.129517 [  160/ 1682]\n",
      "loss: 1900.664429 [  170/ 1682]\n",
      "loss: 1666.885986 [  180/ 1682]\n",
      "loss: 2036.145752 [  190/ 1682]\n",
      "loss: 2093.886719 [  200/ 1682]\n",
      "loss: 2123.445801 [  210/ 1682]\n",
      "loss: 2165.843262 [  220/ 1682]\n",
      "loss: 1923.883423 [  230/ 1682]\n",
      "loss: 1837.229858 [  240/ 1682]\n",
      "loss: 1940.242188 [  250/ 1682]\n",
      "loss: 1599.434692 [  260/ 1682]\n",
      "loss: 1689.225342 [  270/ 1682]\n",
      "loss: 1333.178345 [  280/ 1682]\n",
      "loss: 1469.737549 [  290/ 1682]\n",
      "loss: 1166.192017 [  300/ 1682]\n",
      "loss: 1473.776123 [  310/ 1682]\n",
      "loss: 1215.538330 [  320/ 1682]\n",
      "loss: 1007.216003 [  330/ 1682]\n",
      "loss: 1098.169189 [  340/ 1682]\n",
      "loss: 1203.699951 [  350/ 1682]\n",
      "loss: 1389.488037 [  360/ 1682]\n",
      "loss: 1246.222412 [  370/ 1682]\n",
      "loss: 1134.073975 [  380/ 1682]\n",
      "loss: 1076.186279 [  390/ 1682]\n",
      "loss: 999.122253 [  400/ 1682]\n",
      "loss: 892.476868 [  410/ 1682]\n",
      "loss: 1074.898071 [  420/ 1682]\n",
      "loss: 1001.099121 [  430/ 1682]\n",
      "loss: 1085.297485 [  440/ 1682]\n",
      "loss: 1150.084473 [  450/ 1682]\n",
      "loss: 822.566223 [  460/ 1682]\n",
      "loss: 790.093018 [  470/ 1682]\n",
      "loss: 785.998169 [  480/ 1682]\n",
      "loss: 783.999634 [  490/ 1682]\n",
      "loss: 727.507202 [  500/ 1682]\n",
      "loss: 773.311829 [  510/ 1682]\n",
      "loss: 857.765259 [  520/ 1682]\n",
      "loss: 815.465820 [  530/ 1682]\n",
      "loss: 654.183105 [  540/ 1682]\n",
      "loss: 760.911438 [  550/ 1682]\n",
      "loss: 749.152954 [  560/ 1682]\n",
      "loss: 791.774170 [  570/ 1682]\n",
      "loss: 532.968140 [  580/ 1682]\n",
      "loss: 542.038879 [  590/ 1682]\n",
      "loss: 491.417297 [  600/ 1682]\n",
      "loss: 578.080872 [  610/ 1682]\n",
      "loss: 564.004639 [  620/ 1682]\n",
      "loss: 483.221985 [  630/ 1682]\n",
      "loss: 409.269897 [  640/ 1682]\n",
      "loss: 300.279724 [  650/ 1682]\n",
      "loss: 234.133636 [  660/ 1682]\n",
      "loss: 268.444092 [  670/ 1682]\n",
      "loss: 252.410645 [  680/ 1682]\n",
      "loss: 228.592438 [  690/ 1682]\n",
      "loss: 289.303223 [  700/ 1682]\n",
      "loss: 400.464081 [  710/ 1682]\n",
      "loss: 657.293640 [  720/ 1682]\n",
      "loss: 730.782654 [  730/ 1682]\n",
      "loss: 1065.908691 [  740/ 1682]\n",
      "loss: 1003.805054 [  750/ 1682]\n",
      "loss: 926.189270 [  760/ 1682]\n",
      "loss: 713.555359 [  770/ 1682]\n",
      "loss: 684.251587 [  780/ 1682]\n",
      "loss: 627.325806 [  790/ 1682]\n",
      "loss: 443.579010 [  800/ 1682]\n",
      "loss: 386.460205 [  810/ 1682]\n",
      "loss: 333.531494 [  820/ 1682]\n",
      "loss: 365.617126 [  830/ 1682]\n",
      "loss: 586.314880 [  840/ 1682]\n",
      "loss: 522.790405 [  850/ 1682]\n",
      "loss: 446.839020 [  860/ 1682]\n",
      "loss: 325.078430 [  870/ 1682]\n",
      "loss: 328.131287 [  880/ 1682]\n",
      "loss: 328.683838 [  890/ 1682]\n",
      "loss: 309.674622 [  900/ 1682]\n",
      "loss: 311.649292 [  910/ 1682]\n",
      "loss: 177.271072 [  920/ 1682]\n",
      "loss: 180.056564 [  930/ 1682]\n",
      "loss: 115.823647 [  940/ 1682]\n",
      "loss: 103.192642 [  950/ 1682]\n",
      "loss: 14.433360 [  960/ 1682]\n",
      "loss: 15.792076 [  970/ 1682]\n",
      "loss: 7.530396 [  980/ 1682]\n",
      "loss: 13.467465 [  990/ 1682]\n",
      "loss: 62.879128 [ 1000/ 1682]\n",
      "loss: 94.194443 [ 1010/ 1682]\n",
      "loss: 116.106422 [ 1020/ 1682]\n",
      "loss: 55.149414 [ 1030/ 1682]\n",
      "loss: 24.970921 [ 1040/ 1682]\n",
      "loss: 64.152794 [ 1050/ 1682]\n",
      "loss: 21.920015 [ 1060/ 1682]\n",
      "loss: 6.344465 [ 1070/ 1682]\n",
      "loss: 51.908558 [ 1080/ 1682]\n",
      "loss: 88.595024 [ 1090/ 1682]\n",
      "loss: 184.099197 [ 1100/ 1682]\n",
      "loss: 346.908508 [ 1110/ 1682]\n",
      "loss: 562.491089 [ 1120/ 1682]\n",
      "loss: 716.568665 [ 1130/ 1682]\n",
      "loss: 1202.752441 [ 1140/ 1682]\n",
      "loss: 2092.648926 [ 1150/ 1682]\n",
      "loss: 3189.760498 [ 1160/ 1682]\n",
      "loss: 1833.532227 [ 1170/ 1682]\n",
      "loss: 1847.521851 [ 1180/ 1682]\n",
      "loss: 2351.597656 [ 1190/ 1682]\n",
      "loss: 1972.431641 [ 1200/ 1682]\n",
      "loss: 2281.192871 [ 1210/ 1682]\n",
      "loss: 2226.364990 [ 1220/ 1682]\n",
      "loss: 2962.172607 [ 1230/ 1682]\n",
      "loss: 3732.179688 [ 1240/ 1682]\n",
      "loss: 3641.644531 [ 1250/ 1682]\n",
      "loss: 4476.532227 [ 1260/ 1682]\n",
      "loss: 4155.816895 [ 1270/ 1682]\n",
      "loss: 3296.330566 [ 1280/ 1682]\n",
      "loss: 2682.509521 [ 1290/ 1682]\n",
      "loss: 2567.203369 [ 1300/ 1682]\n",
      "loss: 3365.554199 [ 1310/ 1682]\n",
      "loss: 4130.163086 [ 1320/ 1682]\n",
      "loss: 3554.440918 [ 1330/ 1682]\n",
      "loss: 3156.380371 [ 1340/ 1682]\n",
      "loss: 3052.481689 [ 1350/ 1682]\n",
      "loss: 3541.224121 [ 1360/ 1682]\n",
      "loss: 4762.709473 [ 1370/ 1682]\n",
      "loss: 5841.659668 [ 1380/ 1682]\n",
      "loss: 5886.698730 [ 1390/ 1682]\n",
      "loss: 6046.243164 [ 1400/ 1682]\n",
      "loss: 6508.990723 [ 1410/ 1682]\n",
      "loss: 6566.485352 [ 1420/ 1682]\n",
      "loss: 5508.784668 [ 1430/ 1682]\n",
      "loss: 5020.435059 [ 1440/ 1682]\n",
      "loss: 6331.870117 [ 1450/ 1682]\n",
      "loss: 6160.484375 [ 1460/ 1682]\n",
      "loss: 7642.795898 [ 1470/ 1682]\n",
      "loss: 9922.595703 [ 1480/ 1682]\n",
      "loss: 10994.417969 [ 1490/ 1682]\n",
      "loss: 11068.601562 [ 1500/ 1682]\n",
      "loss: 9264.073242 [ 1510/ 1682]\n",
      "loss: 10251.221680 [ 1520/ 1682]\n",
      "loss: 9524.262695 [ 1530/ 1682]\n",
      "loss: 8557.039062 [ 1540/ 1682]\n",
      "loss: 8665.216797 [ 1550/ 1682]\n",
      "loss: 10834.458984 [ 1560/ 1682]\n",
      "loss: 9069.843750 [ 1570/ 1682]\n",
      "loss: 7807.437500 [ 1580/ 1682]\n",
      "loss: 5698.915039 [ 1590/ 1682]\n",
      "loss: 5873.690430 [ 1600/ 1682]\n",
      "loss: 4281.193359 [ 1610/ 1682]\n",
      "loss: 5031.696777 [ 1620/ 1682]\n",
      "loss: 5994.322754 [ 1630/ 1682]\n",
      "loss: 7637.581055 [ 1640/ 1682]\n",
      "loss: 9766.306641 [ 1650/ 1682]\n",
      "loss: 8946.162109 [ 1660/ 1682]\n",
      "loss: 7344.010254 [ 1670/ 1682]\n",
      "loss: 7059.728516 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8689.325054 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 2155.496338 [    0/ 1682]\n",
      "loss: 2195.732422 [   10/ 1682]\n",
      "loss: 2326.632080 [   20/ 1682]\n",
      "loss: 2137.142578 [   30/ 1682]\n",
      "loss: 2172.798096 [   40/ 1682]\n",
      "loss: 2206.004395 [   50/ 1682]\n",
      "loss: 1795.682861 [   60/ 1682]\n",
      "loss: 2050.533691 [   70/ 1682]\n",
      "loss: 2394.945068 [   80/ 1682]\n",
      "loss: 2545.782471 [   90/ 1682]\n",
      "loss: 2150.663086 [  100/ 1682]\n",
      "loss: 2324.356689 [  110/ 1682]\n",
      "loss: 2174.521484 [  120/ 1682]\n",
      "loss: 2253.508057 [  130/ 1682]\n",
      "loss: 2193.524902 [  140/ 1682]\n",
      "loss: 1769.581299 [  150/ 1682]\n",
      "loss: 1837.315186 [  160/ 1682]\n",
      "loss: 1865.192383 [  170/ 1682]\n",
      "loss: 1904.849243 [  180/ 1682]\n",
      "loss: 1940.305908 [  190/ 1682]\n",
      "loss: 1784.665771 [  200/ 1682]\n",
      "loss: 1990.720093 [  210/ 1682]\n",
      "loss: 2169.956055 [  220/ 1682]\n",
      "loss: 1886.792725 [  230/ 1682]\n",
      "loss: 1906.428711 [  240/ 1682]\n",
      "loss: 1696.158447 [  250/ 1682]\n",
      "loss: 1909.008057 [  260/ 1682]\n",
      "loss: 1432.659180 [  270/ 1682]\n",
      "loss: 1252.434937 [  280/ 1682]\n",
      "loss: 1387.792480 [  290/ 1682]\n",
      "loss: 1263.409668 [  300/ 1682]\n",
      "loss: 1239.206421 [  310/ 1682]\n",
      "loss: 1315.907349 [  320/ 1682]\n",
      "loss: 1169.145508 [  330/ 1682]\n",
      "loss: 976.333191 [  340/ 1682]\n",
      "loss: 1141.829590 [  350/ 1682]\n",
      "loss: 1332.096924 [  360/ 1682]\n",
      "loss: 1196.948242 [  370/ 1682]\n",
      "loss: 1081.280640 [  380/ 1682]\n",
      "loss: 1162.596924 [  390/ 1682]\n",
      "loss: 1017.365356 [  400/ 1682]\n",
      "loss: 1030.939331 [  410/ 1682]\n",
      "loss: 1297.354370 [  420/ 1682]\n",
      "loss: 1023.165222 [  430/ 1682]\n",
      "loss: 1052.770874 [  440/ 1682]\n",
      "loss: 1017.542786 [  450/ 1682]\n",
      "loss: 742.945984 [  460/ 1682]\n",
      "loss: 813.647095 [  470/ 1682]\n",
      "loss: 811.083191 [  480/ 1682]\n",
      "loss: 908.275024 [  490/ 1682]\n",
      "loss: 905.377747 [  500/ 1682]\n",
      "loss: 893.618286 [  510/ 1682]\n",
      "loss: 989.089050 [  520/ 1682]\n",
      "loss: 754.994263 [  530/ 1682]\n",
      "loss: 672.257751 [  540/ 1682]\n",
      "loss: 778.549805 [  550/ 1682]\n",
      "loss: 725.118103 [  560/ 1682]\n",
      "loss: 898.817688 [  570/ 1682]\n",
      "loss: 586.040161 [  580/ 1682]\n",
      "loss: 677.575500 [  590/ 1682]\n",
      "loss: 580.450073 [  600/ 1682]\n",
      "loss: 588.943176 [  610/ 1682]\n",
      "loss: 548.360962 [  620/ 1682]\n",
      "loss: 531.926514 [  630/ 1682]\n",
      "loss: 491.732361 [  640/ 1682]\n",
      "loss: 314.297455 [  650/ 1682]\n",
      "loss: 220.063141 [  660/ 1682]\n",
      "loss: 277.990509 [  670/ 1682]\n",
      "loss: 229.386032 [  680/ 1682]\n",
      "loss: 258.376007 [  690/ 1682]\n",
      "loss: 290.925446 [  700/ 1682]\n",
      "loss: 410.876862 [  710/ 1682]\n",
      "loss: 708.957397 [  720/ 1682]\n",
      "loss: 796.936157 [  730/ 1682]\n",
      "loss: 1040.630249 [  740/ 1682]\n",
      "loss: 1037.235596 [  750/ 1682]\n",
      "loss: 907.828491 [  760/ 1682]\n",
      "loss: 744.957458 [  770/ 1682]\n",
      "loss: 669.980225 [  780/ 1682]\n",
      "loss: 606.923706 [  790/ 1682]\n",
      "loss: 483.045654 [  800/ 1682]\n",
      "loss: 402.334686 [  810/ 1682]\n",
      "loss: 340.603790 [  820/ 1682]\n",
      "loss: 372.506622 [  830/ 1682]\n",
      "loss: 583.197388 [  840/ 1682]\n",
      "loss: 486.188080 [  850/ 1682]\n",
      "loss: 399.245697 [  860/ 1682]\n",
      "loss: 460.153137 [  870/ 1682]\n",
      "loss: 340.137909 [  880/ 1682]\n",
      "loss: 404.771423 [  890/ 1682]\n",
      "loss: 280.866150 [  900/ 1682]\n",
      "loss: 358.888062 [  910/ 1682]\n",
      "loss: 201.783188 [  920/ 1682]\n",
      "loss: 231.445282 [  930/ 1682]\n",
      "loss: 152.529648 [  940/ 1682]\n",
      "loss: 73.542648 [  950/ 1682]\n",
      "loss: 19.196417 [  960/ 1682]\n",
      "loss: 20.085529 [  970/ 1682]\n",
      "loss: 6.229115 [  980/ 1682]\n",
      "loss: 9.272292 [  990/ 1682]\n",
      "loss: 57.912323 [ 1000/ 1682]\n",
      "loss: 84.415512 [ 1010/ 1682]\n",
      "loss: 106.409866 [ 1020/ 1682]\n",
      "loss: 51.165367 [ 1030/ 1682]\n",
      "loss: 40.818195 [ 1040/ 1682]\n",
      "loss: 108.589943 [ 1050/ 1682]\n",
      "loss: 35.146694 [ 1060/ 1682]\n",
      "loss: 11.161381 [ 1070/ 1682]\n",
      "loss: 48.523540 [ 1080/ 1682]\n",
      "loss: 82.080452 [ 1090/ 1682]\n",
      "loss: 158.508896 [ 1100/ 1682]\n",
      "loss: 371.843597 [ 1110/ 1682]\n",
      "loss: 518.764221 [ 1120/ 1682]\n",
      "loss: 721.567200 [ 1130/ 1682]\n",
      "loss: 1119.190186 [ 1140/ 1682]\n",
      "loss: 2150.291016 [ 1150/ 1682]\n",
      "loss: 3190.700439 [ 1160/ 1682]\n",
      "loss: 1833.514282 [ 1170/ 1682]\n",
      "loss: 1854.370728 [ 1180/ 1682]\n",
      "loss: 2412.848389 [ 1190/ 1682]\n",
      "loss: 1986.781982 [ 1200/ 1682]\n",
      "loss: 2148.905762 [ 1210/ 1682]\n",
      "loss: 2289.274902 [ 1220/ 1682]\n",
      "loss: 2938.433105 [ 1230/ 1682]\n",
      "loss: 3693.832764 [ 1240/ 1682]\n",
      "loss: 3279.567139 [ 1250/ 1682]\n",
      "loss: 4447.450195 [ 1260/ 1682]\n",
      "loss: 4335.451172 [ 1270/ 1682]\n",
      "loss: 3184.046143 [ 1280/ 1682]\n",
      "loss: 2649.630127 [ 1290/ 1682]\n",
      "loss: 2739.635498 [ 1300/ 1682]\n",
      "loss: 3391.154785 [ 1310/ 1682]\n",
      "loss: 4187.481445 [ 1320/ 1682]\n",
      "loss: 3296.687012 [ 1330/ 1682]\n",
      "loss: 3113.436035 [ 1340/ 1682]\n",
      "loss: 2889.918213 [ 1350/ 1682]\n",
      "loss: 3503.067627 [ 1360/ 1682]\n",
      "loss: 4730.022461 [ 1370/ 1682]\n",
      "loss: 5711.735840 [ 1380/ 1682]\n",
      "loss: 5626.591309 [ 1390/ 1682]\n",
      "loss: 5791.338867 [ 1400/ 1682]\n",
      "loss: 6371.455566 [ 1410/ 1682]\n",
      "loss: 6637.738281 [ 1420/ 1682]\n",
      "loss: 5557.107910 [ 1430/ 1682]\n",
      "loss: 5187.797363 [ 1440/ 1682]\n",
      "loss: 6076.001465 [ 1450/ 1682]\n",
      "loss: 5991.930664 [ 1460/ 1682]\n",
      "loss: 7704.268066 [ 1470/ 1682]\n",
      "loss: 9854.048828 [ 1480/ 1682]\n",
      "loss: 10758.994141 [ 1490/ 1682]\n",
      "loss: 11159.112305 [ 1500/ 1682]\n",
      "loss: 9369.223633 [ 1510/ 1682]\n",
      "loss: 10513.092773 [ 1520/ 1682]\n",
      "loss: 9308.160156 [ 1530/ 1682]\n",
      "loss: 8645.335938 [ 1540/ 1682]\n",
      "loss: 8438.664062 [ 1550/ 1682]\n",
      "loss: 10576.025391 [ 1560/ 1682]\n",
      "loss: 9184.498047 [ 1570/ 1682]\n",
      "loss: 7903.789062 [ 1580/ 1682]\n",
      "loss: 5515.591797 [ 1590/ 1682]\n",
      "loss: 5678.373047 [ 1600/ 1682]\n",
      "loss: 4491.375000 [ 1610/ 1682]\n",
      "loss: 4725.917480 [ 1620/ 1682]\n",
      "loss: 5783.506836 [ 1630/ 1682]\n",
      "loss: 7931.040527 [ 1640/ 1682]\n",
      "loss: 9027.356445 [ 1650/ 1682]\n",
      "loss: 9098.286133 [ 1660/ 1682]\n",
      "loss: 7127.314941 [ 1670/ 1682]\n",
      "loss: 7023.515137 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8669.103450 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 2055.281006 [    0/ 1682]\n",
      "loss: 2232.380127 [   10/ 1682]\n",
      "loss: 2223.456299 [   20/ 1682]\n",
      "loss: 2031.661133 [   30/ 1682]\n",
      "loss: 2196.869873 [   40/ 1682]\n",
      "loss: 2107.767334 [   50/ 1682]\n",
      "loss: 2090.535400 [   60/ 1682]\n",
      "loss: 2315.470947 [   70/ 1682]\n",
      "loss: 2436.000977 [   80/ 1682]\n",
      "loss: 2525.672607 [   90/ 1682]\n",
      "loss: 2167.941406 [  100/ 1682]\n",
      "loss: 2321.584473 [  110/ 1682]\n",
      "loss: 2725.866943 [  120/ 1682]\n",
      "loss: 2168.549316 [  130/ 1682]\n",
      "loss: 1999.984985 [  140/ 1682]\n",
      "loss: 2032.354492 [  150/ 1682]\n",
      "loss: 2327.633301 [  160/ 1682]\n",
      "loss: 2160.649902 [  170/ 1682]\n",
      "loss: 1934.824463 [  180/ 1682]\n",
      "loss: 2091.727783 [  190/ 1682]\n",
      "loss: 1824.188721 [  200/ 1682]\n",
      "loss: 2390.842285 [  210/ 1682]\n",
      "loss: 2079.651367 [  220/ 1682]\n",
      "loss: 1915.011108 [  230/ 1682]\n",
      "loss: 1834.744751 [  240/ 1682]\n",
      "loss: 1725.469727 [  250/ 1682]\n",
      "loss: 1738.894165 [  260/ 1682]\n",
      "loss: 1451.795654 [  270/ 1682]\n",
      "loss: 1612.960571 [  280/ 1682]\n",
      "loss: 1586.598877 [  290/ 1682]\n",
      "loss: 1446.306763 [  300/ 1682]\n",
      "loss: 1322.182495 [  310/ 1682]\n",
      "loss: 1330.436401 [  320/ 1682]\n",
      "loss: 1406.659790 [  330/ 1682]\n",
      "loss: 1135.528076 [  340/ 1682]\n",
      "loss: 1085.216675 [  350/ 1682]\n",
      "loss: 1193.109009 [  360/ 1682]\n",
      "loss: 1346.799194 [  370/ 1682]\n",
      "loss: 1093.845947 [  380/ 1682]\n",
      "loss: 1037.045166 [  390/ 1682]\n",
      "loss: 961.133179 [  400/ 1682]\n",
      "loss: 1098.471191 [  410/ 1682]\n",
      "loss: 984.703125 [  420/ 1682]\n",
      "loss: 1216.131470 [  430/ 1682]\n",
      "loss: 1237.088623 [  440/ 1682]\n",
      "loss: 958.331848 [  450/ 1682]\n",
      "loss: 904.984558 [  460/ 1682]\n",
      "loss: 814.436890 [  470/ 1682]\n",
      "loss: 912.362122 [  480/ 1682]\n",
      "loss: 764.195618 [  490/ 1682]\n",
      "loss: 662.866089 [  500/ 1682]\n",
      "loss: 754.161316 [  510/ 1682]\n",
      "loss: 1036.248413 [  520/ 1682]\n",
      "loss: 759.302856 [  530/ 1682]\n",
      "loss: 676.895691 [  540/ 1682]\n",
      "loss: 877.249817 [  550/ 1682]\n",
      "loss: 775.911316 [  560/ 1682]\n",
      "loss: 809.261169 [  570/ 1682]\n",
      "loss: 670.991882 [  580/ 1682]\n",
      "loss: 601.650574 [  590/ 1682]\n",
      "loss: 542.976807 [  600/ 1682]\n",
      "loss: 523.774719 [  610/ 1682]\n",
      "loss: 593.396973 [  620/ 1682]\n",
      "loss: 540.369080 [  630/ 1682]\n",
      "loss: 374.114319 [  640/ 1682]\n",
      "loss: 296.983826 [  650/ 1682]\n",
      "loss: 224.011322 [  660/ 1682]\n",
      "loss: 255.616669 [  670/ 1682]\n",
      "loss: 184.141510 [  680/ 1682]\n",
      "loss: 266.477905 [  690/ 1682]\n",
      "loss: 264.123108 [  700/ 1682]\n",
      "loss: 479.614410 [  710/ 1682]\n",
      "loss: 639.775757 [  720/ 1682]\n",
      "loss: 797.232117 [  730/ 1682]\n",
      "loss: 966.621094 [  740/ 1682]\n",
      "loss: 1080.767334 [  750/ 1682]\n",
      "loss: 912.085571 [  760/ 1682]\n",
      "loss: 780.072449 [  770/ 1682]\n",
      "loss: 714.738281 [  780/ 1682]\n",
      "loss: 613.580933 [  790/ 1682]\n",
      "loss: 519.840576 [  800/ 1682]\n",
      "loss: 464.678711 [  810/ 1682]\n",
      "loss: 373.137939 [  820/ 1682]\n",
      "loss: 429.965332 [  830/ 1682]\n",
      "loss: 699.922058 [  840/ 1682]\n",
      "loss: 552.682556 [  850/ 1682]\n",
      "loss: 403.492218 [  860/ 1682]\n",
      "loss: 393.739807 [  870/ 1682]\n",
      "loss: 324.278564 [  880/ 1682]\n",
      "loss: 350.233551 [  890/ 1682]\n",
      "loss: 337.381653 [  900/ 1682]\n",
      "loss: 323.848328 [  910/ 1682]\n",
      "loss: 267.254333 [  920/ 1682]\n",
      "loss: 196.049179 [  930/ 1682]\n",
      "loss: 127.039429 [  940/ 1682]\n",
      "loss: 80.808258 [  950/ 1682]\n",
      "loss: 28.180246 [  960/ 1682]\n",
      "loss: 29.683023 [  970/ 1682]\n",
      "loss: 10.939179 [  980/ 1682]\n",
      "loss: 10.199962 [  990/ 1682]\n",
      "loss: 68.332085 [ 1000/ 1682]\n",
      "loss: 99.702133 [ 1010/ 1682]\n",
      "loss: 120.765907 [ 1020/ 1682]\n",
      "loss: 50.160149 [ 1030/ 1682]\n",
      "loss: 38.524769 [ 1040/ 1682]\n",
      "loss: 92.322189 [ 1050/ 1682]\n",
      "loss: 32.354374 [ 1060/ 1682]\n",
      "loss: 9.725642 [ 1070/ 1682]\n",
      "loss: 32.918812 [ 1080/ 1682]\n",
      "loss: 95.694138 [ 1090/ 1682]\n",
      "loss: 173.349518 [ 1100/ 1682]\n",
      "loss: 328.123260 [ 1110/ 1682]\n",
      "loss: 558.715942 [ 1120/ 1682]\n",
      "loss: 711.327820 [ 1130/ 1682]\n",
      "loss: 1216.466797 [ 1140/ 1682]\n",
      "loss: 2043.514893 [ 1150/ 1682]\n",
      "loss: 3174.666748 [ 1160/ 1682]\n",
      "loss: 1766.628662 [ 1170/ 1682]\n",
      "loss: 1840.013062 [ 1180/ 1682]\n",
      "loss: 2251.685059 [ 1190/ 1682]\n",
      "loss: 1924.177734 [ 1200/ 1682]\n",
      "loss: 2270.713135 [ 1210/ 1682]\n",
      "loss: 2319.910645 [ 1220/ 1682]\n",
      "loss: 2806.186768 [ 1230/ 1682]\n",
      "loss: 3733.268066 [ 1240/ 1682]\n",
      "loss: 3333.301514 [ 1250/ 1682]\n",
      "loss: 4212.217773 [ 1260/ 1682]\n",
      "loss: 4233.283691 [ 1270/ 1682]\n",
      "loss: 3357.962158 [ 1280/ 1682]\n",
      "loss: 2469.719238 [ 1290/ 1682]\n",
      "loss: 2612.867920 [ 1300/ 1682]\n",
      "loss: 3368.281982 [ 1310/ 1682]\n",
      "loss: 4089.685547 [ 1320/ 1682]\n",
      "loss: 3551.813232 [ 1330/ 1682]\n",
      "loss: 3225.400146 [ 1340/ 1682]\n",
      "loss: 2945.887939 [ 1350/ 1682]\n",
      "loss: 3619.221436 [ 1360/ 1682]\n",
      "loss: 4349.971680 [ 1370/ 1682]\n",
      "loss: 5876.575195 [ 1380/ 1682]\n",
      "loss: 5896.071289 [ 1390/ 1682]\n",
      "loss: 5976.220215 [ 1400/ 1682]\n",
      "loss: 6549.889160 [ 1410/ 1682]\n",
      "loss: 6514.446289 [ 1420/ 1682]\n",
      "loss: 5534.114746 [ 1430/ 1682]\n",
      "loss: 5077.382812 [ 1440/ 1682]\n",
      "loss: 6167.669434 [ 1450/ 1682]\n",
      "loss: 6103.821777 [ 1460/ 1682]\n",
      "loss: 7449.514160 [ 1470/ 1682]\n",
      "loss: 9825.150391 [ 1480/ 1682]\n",
      "loss: 10905.842773 [ 1490/ 1682]\n",
      "loss: 11141.455078 [ 1500/ 1682]\n",
      "loss: 8923.248047 [ 1510/ 1682]\n",
      "loss: 10332.068359 [ 1520/ 1682]\n",
      "loss: 9473.433594 [ 1530/ 1682]\n",
      "loss: 8185.090820 [ 1540/ 1682]\n",
      "loss: 8440.747070 [ 1550/ 1682]\n",
      "loss: 10603.239258 [ 1560/ 1682]\n",
      "loss: 8998.865234 [ 1570/ 1682]\n",
      "loss: 7902.002930 [ 1580/ 1682]\n",
      "loss: 5886.112305 [ 1590/ 1682]\n",
      "loss: 5435.221680 [ 1600/ 1682]\n",
      "loss: 4506.752441 [ 1610/ 1682]\n",
      "loss: 4718.139648 [ 1620/ 1682]\n",
      "loss: 5942.215332 [ 1630/ 1682]\n",
      "loss: 7403.955566 [ 1640/ 1682]\n",
      "loss: 9489.640625 [ 1650/ 1682]\n",
      "loss: 9070.833008 [ 1660/ 1682]\n",
      "loss: 7282.264648 [ 1670/ 1682]\n",
      "loss: 6988.827148 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8635.354586 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 2305.287109 [    0/ 1682]\n",
      "loss: 2339.256348 [   10/ 1682]\n",
      "loss: 2354.009277 [   20/ 1682]\n",
      "loss: 2404.798828 [   30/ 1682]\n",
      "loss: 1961.302490 [   40/ 1682]\n",
      "loss: 2101.935547 [   50/ 1682]\n",
      "loss: 1831.656860 [   60/ 1682]\n",
      "loss: 2207.043701 [   70/ 1682]\n",
      "loss: 2664.892578 [   80/ 1682]\n",
      "loss: 2345.827881 [   90/ 1682]\n",
      "loss: 2170.165527 [  100/ 1682]\n",
      "loss: 2466.783936 [  110/ 1682]\n",
      "loss: 2346.325928 [  120/ 1682]\n",
      "loss: 2148.225098 [  130/ 1682]\n",
      "loss: 2113.968262 [  140/ 1682]\n",
      "loss: 2022.874390 [  150/ 1682]\n",
      "loss: 2321.550049 [  160/ 1682]\n",
      "loss: 1904.269287 [  170/ 1682]\n",
      "loss: 1821.070679 [  180/ 1682]\n",
      "loss: 2048.773926 [  190/ 1682]\n",
      "loss: 2011.280273 [  200/ 1682]\n",
      "loss: 2125.059326 [  210/ 1682]\n",
      "loss: 2176.135010 [  220/ 1682]\n",
      "loss: 2013.250732 [  230/ 1682]\n",
      "loss: 2006.552490 [  240/ 1682]\n",
      "loss: 1720.975586 [  250/ 1682]\n",
      "loss: 1650.785156 [  260/ 1682]\n",
      "loss: 1538.446533 [  270/ 1682]\n",
      "loss: 1513.607544 [  280/ 1682]\n",
      "loss: 1330.595825 [  290/ 1682]\n",
      "loss: 1359.556885 [  300/ 1682]\n",
      "loss: 1182.198853 [  310/ 1682]\n",
      "loss: 1465.516724 [  320/ 1682]\n",
      "loss: 1249.119385 [  330/ 1682]\n",
      "loss: 1131.788330 [  340/ 1682]\n",
      "loss: 1290.954468 [  350/ 1682]\n",
      "loss: 1198.237427 [  360/ 1682]\n",
      "loss: 1352.622437 [  370/ 1682]\n",
      "loss: 1161.771240 [  380/ 1682]\n",
      "loss: 1045.088013 [  390/ 1682]\n",
      "loss: 970.100586 [  400/ 1682]\n",
      "loss: 1033.804443 [  410/ 1682]\n",
      "loss: 995.951172 [  420/ 1682]\n",
      "loss: 1098.076416 [  430/ 1682]\n",
      "loss: 954.428711 [  440/ 1682]\n",
      "loss: 859.302917 [  450/ 1682]\n",
      "loss: 760.083801 [  460/ 1682]\n",
      "loss: 868.236023 [  470/ 1682]\n",
      "loss: 914.638672 [  480/ 1682]\n",
      "loss: 777.472778 [  490/ 1682]\n",
      "loss: 804.236816 [  500/ 1682]\n",
      "loss: 920.175659 [  510/ 1682]\n",
      "loss: 1029.334717 [  520/ 1682]\n",
      "loss: 735.705200 [  530/ 1682]\n",
      "loss: 816.878906 [  540/ 1682]\n",
      "loss: 926.716492 [  550/ 1682]\n",
      "loss: 790.255920 [  560/ 1682]\n",
      "loss: 824.825012 [  570/ 1682]\n",
      "loss: 687.535217 [  580/ 1682]\n",
      "loss: 645.728882 [  590/ 1682]\n",
      "loss: 516.069946 [  600/ 1682]\n",
      "loss: 567.716248 [  610/ 1682]\n",
      "loss: 596.292175 [  620/ 1682]\n",
      "loss: 509.631439 [  630/ 1682]\n",
      "loss: 477.488617 [  640/ 1682]\n",
      "loss: 345.956970 [  650/ 1682]\n",
      "loss: 207.056442 [  660/ 1682]\n",
      "loss: 266.744324 [  670/ 1682]\n",
      "loss: 234.254242 [  680/ 1682]\n",
      "loss: 224.213043 [  690/ 1682]\n",
      "loss: 293.266022 [  700/ 1682]\n",
      "loss: 456.208069 [  710/ 1682]\n",
      "loss: 766.426147 [  720/ 1682]\n",
      "loss: 843.421692 [  730/ 1682]\n",
      "loss: 1022.422241 [  740/ 1682]\n",
      "loss: 1007.664734 [  750/ 1682]\n",
      "loss: 1088.806641 [  760/ 1682]\n",
      "loss: 824.013794 [  770/ 1682]\n",
      "loss: 726.583252 [  780/ 1682]\n",
      "loss: 661.292908 [  790/ 1682]\n",
      "loss: 589.545044 [  800/ 1682]\n",
      "loss: 440.125183 [  810/ 1682]\n",
      "loss: 353.104340 [  820/ 1682]\n",
      "loss: 387.870514 [  830/ 1682]\n",
      "loss: 567.264465 [  840/ 1682]\n",
      "loss: 557.366394 [  850/ 1682]\n",
      "loss: 435.596191 [  860/ 1682]\n",
      "loss: 398.793518 [  870/ 1682]\n",
      "loss: 308.236542 [  880/ 1682]\n",
      "loss: 399.953033 [  890/ 1682]\n",
      "loss: 317.691803 [  900/ 1682]\n",
      "loss: 325.415375 [  910/ 1682]\n",
      "loss: 229.879517 [  920/ 1682]\n",
      "loss: 201.130783 [  930/ 1682]\n",
      "loss: 159.748840 [  940/ 1682]\n",
      "loss: 98.598969 [  950/ 1682]\n",
      "loss: 28.352829 [  960/ 1682]\n",
      "loss: 10.877391 [  970/ 1682]\n",
      "loss: 10.104080 [  980/ 1682]\n",
      "loss: 8.721307 [  990/ 1682]\n",
      "loss: 53.156349 [ 1000/ 1682]\n",
      "loss: 80.804443 [ 1010/ 1682]\n",
      "loss: 102.485458 [ 1020/ 1682]\n",
      "loss: 32.055428 [ 1030/ 1682]\n",
      "loss: 21.936275 [ 1040/ 1682]\n",
      "loss: 78.106964 [ 1050/ 1682]\n",
      "loss: 25.563290 [ 1060/ 1682]\n",
      "loss: 5.662807 [ 1070/ 1682]\n",
      "loss: 45.304058 [ 1080/ 1682]\n",
      "loss: 79.658401 [ 1090/ 1682]\n",
      "loss: 146.621323 [ 1100/ 1682]\n",
      "loss: 340.875793 [ 1110/ 1682]\n",
      "loss: 526.240417 [ 1120/ 1682]\n",
      "loss: 671.756714 [ 1130/ 1682]\n",
      "loss: 1240.112915 [ 1140/ 1682]\n",
      "loss: 1937.426025 [ 1150/ 1682]\n",
      "loss: 3097.653809 [ 1160/ 1682]\n",
      "loss: 1757.354736 [ 1170/ 1682]\n",
      "loss: 1760.557007 [ 1180/ 1682]\n",
      "loss: 2326.641113 [ 1190/ 1682]\n",
      "loss: 1866.692139 [ 1200/ 1682]\n",
      "loss: 2209.354736 [ 1210/ 1682]\n",
      "loss: 2192.366455 [ 1220/ 1682]\n",
      "loss: 2785.928467 [ 1230/ 1682]\n",
      "loss: 3380.518799 [ 1240/ 1682]\n",
      "loss: 3485.426270 [ 1250/ 1682]\n",
      "loss: 4241.096191 [ 1260/ 1682]\n",
      "loss: 4285.309570 [ 1270/ 1682]\n",
      "loss: 3268.644287 [ 1280/ 1682]\n",
      "loss: 2540.852539 [ 1290/ 1682]\n",
      "loss: 2589.535645 [ 1300/ 1682]\n",
      "loss: 3275.873535 [ 1310/ 1682]\n",
      "loss: 4067.769531 [ 1320/ 1682]\n",
      "loss: 3476.328125 [ 1330/ 1682]\n",
      "loss: 3084.584961 [ 1340/ 1682]\n",
      "loss: 2998.762207 [ 1350/ 1682]\n",
      "loss: 3608.568359 [ 1360/ 1682]\n",
      "loss: 4594.993652 [ 1370/ 1682]\n",
      "loss: 5756.463379 [ 1380/ 1682]\n",
      "loss: 5878.463867 [ 1390/ 1682]\n",
      "loss: 5833.774414 [ 1400/ 1682]\n",
      "loss: 6198.058105 [ 1410/ 1682]\n",
      "loss: 6377.460449 [ 1420/ 1682]\n",
      "loss: 5492.278809 [ 1430/ 1682]\n",
      "loss: 5239.480957 [ 1440/ 1682]\n",
      "loss: 6020.084961 [ 1450/ 1682]\n",
      "loss: 6502.978027 [ 1460/ 1682]\n",
      "loss: 7449.277344 [ 1470/ 1682]\n",
      "loss: 9526.705078 [ 1480/ 1682]\n",
      "loss: 10853.392578 [ 1490/ 1682]\n",
      "loss: 10786.287109 [ 1500/ 1682]\n",
      "loss: 9309.590820 [ 1510/ 1682]\n",
      "loss: 9966.982422 [ 1520/ 1682]\n",
      "loss: 9592.973633 [ 1530/ 1682]\n",
      "loss: 8037.489746 [ 1540/ 1682]\n",
      "loss: 8421.793945 [ 1550/ 1682]\n",
      "loss: 11076.576172 [ 1560/ 1682]\n",
      "loss: 9092.783203 [ 1570/ 1682]\n",
      "loss: 8032.912598 [ 1580/ 1682]\n",
      "loss: 5605.504883 [ 1590/ 1682]\n",
      "loss: 5531.739746 [ 1600/ 1682]\n",
      "loss: 4495.399902 [ 1610/ 1682]\n",
      "loss: 4704.778809 [ 1620/ 1682]\n",
      "loss: 6193.072754 [ 1630/ 1682]\n",
      "loss: 7374.278320 [ 1640/ 1682]\n",
      "loss: 10039.421875 [ 1650/ 1682]\n",
      "loss: 8870.673828 [ 1660/ 1682]\n",
      "loss: 7106.166504 [ 1670/ 1682]\n",
      "loss: 6954.982422 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8650.347544 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 2548.503174 [    0/ 1682]\n",
      "loss: 2131.046631 [   10/ 1682]\n",
      "loss: 2505.048828 [   20/ 1682]\n",
      "loss: 2301.951172 [   30/ 1682]\n",
      "loss: 1979.301758 [   40/ 1682]\n",
      "loss: 1999.132446 [   50/ 1682]\n",
      "loss: 1955.208618 [   60/ 1682]\n",
      "loss: 1990.778076 [   70/ 1682]\n",
      "loss: 2315.992920 [   80/ 1682]\n",
      "loss: 2117.010742 [   90/ 1682]\n",
      "loss: 2637.318359 [  100/ 1682]\n",
      "loss: 2463.660889 [  110/ 1682]\n",
      "loss: 2224.564453 [  120/ 1682]\n",
      "loss: 2393.902344 [  130/ 1682]\n",
      "loss: 1900.810181 [  140/ 1682]\n",
      "loss: 1925.897827 [  150/ 1682]\n",
      "loss: 2204.261963 [  160/ 1682]\n",
      "loss: 2013.522705 [  170/ 1682]\n",
      "loss: 2130.875732 [  180/ 1682]\n",
      "loss: 1775.281860 [  190/ 1682]\n",
      "loss: 2024.006836 [  200/ 1682]\n",
      "loss: 2129.241699 [  210/ 1682]\n",
      "loss: 2175.723389 [  220/ 1682]\n",
      "loss: 1937.028076 [  230/ 1682]\n",
      "loss: 1750.057617 [  240/ 1682]\n",
      "loss: 1737.990234 [  250/ 1682]\n",
      "loss: 1835.278687 [  260/ 1682]\n",
      "loss: 1706.913818 [  270/ 1682]\n",
      "loss: 1370.989502 [  280/ 1682]\n",
      "loss: 1267.465210 [  290/ 1682]\n",
      "loss: 1448.747803 [  300/ 1682]\n",
      "loss: 1417.058472 [  310/ 1682]\n",
      "loss: 1471.420166 [  320/ 1682]\n",
      "loss: 1208.318848 [  330/ 1682]\n",
      "loss: 1015.549011 [  340/ 1682]\n",
      "loss: 1101.629517 [  350/ 1682]\n",
      "loss: 1276.743530 [  360/ 1682]\n",
      "loss: 1165.962158 [  370/ 1682]\n",
      "loss: 1112.096436 [  380/ 1682]\n",
      "loss: 1175.862061 [  390/ 1682]\n",
      "loss: 980.909973 [  400/ 1682]\n",
      "loss: 1185.579834 [  410/ 1682]\n",
      "loss: 1008.825195 [  420/ 1682]\n",
      "loss: 1108.541260 [  430/ 1682]\n",
      "loss: 1144.985596 [  440/ 1682]\n",
      "loss: 923.329956 [  450/ 1682]\n",
      "loss: 1023.213684 [  460/ 1682]\n",
      "loss: 840.089661 [  470/ 1682]\n",
      "loss: 789.241577 [  480/ 1682]\n",
      "loss: 923.818665 [  490/ 1682]\n",
      "loss: 822.402466 [  500/ 1682]\n",
      "loss: 883.426453 [  510/ 1682]\n",
      "loss: 952.541321 [  520/ 1682]\n",
      "loss: 787.651428 [  530/ 1682]\n",
      "loss: 800.029846 [  540/ 1682]\n",
      "loss: 847.438843 [  550/ 1682]\n",
      "loss: 806.680359 [  560/ 1682]\n",
      "loss: 839.451050 [  570/ 1682]\n",
      "loss: 654.120483 [  580/ 1682]\n",
      "loss: 617.312561 [  590/ 1682]\n",
      "loss: 631.671631 [  600/ 1682]\n",
      "loss: 666.252808 [  610/ 1682]\n",
      "loss: 640.995728 [  620/ 1682]\n",
      "loss: 586.975342 [  630/ 1682]\n",
      "loss: 421.891510 [  640/ 1682]\n",
      "loss: 356.995544 [  650/ 1682]\n",
      "loss: 235.866211 [  660/ 1682]\n",
      "loss: 270.614288 [  670/ 1682]\n",
      "loss: 215.092743 [  680/ 1682]\n",
      "loss: 276.122833 [  690/ 1682]\n",
      "loss: 310.347046 [  700/ 1682]\n",
      "loss: 443.161072 [  710/ 1682]\n",
      "loss: 763.152954 [  720/ 1682]\n",
      "loss: 742.101990 [  730/ 1682]\n",
      "loss: 990.587219 [  740/ 1682]\n",
      "loss: 1060.483276 [  750/ 1682]\n",
      "loss: 935.377441 [  760/ 1682]\n",
      "loss: 767.565491 [  770/ 1682]\n",
      "loss: 733.277039 [  780/ 1682]\n",
      "loss: 690.871155 [  790/ 1682]\n",
      "loss: 532.736694 [  800/ 1682]\n",
      "loss: 423.519440 [  810/ 1682]\n",
      "loss: 438.508209 [  820/ 1682]\n",
      "loss: 418.724762 [  830/ 1682]\n",
      "loss: 571.088318 [  840/ 1682]\n",
      "loss: 600.350525 [  850/ 1682]\n",
      "loss: 428.322693 [  860/ 1682]\n",
      "loss: 384.944122 [  870/ 1682]\n",
      "loss: 403.419464 [  880/ 1682]\n",
      "loss: 391.254669 [  890/ 1682]\n",
      "loss: 349.841949 [  900/ 1682]\n",
      "loss: 291.018829 [  910/ 1682]\n",
      "loss: 236.886642 [  920/ 1682]\n",
      "loss: 247.226852 [  930/ 1682]\n",
      "loss: 166.095779 [  940/ 1682]\n",
      "loss: 58.161793 [  950/ 1682]\n",
      "loss: 37.691307 [  960/ 1682]\n",
      "loss: 12.207355 [  970/ 1682]\n",
      "loss: 10.794998 [  980/ 1682]\n",
      "loss: 12.469246 [  990/ 1682]\n",
      "loss: 51.406425 [ 1000/ 1682]\n",
      "loss: 67.288345 [ 1010/ 1682]\n",
      "loss: 121.559570 [ 1020/ 1682]\n",
      "loss: 30.021671 [ 1030/ 1682]\n",
      "loss: 31.338772 [ 1040/ 1682]\n",
      "loss: 86.742424 [ 1050/ 1682]\n",
      "loss: 27.854959 [ 1060/ 1682]\n",
      "loss: 3.659354 [ 1070/ 1682]\n",
      "loss: 37.309235 [ 1080/ 1682]\n",
      "loss: 93.475983 [ 1090/ 1682]\n",
      "loss: 173.842026 [ 1100/ 1682]\n",
      "loss: 353.205383 [ 1110/ 1682]\n",
      "loss: 539.824829 [ 1120/ 1682]\n",
      "loss: 638.085083 [ 1130/ 1682]\n",
      "loss: 1077.814819 [ 1140/ 1682]\n",
      "loss: 2006.719727 [ 1150/ 1682]\n",
      "loss: 3018.947754 [ 1160/ 1682]\n",
      "loss: 1782.681274 [ 1170/ 1682]\n",
      "loss: 1853.047852 [ 1180/ 1682]\n",
      "loss: 2353.191650 [ 1190/ 1682]\n",
      "loss: 1770.004883 [ 1200/ 1682]\n",
      "loss: 2230.198730 [ 1210/ 1682]\n",
      "loss: 2237.214111 [ 1220/ 1682]\n",
      "loss: 2823.017090 [ 1230/ 1682]\n",
      "loss: 3624.480469 [ 1240/ 1682]\n",
      "loss: 3416.930176 [ 1250/ 1682]\n",
      "loss: 4235.379883 [ 1260/ 1682]\n",
      "loss: 4052.088623 [ 1270/ 1682]\n",
      "loss: 3189.874512 [ 1280/ 1682]\n",
      "loss: 2591.546143 [ 1290/ 1682]\n",
      "loss: 2581.179688 [ 1300/ 1682]\n",
      "loss: 3271.860840 [ 1310/ 1682]\n",
      "loss: 3969.940674 [ 1320/ 1682]\n",
      "loss: 3515.027344 [ 1330/ 1682]\n",
      "loss: 3179.723877 [ 1340/ 1682]\n",
      "loss: 2975.883545 [ 1350/ 1682]\n",
      "loss: 3652.467529 [ 1360/ 1682]\n",
      "loss: 4654.747559 [ 1370/ 1682]\n",
      "loss: 5645.413574 [ 1380/ 1682]\n",
      "loss: 5730.427734 [ 1390/ 1682]\n",
      "loss: 6009.828125 [ 1400/ 1682]\n",
      "loss: 6194.621582 [ 1410/ 1682]\n",
      "loss: 6557.703125 [ 1420/ 1682]\n",
      "loss: 5373.332520 [ 1430/ 1682]\n",
      "loss: 5126.004883 [ 1440/ 1682]\n",
      "loss: 5912.259277 [ 1450/ 1682]\n",
      "loss: 6257.476074 [ 1460/ 1682]\n",
      "loss: 7620.037598 [ 1470/ 1682]\n",
      "loss: 9746.244141 [ 1480/ 1682]\n",
      "loss: 10954.154297 [ 1490/ 1682]\n",
      "loss: 11080.931641 [ 1500/ 1682]\n",
      "loss: 9262.666016 [ 1510/ 1682]\n",
      "loss: 10403.805664 [ 1520/ 1682]\n",
      "loss: 9384.682617 [ 1530/ 1682]\n",
      "loss: 7890.657715 [ 1540/ 1682]\n",
      "loss: 8406.240234 [ 1550/ 1682]\n",
      "loss: 11042.395508 [ 1560/ 1682]\n",
      "loss: 9242.150391 [ 1570/ 1682]\n",
      "loss: 7726.890625 [ 1580/ 1682]\n",
      "loss: 5456.856934 [ 1590/ 1682]\n",
      "loss: 5759.090820 [ 1600/ 1682]\n",
      "loss: 4582.102051 [ 1610/ 1682]\n",
      "loss: 4915.138672 [ 1620/ 1682]\n",
      "loss: 5785.100098 [ 1630/ 1682]\n",
      "loss: 7380.474121 [ 1640/ 1682]\n",
      "loss: 9464.808594 [ 1650/ 1682]\n",
      "loss: 9221.576172 [ 1660/ 1682]\n",
      "loss: 7401.340820 [ 1670/ 1682]\n",
      "loss: 5447.499512 [  336/ 1682]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 8701.801298 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f'Epoch {i+1}\\n-------------------------------')\n",
    "    train_loop(train_dataloader, model, loss, optimizer)\n",
    "    test_loop(test_dataloader, model, loss)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (input_layer): Linear(in_features=960, out_features=480, bias=True)\n",
       "  (hidden_layer1): Linear(in_features=480, out_features=128, bias=True)\n",
       "  (dropout_layer1): Dropout(p=0.2, inplace=False)\n",
       "  (hidden_layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout_layer2): Dropout(p=0.2, inplace=False)\n",
       "  (hidden_layer3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[3.1330e+00, 3.0677e+04, 1.2731e+02,  ..., 1.4300e-01, 9.9883e-01,\n",
       "          9.0000e+00],\n",
       "         [3.1750e+00, 3.1501e+04, 1.3189e+02,  ..., 1.4329e-01, 1.0022e+00,\n",
       "          9.0000e+00],\n",
       "         [3.2580e+00, 3.1438e+04, 1.3100e+02,  ..., 1.4274e-01, 1.0030e+00,\n",
       "          9.0000e+00],\n",
       "         ...,\n",
       "         [2.8150e+00, 3.0968e+04, 1.2750e+02,  ..., 1.3990e-01, 9.6237e-01,\n",
       "          9.0000e+00],\n",
       "         [2.9510e+00, 3.1038e+04, 1.2860e+02,  ..., 1.3934e-01, 9.5962e-01,\n",
       "          9.0000e+00],\n",
       "         [3.0540e+00, 3.1385e+04, 1.3126e+02,  ..., 1.3891e-01, 9.7082e-01,\n",
       "          9.0000e+00]]),\n",
       " tensor([[155],\n",
       "         [152],\n",
       "         [151],\n",
       "         [149],\n",
       "         [149],\n",
       "         [150],\n",
       "         [148],\n",
       "         [141],\n",
       "         [137],\n",
       "         [141]])]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488,\n",
       "       67.84488, 67.84488, 67.84488, 67.84488, 67.84488, 67.84488],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mlp = model(X_test_torch).squeeze().detach().numpy()\n",
    "pred_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.13299990e+00, 3.06773594e+04, 1.27313385e+02, ...,\n",
       "         1.49229228e-01, 1.05641246e+00, 6.00000000e+00],\n",
       "        [3.17499995e+00, 3.15006797e+04, 1.31889236e+02, ...,\n",
       "         1.49313897e-01, 1.05201149e+00, 6.00000000e+00],\n",
       "        [3.25799990e+00, 3.14382598e+04, 1.30999741e+02, ...,\n",
       "         1.49503648e-01, 1.05661333e+00, 6.00000000e+00],\n",
       "        ...,\n",
       "        [3.59500003e+00, 3.11350898e+04, 1.30930573e+02, ...,\n",
       "         1.44423097e-01, 9.98203278e-01, 9.00000000e+00],\n",
       "        [3.67400002e+00, 3.09618203e+04, 1.27916229e+02, ...,\n",
       "         1.43657520e-01, 9.98402536e-01, 9.00000000e+00],\n",
       "        [3.62599993e+00, 3.08224199e+04, 1.27204681e+02, ...,\n",
       "         1.43000141e-01, 9.98831332e-01, 9.00000000e+00]],\n",
       "\n",
       "       [[3.17499995e+00, 3.15006797e+04, 1.31889236e+02, ...,\n",
       "         1.49313897e-01, 1.05201149e+00, 6.00000000e+00],\n",
       "        [3.25799990e+00, 3.14382598e+04, 1.30999741e+02, ...,\n",
       "         1.49503648e-01, 1.05661333e+00, 6.00000000e+00],\n",
       "        [3.25999999e+00, 3.09469902e+04, 1.27076180e+02, ...,\n",
       "         1.49461195e-01, 1.05808914e+00, 6.00000000e+00],\n",
       "        ...,\n",
       "        [3.67400002e+00, 3.09618203e+04, 1.27916229e+02, ...,\n",
       "         1.43657520e-01, 9.98402536e-01, 9.00000000e+00],\n",
       "        [3.62599993e+00, 3.08224199e+04, 1.27204681e+02, ...,\n",
       "         1.43000141e-01, 9.98831332e-01, 9.00000000e+00],\n",
       "        [3.69300008e+00, 3.10196797e+04, 1.28241974e+02, ...,\n",
       "         1.43287003e-01, 1.00223494e+00, 9.00000000e+00]],\n",
       "\n",
       "       [[3.25799990e+00, 3.14382598e+04, 1.30999741e+02, ...,\n",
       "         1.49503648e-01, 1.05661333e+00, 6.00000000e+00],\n",
       "        [3.25999999e+00, 3.09469902e+04, 1.27076180e+02, ...,\n",
       "         1.49461195e-01, 1.05808914e+00, 6.00000000e+00],\n",
       "        [3.15100002e+00, 3.10293105e+04, 1.27333145e+02, ...,\n",
       "         1.49091288e-01, 1.05235457e+00, 6.00000000e+00],\n",
       "        ...,\n",
       "        [3.62599993e+00, 3.08224199e+04, 1.27204681e+02, ...,\n",
       "         1.43000141e-01, 9.98831332e-01, 9.00000000e+00],\n",
       "        [3.69300008e+00, 3.10196797e+04, 1.28241974e+02, ...,\n",
       "         1.43287003e-01, 1.00223494e+00, 9.00000000e+00],\n",
       "        [3.75000000e+00, 3.07062305e+04, 1.27489006e+02, ...,\n",
       "         1.42742947e-01, 1.00295877e+00, 9.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3.95499992e+00, 3.39515195e+04, 1.68759521e+02, ...,\n",
       "         1.39260247e-01, 1.09203684e+00, 6.00000000e+00],\n",
       "        [4.04600000e+00, 3.39467109e+04, 1.70136581e+02, ...,\n",
       "         1.39306813e-01, 1.09926355e+00, 6.00000000e+00],\n",
       "        [3.99499989e+00, 3.37274297e+04, 1.68290527e+02, ...,\n",
       "         1.39304861e-01, 1.09589040e+00, 6.00000000e+00],\n",
       "        ...,\n",
       "        [4.40999985e+00, 3.46459883e+04, 1.70176498e+02, ...,\n",
       "         1.37211859e-01, 1.07503760e+00, 9.00000000e+00],\n",
       "        [4.39099979e+00, 3.45755312e+04, 1.70605591e+02, ...,\n",
       "         1.37157276e-01, 1.07532668e+00, 9.00000000e+00],\n",
       "        [4.41900015e+00, 3.49071094e+04, 1.71842941e+02, ...,\n",
       "         1.37574300e-01, 1.07342207e+00, 9.00000000e+00]],\n",
       "\n",
       "       [[4.04600000e+00, 3.39467109e+04, 1.70136581e+02, ...,\n",
       "         1.39306813e-01, 1.09926355e+00, 6.00000000e+00],\n",
       "        [3.99499989e+00, 3.37274297e+04, 1.68290527e+02, ...,\n",
       "         1.39304861e-01, 1.09589040e+00, 6.00000000e+00],\n",
       "        [3.96300006e+00, 3.37147109e+04, 1.67083115e+02, ...,\n",
       "         1.39304861e-01, 1.09066713e+00, 6.00000000e+00],\n",
       "        ...,\n",
       "        [4.39099979e+00, 3.45755312e+04, 1.70605591e+02, ...,\n",
       "         1.37157276e-01, 1.07532668e+00, 9.00000000e+00],\n",
       "        [4.41900015e+00, 3.49071094e+04, 1.71842941e+02, ...,\n",
       "         1.37574300e-01, 1.07342207e+00, 9.00000000e+00],\n",
       "        [4.45300007e+00, 3.46182383e+04, 1.68539993e+02, ...,\n",
       "         1.37440041e-01, 1.06371665e+00, 9.00000000e+00]],\n",
       "\n",
       "       [[3.99499989e+00, 3.37274297e+04, 1.68290527e+02, ...,\n",
       "         1.39304861e-01, 1.09589040e+00, 6.00000000e+00],\n",
       "        [3.96300006e+00, 3.37147109e+04, 1.67083115e+02, ...,\n",
       "         1.39304861e-01, 1.09066713e+00, 6.00000000e+00],\n",
       "        [4.03399992e+00, 3.39267383e+04, 1.70376083e+02, ...,\n",
       "         1.38188362e-01, 1.09104800e+00, 6.00000000e+00],\n",
       "        ...,\n",
       "        [4.41900015e+00, 3.49071094e+04, 1.71842941e+02, ...,\n",
       "         1.37574300e-01, 1.07342207e+00, 9.00000000e+00],\n",
       "        [4.45300007e+00, 3.46182383e+04, 1.68539993e+02, ...,\n",
       "         1.37440041e-01, 1.06371665e+00, 9.00000000e+00],\n",
       "        [4.46199989e+00, 3.46243008e+04, 1.69350006e+02, ...,\n",
       "         1.37477830e-01, 1.06682599e+00, 9.00000000e+00]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='date'>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df(y_test, pred_mlp).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
