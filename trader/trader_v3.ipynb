{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db connection\n",
    "\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import keyring\n",
    "import platform\n",
    "import numpy as np\n",
    "\n",
    "user = 'root'\n",
    "pw = keyring.get_password('macmini_db', user)\n",
    "host = '192.168.219.106' if platform.system() == 'Windows' else '127.0.0.1'\n",
    "port = 3306\n",
    "db = 'stock'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data\n",
    "COLUMNS_STOCK_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "COLUMNS_TRAINING_DATA = ['open', 'high', 'low', 'close', 'volume', 'close_ma5', 'volume_ma5', 'close_ma5_ratio', 'volume_ma5_ratio',\n",
    "       'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio',\n",
    "       'low_close_ratio', 'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "       'close_ma10', 'volume_ma10', 'close_ma10_ratio', 'volume_ma10_ratio',\n",
    "       'close_ma20', 'volume_ma20', 'close_ma20_ratio', 'volume_ma20_ratio',\n",
    "       'close_ma60', 'volume_ma60', 'close_ma60_ratio', 'volume_ma60_ratio',\n",
    "       'close_ma120', 'volume_ma120', 'close_ma120_ratio',\n",
    "       'volume_ma120_ratio', 'close_ma240', 'volume_ma240',\n",
    "       'close_ma240_ratio', 'volume_ma240_ratio', 'upper_bb',\n",
    "       'lower_bb', 'bb_pb', 'bb_width', 'macd',\n",
    "       'macd_signal', 'macd_oscillator', 'rs', 'rsi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stock price\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# get us stock price of a specific ticker\n",
    "def get_stock_data(ticker, fro=None, to=None):\n",
    "\n",
    "    # connect DB\n",
    "    engine = create_engine(f'mysql+pymysql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    con = pymysql.connect(\n",
    "        user=user,\n",
    "        passwd=pw,\n",
    "        host=host,\n",
    "        db=db,\n",
    "        charset='utf8'\n",
    "    )\n",
    "            \n",
    "    mycursor = con.cursor()\n",
    "    \n",
    "    if fro is not None:\n",
    "        if to is not None:               \n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date BETWEEN '{fro}' AND '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date >= '{fro}'\n",
    "                    \"\"\"\n",
    "    \n",
    "    else:\n",
    "        if to is not None:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date <= '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    \"\"\"\n",
    "            \n",
    "    print(query)\n",
    "    stock_data = pd.read_sql(query, con=engine)\n",
    "    con.close()\n",
    "    engine.dispose()\n",
    "    return stock_data[['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'ticker']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2018-01-01' AND '2022-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "stock_code = 'AAPL'\n",
    "fro = '2018-01-01'\n",
    "to = '2022-12-31'\n",
    "df = get_stock_data(stock_code, fro=fro, to=to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    # moving average\n",
    "    windows = [5, 10, 20, 60, 120, 240]\n",
    "    for window in windows:\n",
    "        data[f'close_ma{window}'] = data['close'].rolling(window).mean()\n",
    "        data[f'volume_ma{window}'] = data['volume'].rolling(window).mean()\n",
    "        data[f'close_ma{window}_ratio'] = (data['close'] - data[f'close_ma{window}']) / data[f'close_ma{window}']\n",
    "        data[f'volume_ma{window}_ratio'] = (data['volume'] - data[f'volume_ma{window}']) / data[f'volume_ma{window}']\n",
    "        data['open_close_ratio'] = (data['open'].values - data['close'].values) / data['close'].values\n",
    "        data['open_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'open_prev_close_ratio'] = (data['open'][1:].values - data['close'][:-1].values) / data['close'][:-1].values\n",
    "        data['high_close_ratio'] = (data['high'].values - data['close'].values) / data['close'].values\n",
    "        data['low_close_ratio'] = (data['low'].values - data['close'].values) / data['close'].values\n",
    "        data['close_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'close_prev_close_ratio'] = (data['close'][1:].values - data['close'][:-1].values) / data['close'][:-1].values \n",
    "        data['volume_prev_volume_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'volume_prev_volume_ratio'] = (\n",
    "            # if volume is 0, change it into non zero value exploring previous volume continuously\n",
    "            (data['volume'][1:].values - data['volume'][:-1].values) / data['volume'][:-1].replace(to_replace=0, method='ffill').replace(to_replace=0, method='bfill').values\n",
    "        )\n",
    "    \n",
    "    # Bollinger band\n",
    "    data['middle_bb'] = data['close'].rolling(20).mean()\n",
    "    data['upper_bb'] = data['middle_bb'] + 2 * data['close'].rolling(20).std()\n",
    "    data['lower_bb'] = data['middle_bb'] - 2 * data['close'].rolling(20).std()\n",
    "    data['bb_pb'] = (data['close'] - data['lower_bb']) / (data['upper_bb'] - data['lower_bb'])\n",
    "    data['bb_width'] = (data['upper_bb'] - data['lower_bb']) / data['middle_bb']\n",
    "    \n",
    "    # MACD\n",
    "    macd_short, macd_long, macd_signal = 12, 26, 9\n",
    "    data['ema_short'] = data['close'].ewm(macd_short).mean()\n",
    "    data['ema_long'] = data['close'].ewm(macd_long).mean()\n",
    "    data['macd'] = data['ema_short'] - data['ema_long']\n",
    "    data['macd_signal'] = data['macd'].ewm(macd_signal).mean()\n",
    "    data['macd_oscillator'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # RSI\n",
    "    data['close_change'] = data['close'].diff()\n",
    "    # data['close_up'] = np.where(data['close_change'] >=0, df['close_change'], 0)\n",
    "    data['close_up'] = data['close_change'].apply(lambda x: x if x >= 0 else 0)\n",
    "    # data['close_down'] = np.where(data['close_change'] < 0, df['close_change'].abs(), 0)\n",
    "    data['close_down'] = data['close_change'].apply(lambda x: -x if x < 0 else 0)\n",
    "    data['rs'] = data['close_up'].ewm(alpha=1/14, min_periods=14).mean() / data['close_down'].ewm(alpha=1/14, min_periods=14).mean()\n",
    "    data['rsi'] = 100 - (100 / (1 + data['rs']))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data function\n",
    "\n",
    "load_data() function is a combined function for getting data from databases and preprocessing it into training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock_code, fro, to):\n",
    "    ''' \n",
    "    Arguments\n",
    "    ----------\n",
    "    - stock_code : unique stock code\n",
    "    - fro : start date\n",
    "    - to : end data\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    df_adj : entire prerprocessed data\n",
    "    stock_data : data for plotting chart\n",
    "    training_data : data for training a model\n",
    "    '''\n",
    "    \n",
    "    df = get_stock_data(stock_code, fro, to)\n",
    "    df_adj = preprocess(df).dropna().reset_index(drop=True)\n",
    "    # df_adj.dropna(inplace=True).reset_index(drop=True)\n",
    "    \n",
    "    stock_data = df_adj[COLUMNS_STOCK_DATA]\n",
    "    training_data = df_adj[COLUMNS_TRAINING_DATA]\n",
    "    \n",
    "    return df_adj, stock_data, training_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2018-01-01' AND '2022-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "df_adj, df_stock_data, df_training_data = load_data(stock_code, fro, to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "- function for calculating probabilities based on the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = max(min(x, 10), -10)\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>close_ma5</th>\n",
       "      <th>volume_ma5</th>\n",
       "      <th>...</th>\n",
       "      <th>ema_short</th>\n",
       "      <th>ema_long</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>macd_oscillator</th>\n",
       "      <th>close_change</th>\n",
       "      <th>close_up</th>\n",
       "      <th>close_down</th>\n",
       "      <th>rs</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>42.314999</td>\n",
       "      <td>42.540001</td>\n",
       "      <td>43.075001</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>40.670979</td>\n",
       "      <td>102223600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>42.990002</td>\n",
       "      <td>43.132500</td>\n",
       "      <td>43.637501</td>\n",
       "      <td>43.057499</td>\n",
       "      <td>40.663895</td>\n",
       "      <td>118071600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.061099</td>\n",
       "      <td>43.061178</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.007500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>43.020000</td>\n",
       "      <td>43.134998</td>\n",
       "      <td>43.367500</td>\n",
       "      <td>43.257500</td>\n",
       "      <td>40.852776</td>\n",
       "      <td>89738400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.131870</td>\n",
       "      <td>43.129103</td>\n",
       "      <td>0.002767</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>43.262501</td>\n",
       "      <td>43.360001</td>\n",
       "      <td>43.842499</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>41.317909</td>\n",
       "      <td>94640000.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.305420</td>\n",
       "      <td>43.293222</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.007946</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>43.482498</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>43.902500</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>41.164436</td>\n",
       "      <td>82271200.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>43.343500</td>\n",
       "      <td>97388960.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.371209</td>\n",
       "      <td>43.356602</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>-0.162498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>129.639999</td>\n",
       "      <td>130.919998</td>\n",
       "      <td>132.419998</td>\n",
       "      <td>131.860001</td>\n",
       "      <td>130.959976</td>\n",
       "      <td>63814900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>132.841998</td>\n",
       "      <td>76924080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.258514</td>\n",
       "      <td>143.745042</td>\n",
       "      <td>-3.486528</td>\n",
       "      <td>-2.163850</td>\n",
       "      <td>-1.322678</td>\n",
       "      <td>-0.369995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369995</td>\n",
       "      <td>0.554614</td>\n",
       "      <td>35.675357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>128.720001</td>\n",
       "      <td>131.380005</td>\n",
       "      <td>131.410004</td>\n",
       "      <td>130.029999</td>\n",
       "      <td>129.142441</td>\n",
       "      <td>69007800.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>132.373999</td>\n",
       "      <td>74807120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>139.471705</td>\n",
       "      <td>143.237077</td>\n",
       "      <td>-3.765373</td>\n",
       "      <td>-2.324002</td>\n",
       "      <td>-1.441370</td>\n",
       "      <td>-1.830002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.830002</td>\n",
       "      <td>0.507354</td>\n",
       "      <td>33.658569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>125.870003</td>\n",
       "      <td>129.669998</td>\n",
       "      <td>131.029999</td>\n",
       "      <td>126.040001</td>\n",
       "      <td>125.179680</td>\n",
       "      <td>85438400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>131.121999</td>\n",
       "      <td>76408240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>138.438497</td>\n",
       "      <td>142.600149</td>\n",
       "      <td>-4.161652</td>\n",
       "      <td>-2.507767</td>\n",
       "      <td>-1.653885</td>\n",
       "      <td>-3.989998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.989998</td>\n",
       "      <td>0.422765</td>\n",
       "      <td>29.714325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>127.730003</td>\n",
       "      <td>127.989998</td>\n",
       "      <td>130.479996</td>\n",
       "      <td>129.610001</td>\n",
       "      <td>128.725311</td>\n",
       "      <td>75703700.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>129.953999</td>\n",
       "      <td>74363380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>137.759382</td>\n",
       "      <td>142.119032</td>\n",
       "      <td>-4.359650</td>\n",
       "      <td>-2.692956</td>\n",
       "      <td>-1.666695</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583415</td>\n",
       "      <td>36.845374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>127.430000</td>\n",
       "      <td>128.410004</td>\n",
       "      <td>129.949997</td>\n",
       "      <td>129.929993</td>\n",
       "      <td>129.043137</td>\n",
       "      <td>77034200.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>129.493999</td>\n",
       "      <td>74199800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>137.157121</td>\n",
       "      <td>141.667586</td>\n",
       "      <td>-4.510465</td>\n",
       "      <td>-2.874706</td>\n",
       "      <td>-1.635759</td>\n",
       "      <td>0.319992</td>\n",
       "      <td>0.319992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598923</td>\n",
       "      <td>37.457887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1259 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close   adj_close  \\\n",
       "0     2018-01-02   42.314999   42.540001   43.075001   43.064999   40.670979   \n",
       "1     2018-01-03   42.990002   43.132500   43.637501   43.057499   40.663895   \n",
       "2     2018-01-04   43.020000   43.134998   43.367500   43.257500   40.852776   \n",
       "3     2018-01-05   43.262501   43.360001   43.842499   43.750000   41.317909   \n",
       "4     2018-01-08   43.482498   43.587502   43.902500   43.587502   41.164436   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1254  2022-12-23  129.639999  130.919998  132.419998  131.860001  130.959976   \n",
       "1255  2022-12-27  128.720001  131.380005  131.410004  130.029999  129.142441   \n",
       "1256  2022-12-28  125.870003  129.669998  131.029999  126.040001  125.179680   \n",
       "1257  2022-12-29  127.730003  127.989998  130.479996  129.610001  128.725311   \n",
       "1258  2022-12-30  127.430000  128.410004  129.949997  129.929993  129.043137   \n",
       "\n",
       "           volume ticker   close_ma5  volume_ma5  ...   ema_short    ema_long  \\\n",
       "0     102223600.0   AAPL         NaN         NaN  ...   43.064999   43.064999   \n",
       "1     118071600.0   AAPL         NaN         NaN  ...   43.061099   43.061178   \n",
       "2      89738400.0   AAPL         NaN         NaN  ...   43.131870   43.129103   \n",
       "3      94640000.0   AAPL         NaN         NaN  ...   43.305420   43.293222   \n",
       "4      82271200.0   AAPL   43.343500  97388960.0  ...   43.371209   43.356602   \n",
       "...           ...    ...         ...         ...  ...         ...         ...   \n",
       "1254   63814900.0   AAPL  132.841998  76924080.0  ...  140.258514  143.745042   \n",
       "1255   69007800.0   AAPL  132.373999  74807120.0  ...  139.471705  143.237077   \n",
       "1256   85438400.0   AAPL  131.121999  76408240.0  ...  138.438497  142.600149   \n",
       "1257   75703700.0   AAPL  129.953999  74363380.0  ...  137.759382  142.119032   \n",
       "1258   77034200.0   AAPL  129.493999  74199800.0  ...  137.157121  141.667586   \n",
       "\n",
       "          macd  macd_signal  macd_oscillator  close_change  close_up  \\\n",
       "0     0.000000     0.000000         0.000000           NaN  0.000000   \n",
       "1    -0.000079    -0.000042        -0.000038     -0.007500  0.000000   \n",
       "2     0.002767     0.000995         0.001772      0.200001  0.200001   \n",
       "3     0.012198     0.004252         0.007946      0.492500  0.492500   \n",
       "4     0.014607     0.006781         0.007826     -0.162498  0.000000   \n",
       "...        ...          ...              ...           ...       ...   \n",
       "1254 -3.486528    -2.163850        -1.322678     -0.369995  0.000000   \n",
       "1255 -3.765373    -2.324002        -1.441370     -1.830002  0.000000   \n",
       "1256 -4.161652    -2.507767        -1.653885     -3.989998  0.000000   \n",
       "1257 -4.359650    -2.692956        -1.666695      3.570000  3.570000   \n",
       "1258 -4.510465    -2.874706        -1.635759      0.319992  0.319992   \n",
       "\n",
       "      close_down        rs        rsi  \n",
       "0       0.000000       NaN        NaN  \n",
       "1       0.007500       NaN        NaN  \n",
       "2       0.000000       NaN        NaN  \n",
       "3       0.000000       NaN        NaN  \n",
       "4       0.162498       NaN        NaN  \n",
       "...          ...       ...        ...  \n",
       "1254    0.369995  0.554614  35.675357  \n",
       "1255    1.830002  0.507354  33.658569  \n",
       "1256    3.989998  0.422765  29.714325  \n",
       "1257    0.000000  0.583415  36.845374  \n",
       "1258    0.000000  0.598923  37.457887  \n",
       "\n",
       "[1259 rows x 53 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENTS\n",
    "\n",
    "- Environment has stock market data and return the current and next price to the agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# environment\n",
    "\n",
    "class Environment:\n",
    "    ''' \n",
    "    Attribute\n",
    "    ---------\n",
    "    - stock_data : stock price data such as 'open', 'close', 'high', 'low', 'volume'\n",
    "    - state : current state\n",
    "    - idx : current postion of stock data\n",
    "    \n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize idx and state\n",
    "    - observe() : move idx into next postion and get a new state\n",
    "    - get_close_price() : get close price of current state\n",
    "    - get_next_close_price() : get close price of next index state\n",
    "    - get_open_price() : get open price of current state\n",
    "    - get_next_open_price() : get open price of next indext state\n",
    "    - get_state() : get current state\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, stock_data=None):\n",
    "        self.close_price_idx = 4    # index postion of close price\n",
    "        self.open_price_idx = 1     # index position of open price\n",
    "        self.stock_data = stock_data\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        self.max_idx = len(stock_data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        # self.idx = 0\n",
    "        # self.state = self.stock_data.iloc[self.idx]\n",
    "        \n",
    "    def observe(self):\n",
    "        # move to next day and get price data\n",
    "        # if there is no more idx, return None\n",
    "        if len(self.stock_data) > self.idx + 1:\n",
    "            self.idx += 1\n",
    "            self.state = self.stock_data.iloc[self.idx]\n",
    "            return self.state\n",
    "        return None\n",
    "    \n",
    "    def get_close_price(self):\n",
    "        # return close price\n",
    "        if self.state is not None:\n",
    "            return self.state[self.close_price_idx]\n",
    "        return None\n",
    "    \n",
    "    def get_next_close_price(self):\n",
    "        # return tomorrow close price\n",
    "        if self.idx < self.max_idx - 1:\n",
    "            return self.stock_data.iloc[self.idx + 1, self.close_price_idx]\n",
    "        else:\n",
    "            return self.stock_data.iloc[self.idx, self.close_price_idx]\n",
    "    \n",
    "    def get_open_price(self):\n",
    "        # return open price\n",
    "        if self.state is not None:\n",
    "            return self.state[self.open_price_idx]\n",
    "        \n",
    "    def get_next_open_price(self):\n",
    "        # return tomorrow open price\n",
    "        if self.idx < self.max_idx - 1:\n",
    "            return self.stock_data.iloc[self.idx + 1, self.open_price_idx]\n",
    "        else:\n",
    "            return self.stock_data.iloc[self.idx, self.open_price_idx] \n",
    "    \n",
    "    def get_state(self):\n",
    "        # return current state\n",
    "        if self.state is not None:\n",
    "            return self.state\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment(df_adj)\n",
    "e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                              2018-12-13\n",
       "open                               42.387501\n",
       "high                               42.622501\n",
       "low                                43.142502\n",
       "close                              42.737499\n",
       "adj_close                          40.967648\n",
       "volume                           127594400.0\n",
       "ticker                                  AAPL\n",
       "close_ma5                          42.338501\n",
       "volume_ma5                       175292480.0\n",
       "close_ma5_ratio                     0.009424\n",
       "volume_ma5_ratio                   -0.272106\n",
       "open_close_ratio                   -0.008189\n",
       "open_prev_close_ratio               0.002661\n",
       "high_close_ratio                   -0.002691\n",
       "low_close_ratio                     0.009477\n",
       "close_prev_close_ratio               0.01094\n",
       "volume_prev_volume_ratio           -0.104669\n",
       "close_ma10                         43.528251\n",
       "volume_ma10                      170264920.0\n",
       "close_ma10_ratio                   -0.018166\n",
       "volume_ma10_ratio                  -0.250613\n",
       "close_ma20                          44.43225\n",
       "volume_ma20                      173363500.0\n",
       "close_ma20_ratio                   -0.038142\n",
       "volume_ma20_ratio                  -0.264007\n",
       "close_ma60                         51.035625\n",
       "volume_ma60                 158317206.666667\n",
       "close_ma60_ratio                   -0.162595\n",
       "volume_ma60_ratio                  -0.194059\n",
       "close_ma120                        51.088896\n",
       "volume_ma120                133001283.333333\n",
       "close_ma120_ratio                  -0.163468\n",
       "volume_ma120_ratio                 -0.040653\n",
       "close_ma240                        47.619115\n",
       "volume_ma240                     133074575.0\n",
       "close_ma240_ratio                  -0.102514\n",
       "volume_ma240_ratio                 -0.041181\n",
       "middle_bb                           44.43225\n",
       "upper_bb                           48.180116\n",
       "lower_bb                           40.684385\n",
       "bb_pb                               0.273904\n",
       "bb_width                              0.1687\n",
       "ema_short                          45.636882\n",
       "ema_long                           48.458697\n",
       "macd                               -2.821815\n",
       "macd_signal                         -2.03907\n",
       "macd_oscillator                    -0.782745\n",
       "close_change                        0.462498\n",
       "close_up                            0.462498\n",
       "close_down                               0.0\n",
       "rs                                   0.56144\n",
       "rsi                                35.956552\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                              2018-12-13\n",
       "open                               42.387501\n",
       "high                               42.622501\n",
       "low                                43.142502\n",
       "close                              42.737499\n",
       "adj_close                          40.967648\n",
       "volume                           127594400.0\n",
       "ticker                                  AAPL\n",
       "close_ma5                          42.338501\n",
       "volume_ma5                       175292480.0\n",
       "close_ma5_ratio                     0.009424\n",
       "volume_ma5_ratio                   -0.272106\n",
       "open_close_ratio                   -0.008189\n",
       "open_prev_close_ratio               0.002661\n",
       "high_close_ratio                   -0.002691\n",
       "low_close_ratio                     0.009477\n",
       "close_prev_close_ratio               0.01094\n",
       "volume_prev_volume_ratio           -0.104669\n",
       "close_ma10                         43.528251\n",
       "volume_ma10                      170264920.0\n",
       "close_ma10_ratio                   -0.018166\n",
       "volume_ma10_ratio                  -0.250613\n",
       "close_ma20                          44.43225\n",
       "volume_ma20                      173363500.0\n",
       "close_ma20_ratio                   -0.038142\n",
       "volume_ma20_ratio                  -0.264007\n",
       "close_ma60                         51.035625\n",
       "volume_ma60                 158317206.666667\n",
       "close_ma60_ratio                   -0.162595\n",
       "volume_ma60_ratio                  -0.194059\n",
       "close_ma120                        51.088896\n",
       "volume_ma120                133001283.333333\n",
       "close_ma120_ratio                  -0.163468\n",
       "volume_ma120_ratio                 -0.040653\n",
       "close_ma240                        47.619115\n",
       "volume_ma240                     133074575.0\n",
       "close_ma240_ratio                  -0.102514\n",
       "volume_ma240_ratio                 -0.041181\n",
       "middle_bb                           44.43225\n",
       "upper_bb                           48.180116\n",
       "lower_bb                           40.684385\n",
       "bb_pb                               0.273904\n",
       "bb_width                              0.1687\n",
       "ema_short                          45.636882\n",
       "ema_long                           48.458697\n",
       "macd                               -2.821815\n",
       "macd_signal                         -2.03907\n",
       "macd_oscillator                    -0.782745\n",
       "close_change                        0.462498\n",
       "close_up                            0.462498\n",
       "close_down                               0.0\n",
       "rs                                   0.56144\n",
       "rsi                                35.956552\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.369998931884766"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_next_close_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                              2018-12-14\n",
       "open                                   41.32\n",
       "high                                   42.25\n",
       "low                                    42.27\n",
       "close                              41.369999\n",
       "adj_close                          39.656776\n",
       "volume                           162814800.0\n",
       "ticker                                  AAPL\n",
       "close_ma5                             42.188\n",
       "volume_ma5                       174030160.0\n",
       "close_ma5_ratio                    -0.019389\n",
       "volume_ma5_ratio                   -0.064445\n",
       "open_close_ratio                   -0.001209\n",
       "open_prev_close_ratio              -0.033168\n",
       "high_close_ratio                    0.021271\n",
       "low_close_ratio                     0.021755\n",
       "close_prev_close_ratio             -0.031998\n",
       "volume_prev_volume_ratio            0.276034\n",
       "close_ma10                         43.176501\n",
       "volume_ma10                      169838400.0\n",
       "close_ma10_ratio                    -0.04184\n",
       "volume_ma10_ratio                  -0.041355\n",
       "close_ma20                          44.16575\n",
       "volume_ma20                      169344040.0\n",
       "close_ma20_ratio                   -0.063301\n",
       "volume_ma20_ratio                  -0.038556\n",
       "close_ma60                          50.81525\n",
       "volume_ma60                 159222533.333333\n",
       "close_ma60_ratio                   -0.185874\n",
       "volume_ma60_ratio                   0.022561\n",
       "close_ma120                        51.054125\n",
       "volume_ma120                133302636.666667\n",
       "close_ma120_ratio                  -0.189684\n",
       "volume_ma120_ratio                  0.221392\n",
       "close_ma240                        47.612052\n",
       "volume_ma240                133327038.333333\n",
       "close_ma240_ratio                  -0.131102\n",
       "volume_ma240_ratio                  0.221169\n",
       "middle_bb                           44.16575\n",
       "upper_bb                           47.991841\n",
       "lower_bb                            40.33966\n",
       "bb_pb                               0.134646\n",
       "bb_width                            0.173261\n",
       "ema_short                           45.30866\n",
       "ema_long                           48.196123\n",
       "macd                               -2.887462\n",
       "macd_signal                        -2.123909\n",
       "macd_oscillator                    -0.763554\n",
       "close_change                         -1.3675\n",
       "close_up                                 0.0\n",
       "close_down                            1.3675\n",
       "rs                                  0.472492\n",
       "rsi                                32.087938\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.369998931884766"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_state()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.369998931884766"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_close_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.682498931884766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_next_open_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                              2018-12-17\n",
       "open                               40.682499\n",
       "high                               41.362499\n",
       "low                                42.087502\n",
       "close                              40.985001\n",
       "adj_close                          39.287712\n",
       "volume                           177151600.0\n",
       "ticker                                  AAPL\n",
       "close_ma5                             41.905\n",
       "volume_ma5                       159839680.0\n",
       "close_ma5_ratio                    -0.021954\n",
       "volume_ma5_ratio                    0.108308\n",
       "open_close_ratio                   -0.007381\n",
       "open_prev_close_ratio              -0.016618\n",
       "high_close_ratio                    0.009211\n",
       "low_close_ratio                       0.0269\n",
       "close_prev_close_ratio             -0.009306\n",
       "volume_prev_volume_ratio            0.088056\n",
       "close_ma10                         42.810501\n",
       "volume_ma10                      171740960.0\n",
       "close_ma10_ratio                   -0.042641\n",
       "volume_ma10_ratio                   0.031505\n",
       "close_ma20                         43.822375\n",
       "volume_ma20                      168905860.0\n",
       "close_ma20_ratio                   -0.064747\n",
       "volume_ma20_ratio                   0.048819\n",
       "close_ma60                         50.581542\n",
       "volume_ma60                      160401140.0\n",
       "close_ma60_ratio                   -0.189724\n",
       "volume_ma60_ratio                   0.104429\n",
       "close_ma120                        51.011438\n",
       "volume_ma120                133959926.666667\n",
       "close_ma120_ratio                  -0.196553\n",
       "volume_ma120_ratio                  0.322422\n",
       "close_ma240                        47.603417\n",
       "volume_ma240                     133573205.0\n",
       "close_ma240_ratio                  -0.139032\n",
       "volume_ma240_ratio                  0.326251\n",
       "middle_bb                          43.822375\n",
       "upper_bb                           47.484471\n",
       "lower_bb                           40.160279\n",
       "bb_pb                               0.112602\n",
       "bb_width                            0.167134\n",
       "ema_short                          44.976071\n",
       "ema_long                           47.929015\n",
       "macd                               -2.952944\n",
       "macd_signal                        -2.206812\n",
       "macd_oscillator                    -0.746132\n",
       "close_change                       -0.384998\n",
       "close_up                                 0.0\n",
       "close_down                          0.384998\n",
       "rs                                  0.450837\n",
       "rsi                                31.074273\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.682498931884766"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_state()['open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.682498931884766"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.get_open_price()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agent decides and validates an action accoding to the value of networks from learners class.\n",
    "\n",
    "- Agent also has initial balance and current portfolio value.\n",
    "\n",
    "- Agent has its own states : ratio of holding stocks, profit/loss ratio, current current price to average buying price ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - enviroment : instance of environment\n",
    "    - initial_balance : initial capital balance\n",
    "    - min_trading_price : minimum trading price\n",
    "    - max_trading_price : maximum trading price\n",
    "    - balance : cash balance\n",
    "    - num_stocks : obtained stocks\n",
    "    - portfolio_value : value of portfolios (balance + price * num_stocks)\n",
    "    - num_buy : number of buying\n",
    "    - num_sell : number of selling\n",
    "    - num_hold : number of holding\n",
    "    - ratio_hold : ratio of holding stocks\n",
    "    - profitloss : current profit or loss\n",
    "    - avg_buy_price_ratio : the ratio average price of a stock bought to the current price\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize an agent\n",
    "    - set_balance() : initialize balance\n",
    "    - get_states() : get the state of an agent\n",
    "    - decide_action() : exploration or exploitation behavior according to the policy net\n",
    "    - validate_action() : validate actions\n",
    "    - decide_trading_unit() : decide how many stocks are sold or bought\n",
    "    - act() : act the actions\n",
    "    '''\n",
    "    \n",
    "    # agent stste dimensions\n",
    "    ## (ratio_hold, profitloss, current price to avg_buy_price ratio)\n",
    "    STATE_DIM = 3\n",
    "    \n",
    "    # trading charge and tax\n",
    "    TRADING_CHARGE = 0.00015    # trading charge 0.015%\n",
    "    TRADING_TAX = 0.002          # trading tax = 0.2% \n",
    "    \n",
    "    # action space\n",
    "    ACTION_BUY = 0      # buy\n",
    "    ACTION_SELL = 1     # sell\n",
    "    ACTION_HOLD = 2     # hold\n",
    "    \n",
    "    # get probabilities from neural nets\n",
    "    ACTIONS = [ACTION_BUY, ACTION_SELL, ACTION_HOLD]\n",
    "    NUM_ACTIONS = len(ACTIONS)      # output number from nueral nets\n",
    "    \n",
    "    def __init__(self, env,\n",
    "                 initial_balance=None, min_trading_money=None, max_trading_money=None):        \n",
    "        \n",
    "        # get current price from the environment\n",
    "        self.env = env\n",
    "        self.initial_balance = initial_balance\n",
    "        \n",
    "        # minumum and maximum trainding price\n",
    "        self.min_trading_money = min_trading_money\n",
    "        self.max_trading_money = max_trading_money\n",
    "        \n",
    "        # attributes for an agent class\n",
    "        self.balance = initial_balance\n",
    "        self.num_stocks = 0\n",
    "        \n",
    "        # value of portfolio : balance + num_stocks * {current stock price}\n",
    "        self.portfolio_value = self.balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        \n",
    "        # three states of Agent class\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        self.done = False\n",
    "        # self.env.reset()\n",
    "        \n",
    "    def set_initial_balance(self, balance):\n",
    "        self.initial_balance = balance\n",
    "        \n",
    "    def get_states(self):\n",
    "        # return current profitloss based on close price\n",
    "        close_price = self.env.get_close_price()\n",
    "        self.ratio_hold = self.num_stocks * close_price / self.portfolio_value\n",
    "        self.portfolio_value = self.balance + close_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        return (\n",
    "            self.ratio_hold,\n",
    "            self.profitloss,        # profitloss = (portfolio_value / initial_balance) - 1\n",
    "            (self.env.get_close_price() / self.avg_buy_price) if self.avg_buy_price > 0 else 0\n",
    "        )\n",
    "        \n",
    "    def decide_action(self, pred_value, pred_policy, eps):\n",
    "        # act randomly with epsilon probability, act according to neural network  with (1 - epsilon) probability\n",
    "        confidence = 0\n",
    "        \n",
    "        # if theres is a pred_policy, follow it, otherwise follow a pred_value\n",
    "        pred = pred_policy\n",
    "        if pred is None:\n",
    "            pred = pred_value\n",
    "            \n",
    "        # there is no prediction from both pred_policy and pred_value, explore!\n",
    "        if pred is None:\n",
    "            eps = 1\n",
    "        else:\n",
    "            maxpred = np.max(pred)\n",
    "            # if values for actions are euqal, explore!\n",
    "            if (pred == maxpred).all():\n",
    "                eps = 1\n",
    "                \n",
    "            # if the difference between buying and selling prediction policy value is less than 0.05, explore!\n",
    "            if pred_policy is not None:\n",
    "                if np.max(pred_policy) - np.min(pred_policy) < 0.05:\n",
    "                    eps = 1\n",
    "                    \n",
    "        # decide whether exploration will be done or not\n",
    "        if np.random.rand() < eps:\n",
    "            exploration = True\n",
    "            action = np.random.randint(self.NUM_ACTIONS) \n",
    "        else: \n",
    "            exploration = False\n",
    "            action = np.argmax(pred)\n",
    "            \n",
    "        confidence = .5\n",
    "        if pred_policy is not None:\n",
    "            confidence = pred[action]\n",
    "        elif pred_value is not None:\n",
    "            confidence = sigmoid(pred[action])\n",
    "            \n",
    "        return action, confidence, exploration\n",
    "    \n",
    "    def validate_action(self, action):\n",
    "        # validate if the action is available\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # check if al least one stock can be bought.\n",
    "            if self.balance < self.env.get_next_open_price() * (1 + self.TRADING_CHARGE):\n",
    "                return False\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # check if there is any sotck that can be sold\n",
    "            if self.num_stocks <= 0:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def decide_trading_unit(self, confidence):\n",
    "        # adjust number of stocks for buying and selling according to confidence level\n",
    "        if np.isnan(confidence):\n",
    "            return self.min_trading_money\n",
    "        \n",
    "        # set buying price range between self.min_trading_money + added_trading_price [min_trading_money, max_trading_money]\n",
    "        # in case that confidence > 1 causes the price over max_trading_money, we set min() so that the value cannot have larger value than self.max_trading_money - self.min_trading_money\n",
    "        # in case that confidence < 0, we set max() so that added_trading_price cannot have negative value.\n",
    "        added_trading_money = max(min(\n",
    "            int(confidence * (self.max_trading_money - self.min_trading_money)),\n",
    "            self.max_trading_money - self.min_trading_money\n",
    "        ), 0)\n",
    "        \n",
    "        trading_price = self.min_trading_money + added_trading_money\n",
    "        \n",
    "        return max(int(trading_price / self.env.get_next_open_price()), 1)\n",
    "    \n",
    "    def step(self, action, confidence):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - action : decided action from decide_action() method based on exploration or exploitation (0 or 1)\n",
    "        - confidence : probability from decide_action() method, the probability from policy network or the softmax probability from value network\n",
    "        '''\n",
    "        \n",
    "        # get the next open price from the environment\n",
    "        \n",
    "        open_price = self.env.get_next_open_price()\n",
    "        \n",
    "        if not self.validate_action(action):\n",
    "            action = Agent.ACTION_HOLD\n",
    "        \n",
    "        # buy\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # decide how many stocks will be bought\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            balance = (\n",
    "                self.balance - open_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            )\n",
    "            \n",
    "            # if lacks of balance, buy maximum units within the amount of money available\n",
    "            if balance < 0:\n",
    "                trading_unit = min(\n",
    "                    int(self.balance / (open_price * (1 + self.TRADING_CHARGE))),\n",
    "                    int(self.max_trading_money / open_price)\n",
    "                )\n",
    "                \n",
    "            # total amount of money with trading charge\n",
    "            invest_amount = open_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            if invest_amount > 0:\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks + open_price * trading_unit) / (self.num_stocks + trading_unit)\n",
    "                self.balance -= invest_amount\n",
    "                self.num_stocks += trading_unit\n",
    "                self.num_buy += 1\n",
    "                \n",
    "        # sell\n",
    "        elif action == self.ACTION_SELL:\n",
    "            # decide how many stocks will be sold\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            \n",
    "            # if lacks of stocks, sell maximum units available\n",
    "            trading_unit = min(trading_unit, self.num_stocks)\n",
    "            \n",
    "            # selling amount\n",
    "            invest_amount = open_price * (\n",
    "                1 - (self.TRADING_TAX + self.TRADING_CHARGE)\n",
    "            ) * trading_unit\n",
    "            \n",
    "            if invest_amount > 0:\n",
    "                # update average buy price\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks - open_price * trading_unit) / (self.num_stocks - trading_unit) if self.num_stocks > trading_unit else 0\n",
    "                self.num_stocks -= trading_unit\n",
    "                self.balance += invest_amount\n",
    "                self.num_sell += 1\n",
    "                \n",
    "        # hold\n",
    "        elif action == self.ACTION_HOLD:\n",
    "            self.num_hold += 1\n",
    "            \n",
    "        # update portfolio value with close price\n",
    "        close_price = self.env.get_next_close_price()\n",
    "        \n",
    "        self.portfolio_value = self.balance + close_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        \n",
    "        # info = {\n",
    "        #     'num_stocks': self.num_stocks,\n",
    "        #     'num_hold': self.num_hold,\n",
    "        #     'num_buy': self.num_buy,\n",
    "        #     'num_sell': self.num_sell\n",
    "        # }\n",
    "        \n",
    "        # return next_state, self.profitloss, self.done, info             # (next_states, profitloss, done, info)\n",
    "        return self.profitloss\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    '''\n",
    "    Common attributes and methods for neural networks\n",
    "    \n",
    "    Attributes\n",
    "    --------\n",
    "    - input_dim\n",
    "    - output_dim\n",
    "    - lr : learning rate\n",
    "    - shared_network : head of neural network which is shared with various networks (e.g., A2C)\n",
    "    - activation : activation layer function ('linear', 'sigmoid', 'tanh', 'softmax')\n",
    "    - loss : loss function for networks\n",
    "    - model : final neural network model\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - predict() : calculate value or probability of actions\n",
    "    - train_on_batch() : generate batch data for training\n",
    "    - save_model()\n",
    "    - load_model()\n",
    "    - get_shared_network() : generate network head for the networks\n",
    "    '''\n",
    "    \n",
    "    # threading lock for A3C\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim=0, output_dim=0, num_steps=1, lr=0.001,\n",
    "        net=None, shared_network=None, activation='sigmoid', loss='mse'\n",
    "    ):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.shared_network = shared_network\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.net = net\n",
    "        \n",
    "        # data shape for various network\n",
    "        # CNN, LSTM have 3 dimensional shape, so we set input shape as (num_stpes, input_dim)\n",
    "        # DNN have 2 dimensional shape and we set input shape as (input_dim,)\n",
    "        inp = None\n",
    "        if self.num_steps > 1:\n",
    "            inp = (self.num_steps, input_dim)\n",
    "        else:\n",
    "            inp = (self.input_dim,)\n",
    "            \n",
    "        # in case that shared network is used,\n",
    "        self.head = None\n",
    "        if self.shared_network is None:\n",
    "            self.head = self.get_network_head(inp, self.output_dim)\n",
    "        else:\n",
    "            self.head = self.shared_network\n",
    "            \n",
    "        # neual network model\n",
    "        ## generate network model for head\n",
    "        self.model = nn.Sequential(self.head)\n",
    "        \n",
    "        # add activation layer\n",
    "        if self.activation == 'linear':\n",
    "            pass\n",
    "        elif self.activation == 'relu':\n",
    "            self.model.add_module('activation', nn.ReLU())\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            self.model.add_module('activation', nn.LeakyReLU())\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.model.add_module('activation', nn.Sigmoid())\n",
    "        elif self.activation == 'tanh':\n",
    "            self.model.add_module('activation', nn.Tanh())\n",
    "        elif self.activation == 'softmax':\n",
    "            self.model.add_module('activation', nn.Softmax(dim=1))\n",
    "        self.model.apply(Network.init_weights)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.NAdam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = None\n",
    "        if loss == 'mse':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif loss == 'binary_crossentropy':\n",
    "            self.criterion = nn.BCELoss()\n",
    "            \n",
    "    def predict(self, sample):\n",
    "        # return prediction of buy, sell and hold on given sample\n",
    "        # value network returns each actions' value on sample and policy network returns each actions' probabilities on sample\n",
    "        with self.lock:\n",
    "            # transform evaluation mode: deactivate module used only on training such as Dropout\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(sample).float().to(device)\n",
    "                pred = self.model(x).detach().cpu().numpy()\n",
    "                pred = pred.faltten()\n",
    "            return pred\n",
    "        \n",
    "    def train_on_batch(self, x, y, a=None, eps=None, K=None):\n",
    "        if self.num_steps > 1:\n",
    "            x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        else:\n",
    "            x = np.array(x).reshape((-1, self.input_dim))\n",
    "            \n",
    "        loss = 0.\n",
    "        \n",
    "        if self.net == 'ppo':\n",
    "            with self.lock():\n",
    "                self.model.train()\n",
    "                _x = torch.from_numpy(x).float().to(device)\n",
    "                _y = torch.from_numpy(y).float().to(device)\n",
    "                probs = F.softmax(_y, dim=1)\n",
    "                for _ in range(K):\n",
    "                    y_pred = self.model(_x)\n",
    "                    probs_pred = F.softmax(y_pred, dim=1)\n",
    "                    rto = torch.exp(torch.log(probs[:, a]) - torch.log(probs_pred[:, a]))\n",
    "                    rto_adv = rto * _y[:, a]\n",
    "                    clp_adv = torch.clamp(rto, 1 - eps, 1 + eps) * _y[:, a]\n",
    "                    _loss = -torch.min(rto_adv, clp_adv).mean()\n",
    "                    _loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    loss += _loss.item()\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.model.train()\n",
    "                _x = torch.from_numpy(x).float().to(device)\n",
    "                _y = torch.from_numpy(y).float().to(device)\n",
    "                y_pred = self.model(_x)\n",
    "                _loss = self.criterion(y_pred, _y)\n",
    "                _loss.backward()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss += _loss.item()\n",
    "            return loss\n",
    "        \n",
    "    @classmethod\n",
    "    def get_shared_network(cls, net='dnn', num_steps=1, input_dim=0, output_dim=0):\n",
    "        if net == 'dnn':\n",
    "            return DNN.get_network_head((input_dim,), output_dim)\n",
    "        elif net == 'lstm':\n",
    "            return LSTMNetwork.get_network_head((num_steps, input_dim), output_dim)\n",
    "        elif net == 'cnn':\n",
    "            return CNN.get_network_head((num_steps, input_dim), output_dim)\n",
    "        elif net == 'alex':\n",
    "            return AlexNet.get_network_head((num_steps, input_dim), output_dim)\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        # initialize weights as weighted normal distribution\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, torch.nn.Conv1d):\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for weights in m.all_weights:\n",
    "                for weight in weights:\n",
    "                    nn.init.normal_(weight, std=0.01)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli-layer percetpron\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/b183f232a889b22d3fca742818c23e85bd234de24d94365275cab1c983e94c95/68747470733a2f2f6769746875622e636f6d2f61686e3238332f66696e616e63652f626c6f622f6d61696e2f646565705f6c6561726e696e672f696d672f444c5f4d4c505f437573746f6d2e504e473f7261773d74727565'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DNNNetwork(Network):\n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm1d(inp[0]),         # input.shape = (input_dim, )\n",
    "            nn.Linear(inp[0], 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32, output_dim),\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.input_dim))\n",
    "        return super().predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://camo.githubusercontent.com/6dcbb84c9583b07911dccfcfae04251b1a2c81de1ead2b15ea76f5a7925701f2/68747470733a2f2f6769746875622e636f6d2f61686e3238332f66696e616e63652f626c6f622f6d61696e2f6d616368696e655f6c6561726e696e672f696d672f4c53544d5f4172636869746563747572652e706e673f7261773d74727565'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM (Long Stort-Term Memory) is a kind of recerrent neural network (RNN).\n",
    "\n",
    "- Forget gate layer decides what information should be forgotten from cell state by sigmoid layer. It gets previous hidden state $h_{t-1}$ and input $x_t$ and transforms the value into new value between 0 and 1 through sigmoid function to the $c_{t-1}$.\n",
    "\n",
    "    - 0 : all should be forgotten.\n",
    "\n",
    "    - 1 : all should be remembered.\n",
    "\n",
    "\n",
    "<img src='./image/lstm_1.png'>\n",
    "\n",
    "- Input gate layer decides what information should be stored in a cell state. Sigmoid layer decides what new information should be stored and tanh layer makes a vetor $\\tilde{C}_t$ and prepares for adding to the cell state.\n",
    "\n",
    "<img src='./image/lstm_2.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we update previous $C_{t-1}$ and get a new cell state $C_t$.\n",
    "\n",
    "- $f_t$ is multiplied to the previous state for forgetting what should be forgotten decided by forget gate layer.\n",
    "\n",
    "- And next $i_t\\times \\tilde C_t$ is added.\n",
    "\n",
    "<img src='./image/lstm_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lastly, we should decide what should be sent to output, the value filtered by a cell state, which is called output layer.\n",
    "\n",
    "- Sigmoid layer decides to which part input data will be sent. It applies tanh layer to the cell state and turns into a value between -1 and 1.\n",
    "\n",
    "- The output layer multiplies sigmoid gate and output.\n",
    "\n",
    "<img src='./image/lstm_4.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(Network):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm1d(inp[0]),\n",
    "            LSTMModule(inp[1], 128, batch_first=True, use_last_only=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((-1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "        \n",
    "class LSTMModule(nn.LSTM):\n",
    "    def __init__(self, *args, use_last_only=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_last_only = use_last_only\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, (h_n, _) = super().forward(x)\n",
    "        if self.use_last_only:\n",
    "            return h_n[-1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolustion layer\n",
    "\n",
    "- All pixels in the input layer are not connected one by one to the next layer and filtered. So lower layer focuses on the lower features and upper layer focusees on the higher features.\n",
    "\n",
    "- Filters are weighted parameters that are optimized throught training process.\n",
    "\n",
    "<img src='./image/cnn_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer\n",
    "\n",
    "- Pooling layer is called sub-sampling which reduces image size.\n",
    "\n",
    "<img src='./image/cnn_2.jpeg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm1d(inp[0]),\n",
    "            nn.Conv1d(inp[0], 1, kernel_size),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(inp[1] - (kernel_size - 1), 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet competed in the ImageNet Larget Scale Visual Recognition Challenge on Sep. 30, 2012. The network achieved a top 5 error of 15.3% more than 10.8 percentage points lower that of the runner up.\n",
    "\n",
    "<img src='./image/alexnet.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Do print / debug stuff\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2,\n",
    "        stride = 2,\n",
    "        padding = 1,\n",
    "        return nn.Sequential(\n",
    "            nn.BatchNorm1d(inp[0]),\n",
    "            nn.Conv1d(inp[0], 96, kernel_size=kernel_size, stride=stride),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            PrintLayer(),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.Conv1d(96, 256, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding),  \n",
    "            PrintLayer(),\n",
    "            nn.Conv1d(256, 384, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(384, 256, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            PrintLayer(),\n",
    "            \n",
    "            # classifier\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, output_dim)          \n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "class Visualizer:\n",
    "    '''\n",
    "    Attributes\n",
    "    ----------\n",
    "    - fig : matplotlib Figure instance plays like a canvas\n",
    "    - plot() : print charts except daily stock data chart\n",
    "    - save() : save Figure as an image file\n",
    "    - clear() : initialize all chart but daily stock data chart\n",
    "    Returns\n",
    "    ---------\n",
    "    - Figure title : parameter, epsilon\n",
    "    - Axes 1 : daily price chart\n",
    "    - Axes 2 : number of stocks and agent action chart\n",
    "    - Axes 3 : value network chart\n",
    "    - Axes 4 : policy network and epsilon chart\n",
    "    - Axes 5 : Portfolio value and learning point chart\n",
    "    '''\n",
    "    \n",
    "    COLORS = ['r', 'b', 'g']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.canvas = None\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.title = ''\n",
    "        self.x = []\n",
    "        self.xticks = []\n",
    "        self.xlabels = []\n",
    "        \n",
    "    def prepare(self, stock_data, title):\n",
    "        self.title = title\n",
    "        \n",
    "        # shares x-axis among all charts\n",
    "        # self.x =np.arange(stock_data['date])\n",
    "        # self.x_label = [datetime.strptime(date, '%Y%m%d').date() for date in stock_data['date']]\n",
    "        with lock:\n",
    "            # prepare for printing five charts\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                nrows=5, ncols=1, facecolor='w', sharex=True\n",
    "            )\n",
    "            for ax in self.axes:\n",
    "                # deactivate scientific marks\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # change y-axis to the right\n",
    "                ax.yaxis.tick_right()\n",
    "                \n",
    "            # chart 1. plot daily stock data\n",
    "            self.axes[0].set_ylabel('Env.')\n",
    "            x = np.arange(len(stock_data))\n",
    "            # make two dimensional array with open, high, low and close order\n",
    "            ohlc = np.hstack((\n",
    "                x.reshape(-1, 1), np.array(stock_data)[:, 1:5]\n",
    "            ))\n",
    "            # red for positive, blue for negative\n",
    "            candlestick_ohlc(self.axes[0], ohlc, colorup='r', colordown='b')\n",
    "            \n",
    "            # visualize volume\n",
    "            ax = self.axes[0].twinx()\n",
    "            volume = np.array(stock_data)[:, 5].tolist()\n",
    "            ax.bar(x, volume, color='b', alpha=0.3)\n",
    "            \n",
    "            # set x-axis\n",
    "            self.x = np.arange(len(stock_data['date']))\n",
    "            self.xticks = stock_data.index[[0, -1]]\n",
    "            self.xlabels = stock_data.iloc[[0, -1]]['date']\n",
    "            \n",
    "    def plot(self, epoch_str=None, num_epochs=None, eps=None,\n",
    "             action_list=None, actions=None, num_stocks=None,\n",
    "             outvals_value=[], outvals_policy=[], exps=None,\n",
    "             initial_balance=None, pvs=None):\n",
    "        ''' \n",
    "        Attributes\n",
    "        ---------\n",
    "        - epoch_str : epoch for Figure title\n",
    "        - num_epochs : number of total epochs\n",
    "        - epsilon : exploration rate\n",
    "        - action_list : total action list of an agent\n",
    "        - num_stocks : number of stocks \n",
    "        - outvals_value : output array of value network\n",
    "        - outvals_policy : output array of policy network\n",
    "        - exps : array whether exploration is true or not\n",
    "        - initial_balance\n",
    "        - pvs : array of portfolio values\n",
    "        '''\n",
    "        \n",
    "        with lock:\n",
    "            # action, num_stocks, outvals_value, outvals_policy, pvs has same size\n",
    "            # create an array with same size as actions and use same x-axis\n",
    "            actions = np.array(actions)         # action array of an agent\n",
    "            \n",
    "            # turn value network output into an array\n",
    "            outvals_value = np.array(outvals_value)\n",
    "            \n",
    "            # turn policy network output into an array\n",
    "            outvals_policy = np.array(outvals_policy)\n",
    "            \n",
    "            # turn initial balance into an array\n",
    "            pvs_base = np.zeros(len(actions)) + initial_balance     # array([initial_balance, initial_balance, initial_balance, ...])\n",
    "            \n",
    "            # chart 2. plot agent states (action, num_stocks)\n",
    "            for action, color in zip(action_list, self.COLORS):\n",
    "                for i in self.x[actions == action]:\n",
    "                    # express actions as background color : red for buying, blue for selling\n",
    "                    self.axes[1].axvline(i, color=color, alpha=0.1)\n",
    "            self.axes[1].plot(self.x, num_stocks, '-k')     # plot number of stocks\n",
    "            \n",
    "            # chart 3. plot value network (prediction value for action)\n",
    "            if (len(outvals_value)) > 0:\n",
    "                max_actions = np.argmax(outvals_value, axis=1)\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    # plot background\n",
    "                    for idx in self.x:\n",
    "                        if max_actions[idx] == action:\n",
    "                            self.axes[2].axvline(idx, color=color, alpha=0.1)\n",
    "                    \n",
    "                    # plot value network\n",
    "                    ## red for buying, blue for selling, green for holding\n",
    "                    ## if there are no predicions for action, plot green chart\n",
    "                    self.axes[2].plot(self.x, outvals_value[:, action], color=color, linestyle='-')\n",
    "                    \n",
    "            # chart 4. plot policy network\n",
    "            # plot exploration as yellow background\n",
    "            for exp_idx in exps:\n",
    "                self.axes[3].axvline(exp_idx, color='y')\n",
    "            \n",
    "            # plot action as background color\n",
    "            _outvals = outvals_policy if len(outvals_policy) > 0 else outvals_value\n",
    "            for idx, outval in zip(self.x, _outvals):\n",
    "                color = 'white'\n",
    "                if np.isnan(outval.max()):\n",
    "                    continue\n",
    "                # with no exploration area, red for buying, blie for selling\n",
    "                if outval.argmax() == Agent.ACTION_BUY:\n",
    "                    color = self.COLORS[0]      # red for buying\n",
    "                elif outval.argmax() == Agent.ACTION_SELL:\n",
    "                    color = self.COLORS[1]      # blue for selling\n",
    "                elif outval.argmax() == Agent.ACTION_HOLD:\n",
    "                    color = self.COLORS[2]      # green for holding\n",
    "                self.axes[3].axvline(idx, color=color, alpha=0.1)\n",
    "                \n",
    "            # plot policy network\n",
    "            # red for buying policy network output, blue for selling policy network output\n",
    "            # when red line is above blue line, buy stocks, otherwise sell stocks\n",
    "            if len(outvals_policy) > 0:\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    self.axes[3].plot(\n",
    "                        self.x, outvals_policy[:, action],\n",
    "                        color=color, linestyle='-'\n",
    "                    )\n",
    "                    \n",
    "            # chart 5. portfolio value\n",
    "            # horizontal line for initial balance\n",
    "            self.axes[4].axhline(\n",
    "                initial_balance, linestyle='-', color='gray'\n",
    "            )\n",
    "            \n",
    "            self.axes[4].fill_between(\n",
    "                self.x, pvs, pvs_base,\n",
    "                where=pvs > pvs_base, facecolor='r', alpha=0.1\n",
    "            )\n",
    "            self.axes[4].plot(self.x, pvs, '-k')\n",
    "            self.axes[4].xaxis.set_ticks(self.xticks)\n",
    "            self.axes[4].xaxis.set_ticklabels(self.xlabels)\n",
    "            \n",
    "            # epoch and exploration rate\n",
    "            self.fig.suptitle(f'{self.titlt}\\nEPOCH:{epoch_str}/{num_epochs} EPSILON:{eps:.2f}')\n",
    "            # adjust canvas layout\n",
    "            self.fig.tight_layout()\n",
    "            self.fig.subplots_adjust(top=0.85)\n",
    "            \n",
    "    def clear(self, xlim):\n",
    "        with lock:\n",
    "            _axes = self.axes.tolist()\n",
    "            # initialize charts except non changeable value (stock data)\n",
    "            for ax in _axes[1:]:\n",
    "                ax.cla()            # initialize chart\n",
    "                ax.relim()          # initialize limit\n",
    "                ax.autoscale()      # reset scale\n",
    "            \n",
    "            # reset y-axis label\n",
    "            self.axes[1].set_ylabel('Agent')\n",
    "            self.axes[2].set_ylabel('Value')\n",
    "            self.axes[3].set_ylabel('Policy')\n",
    "            self.axes[4].set_ylabel('Portfolio')\n",
    "            for ax in _axes:\n",
    "                ax.set_xlim(xlim)       # set limit in x-axis\n",
    "                ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # set equal width horizontally\n",
    "                ax.ticklabel_format(useOffset=False)\n",
    "                \n",
    "    def save(self, path):\n",
    "        with lock:\n",
    "            self.fig.savefig(path)\n",
    "            \n",
    "        \n",
    "           \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce - Basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Policy $\\pi$ is a function that connectes states to action probabilities. Action probabilities are for getting $a\\sim\\pi(s)$. In the REINFORCE algorithm, agent trains policies and acts as trained policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "- Reward $R_t(\\tau)$\n",
    "$$R_t(\\tau)=\\sum_{t^{\\\\prime}=t}^T\\gamma^{t^{\\prime}-t}r_{t^\\prime}$$\n",
    "\n",
    "- If $t=0$, the equation above is the reward of a full episode, and an object can be defined as an expectated reward of total episodes.\n",
    "\n",
    "$$J(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^T \\gamma^t r_t\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient\n",
    "\n",
    "- Policy gradient algorithm solves the problem below.\n",
    "$$\\max_{\\theta}J(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "- We perform policy gradient for maximizing the object.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha\\triangledown_\\theta j(\\pi_\\theta)$$\n",
    "\n",
    "- And policy gradient can be defined as follows.\n",
    "$$\\triangledown_{\\theta}J(\\pi_\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t=0}^T R_r(\\tau)\\triangledown_\\theta\\log\\pi_\\theta(a_t|s_t)\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm\n",
    "\n",
    "#### pseudo code\n",
    "\n",
    "- Initialize learning rate $\\alpha$\n",
    "\n",
    "- Initialize the weights $\\theta$ of policy network $\\pi_\\theta$\n",
    "\n",
    "- For episode = 0, ..., MAX_EPSIODE do:\n",
    "\n",
    "    - Get samples $\\tau=(s_0,a_0,r_0), ..., (s_T, a_T, r_T)$\n",
    "\n",
    "    - Set $\\triangledown_\\theta J(\\pi_\\theta)=0$\n",
    "\n",
    "    - For $t=0, ..., T$ do:\n",
    "\n",
    "        - $R_t(\\tau)=\\sum_{t^\\prime=t}^T\\gamma^{t^\\prime-t}r^\\prime_t$\n",
    "\n",
    "        - $\\triangledown_\\theta J(\\pi_\\theta)=\\triangle_\\theta J(\\pi_\\theta)+R_t(\\tau)\\triangledown_\\theta\\log\\pi_\\theta(a_t|s_t)$\n",
    "    \n",
    "    - End for\n",
    "\n",
    "    - $\\theta=\\theta+\\alpha\\triangledown_\\theta J(\\pi_\\theta)$\n",
    "\n",
    "- End for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import abc\n",
    "import collections\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "LOGGER_NAME = 'trader'\n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.path.pardir))\n",
    "\n",
    "class ReinforcementLearner:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - stock_code\n",
    "    - stock_data : stock data for plotting chart\n",
    "    - environment\n",
    "    - agent\n",
    "    - training_data : data for training a model\n",
    "    - value_network : value network for a model if needed\n",
    "    - policy_network : policy network for a model if needed\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - init_value_network() : function for creating value network\n",
    "    - init_policy_network() : function for creating policy network\n",
    "    - build_sample() : get samples from environment instances\n",
    "    - get_batch() : create batch training data\n",
    "    - update_network() : training vlue network and policy network\n",
    "    - fit() : request train value network and policy network\n",
    "    - run() : perform reinfocement learning\n",
    "    - save_models() : save value network and policy network\n",
    "    '''\n",
    "    \n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    def __init__(self, rl_method='dqn', stock_code=None,\n",
    "                 stock_data=None, training_data=None,\n",
    "                 min_trading_money=100, max_trading_money=10000,\n",
    "                 value_net='cnn', policy_net='cnn', num_steps=1, lr=0.05,\n",
    "                 gamma=0.9, num_epochs=1000,\n",
    "                 balance=100000, eps_init=1,\n",
    "                 value_network=None, policy_network=None,\n",
    "                 output_path='', reuse_models=True, gen_output=True):\n",
    "        ''' \n",
    "        Attributes\n",
    "        --------\n",
    "        - rl_method : reinforcement learning method - 'dqn', 'pg', 'ac', 'a2c', 'a3c', 'ppo', 'acer', ...\n",
    "        - stock_code\n",
    "        - stock_data\n",
    "        - training_data\n",
    "        - min_trading_money\n",
    "        - max_trading_money\n",
    "        - net : neural network - 'dnn', 'lstm', 'cnn', 'rnn', 'alex', ...\n",
    "        - n_steps : sequence length of 2 dimensional networks such as CNN, RNN, LSTM, AlexNet\n",
    "        - lr : learning rate\n",
    "        - gamma : discount rate\n",
    "        - num_epochs : number of traiding epochs (scenarios)\n",
    "        - balance : initial balance\n",
    "        - eps_init : initial exploration rate\n",
    "        - value_network\n",
    "        - policy_network\n",
    "        - output_path\n",
    "        - reuse_model\n",
    "        - gen_output Whether the results are visualized or not\n",
    "        '''\n",
    "        \n",
    "        # check arguments\n",
    "        assert min_trading_money > 0\n",
    "        assert max_trading_money > 0\n",
    "        assert max_trading_money >= min_trading_money\n",
    "        assert num_epochs > 0\n",
    "        assert lr > 0\n",
    "        \n",
    "        # set reinforcement learning\n",
    "        self.rl_method = rl_method\n",
    "        self.gamma = gamma\n",
    "        self.num_epochs = num_epochs\n",
    "        self.eps_init = eps_init\n",
    "        \n",
    "        # set environment\n",
    "        self.stock_code = stock_code\n",
    "        self.stock_dasa = stock_data\n",
    "        self.env = Environment(stock_data)\n",
    "        \n",
    "        # set agent\n",
    "        self.agent = Agent(self.env, balance, min_trading_money, max_trading_money)\n",
    "        \n",
    "        # training data\n",
    "        self.training_data = training_data\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # vector size = training data vector size + agent state vector size\n",
    "        self.num_features = self.agent.STATE_DIM\n",
    "        if self.training_data is not None:\n",
    "            self.num_features += self.training_data.shape[1]\n",
    "            \n",
    "        # set nework\n",
    "        self.value_net = value_net\n",
    "        self.policy_net = policy_net\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "        self.reuse_models = reuse_models\n",
    "        \n",
    "        # visualize module\n",
    "        self.visualier = Visualizer()\n",
    "        \n",
    "        # memory \n",
    "        self.memory_sample = []         # training data sample\n",
    "        self.memroy_action = []         # actions taken\n",
    "        self.memory_reward = []         # reward obtained\n",
    "        self.memory_value = []          # predictiojn values for actions\n",
    "        self.memory_policy = []         # prediction probabilities for actions\n",
    "        self.memory_pv = []             # portfolio value\n",
    "        self.memory_num_stocks = []     # number of stocks\n",
    "        self.memory_exp_idx = []        # exploration index\n",
    "        \n",
    "        # exploration epoch info\n",
    "        self.loss = 0               # loss during epoch\n",
    "        self.itr_cnt = 0            # number of iterations with profit\n",
    "        self.exploration_cnt = 0    # number of exploration\n",
    "        self.batch_size = 0         # number of training\n",
    "        \n",
    "        # log output\n",
    "        self.output_path = output_path\n",
    "        self.gen_output = gen_output\n",
    "        \n",
    "    def init_value_network(self, shared_network=None, activation='linear', loss='mse'):\n",
    "        if self.value_net == 'dnn':\n",
    "            self.value_network = DNNNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr,\n",
    "                # num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.value_net == 'cnn':\n",
    "            self.value_network = CNNNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.value_net == 'lstm':\n",
    "            self.value_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.value == 'alex':\n",
    "            self.value_network = AlexNet(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        if self.reuse_models and os.path.exists(self.value_network_path):\n",
    "            self.value_network.load_model(model_path=self.value_network_path)\n",
    "            \n",
    "    def init_policy_network(self, shared_network=None, activation='sigmoid', loss='binary_crossentropy'):\n",
    "        if self.policy_net == 'dnn':\n",
    "            self.policy_network = DNNNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            ) \n",
    "            \n",
    "        elif self.policy_net == 'lstm':\n",
    "            self.policy_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr,\n",
    "                num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'alex':\n",
    "            self.policy_network = AlexNet(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=Agent.NUM_ACTIONS,\n",
    "                lr=self.lr,\n",
    "                num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        if self.reuse_models and os.path.exists(self.policy_network_path):\n",
    "            self.policy_network.load_model(model_path=self.policy_network_path)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # reset env\n",
    "        self.env.reset()\n",
    "        \n",
    "        # reset agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # reset visualizer\n",
    "        self.visualier.clear([0, len(self.stock_data)])\n",
    "        \n",
    "        # reset memories\n",
    "        self.memory_sample = []\n",
    "        self.memroy_action = []\n",
    "        self.memory_reward = []\n",
    "        self.memory_value = []\n",
    "        self.memory_policy = []\n",
    "        self.memory_pv = []\n",
    "        self.memory_num_stocks = []\n",
    "        self.memory_exp_idx = []\n",
    "        \n",
    "        # reset epoch info\n",
    "        self.loss = 0.\n",
    "        self.itr_cnt = 0\n",
    "        self.exploration_cnt = 0\n",
    "        self.batch_size = 0\n",
    "        \n",
    "    def build_sample(self):\n",
    "        # get next index data\n",
    "        self.env.observe()\n",
    "        # 44 samples + 3 agent states = 47 features\n",
    "        if len(self.training_data) > self.training_data_idx + 1:\n",
    "            self.training_data_idx += 1\n",
    "            self.sample = self.training_data[self.training_data_idx].tolist()\n",
    "            self.sample.extend(self.agent.get_states())\n",
    "            return self.sample\n",
    "        return None\n",
    "    \n",
    "    # abstract method\n",
    "    @abc.abstractmethod\n",
    "    def get_batch(self):\n",
    "        pass\n",
    "    \n",
    "    # after generate batch data, call train_on_batch() method to train value network and policy network.\n",
    "    # value network : DQNLearner, ActorCriticLearner, A2CLearner\n",
    "    # policy network : PolicyGradientLearner, ActorCrotocLearner, A2CLearner\n",
    "    # loss value after training is saved as instance. in case of training value and policy probabilities\n",
    "    def fit(self):\n",
    "        # generate batch data\n",
    "        x, y_value, y_policy = self.get_batch()\n",
    "        # initialize loss\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # update value network\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            if y_policy is not None:\n",
    "                # update policy network\n",
    "                loss += self.policy_network.train_on_batch(x, y_policy)\n",
    "            self.loss = loss\n",
    "            \n",
    "    # visualize one complete epoch\n",
    "    # in case of LSTM, CNN agent, the number of agent's actions, num_stocks, output of value network, output of policy network and portfolio value is less than daily stock data by (num_steps - 1). So we fill (num_steps - 1) meaningless data.\n",
    "    def visualzie(self, epoch_str, num_epochs, eps):\n",
    "        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memroy_action\n",
    "        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks\n",
    "        if self.value_network is not None:\n",
    "            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] * (self.num_steps - 1) + self.memory_value\n",
    "        if self.policy_network is not None:\n",
    "            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] * (self.num_steps - 1) + self.memory_policy\n",
    "            \n",
    "        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv\n",
    "        self.visualier.plot(\n",
    "            epoch_str=epoch_str, num_epochs=num_epochs,\n",
    "            eps=eps, action_list=Agent.ACTIONS,\n",
    "            actions=self.memory_action,\n",
    "            num_stocks=self.memory_num_stocks,\n",
    "            outvals_value=self.memory_value,\n",
    "            outvals_policy=self.memory_policy,\n",
    "            exps=self.memory_exp_idx,\n",
    "            initial_balance=self.agent.initial_balance,\n",
    "            pvs=self.memory_pv,\n",
    "        )\n",
    "        self.visualier.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))\n",
    "    \n",
    "    def run(self, learning=True):\n",
    "        ''' \n",
    "        Arguments\n",
    "        --------\n",
    "        - learning : boolean if learning will be done or not\n",
    "            - True : after training, build value and policy network\n",
    "            - False : simulation with pretrained model\n",
    "        '''      \n",
    "        info = {\n",
    "            f'[{self.stock_code}] RL:{self.rl_method} VALUE NET:{self.value_net} POLICY NET:{self.policy_net}'\n",
    "            f' LR:{self.lr} DF:{self.gamma}'\n",
    "        }\n",
    "        with self.lock:\n",
    "            logger.debug(info)\n",
    "            \n",
    "        # start time\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # prepare folders for saving results\n",
    "        if self.gen_output:\n",
    "            self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}')\n",
    "            if not os.path.isdir(self.epoch_summary_dir):\n",
    "                os.makedirs(self.epoch_summary_dir)\n",
    "            else:\n",
    "                for f in os.listdir(self.epoch_summary_dir):\n",
    "                    os.remove(os.path.join(self.epoch_summary_dir, f))\n",
    "                    \n",
    "        # reset info about training\n",
    "        # save the most highest porfolio value at max_portfolio_value variable\n",
    "        max_portfolio_value = 0\n",
    "        # save the count of epochs with profit\n",
    "        epoch_win_cnt = 0\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # start time of an epoch\n",
    "            time_start_epoch = time.time()\n",
    "            \n",
    "            # queue for making step samples\n",
    "            q_sample = collections.deque(maxlen=self.num_steps)\n",
    "            \n",
    "            self.reset()\n",
    "            \n",
    "            # decaying exploration rate\n",
    "            if learning:\n",
    "                eps = self.eps_init * (1 - (epoch / (self.num_epochs - 1)))\n",
    "            else:\n",
    "                eps = self.eps_init\n",
    "                \n",
    "            for i in tqdm(range(len(self.training_data)), leave=False):\n",
    "                # create samples\n",
    "                next_sample = self.build_sample()\n",
    "                if next_sample is None:\n",
    "                    break\n",
    "                \n",
    "                # save samples until its size becomes as num_steps\n",
    "                q_sample.append(next_sample)\n",
    "                if len(q_sample) < self.num_steps:\n",
    "                    continue\n",
    "                \n",
    "                # get predicted value of actions\n",
    "                if self.value_network is not None:\n",
    "                    pred_value = self.value_network.predict(list(q_sample))\n",
    "                # get predicted probabilities of actions\n",
    "                if self.policy_network is not None:\n",
    "                    pred_policy = self.policy_network.predict(list(q_sample))\n",
    "                    \n",
    "                # make decistions based on predicted value and probabilities\n",
    "                # decide actions based on networks or exploration\n",
    "                # decide actions randomly with epsilon probability or accoring to network output with (1 - epsilon) probability.\n",
    "                # policy network output is the probabilities that selling or buying increase portfolio value. If output for buying is larger than that for selling, buy the stock. Otherwise sell it.\n",
    "                # If there is no output of policy network, select the action with the highest output of value network.\n",
    "                action, confidence, exploration = self.agent.decide_action(pred_value, pred_policy, eps)\n",
    "                \n",
    "                # get rewards from action\n",
    "                reward = self.agent.step(action, confidence)\n",
    "                \n",
    "                # save action and the results in the memory\n",
    "                self.memory_sample.append(list(q_sample))\n",
    "                self.memory_action.append(action)\n",
    "                self.memory_reward.append(reward)\n",
    "                if self.value_network is not None:\n",
    "                    self.memory_value.append(pred_value)\n",
    "                if self.policy_network is not None:\n",
    "                    self.memory_policy.append(pred_policy)\n",
    "                self.memory_pv.append(self.agent.portfolio_value)\n",
    "                self.memory_num_stocks.append(self.agent.num_stocks)\n",
    "                if exploration:\n",
    "                    self.memory_exp_idx.append(self.training_data_idx)\n",
    "                    \n",
    "                # update iteration info\n",
    "                self.batch_size += 1\n",
    "                self.itr_cnt += 1\n",
    "                self.exploration_cnt += 1 if exploration else 0\n",
    "                \n",
    "            # training network after completin an epoch\n",
    "            if learning:\n",
    "                self.fit()\n",
    "                \n",
    "            # log about an epoch info\n",
    "            # check the length of epoch number string\n",
    "            num_epochs_digit = len(str(self.num_epochs))\n",
    "            # fill '0  as same size as the length of number of epochs\n",
    "            epoch_str = str(epoch + 1).rjust(num_epochs_digit, '0')\n",
    "            time_end_epoch = time.time()\n",
    "            # save time of an epoch\n",
    "            elapsed_time_epoch = time_end_epoch - time_start_epoch\n",
    "            logger.debug(\n",
    "                f'[{self.stock_code}][Epoch {epoch_str}] '\n",
    "                f'Epsilon:{eps:.4f} # Expl.:{self.exploration_cnt}/{self.itr_cnt} '\n",
    "                f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '\n",
    "                f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}'                \n",
    "            ) \n",
    "            \n",
    "            # visualize epoch information\n",
    "            if self.gen_output:\n",
    "                if self.num_epochs == 1 or (epoch + 1) % max(int(self.num_epochs / 100), 1) == 0:\n",
    "                    self.visualzie(epoch_str, self.num_epochs, eps)\n",
    "            \n",
    "            # update training info\n",
    "            max_portfolio_value = max(\n",
    "                max_portfolio_value, self.agent.portfolio_value\n",
    "            )\n",
    "            if self.agent.portfolio_value > self.agent.initial_balance:\n",
    "                epoch_win_cnt += 1\n",
    "                \n",
    "        # end time\n",
    "        time_end = time.time()\n",
    "        elapse_time = time_end - time_start\n",
    "        \n",
    "        # log about training\n",
    "        with self.lock:\n",
    "            logger.debug(\n",
    "                f'[{self.stock_code} Elapsed Time:{elapse_time:.4f}] '\n",
    "                f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}'\n",
    "            )\n",
    "            \n",
    "    def save_models(self):\n",
    "        if self.value_network is not None and self.value_network_path is not None:\n",
    "            self.value_network.save_model(self.value_network_path)\n",
    "        if self.policy_network is not None and self.policy_network_path is not None:\n",
    "            self.policy_network.save_model(self.policy_network_path)\n",
    "            \n",
    "    # without training, just predict actions based on samples\n",
    "    def predict(self):\n",
    "        # initiate an agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # queue for step samples\n",
    "        q_sample = collections.deque(maxlen=self.num_steps)\n",
    "        \n",
    "        result = []\n",
    "        while True:\n",
    "            # create samples\n",
    "            next_sample = self.build_sample()\n",
    "            if next_sample is None:\n",
    "                break\n",
    "            \n",
    "            # save samples as many as num_steps\n",
    "            q_sample.append(next_sample)\n",
    "            if len(q_sample) < self.num_steps:\n",
    "                continue\n",
    "            \n",
    "            # prediction based on value and policy network\n",
    "            pred_value = None\n",
    "            pred_policy = None\n",
    "            if self.value_network is not None:\n",
    "                pred_value = self.value_network.predict(list(q_sample)).tolist()\n",
    "            if self.policy_network is not None:\n",
    "                pred_policy = self.policy_network.predict(list(q_sample)).tolist()\n",
    "                \n",
    "            # decide action based on the network\n",
    "            result.append((self.env.observe[0], pred_value, pred_policy))\n",
    "            \n",
    "        if self.gen_output:\n",
    "            with open(os.path.join(self.output_path, f'pred_{self.stock_code}.json'), 'w') as f:\n",
    "                print(json.dump(result), file=F)\n",
    "                \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ReinforcementLearner(stock_code=stock_code, stock_data=df_stock_data, training_data=df_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.38750076293945,\n",
       " 42.622501373291016,\n",
       " 43.14250183105469,\n",
       " 42.73749923706055,\n",
       " 127594400.0,\n",
       " 42.3385009765625,\n",
       " 175292480.0,\n",
       " 0.009424005368516047,\n",
       " -0.2721056830275891,\n",
       " -0.008189493544760023,\n",
       " 0.0026611291070369274,\n",
       " -0.002690795339513195,\n",
       " 0.00947651596897686,\n",
       " 0.010940217492328647,\n",
       " -0.10466855845311374,\n",
       " 43.52825088500977,\n",
       " 170264920.0,\n",
       " -0.018166400713830202,\n",
       " -0.25061251607201296,\n",
       " 44.43225040435791,\n",
       " 173363500.0,\n",
       " -0.03814236622890343,\n",
       " -0.26400655270573103,\n",
       " 51.03562507629395,\n",
       " 158317206.66666666,\n",
       " -0.16259477231499375,\n",
       " -0.1940585443207878,\n",
       " 51.0888960202535,\n",
       " 133001283.33333333,\n",
       " -0.16346794379510873,\n",
       " -0.04065286588086803,\n",
       " 47.61911471684774,\n",
       " 133074575.0,\n",
       " -0.10251378062809866,\n",
       " -0.04118123240295902,\n",
       " 48.18011567132509,\n",
       " 40.68438513739073,\n",
       " 0.27390447006800017,\n",
       " 0.16870022260225612,\n",
       " -2.821814528255821,\n",
       " -2.0390696683620253,\n",
       " -0.7827448598937958,\n",
       " 0.5614399715311799,\n",
       " 35.956551757838014,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.build_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "##### Experience buffer\n",
    "\n",
    "- We get transition data $(s,a,r,s^\\prime)$, save into buffer, and train with uniform random sampling data from buffer.\n",
    "\n",
    "- Q-learning is off-poilcy learning.\n",
    "\n",
    "#### Target netwrok\n",
    "\n",
    "- We set separate target network and update it periodically.\n",
    "\n",
    "    - $\\theta_i$ : training parameter for $i$th iteration\n",
    "\n",
    "    - $\\theta^-_i$ : target calculation network for $i$th iteration\n",
    "\n",
    "    - $U(D)$ : replay memory of transitions $\\rightarrow \\pi_0, \\cdots, \\pi_i$ dataset.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "$$L_i(\\theta_i)=E_{s,a,r,s^\\prime}\\left[(r+\\gamma\\max_{a^\\prime}Q(s^\\prime, a^\\prime, Q^-_i)-Q(s,a;\\theta_i))^2\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "\n",
    "- To reudce compuration complexity, neural network get states and return all action-values.\n",
    "\n",
    "<img src='./image/3-s2.0-B9780323857871000117-f06-02-9780323857871.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, value_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.value_network_path = value_network_path\n",
    "        # create value network\n",
    "        self.init_value_network()\n",
    "        \n",
    "    # abstract method\n",
    "    def get_batch(self):\n",
    "        # reversed memory array\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        \n",
    "        # prepare sample array 'x' and label array 'y_value' with 0 value\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), Agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        \n",
    "        # we can handle from the last data because of reversed memory\n",
    "        for i, (sample, action, value, reward) in enumerate(memory):\n",
    "            # sample\n",
    "            x[i] = sample\n",
    "            ## reward for training\n",
    "            ## memory_reward[-1] : last profit/loss in the batch data\n",
    "            ## reward : profit/loss at the time of action\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # value network output\n",
    "            y_value[i] = value\n",
    "            # state-action value\n",
    "            y_value[i, action] = r + self.gamma * value_max_next\n",
    "            # save the maximum next state value\n",
    "            value_max_next = value.max()\n",
    "            \n",
    "        # return sample array, value network label, policy network label\n",
    "        # DQN has no policy network, return None\n",
    "        return x, y_value, None\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value-based RL\n",
    "    \n",
    "- Neural network defined Q-function is trained as Q-learning. $Q^\\pi(s,a)\\approx Q_\\theta(s,a)$\n",
    "\n",
    "- When Q-function is used, policies are simple argmax.\n",
    "\n",
    "    - behavior policy : $\\varepsilon$-greedy\n",
    "\n",
    "    - target policy : $\\argmax$ policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanteges of Policy-based RL\n",
    "\n",
    "- Advantages\n",
    "\n",
    "    - Stable convergence\n",
    "\n",
    "    - Effective on many action space or continuous action space\n",
    "\n",
    "    - Better exploration : Various experiences are trained without $\\varepsilon$-greedy by stochastic policy.\n",
    "\n",
    "- Disadvantage\n",
    "\n",
    "    - Frequent local optimum\n",
    "\n",
    "    - additional cost from evaluating policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks for policy\n",
    "\n",
    "- Discrete action space\n",
    "\n",
    "<img src='./image/pg_01.jpg'>\n",
    "\n",
    "- Continuous action space\n",
    "\n",
    "<img src='./image/pg_02.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
