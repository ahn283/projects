{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db connection\n",
    "\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import keyring\n",
    "import platform\n",
    "import numpy as np\n",
    "\n",
    "user = 'root'\n",
    "pw = keyring.get_password('macmini_db', user)\n",
    "host = '192.168.219.106' if platform.system() == 'Windows' else '127.0.0.1'\n",
    "port = 3306\n",
    "db = 'stock'\n",
    "\n",
    "\n",
    "# # connect DB\n",
    "# engine = create_engine(f'mysql+pymysql://{self.user}:{self.pw}@{self.host}:{self.port}/{self.db}')\n",
    "\n",
    "# con = pymysql.connect(\n",
    "#     user=user,\n",
    "#     passwd=pw,\n",
    "#     host=host,\n",
    "#     db=db,\n",
    "#     charset='utf8'\n",
    "# )\n",
    "        \n",
    "# mycursor = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data\n",
    "COLUMNS_STOCK_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "COLUMNS_TRAINING_DATA = ['open', 'high', 'low', 'close', 'volume', 'close_ma5', 'volume_ma5', 'close_ma5_ratio', 'volume_ma5_ratio',\n",
    "       'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio',\n",
    "       'low_close_ratio', 'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "       'close_ma10', 'volume_ma10', 'close_ma10_ratio', 'volume_ma10_ratio',\n",
    "       'close_ma20', 'volume_ma20', 'close_ma20_ratio', 'volume_ma20_ratio',\n",
    "       'close_ma60', 'volume_ma60', 'close_ma60_ratio', 'volume_ma60_ratio',\n",
    "       'close_ma120', 'volume_ma120', 'close_ma120_ratio',\n",
    "       'volume_ma120_ratio', 'close_ma240', 'volume_ma240',\n",
    "       'close_ma240_ratio', 'volume_ma240_ratio', 'upper_bb',\n",
    "       'lower_bb', 'bb_pb', 'bb_width', 'macd',\n",
    "       'macd_signal', 'macd_oscillator', 'rs', 'rsi']\n",
    "# COLUMNS_TRAINING_DATA = ['open', 'high', 'low', 'close', 'volume', 'close_ma5', 'volume_ma5', 'close_ma5_ratio', 'volume_ma5_ratio',\n",
    "#        'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio',\n",
    "#        'low_close_ratio', 'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "#        'close_ma10', 'volume_ma10', 'close_ma10_ratio', 'volume_ma10_ratio',\n",
    "#        'close_ma20', 'volume_ma20', 'close_ma20_ratio', 'volume_ma20_ratio',\n",
    "#        'close_ma60', 'volume_ma60', 'close_ma60_ratio', 'volume_ma60_ratio',\n",
    "#        'close_ma120', 'volume_ma120', 'close_ma120_ratio',\n",
    "#        'volume_ma120_ratio', 'close_ma240', 'volume_ma240',\n",
    "#        'close_ma240_ratio', 'volume_ma240_ratio', 'middle_bb', 'upper_bb',\n",
    "#        'lower_bb', 'bb_pb', 'bb_width', 'ema_short', 'ema_long', 'macd',\n",
    "#        'macd_signal', 'macd_oscillator', 'close_change', 'close_up',\n",
    "#        'close_down', 'rs', 'rsi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# get us stock price of a specific ticker\n",
    "def get_prices_from_ticker(ticker, fro=None, to=None):\n",
    "\n",
    "    # connect DB\n",
    "    engine = create_engine(f'mysql+pymysql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    con = pymysql.connect(\n",
    "        user=user,\n",
    "        passwd=pw,\n",
    "        host=host,\n",
    "        db=db,\n",
    "        charset='utf8'\n",
    "    )\n",
    "            \n",
    "    mycursor = con.cursor()\n",
    "    \n",
    "    if fro is not None:\n",
    "        if to is not None:               \n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date BETWEEN '{fro}' AND '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date >= '{fro}'\n",
    "                    \"\"\"\n",
    "    \n",
    "    else:\n",
    "        if to is not None:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date <= '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    \"\"\"\n",
    "            \n",
    "    print(query)\n",
    "    prices = pd.read_sql(query, con=engine)\n",
    "    con.close()\n",
    "    engine.dispose()\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# str format on date, time\n",
    "FORMAT_DATE = '%Y%m%d'\n",
    "FORMAT_DATETIME = '%Y%m%d%H%M%S'\n",
    "\n",
    "def get_today_str():\n",
    "    today = datetime.datetime.combine(\n",
    "        datetime.date.today(), datetime.datetime.min.time()\n",
    "    )\n",
    "    today_str = today.strftime(FORMAT_DATE)\n",
    "    return today_str\n",
    "\n",
    "def get_time_str():\n",
    "    return datetime.datetime.fromtimestamp(\n",
    "        int(time.time())\n",
    "    ).strftime(FORMAT_DATETIME)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    x = max(min(x, 10), -10)\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2010-01-01' AND '2020-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "stock_code = 'AAPL'\n",
    "fro = '2010-01-01'\n",
    "to = '2020-12-31'\n",
    "df = get_prices_from_ticker(stock_code, fro=fro, to=to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_STOCK_RATIO_DATA = [\n",
    "    'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio', 'low_close_ratio',\n",
    "    'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "]\n",
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    # moving average\n",
    "    windows = [5, 10, 20, 60, 120, 240]\n",
    "    for window in windows:\n",
    "        data[f'close_ma{window}'] = data['close'].rolling(window).mean()\n",
    "        data[f'volume_ma{window}'] = data['volume'].rolling(window).mean()\n",
    "        data[f'close_ma{window}_ratio'] = (data['close'] - data[f'close_ma{window}']) / data[f'close_ma{window}']\n",
    "        data[f'volume_ma{window}_ratio'] = (data['volume'] - data[f'volume_ma{window}']) / data[f'volume_ma{window}']\n",
    "        data['open_close_ratio'] = (data['open'].values - data['close'].values) / data['close'].values\n",
    "        data['open_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'open_prev_close_ratio'] = (data['open'][1:].values - data['close'][:-1].values) / data['close'][:-1].values\n",
    "        data['high_close_ratio'] = (data['high'].values - data['close'].values) / data['close'].values\n",
    "        data['low_close_ratio'] = (data['low'].values - data['close'].values) / data['close'].values\n",
    "        data['close_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'close_prev_close_ratio'] = (data['close'][1:].values - data['close'][:-1].values) / data['close'][:-1].values \n",
    "        data['volume_prev_volume_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'volume_prev_volume_ratio'] = (\n",
    "            # if volume is 0, change it into non zero value exploring previous volume continuously\n",
    "            (data['volume'][1:].values - data['volume'][:-1].values) / data['volume'][:-1].replace(to_replace=0, method='ffill').replace(to_replace=0, method='bfill').values\n",
    "        )\n",
    "    \n",
    "    # Bollinger band\n",
    "    data['middle_bb'] = data['close'].rolling(20).mean()\n",
    "    data['upper_bb'] = data['middle_bb'] + 2 * data['close'].rolling(20).std()\n",
    "    data['lower_bb'] = data['middle_bb'] - 2 * data['close'].rolling(20).std()\n",
    "    data['bb_pb'] = (data['close'] - data['lower_bb']) / (data['upper_bb'] - data['lower_bb'])\n",
    "    data['bb_width'] = (data['upper_bb'] - data['lower_bb']) / data['middle_bb']\n",
    "    \n",
    "    # MACD\n",
    "    macd_short, macd_long, macd_signal = 12, 26, 9\n",
    "    data['ema_short'] = data['close'].ewm(macd_short).mean()\n",
    "    data['ema_long'] = data['close'].ewm(macd_long).mean()\n",
    "    data['macd'] = data['ema_short'] - data['ema_long']\n",
    "    data['macd_signal'] = data['macd'].ewm(macd_signal).mean()\n",
    "    data['macd_oscillator'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # RSI\n",
    "    data['close_change'] = data['close'].diff()\n",
    "    # data['close_up'] = np.where(data['close_change'] >=0, df['close_change'], 0)\n",
    "    data['close_up'] = data['close_change'].apply(lambda x: x if x >= 0 else 0)\n",
    "    # data['close_down'] = np.where(data['close_change'] < 0, df['close_change'].abs(), 0)\n",
    "    data['close_down'] = data['close_change'].apply(lambda x: -x if x < 0 else 0)\n",
    "    data['rs'] = data['close_up'].ewm(alpha=1/14, min_periods=14).mean() / data['close_down'].ewm(alpha=1/14, min_periods=14).mean()\n",
    "    data['rsi'] = 100 - (100 / (1 + data['rs']))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock_code, fro, to):\n",
    "    df = get_prices_from_ticker(stock_code, fro, to)\n",
    "    df_adj = preprocess(df).dropna().reset_index(drop=True)\n",
    "    # df_adj.dropna(inplace=True).reset_index(drop=True)\n",
    "    \n",
    "    stock_data = df_adj[COLUMNS_STOCK_DATA]\n",
    "    training_data = df_adj[COLUMNS_TRAINING_DATA]\n",
    "    \n",
    "    return df_adj, stock_data, training_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2010-01-01' AND '2020-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "df_adj, stock_data, training_data = load_data(stock_code, fro, to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# environment\n",
    "\n",
    "class Environment:\n",
    "    ''' \n",
    "    Attribute\n",
    "    ---------\n",
    "    - stock_data : stock price data such as 'open', 'close', 'volume', 'bb', 'rsi', etc.\n",
    "    - state : current state\n",
    "    - idx : current postion of stock data\n",
    "    \n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize idx and state\n",
    "    - step() : move idx into next postion and get a new state\n",
    "    - get_price() : get close price of current state\n",
    "    - get_state() : get current state\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, stock_data=None):\n",
    "        self.PRICE_IDX = 4  # index postion of close price\n",
    "        self.stock_data = stock_data\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        \n",
    "    def step(self):\n",
    "        # if there is no more idx, return None\n",
    "        if len(self.stock_data) > self.idx + 1:\n",
    "            self.idx += 1\n",
    "            self.state = self.stock_data.iloc[self.idx]\n",
    "            return self.state\n",
    "        return None\n",
    "    \n",
    "    def get_price(self):\n",
    "        # return close price\n",
    "        if self.state is not None:\n",
    "            return self.state[self.PRICE_IDX]\n",
    "        return None\n",
    "    \n",
    "    def get_state(self):\n",
    "        # return current state\n",
    "        if self.state is not None:\n",
    "            return self.state\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.441429138183594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Environment(df_adj)\n",
    "a.reset()\n",
    "a.step()\n",
    "a.step()\n",
    "a.get_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                              2010-12-16\n",
       "high                                 11.4675\n",
       "low                                11.521786\n",
       "open                               11.432143\n",
       "close                              11.473214\n",
       "adj_close                           9.713216\n",
       "volume                           322030800.0\n",
       "ticker                                  AAPL\n",
       "close_ma5                          11.458071\n",
       "volume_ma5                       358535520.0\n",
       "close_ma5_ratio                     0.001322\n",
       "volume_ma5_ratio                   -0.101816\n",
       "open_close_ratio                    -0.00358\n",
       "open_prev_close_ratio              -0.000812\n",
       "high_close_ratio                   -0.000498\n",
       "low_close_ratio                     0.004233\n",
       "close_prev_close_ratio              0.002778\n",
       "volume_prev_volume_ratio           -0.228321\n",
       "close_ma10                         11.431071\n",
       "volume_ma10                      359079280.0\n",
       "close_ma10_ratio                    0.003687\n",
       "volume_ma10_ratio                  -0.103176\n",
       "close_ma20                         11.304143\n",
       "volume_ma20                      395309600.0\n",
       "close_ma20_ratio                    0.014957\n",
       "volume_ma20_ratio                  -0.185371\n",
       "close_ma60                         10.969405\n",
       "volume_ma60                 505909366.666667\n",
       "close_ma60_ratio                    0.045929\n",
       "volume_ma60_ratio                  -0.363461\n",
       "close_ma120                        10.071384\n",
       "volume_ma120                543663213.333333\n",
       "close_ma120_ratio                   0.139189\n",
       "volume_ma120_ratio                 -0.407665\n",
       "close_ma240                         9.198582\n",
       "volume_ma240                614464328.333333\n",
       "close_ma240_ratio                   0.247281\n",
       "volume_ma240_ratio                 -0.475916\n",
       "middle_bb                          11.304143\n",
       "upper_bb                           11.636183\n",
       "lower_bb                           10.972103\n",
       "bb_pb                               0.754594\n",
       "bb_width                            0.058747\n",
       "ema_short                          11.278047\n",
       "ema_long                           10.968925\n",
       "macd                                0.309122\n",
       "macd_signal                         0.335923\n",
       "macd_oscillator                    -0.026801\n",
       "close_change                        0.031785\n",
       "close_up                            0.031785\n",
       "close_down                               0.0\n",
       "rs                                  1.487603\n",
       "rsi                                59.800667\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.step()\n",
    "a.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - enviroment : instance of environment\n",
    "    - initial_balance : initial capital balance\n",
    "    - min_trading_price : minimum trading price\n",
    "    - max_trading_price : maximum trading price\n",
    "    - balance : cash balance\n",
    "    - num_stocks : obtained stocks\n",
    "    - portfolio_value : value of portfolios (balance + price * num_stocks)\n",
    "    - num_buy : number of buying\n",
    "    - num_sell : number of selling\n",
    "    - num_hold : number of holding\n",
    "    - ratio_hold : ratio of holding stocks\n",
    "    - profitloss : current profit or loss\n",
    "    - avg_buy_price_ratio : the ratio average price of a stock bought to the current price\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize an agent\n",
    "    - set_balance() : initialize balance\n",
    "    - get_states() : get the state of an agent\n",
    "    - decide_action() : exploration or exploitation behavior according to the policy net\n",
    "    - validate_action() : validate actions\n",
    "    - decide_trading_unit() : decide how many stocks are sold or bought\n",
    "    - act() : act the actions\n",
    "    '''\n",
    "    \n",
    "    # agent state dimensions\n",
    "    ## (ratio_hold, profit-loss ratio, current price to avg_buy_price ratio)\n",
    "    STATE_DIM = 3\n",
    "    \n",
    "    # trading charge and tax\n",
    "    TRADING_CHARGE = 0.00015    # trading charge 0.015%\n",
    "    TRADING_TAX = 0.02          # trading tax = 0.2%\n",
    "    \n",
    "    # action space\n",
    "    ACTION_BUY = 0      # buy\n",
    "    ACTION_SELL = 1     # sell\n",
    "    ACTION_HOLD = 2     # hold\n",
    "    \n",
    "    # get probabilities from neural nets\n",
    "    ACTIONS = [ACTION_BUY, ACTION_SELL, ACTION_HOLD]\n",
    "    NUM_ACTIONS = len(ACTIONS)      # output number from nueral nets\n",
    "    \n",
    "    def __init__(self, environment, initial_balance, min_trading_price, max_trading_price):\n",
    "        # get current price from the environment\n",
    "        self.environment = environment\n",
    "        self.initial_balance = initial_balance\n",
    "        \n",
    "        # minumum and maximum trainding price\n",
    "        self.min_trading_price = min_trading_price\n",
    "        self.max_trading_price = max_trading_price\n",
    "        \n",
    "        # attributes for an agent class\n",
    "        self.balance = initial_balance\n",
    "        self.num_stocks = 0\n",
    "        \n",
    "        # value of portfolio : balance + num_stocks * {current stock price}\n",
    "        self.portfolio_value = 0\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        \n",
    "        # the state of Agent class\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        \n",
    "    def set_initial_balance(self, balance):\n",
    "        self.initial_balance = balance\n",
    "        \n",
    "    def get_states(self):\n",
    "        # return ratio hold, profit/loss ratio and \n",
    "        # ratio_hold = num_stokcs / (portfoilo_value / price) = (num_stocks * price) / portfolio_value\n",
    "        self.ratio_hold = self.num_stocks * self.environment.get_price() / self.portfolio_value\n",
    "        \n",
    "        return (\n",
    "            self.ratio_hold,\n",
    "            self.profitloss,        # profitloss = (portfolio_value / initial_balance) - 1\n",
    "            (self.environment.get_price() / self.avg_buy_price) - 1 if self.avg_buy_price > 0 else 0\n",
    "        )\n",
    "        \n",
    "    def decide_action(self, pred_value, pred_policy, epsilon):\n",
    "        # act randomly with epsilon probability, act according to neural network  with (1 - epsilon) probability\n",
    "        confidence = 0\n",
    "        \n",
    "        # if theres is a pred_policy, follow it, otherwise follow a pred_value\n",
    "        pred = pred_policy\n",
    "        if pred is None:\n",
    "            pred = pred_value\n",
    "            \n",
    "        # there is no prediction from both pred_policy and pred_value, explore!\n",
    "        if pred is None:\n",
    "            epsilon = 1\n",
    "        else:\n",
    "            maxpred = np.max(pred)\n",
    "            # if values for actions are euqal, explore!\n",
    "            if (pred == maxpred).all():\n",
    "                epsilon = 1\n",
    "        \n",
    "            # if the diffrence between buying and selling prediction policy value is less than 0.05, explore!\n",
    "            if pred_policy is not None:\n",
    "                if np.max(pred_policy) - np.min(pred_policy) < 0.05:\n",
    "                    epsilon = 1\n",
    "            # if pred is not None:\n",
    "            #     if np.max(pred) - np.min(pred) < 0.05:\n",
    "            #         epsilon = 1     \n",
    "                    \n",
    "        # decide whether exploration will be done or not\n",
    "        if np.random.rand() < epsilon:\n",
    "            exploration = True\n",
    "            action = np.random.randint(self.NUM_ACTIONS) \n",
    "        else: \n",
    "            exploration = False\n",
    "            action = np.argmax(pred)\n",
    "            \n",
    "        confidence = .5\n",
    "        if pred_policy is not None:\n",
    "            confidence = pred[action]  \n",
    "        elif pred_value is not None:\n",
    "            confidence = sigmoid(pred[action])\n",
    "            \n",
    "        return action, confidence, exploration\n",
    "    \n",
    "    def validate_action(self, action):\n",
    "        # validate if the action is available\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # check if al least one stock can be bought.\n",
    "            if self.balance < self.environment.get_price() * (1 + self.TRADING_CHARGE):\n",
    "                return False\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # check if there is any sotck that can be sold\n",
    "            if self.num_stocks <= 0:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def decide_trading_unit(self, confidence):\n",
    "        # adjust number of stocks for buying and selling according to confidence level\n",
    "        if np.isnan(confidence):\n",
    "            return self.min_trading_price\n",
    "        \n",
    "        # set buying price range between self.min_trading_price + added_trading_price [min_trading_price, max_trading_price]\n",
    "        # in case that confidence > 1 causes the price over max_trading_price, we set min() so that the value cannot have larger value than self.max_trading_price - self.min_trading_price\n",
    "        # in case that confidence < 0, we set max() so that added_trading_price cannot have negative value.\n",
    "        added_trading_price = max(min(\n",
    "            int(confidence * (self.max_trading_price - self.min_trading_price)),\n",
    "            self.max_trading_price - self.min_trading_price\n",
    "        ), 0)\n",
    "        \n",
    "        trading_price = self.min_trading_price + added_trading_price\n",
    "        \n",
    "        return max(int(trading_price / self.environment.get_price()), 1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def act(self, action, confidence):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - action : decided action from decide_action() method based on exploration or exploitation (0 or 1)\n",
    "        - confidence : probabilitu from decide_action() method, the probability from policy network or the softmax probability from value network\n",
    "        '''\n",
    "        \n",
    "        if not self.validate_action(action):\n",
    "            action = Agent.ACTION_HOLD\n",
    "        \n",
    "        # get the price from the environment\n",
    "        curr_price = self.environment.get_price()\n",
    "        \n",
    "        # buy\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # decide how many stocks will be bought\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            balance = (\n",
    "                self.balance - curr_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            )\n",
    "            \n",
    "            # if lacks of balance, buy maximum units within the amount of money available\n",
    "            if balance < 0:\n",
    "                trading_unit = min(\n",
    "                    int(self.balance / (curr_price * (1 + self.TRADING_CHARGE))),\n",
    "                    int(self.max_trading_price / curr_price)\n",
    "                )\n",
    "                \n",
    "            # total amount of money with trading charge\n",
    "            invest_amount = curr_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            if invest_amount > 0:\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks + curr_price * trading_unit) / (self.num_stocks + trading_unit)\n",
    "                self.balance -= invest_amount\n",
    "                self.num_stocks += trading_unit\n",
    "                self.num_buy += 1\n",
    "                \n",
    "        # sell\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # decide how many stocks will be sold\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            \n",
    "            # if lacks of stocks, sell maximum units available\n",
    "            trading_unit = min(trading_unit, self.num_stocks)\n",
    "            \n",
    "            # selling amount\n",
    "            invest_amount = curr_price * (\n",
    "                1 - (self.TRADING_TAX + self.TRADING_CHARGE)\n",
    "            ) * trading_unit\n",
    "            \n",
    "            if invest_amount > 0:\n",
    "                # update average buy price\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks - curr_price * trading_unit) / (self.num_stocks - trading_unit) if self.num_stocks > trading_unit else 0\n",
    "                self.num_stocks -= trading_unit\n",
    "                self.balance += invest_amount\n",
    "                self.num_sell += 1\n",
    "                \n",
    "        # hold\n",
    "        elif action == Agent.ACTION_HOLD:\n",
    "            self.num_hold += 1\n",
    "            \n",
    "        # update portfolio value\n",
    "        self.portfolio_value = self.balance + curr_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        \n",
    "        return self.profitloss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    ''' \n",
    "    Common attributes and methods for neural networks\n",
    "    \n",
    "    Attributes\n",
    "    ---------\n",
    "    - input_dim\n",
    "    - output_dim\n",
    "    - lr : learning rate\n",
    "    - shared_network : head of neural network which is shared with various networks (e.g., A2C)\n",
    "    - activation : activation layer function ('linear', 'sigmoid', 'tanh', 'softmax')\n",
    "    - loss : loss function for networks\n",
    "    - model : final neural network model\n",
    "    \n",
    "    Functions\n",
    "    ---------\n",
    "    - predict() : calculate value or probability of actions\n",
    "    - train_on_batch() : generate batch data for training\n",
    "    - save_model()\n",
    "    - load_model()\n",
    "    - get_share_network() : generate network head according to the networks\n",
    "    '''\n",
    "    \n",
    "    # thread lock for A3C\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    def __init__(self, input_dim=0, output_dim=0, num_steps=1, lr=0.001,\n",
    "                 shared_network=None, activation='sigmoid', loss='mse'):\n",
    "        self.input_dim = input_dim\n",
    "        self.outpu_dim = output_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.shared_network = shared_network\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        \n",
    "        # data shape for various networks\n",
    "        # CNN, LSTMNetwork has 3 dimensional shape, so we set input shape as (num_stpes, input_dim). In DNN, we set input shape as (input_dim, )\n",
    "        inp = None\n",
    "        if self.num_steps > 1:\n",
    "            inp = (self.num_steps, input_dim)\n",
    "        else:\n",
    "            inp = (self.input_dim,)\n",
    "        \n",
    "        # in case that shared_network is used\n",
    "        self.head = None\n",
    "        if self.shared_network is None:\n",
    "            self.head = self.get_network_head(inp, self.outpu_dim)\n",
    "        else:\n",
    "            self.head = self.shared_network\n",
    "            \n",
    "        # neural network model\n",
    "        ## generate network model for head\n",
    "        self.model = torch.nn.Sequential(self.head)\n",
    "        if self.activation == 'linear':\n",
    "            pass\n",
    "        elif self.activation == 'relu':        \n",
    "            self.model.add_module('activation', torch.nn.ReLU())   \n",
    "        elif self.activation == 'leaky_relu':\n",
    "            self.model.add_module('activation', torch.nn.LeakyReLU()) \n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.model.add_module('activation', torch.nn.Sigmoid())\n",
    "        elif self.activation == 'tanh':\n",
    "            self.model.add_module('activation', torch.nn.Tanh())\n",
    "        elif self.activation == 'softmax':\n",
    "            self.model.add_module('activation', torch.nn.Softmax(dim=1))\n",
    "        self.model.apply(Network.init_weights)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.NAdam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = None\n",
    "        if loss == 'mse':\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "        elif loss == 'binary_crossentropy':\n",
    "            self.criterion = torch.nn.BCELoss()\n",
    "            \n",
    "    def predict(self, sample):\n",
    "        # return prediction of buy, sell and hold on given sample\n",
    "        # value network returns each actions' values on sample and policy network returns each actions' probabilities on sample\n",
    "        with self.lock:\n",
    "            # transform evaluation mode : deavtivate module used only on traininig such as Drop out\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(sample).float().to(device)\n",
    "                pred = self.model(x).detach().cpu().numpy()\n",
    "                pred = pred.flatten()\n",
    "            return pred\n",
    "        \n",
    "    def train_on_batch(self, x, y):\n",
    "        if self.num_steps > 1:\n",
    "            x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        else:\n",
    "            x = np.array(x).reshape((-1, self.input_dim))\n",
    "        loss = 0\n",
    "        with self.lock:\n",
    "            # transform training mode\n",
    "            self.model.train()\n",
    "            _x = torch.from_numpy(x).float().to(device)\n",
    "            _y = torch.from_numpy(y).float().to(device)\n",
    "            y_pred = self.model(_x)\n",
    "            _loss = self.criterion(y_pred, _y)\n",
    "            self.optimizer.zero_grad()\n",
    "            _loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += _loss.item()\n",
    "        return loss\n",
    "    \n",
    "    def train_on_batch_for_ppo(self, x, y, a, eps, K):\n",
    "        if self.num_steps > 1:\n",
    "            x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        else:\n",
    "            x = np.array(x).reshape((-1, self.input_dim))\n",
    "            \n",
    "        loss = 0.\n",
    "        with self.lock:\n",
    "            self.model.train()\n",
    "            _x = torch.from_numpy(x).float().to(device)\n",
    "            _y = torch.from_numpy(y).float().to(device)\n",
    "            probs = F.softmax(_y, dim=1)\n",
    "            for _ in range(K):\n",
    "                y_pred = self.model(_x)\n",
    "                probs_pred = F.softmax(y_pred, dim=1)\n",
    "                rto = torch.exp(torch.log(probs[:, a]) - torch.log(probs_pred[:, a]))\n",
    "                rto_adv = rto * _y[:, a]\n",
    "                clp_adv = torch.clamp(rto, 1 - eps, 1 + eps) * _y[:, a]\n",
    "                _loss = -torch.min(rto_adv, clp_adv).mean()\n",
    "                _loss.backward()\n",
    "                self.optimizer.step()\n",
    "                loss += _loss.item()\n",
    "        return loss\n",
    "    \n",
    "    @classmethod\n",
    "    def get_shared_network(cls, net='dnn', num_steps=1, input_dim=0, output_dim=0):\n",
    "        if net == 'dnn':\n",
    "            return DNN.get_network_head((input_dim, ), output_dim)\n",
    "        elif net == 'lstm':\n",
    "            return LSTMNetwork.get_network_head((num_steps, input_dim), output_dim)\n",
    "        elif net == 'cnn':\n",
    "            return CNN.get_network_head((num_steps, input_dim), output_dim)\n",
    "        elif net == 'alex':\n",
    "            return AlexNet.get_network_head((num_steps, input_dim), output_dim)\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        # initialize weights as weighted normal distribution\n",
    "        if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=0.01)\n",
    "        elif isinstance(m, torch.nn.LSTM):\n",
    "            for weights in m.all_weights:\n",
    "                for weight in weights:\n",
    "                    torch.nn.init.normal_(weight, std=0.01)\n",
    "                    \n",
    "    def save_model(self, model_path):\n",
    "        if model_path is not None and self.model is not None:\n",
    "            torch.save(self.model, model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        if model_path is not None:\n",
    "            self.model = torch.load(model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Network):\n",
    "    # @staticmethod\n",
    "    # def get_network_head(inp, outpu_dim):\n",
    "    #     return torch.nn.Sequential(\n",
    "    #         torch.nn.BatchNorm1d(inp[0]),   # input shape = (input_dim, )\n",
    "    #         torch.nn.Linear(inp[0], 256),\n",
    "    #         torch.nn.BatchNorm1d(256),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(256, 128),\n",
    "    #         torch.nn.BatchNorm1d(128),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(128, 64),\n",
    "    #         torch.nn.BatchNorm1d(64),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(64, 32),\n",
    "    #         torch.nn.BatchNorm1d(32),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(32, outpu_dim),\n",
    "    #     )\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, outpu_dim):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),   # input shape = (input_dim, )\n",
    "            torch.nn.Linear(inp[0], 1024),\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, outpu_dim),\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),\n",
    "            LSTMModule(inp[1], 128, batch_first=True, use_last_only=True),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, output_dim),\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((-1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "    \n",
    "class LSTMModule(torch.nn.LSTM):\n",
    "    def __init__(self, *args, use_last_only=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_last_only = use_last_only\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, (h_n, _) = super().forward(x)\n",
    "        if self.use_last_only:\n",
    "            return h_n[-1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),\n",
    "            torch.nn.Conv1d(inp[0], 1, kernel_size),\n",
    "            torch.nn.BatchNorm1d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(inp[1] - (kernel_size - 1), 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Do print / debug stuff\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2,\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        print(inp)\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(inp, 96, kernel_size=kernel_size, stride=stride),        # (5, 96)\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.Conv2d(96, 256, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.Conv2d(256, 384, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 384, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 256, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            PrintLayer(),\n",
    "            \n",
    "            # classifier\n",
    "            torch.nn.Linear(6, 32),\n",
    "            PrintLayer(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(32, output_dim),\n",
    "            PrintLayer()\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "class Visualizer:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - fig : matplotlib Figure instance plays like a canvas\n",
    "    - plot() : print charts except daily price chart\n",
    "    - save() : save Figure as an image file\n",
    "    - clear() : initialze all chart but daily price chart\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    - Figure title : parameter, epsilon\n",
    "    - Axes 1 : daily price chart\n",
    "    - Axes 2 : number of stocks and agent action chart\n",
    "    - Axes 3 : value network chart\n",
    "    - Axes 4 : policy network and epsilon chart\n",
    "    - Axes 5 : Portfolio value and learning point chart\n",
    "    '''\n",
    "    \n",
    "    COLORS = ['r', 'b', 'g']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.canvas = None \n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.title = ''\n",
    "        self.x = []\n",
    "        self.xticks = []\n",
    "        self.xlabels = []\n",
    "        \n",
    "    def prepare(self, stock_data, title):\n",
    "        self.title = title\n",
    "        # shared x-axis among all charts\n",
    "        # self.x = np.arange(stock_data['date'])\n",
    "        # self.x_label = [datetime.strptime(date, '%Y%m%d').date() for date in stock_data['date']]\n",
    "        with lock:\n",
    "            # preare for printing five charts\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                nrows=5, ncols=1, facecolor='w', sharex=True\n",
    "            )\n",
    "            for ax in self.axes:\n",
    "                # deactivate scientific marks\n",
    "                ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # change y-axis to the right\n",
    "                ax.yaxis.tick_right()\n",
    "            \n",
    "            # chart 1. daily price data\n",
    "            self.axes[0].set_ylabel('Env.')\n",
    "            x = np.arange(len(stock_data))\n",
    "            # make two dimensional array with open, high, low and close order\n",
    "            ohlc = np.hstack((\n",
    "                x.reshape(-1, 1), np.array(stock_data)[:, 1:5]\n",
    "            ))\n",
    "            # red for positive, blue for negative\n",
    "            candlestick_ohlc(self.axes[0], ohlc, colorup='r', colordown='b')\n",
    "            # visualize volume\n",
    "            ax = self.axes[0].twinx()\n",
    "            volume = np.array(stock_data)[:, 5].tolist()\n",
    "            ax.bar(x, volume, color='b', alpha=0.3)\n",
    "            # set x-axis\n",
    "            self.x = np.arange(len(stock_data['date']))\n",
    "            self.xticks = stock_data.index[[0, -1]]\n",
    "            self.xlabels = stock_data.iloc[[0, -1]]['date']\n",
    "            \n",
    "            \n",
    "    def plot(self, epoch_str=None, num_epochs=None, epsilon=None,\n",
    "             action_list=None, actions=None, num_stocks=None,\n",
    "             outvals_value=[], outvals_policy=[], exps=None,\n",
    "             initial_balance=None, pvs=None):\n",
    "        ''' \n",
    "        Attributes\n",
    "        ---------\n",
    "        - epoch_str : epoch for Figure title\n",
    "        - num_epochs : number of total epochs\n",
    "        - epsilon : exploration rate\n",
    "        - action_list : total action list of an agent\n",
    "        - num_stocks : number of stocks\n",
    "        - outvals_value : output array of value network\n",
    "        - outvals_policy : ouput array of policy network\n",
    "        - exps : array whether exploration is true or not\n",
    "        - initial_balance \n",
    "        - pvs : array of portfolio value\n",
    "        '''\n",
    "        \n",
    "        with lock:\n",
    "            # action, num_stocks, outvals_value, outvals_policy, pvs has same size\n",
    "            # create an array with same size as actions and use as x-axis\n",
    "            actions = np.array(actions)     # action array of an agent\n",
    "            # turn value network output as an array\n",
    "            outvals_value = np.array(outvals_value)\n",
    "            # turn policy network output as an array\n",
    "            outvals_policy = np.array(outvals_policy)\n",
    "            # turn initial balance as an array\n",
    "            pvs_base = np.zeros(len(actions)) + initial_balance     # array([initial_balance, initial_balance, initial_balance, ...])\n",
    "            \n",
    "            # chart 2. agent states (action, num_stocks)\n",
    "            for action, color in zip(action_list, self.COLORS):\n",
    "                for i in self.x[actions == action]:\n",
    "                    # express actions as background color : red for buying, blue for selling\n",
    "                    self.axes[1].axvline(i, color=color, alpha=0.1)\n",
    "            self.axes[1].plot(self.x, num_stocks, '-k')     # plot number of stocks\n",
    "            \n",
    "            # chart 3. value network (prediction value for action)\n",
    "            if (len(outvals_value)) > 0:\n",
    "                max_actions = np.argmax(outvals_value, axis=1)\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    # plot background\n",
    "                    for idx in self.x:\n",
    "                        if max_actions[idx] == action:\n",
    "                            self.axes[2].axvline(idx, color=color, alpha=0.1)\n",
    "                    # plot value network\n",
    "                    ## red for buying, blue for selling, green for holding\n",
    "                    ## if no prediction for action, no plot green chart\n",
    "                    self.axes[2].plot(self.x, outvals_value[:, action], color=color, linestyle='-')\n",
    "            \n",
    "            # chart 4. policy network\n",
    "            # plot exploration as yellow background\n",
    "            for exp_idx in exps:\n",
    "                self.axes[3].axvline(exp_idx, color='y')\n",
    "            # plot action as background\n",
    "            _outvals = outvals_policy if len(outvals_policy) > 0 else outvals_value\n",
    "            for idx, outval in zip(self.x, _outvals):\n",
    "                color = 'white'\n",
    "                if np.isnan(outval.max()):\n",
    "                    continue\n",
    "                # with no exploration area, red for buying, blue for selling\n",
    "                if outval.argmax() == Agent.ACTION_BUY:\n",
    "                    color = self.COLORS[0]      # red for buying\n",
    "                elif outval.argmax() == Agent.ACTION_SELL:\n",
    "                    color = self.COLORS[1]      # blue for selling\n",
    "                elif outval.argmax() == Agent.ACTION_HOLD:\n",
    "                    color = self.COLORS[2]      # green for holding\n",
    "                self.axes[3].axvline(idx, color=color, alpha=0.1)\n",
    "                \n",
    "            # plot policy network\n",
    "            # red for buying policy network output, blue for selling policy network\n",
    "            # when red line is above blue line, buy stocks, otherwise sell stocks\n",
    "            if len(outvals_policy) > 0:\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    self.axes[3].plot(\n",
    "                        self.x, outvals_policy[:, action],\n",
    "                        color=color, linestyle='-'\n",
    "                    )\n",
    "                    \n",
    "            # chart 5. portfolio value\n",
    "            # horzontal line for initial balance\n",
    "            self.axes[4].axhline(\n",
    "                initial_balance, linestyle='-', color='gray'\n",
    "            )\n",
    "            \n",
    "            self.axes[4].fill_between(\n",
    "                self.x, pvs, pvs_base,\n",
    "                where=pvs > pvs_base, facecolor='r', alpha=0.1\n",
    "            )\n",
    "            self.axes[4].plot(self.x, pvs, '-k')\n",
    "            self.axes[4].xaxis.set_ticks(self.xticks)\n",
    "            self.axes[4].xaxis.set_ticklabels(self.xlabels)\n",
    "            \n",
    "            # epoch and exploration rate\n",
    "            self.fig.suptitle(f'{self.title}\\nEPOCH:{epoch_str}/{num_epochs} EPSILON:{epsilon:.2f}')\n",
    "            # adjust canvas layout\n",
    "            self.fig.tight_layout()\n",
    "            self.fig.subplots_adjust(top=0.85)\n",
    "            \n",
    "    def clear(self, xlim):\n",
    "        with lock:\n",
    "            _axes = self.axes.tolist()\n",
    "            # intial chart except non changeable value\n",
    "            for ax in _axes[1:]:\n",
    "                ax.cla()        # initialize chart\n",
    "                ax.relim()      # initialize limit\n",
    "                ax.autoscale()  # reset scale\n",
    "            \n",
    "            # reset y-axis label\n",
    "            self.axes[1].set_ylabel('Agent')\n",
    "            self.axes[2].set_ylabel('V')\n",
    "            self.axes[3].set_ylabel('P')\n",
    "            self.axes[4].set_ylabel('PV')\n",
    "            for ax in _axes:\n",
    "                ax.set_xlim(xlim)       # reset limit in x-axis\n",
    "                ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # set equal width horizontally\n",
    "                ax.ticklabel_format(useOffset=False)\n",
    "                \n",
    "    def save(self, path):\n",
    "        with lock:\n",
    "            self.fig.savefig(path)\n",
    "            \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Policy $\\pi$ is a function that connectes states to action probabilities. Action probabilities are for getting $a\\sim\\pi(s)$. In the REINFORCE algorithm, agent trains policies and acts as trained policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "- Reward $R_t(\\tau)$\n",
    "$$R_t(\\tau)=\\sum_{t^{\\\\prime}=t}^T\\gamma^{t^{\\prime}-t}r_{t^\\prime}$$\n",
    "\n",
    "- If $t=0$, the equation above is the reward of a full episode, and an object can be defined as an expectated reward of total episodes.\n",
    "\n",
    "$$J(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^T \\gamma^t r_t\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient\n",
    "\n",
    "- Policy gradient algorithm solves the problem below.\n",
    "$$\\max_{\\theta}J(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "- We perform policy gradient for maximizing the object.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha\\triangledown_\\theta j(\\pi_\\theta)$$\n",
    "\n",
    "- And policy gradient can be defined as follows.\n",
    "$$\\triangledown_{\\theta}J(\\pi_\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t=0}^T R_r(\\tau)\\triangledown_\\theta\\log\\pi_\\theta(a_t|s_t)\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm\n",
    "\n",
    "#### pseudo code\n",
    "\n",
    "- Initialize learning rate $\\alpha$\n",
    "\n",
    "- Initialize the weights $\\theta$ of policy network $\\pi_\\theta$\n",
    "\n",
    "- For episode = 0, ..., MAX_EPSIODE do:\n",
    "\n",
    "    - Get samples $\\tau=(s_0,a_0,r_0), ..., (s_T, a_T, r_T)$\n",
    "\n",
    "    - Set $\\triangledown_\\theta J(\\pi_\\theta)=0$\n",
    "\n",
    "    - For $t=0, ..., T$ do:\n",
    "\n",
    "        - $R_t(\\tau)=\\sum_{t^\\prime=t}^T\\gamma^{t^\\prime-t}r^\\prime_t$\n",
    "\n",
    "        - $\\triangledown_\\theta J(\\pi_\\theta)=\\triangle_\\theta J(\\pi_\\theta)+R_t(\\tau)\\triangledown_\\theta\\log\\pi_\\theta(a_t|s_t)$\n",
    "    \n",
    "    - End for\n",
    "\n",
    "    - $\\theta=\\theta+\\alpha\\triangledown_\\theta J(\\pi_\\theta)$\n",
    "\n",
    "- End for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import abc \n",
    "import collections\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "LOGGER_NAME = 'rltrader'\n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.path.pardir))\n",
    "\n",
    "class ReinforcementLearner:\n",
    "    ''' \n",
    "    Attributes\n",
    "    ----------\n",
    "    - stock_code\n",
    "    - stock_data : stock data\n",
    "    - environment\n",
    "    - agent\n",
    "    - training_data\n",
    "    - value_network\n",
    "    - policy_network\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - init_value_network() : function for creating value network\n",
    "    - init_policy_network() : function for creating policy network\n",
    "    - build_sample() : get samples from environment instances\n",
    "    - get_batch() : create batch training data\n",
    "    - update_network() : training value network and policy network\n",
    "    - fit() : request train value and policy network\n",
    "    - run() : perform reinforcement learning\n",
    "    - save_models() : save value and policy network\n",
    "    '''\n",
    "    \n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    def __init__(self, rl_method='dqn', stock_code=None,\n",
    "                 stock_data=None, training_data=None, \n",
    "                 min_trading_price=100, max_trading_price=10000,\n",
    "                 net='cnn', num_steps=1, lr=0.0005,\n",
    "                 discount_factor=0.9, num_epochs=1000,\n",
    "                 balance=100000, start_epsilon=1,\n",
    "                 value_network=None, policy_network=None,\n",
    "                 output_path='', reuse_models=True, gen_output=True):\n",
    "        ''' \n",
    "        Attributes\n",
    "        ---------\n",
    "        - rl_method : reinforcement learning method 'dqn', 'pg', 'ac', 'a2c', 'a3c', 'ppo', ...\n",
    "        - stock_code\n",
    "        - stock_data\n",
    "        - training_data : preprocessed data\n",
    "        - min_trading_price, max_trading_price\n",
    "        - net : network. 'dnn', 'lstm', 'cnn', 'alex'\n",
    "        - n_steps : LTSM, CNN sequence length\n",
    "        - lr : learning rate\n",
    "        - discount_rate\n",
    "        - num_epochs : number of training epochs\n",
    "        - balance : initial balance\n",
    "        - start_epsilon\n",
    "        - value_network, policy_network\n",
    "        - output_path \n",
    "        - reuse_models\n",
    "        '''\n",
    "        \n",
    "        # check arguments\n",
    "        assert min_trading_price > 0\n",
    "        assert max_trading_price > 0\n",
    "        assert max_trading_price >= min_trading_price\n",
    "        assert num_epochs > 0\n",
    "        assert lr > 0\n",
    "        \n",
    "        # set reinforcement learning\n",
    "        self.rl_method = rl_method\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_epochs = num_epochs\n",
    "        self.start_epsilon = start_epsilon\n",
    "        \n",
    "        # set environment\n",
    "        self.stock_code = stock_code\n",
    "        self.stock_data = stock_data\n",
    "        self.environment = Environment(stock_data)\n",
    "        \n",
    "        # set agent\n",
    "        self.agent = Agent(self.environment, balance, min_trading_price, max_trading_price)\n",
    "        \n",
    "        # training data\n",
    "        self.training_data = training_data\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # vector size = training vector size + agent state size\n",
    "        self.num_features = self.agent.STATE_DIM\n",
    "        if self.training_data is not None:\n",
    "            self.num_features += self.training_data.shape[1]\n",
    "        \n",
    "        # set network\n",
    "        self.net = net\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "        self.reuse_models = reuse_models\n",
    "        \n",
    "        # visualization module\n",
    "        self.visualizer = Visualizer()\n",
    "        \n",
    "        # memeory\n",
    "        self.memory_sample = []     # training data sample\n",
    "        self.memory_action = []     # actions taken\n",
    "        self.memory_reward = []     # reward obtained\n",
    "        self.memory_value = []      # prediction value for action\n",
    "        self.memory_policy = []     # prediction probability for action\n",
    "        self.memory_pv = []         # portfolio value\n",
    "        self.memory_num_stocks = [] # number of stocks\n",
    "        self.memory_exp_idx = []    # exploration index\n",
    "        \n",
    "        # exploration epoch info\n",
    "        self.loss = 0               # loss during epoch\n",
    "        self.itr_cnt = 0            # number of iterations with profit\n",
    "        self.exploration_cnt = 0    # count of exploration\n",
    "        self.batch_size = 0         # number of training\n",
    "        \n",
    "        # log output\n",
    "        self.output_path = output_path\n",
    "        self.gen_output = gen_output\n",
    "        \n",
    "    def init_value_network(self, shared_network=None, activation='linear', loss='mse'):\n",
    "        if self.net == 'dnn':\n",
    "            self.value_network = DNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, \n",
    "                # num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'cnn':\n",
    "            self.value_network = CNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'lstm':\n",
    "            self.value_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'alex':\n",
    "            self.value_network = AlexNet(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        if self.reuse_models and os.path.exists(self.value_network_path):\n",
    "            self.value_network.load_model(model_path=self.value_network_path)\n",
    "            \n",
    "    def init_policy_network(self, shared_network=None, activation='sigmoid', loss='binary_crossentropy'):\n",
    "        if self.net == 'dnn':\n",
    "            self.policy_network = DNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, \n",
    "                # num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "        \n",
    "        elif self.net == 'lstm':\n",
    "            self.policy_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'cnn':\n",
    "            self.policy_network = CNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "        elif self.net == 'alex':\n",
    "            self.policy_network = AlexNet(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        if self.reuse_models and os.path.exists(self.policy_network_path):\n",
    "            self.policy_network.load_model(model_path=self.policy_network_path)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # reset environment\n",
    "        self.environment.reset()\n",
    "        \n",
    "        # reset agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # reset visualizer\n",
    "        self.visualizer.clear([0, len(self.stock_data)])\n",
    "        \n",
    "        # reset memories\n",
    "        self.memory_sample = []\n",
    "        self.memory_action = []\n",
    "        self.memory_reward = []\n",
    "        self.memory_value = []\n",
    "        self.memory_policy = []\n",
    "        self.memory_pv = []\n",
    "        self.memory_num_stocks = []\n",
    "        self.memory_exp_idx = []\n",
    "        \n",
    "        # reset epoch info\n",
    "        self.loss = 0.\n",
    "        self.itr_cnt = 0\n",
    "        self.exploration_cnt = 0\n",
    "        self.batch_size = 0\n",
    "        \n",
    "    def build_sample(self):\n",
    "        # get next index data\n",
    "        self.environment.step()\n",
    "        # 47 samples + 3 agnet states = 50 features\n",
    "        if len(self.training_data) > self.training_data_idx + 1:\n",
    "            self.training_data_idx += 1\n",
    "            self.sample = self.training_data[self.training_data_idx].tolist()\n",
    "            self.sample.extend(self.agent.get_states())\n",
    "            return self.sample\n",
    "        return None\n",
    "    \n",
    "    # abstract method\n",
    "    @abc.abstractmethod\n",
    "    def get_batch(self):\n",
    "        pass\n",
    "    \n",
    "    # after generate batch data, call train_on_batch() medho to train value network and policy network\n",
    "    # value network : DQNLearner, ActorCriticLearner, A2CLearner\n",
    "    # policy network : PolicyGradientLearner, ActorCriticLearner, A2CLearner\n",
    "    # loss value after training is saves as instance. in case of training value and policy network return sum of both loss\n",
    "    def fit(self):\n",
    "        # generate batch data\n",
    "        x, y_value, y_policy = self.get_batch()\n",
    "        # initialize loss\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # update value network\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            if y_policy is not None:\n",
    "                # update policy network\n",
    "                loss += self.policy_network.train_on_batch(x, y_policy)\n",
    "            self.loss = loss\n",
    "            \n",
    "    # visualize one complete epoch\n",
    "    # in case of LSTM, CNN agent, the number of agent's actions, num_stocks, output of value network, output of policy network and portfolio value is less than daily price data by (num_steps -1). So we fill (num_steps -1) meaningless data \n",
    "    def visualize(self, epoch_str, num_epochs, epsilon):\n",
    "        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action\n",
    "        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks\n",
    "        if self.value_network is not None:\n",
    "            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] * (self.num_steps - 1) + self.memory_value\n",
    "        if self.policy_network is not None:\n",
    "            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] * (self.num_steps - 1) + self.memory_policy\n",
    "        \n",
    "        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv\n",
    "        self.visualizer.plot(\n",
    "            epoch_str=epoch_str, num_epochs=num_epochs,\n",
    "            epsilon=epsilon, action_list=Agent.ACTIONS,\n",
    "            actions=self.memory_action,\n",
    "            num_stocks=self.memory_num_stocks,\n",
    "            outvals_value=self.memory_value,\n",
    "            outvals_policy=self.memory_policy,\n",
    "            exps=self.memory_exp_idx,\n",
    "            initial_balance=self.agent.initial_balance,\n",
    "            pvs=self.memory_pv,\n",
    "        )\n",
    "        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))\n",
    "                             \n",
    "    def run(self, learning=True):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - learning : boolean if learning will be done or not\n",
    "            - True : after training, build value and policy network\n",
    "            - False: simulation with pretrined model\n",
    "        '''\n",
    "        info = (\n",
    "            f'[{self.stock_code}] RL:{self.rl_method} NET:{self.net}'\n",
    "            f' LR:{self.lr} DF:{self.discount_factor}'\n",
    "        )\n",
    "        with self.lock:\n",
    "            logger.debug(info)\n",
    "        \n",
    "        # start time\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # prepare visualization\n",
    "        self.visualizer.prepare(self.environment.stock_data, info)\n",
    "        \n",
    "        # prepare folders foe saving results\n",
    "        if self.gen_output:\n",
    "            self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}')\n",
    "            if not os.path.isdir(self.epoch_summary_dir):\n",
    "                os.makedirs(self.epoch_summary_dir)\n",
    "            else:\n",
    "                for f in os.listdir(self.epoch_summary_dir):\n",
    "                    os.remove(os.path.join(self.epoch_summary_dir, f))\n",
    "                    \n",
    "        # reset info about training\n",
    "        # save the most highest portfolio value at max_portfolio_value variable\n",
    "        max_portfolio_value = 0\n",
    "        # save the count of epochs with profit\n",
    "        epoch_win_cnt = 0\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # start time of an epoch\n",
    "            time_start_epoch = time.time()\n",
    "            \n",
    "            # queue for making step samples\n",
    "            q_sample = collections.deque(maxlen=self.num_steps)\n",
    "            \n",
    "            # reset environment, networks, visualizer and memories\n",
    "            self.reset()\n",
    "            \n",
    "            # decaying exploration rate\n",
    "            if learning:\n",
    "                epsilon = self.start_epsilon * (1 - (epoch / (self.num_epochs - 1)))\n",
    "            else:\n",
    "                epsilon = self.start_epsilon\n",
    "                \n",
    "            for i in tqdm(range(len(self.training_data)), leave=False):\n",
    "                # create samples\n",
    "                next_sample = self.build_sample()\n",
    "                if next_sample is None:\n",
    "                    break\n",
    "                \n",
    "                # save samples until its size becomes as num_steps\n",
    "                q_sample.append(next_sample)\n",
    "                if len(q_sample) < self.num_steps:\n",
    "                    continue\n",
    "                \n",
    "                # prediction of value and policyn entwork\n",
    "                pred_value = None\n",
    "                pred_policy = None\n",
    "                # get predicted value of actions\n",
    "                if self.value_network is not None:\n",
    "                    pred_value = self.value_network.predict(list(q_sample))\n",
    "                # get predicted probabilities of actions\n",
    "                if self.policy_network is not None:\n",
    "                    pred_policy = self.policy_network.predict(list(q_sample))\n",
    "                    \n",
    "                # make decisions based on predicted value and probabilities\n",
    "                # decide actions based on based on networks or exploration\n",
    "                # decide actions randomly with epsilon probability or according to network output with (1 - epsilon)\n",
    "                # policy network output is the probabilities that selling or buying increase portfolio value. if output for buying is larger than that for selling, then buy the stock. Otherwise, sell it.\n",
    "                # if there is no output of policy network, select the action with the hightes output of value network.\n",
    "                action, confidence, exploration = self.agent.decide_action(pred_value, pred_policy, epsilon)\n",
    "                \n",
    "                # get rewards from action\n",
    "                reward = self.agent.act(action, confidence)\n",
    "                \n",
    "                # save action and the results in the memory\n",
    "                self.memory_sample.append(list(q_sample))\n",
    "                self.memory_action.append(action)\n",
    "                self.memory_reward.append(reward)\n",
    "                if self.value_network is not None:\n",
    "                    self.memory_value.append(pred_value)\n",
    "                if self.policy_network is not None:\n",
    "                    self.memory_policy.append(pred_policy)\n",
    "                self.memory_pv.append(self.agent.portfolio_value)\n",
    "                self.memory_num_stocks.append(self.agent.num_stocks)\n",
    "                if exploration:\n",
    "                    self.memory_exp_idx.append(self.training_data_idx)\n",
    "                    \n",
    "                # update iteration info\n",
    "                self.batch_size += 1\n",
    "                self.itr_cnt += 1\n",
    "                self.exploration_cnt +=1 if exploration else 0\n",
    "                \n",
    "            # training network after completing an epoch\n",
    "            if learning:\n",
    "                self.fit()\n",
    "            \n",
    "            # log about an epoch info\n",
    "            # check the length of epoch number string\n",
    "            num_epochs_digit = len(str(self.num_epochs))\n",
    "            # fill '0' as same size as the length of number of epochs\n",
    "            epoch_str = str(epoch + 1).rjust(num_epochs_digit, '0')\n",
    "            time_end_epoch = time.time()\n",
    "            # save time of an epoch\n",
    "            elapsed_time_epoch = time_end_epoch - time_start_epoch\n",
    "            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}]'\n",
    "                         f'Epsilon:{epsilon:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '\n",
    "                         f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '\n",
    "                         f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '\n",
    "                         f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')\n",
    "            \n",
    "            # visualize epoch information\n",
    "            if self.gen_output:\n",
    "                if self.num_epochs == 1 or (epoch + 1) % max(int(self.num_epochs / 100), 1) == 0:\n",
    "                    self.visualize(epoch_str, self.num_epochs, epsilon)\n",
    "            \n",
    "            # update training info\n",
    "            max_portfolio_value = max(\n",
    "                max_portfolio_value, self.agent.portfolio_value\n",
    "            )\n",
    "            if self.agent.portfolio_value > self.agent.initial_balance:\n",
    "                epoch_win_cnt += 1\n",
    "                \n",
    "        # end time\n",
    "        time_end = time.time()\n",
    "        elapsed_time = time_end - time_start\n",
    "        \n",
    "        # log about training\n",
    "        with self.lock:\n",
    "            logger.debug(f'[{self.stock_code} Elapsed Time:{elapsed_time:.4f}]'\n",
    "                         f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')\n",
    "        \n",
    "    def save_models(self):\n",
    "        if self.value_network is not None and self.value_network_path is not None:\n",
    "            self.value_network.save_model(self.value_network_path)\n",
    "        if self.policy_network is not None and self.policy_network_path is not None:\n",
    "            self.policy_network.save_model(self.policy_network_path)\n",
    "            \n",
    "    # wihtou training, just predict actions based on samples\n",
    "    def predict(self):\n",
    "        # initiate an agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # queue for step samples\n",
    "        q_sample = collections.deque(maxlen=self.num_steps)\n",
    "        \n",
    "        result = []\n",
    "        while True:\n",
    "            # create samples\n",
    "            next_sample = self.build_sample()\n",
    "            if next_sample is None:\n",
    "                break\n",
    "            \n",
    "            # save samples as many as num_steps\n",
    "            q_sample.append(next_sample)\n",
    "            if len(q_sample) < self.num_steps:\n",
    "                continue\n",
    "            \n",
    "            # prediction based on value and policy network\n",
    "            pred_value = None\n",
    "            pred_policy = None\n",
    "            if self.value_network is not None:\n",
    "                pred_value = self.value_network.predict(list(q_sample)).tolist()\n",
    "            if self.policy_network is not None:\n",
    "                pred_policy = self.policy_network.predict(list(q_sample)).tolist()\n",
    "                \n",
    "            # decide action based on the network\n",
    "            result.append((self.environment.step[0]. pred_value, pred_policy))\n",
    "            \n",
    "        if self.gen_output:\n",
    "            with open(os.path.join(self.output_path, f'pred_{self.stock_code}.json'), 'w') as f:\n",
    "                print(json.dumps(result), file=f)\n",
    "                \n",
    "        return result\n",
    "             \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, value_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.value_network_path = value_network_path\n",
    "        # create value network\n",
    "        self.init_value_network()\n",
    "    \n",
    "    # abstract method\n",
    "    def get_batch(self):\n",
    "        # reverse memory array\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        \n",
    "        # prepare sample array 'x' and label array 'y_value' with 0 value\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        \n",
    "        # we can handle from the last data becase of reversed memory\n",
    "        for i, (sample, action, value, reward) in enumerate(memory):\n",
    "            # sample\n",
    "            x[i] = sample\n",
    "            # # reward for training\n",
    "            ## memory_reward[-1] : last profit/loss in the batch data\n",
    "            ## reward : profit/loss at the time of action\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # value network output\n",
    "            y_value[i] = value\n",
    "            # state-action value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next\n",
    "            # save the maximum next state value\n",
    "            value_max_next = value.max()\n",
    "            \n",
    "        # return sample array, value network label, policy network label\n",
    "        # DQN has no policy network, return None\n",
    "        return x, y_value, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, policy_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.policy_network_path = policy_network_path\n",
    "        self.init_policy_network()\n",
    "        \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        # sample array composed of training data and agent states\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        # label array for training policy network\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        for i, (sample, action, policy, reward) in enumerate(memory):\n",
    "            # feature vector\n",
    "            x[i] = sample\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # policy network output\n",
    "            y_policy[i, :] = policy\n",
    "            # make it label with sigmoid to the reward\n",
    "            y_policy[i, action] = sigmoid(r)\n",
    "        \n",
    "        # there is no value network in PG\n",
    "        return x, None, y_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, shared_network=None,\n",
    "                 value_network_path=None, policy_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # shared layer of policy and value network\n",
    "        if shared_network is None:\n",
    "            self.shared_network = Network.get_shared_network(\n",
    "                net=self.net, num_steps=self.num_steps,\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS\n",
    "            )\n",
    "        else:\n",
    "            self.shared_network = shared_network\n",
    "        self.value_network_path = value_network_path\n",
    "        self.policy_network_path = policy_network_path\n",
    "        if self.value_network is None:\n",
    "            self.init_value_network(shared_network=self.shared_network)\n",
    "        if self.policy_network is None:\n",
    "            self.init_policy_network(shared_network=self.shared_network)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # value network label\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next\n",
    "            y_policy[i, :] = policy\n",
    "            # policy network label\n",
    "            y_policy[i, action] = sigmoid(r)\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CLearner(ActorCriticLearner):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward)\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        reward_next = self.memory_reward[-1]\n",
    "        \n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = reward_next + self.memory_reward[-1] - reward * 2\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = np.tanh(r + self.discount_factor * value_max_next)\n",
    "            advantage = y_value[i, action] - y_value[i].mean()\n",
    "            y_policy[i, :] = policy\n",
    "            y_policy[i, action] = sigmoid(advantage)\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, list_stock_code=None,\n",
    "                 list_stock_data=None, list_training_data=None,\n",
    "                 list_min_trading_price=None, list_max_trading_price=None,\n",
    "                 value_network_path=None, policy_network_path=None,\n",
    "                 **kwargs):\n",
    "        assert len(list_training_data) > 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_features += list_training_data[0].shape[1]\n",
    "        \n",
    "        ''' \n",
    "        create A2C learner instances as many as list size\n",
    "        each A2C learner shares value and policy network\n",
    "        '''\n",
    "        \n",
    "        # create shared network\n",
    "        self.shared_network = Network.get_shared_network(\n",
    "            net=self.net, num_steps=self.num_steps,\n",
    "            input_dim=self.num_features,\n",
    "            output_dim=self.agent.NUM_ACTIONS\n",
    "        )\n",
    "        self.value_network_path = value_network_path\n",
    "        self.policy_network_path = policy_network_path\n",
    "        if self.value_network is None:\n",
    "            self.init_value_network(shared_network=self.shared_network)\n",
    "        if self.policy_network is None:\n",
    "            self.init_policy_network(shared_network=self.shared_network)\n",
    "            \n",
    "        # create A2C instances\n",
    "        self.learners = []\n",
    "        for (stock_code, stock_data, training_data,\n",
    "             min_trading_price, max_trading_price) in zip(\n",
    "                 list_stock_code, list_stock_data, list_training_data,\n",
    "                 list_min_trading_price, list_max_trading_price\n",
    "             ):\n",
    "                 learner = A2CLearner(*args, stock_code=stock_code,\n",
    "                                      stock_data=stock_data,\n",
    "                                      training_data=training_data,\n",
    "                                      min_trading_price=min_trading_price,\n",
    "                                      max_trading_price=max_trading_price,\n",
    "                                      shared_network=self.shared_network,\n",
    "                                      value_network=self.value_network,\n",
    "                                      policy_network=self.policy_network, **kwargs)\n",
    "                 self.learners.append(learner)\n",
    "    \n",
    "    def run(self, learning=True):\n",
    "        threads = []\n",
    "        # execute run() method of each A2CLearne instances simultaneiously\n",
    "        for learner in self.learners:\n",
    "            threads.append(threading.Thread(\n",
    "                target=learner.run, daemon=True, kwargs={'learning': learning}\n",
    "            ))\n",
    "        \n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "            \n",
    "    def predict(self):\n",
    "        threads = []\n",
    "        for learner in self.learners:\n",
    "            threads.append(threading.Thread(\n",
    "                target=learner.predict, daemon=True\n",
    "            ))\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOLearner(A2CLearner):\n",
    "    def __init__(self, *args, lmb=0.95, eps=0.1, K=3, **kwargs):\n",
    "        # kwargs['value_network_activation'] = 'tanh'\n",
    "        # kwargs['policy_network_activation'] = 'tanh'\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lmb = lmb\n",
    "        self.eps = eps\n",
    "        self.K = K\n",
    "        \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        reward_next = self.memory_reward[-1]\n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = np.tanh(reward + self.discount_factor * value_max_next)\n",
    "            advantage = y_value[i, action] - y_value[i].mean()\n",
    "            y_policy[i, :] = policy\n",
    "            y_policy[i, action] = advantage\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy\n",
    "    \n",
    "    def fit(self):\n",
    "        x, y_value, y_policy = self.get_batch()\n",
    "        # initialize loss\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # update value network\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            if y_policy is not None:\n",
    "                # update policy network\n",
    "                loss += self.policy_network.train_on_batch_for_ppo(x, y_policy, list(reversed(self.memory_action)), self.eps, self.K)\n",
    "            self.loss = loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import logging\n",
    "# import argparse\n",
    "# import json\n",
    "\n",
    "# os.environ['RLTRADER_BASE'] = 'C:\\project\\github\\projects\\trader'\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     mode = 'train'      # 'train', 'test', 'update', 'predict'\n",
    "#     name = 'Apple'\n",
    "#     rl_method = 'dqn'\n",
    "#     net = 'cnn'\n",
    "#     start_date = '2018-01-01'\n",
    "#     end_date = '2022-12-31'\n",
    "#     lr = 0.01\n",
    "#     discount_factor = 0.7\n",
    "#     initial_balance = 1000000\n",
    "#     stock_codes = ['AAPL']\n",
    "    \n",
    "#     # learner's parameter\n",
    "#     output_name = f'{mode}_{name}_{rl_method}_{net}'\n",
    "#     # reinforcement learning True or False\n",
    "#     learning = mode in ['train', 'update']\n",
    "#     # use model flag\n",
    "#     reuse_models = mode in ['test', 'update', 'predict']\n",
    "#     value_network_name = f'{name}_{rl_method}_{net}_value.mdl'\n",
    "#     policy_network_name = f'{name}_{rl_method}_{net}_policy.mdl'\n",
    "#     start_epsilon = 1 if mode in ['train', 'update'] else 0\n",
    "#     num_epochs = 1000 if mode in ['train', 'update'] else 0\n",
    "#     num_steps = 5 if net in ['lstm', 'cnn'] else 1\n",
    "    \n",
    "#     # output path\n",
    "#     output_path = os.path.join(BASE_DIR, 'output', output_name)\n",
    "#     if not os.path.isdir(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "    \n",
    "#     # model path\n",
    "#     value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "#     policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name)\n",
    "    \n",
    "#     # setting for logging\n",
    "#     # log level DEBUG < INFO < WARNING < CRITICAL. more than DEBUG\n",
    "#     log_path = os.path.join(output_path, f'{output_name}.log')\n",
    "#     if os.path.exists(log_path):\n",
    "#         os.remove(log_path)\n",
    "#     logging.basicConfig(format='%(message)s')\n",
    "#     logger = logging.getLogger(LOGGER_NAME)\n",
    "#     logger.setLevel(logging.DEBUG)\n",
    "#     logger.propagate = False\n",
    "#     stream_handler = logging.StreamHandler(sys.stdout)\n",
    "#     stream_handler.setLevel(logging.INFO)\n",
    "#     file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "#     file_handler.setLevel(logging.DEBUG)\n",
    "#     logger.addHandler(stream_handler)\n",
    "#     logger.addHandler(file_handler)\n",
    "    \n",
    "#     common_params = {}\n",
    "#     list_stock_code = []\n",
    "#     list_training_data = []\n",
    "#     list_min_trading_price = []\n",
    "#     list_max_trading_price = []\n",
    "    \n",
    "#     for stock_code in stock_codes:\n",
    "#         _, stock_data, training_data = load_data(\n",
    "#             stock_code, start_date, end_date\n",
    "#         )\n",
    "        \n",
    "#         assert len(stock_data) >= num_steps\n",
    "        \n",
    "#         # minimum and maximum trading price policy\n",
    "#         min_trading_price = 1\n",
    "#         max_trading_price = 10000\n",
    "        \n",
    "#         # common parameters\n",
    "#         common_params = {\n",
    "#             'rl_method': rl_method,\n",
    "#             'net': net, 'num_steps': num_steps,  'lr': lr,\n",
    "#             'balance': initial_balance, 'num_epochs': num_epochs,\n",
    "#             'discount_factor': discount_factor, 'start_epsilon': start_epsilon,\n",
    "#             'output_path': output_path, 'reuse_models': reuse_models\n",
    "#         }\n",
    "        \n",
    "#         # start reinforcement learning\n",
    "#         learner = None\n",
    "#         common_params.update({\n",
    "#             'stock_code': stock_code,\n",
    "#             'stock_data': stock_data,\n",
    "#             'training_data': training_data,\n",
    "#             'min_trading_price': min_trading_price,\n",
    "#             'max_trading_price': max_trading_price\n",
    "#         })\n",
    "        \n",
    "#         learner = DQNLearner(**{**common_params, 'value_network_path': value_network_path})\n",
    "        \n",
    "#         # check learner is not None\n",
    "#         assert learner is not None\n",
    "        \n",
    "#         if mode in ['train', 'test', 'update']:\n",
    "#             learner.run(learning=learning)\n",
    "            \n",
    "#             if mode in ['train', 'update']:\n",
    "#                 learner.save_models()\n",
    "        \n",
    "#         elif mode == 'predict':\n",
    "#             learner.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import logging\n",
    "# import argparse\n",
    "# import json\n",
    "\n",
    "# def run_trader(mode='train', stock_name='Apple', rl_method='dqn', net='cnn',\n",
    "#                start_date='2020-01-01', end_date='2023-12-31',\n",
    "#                lr=0.01, discount_factor=0.9,\n",
    "#                initial_balance=1000000, stock_code='AAPL',\n",
    "#                min_trading_price=1, max_trading_price=10000, num_epochs=1000):\n",
    "    \n",
    "#     os.environ['RLTRADER_BASE'] = 'C:\\project\\github\\projects\\trader'\n",
    "#     ''' \n",
    "#     Arguments\n",
    "#     --------\n",
    "#     mode : 'train', 'test', 'update', 'predict'\n",
    "#     rl_method : 'dqn', 'a2c', 'a3c', 'pg', 'ppo'\n",
    "#     '''\n",
    "\n",
    "#     # learner's parameter\n",
    "#     output_name = f'{mode}_{stock_name}_{rl_method}_{net}'\n",
    "#     # reinforcement learning True or False\n",
    "#     learning = mode in ['train', 'update']\n",
    "#     # use model flag\n",
    "#     reuse_models = mode in ['test', 'update', 'predict']\n",
    "#     value_network_name = f'{stock_name}_{rl_method}_{net}_value.mdl'\n",
    "#     policy_network_name = f'{stock_name}_{rl_method}_{net}_policy.mdl'\n",
    "#     start_epsilon = 1 if mode in ['train', 'update'] else 0\n",
    "#     num_epochs = num_epochs if mode in ['train', 'update'] else 0\n",
    "#     num_steps = 5 if net in ['lstm', 'cnn'] else 1\n",
    "    \n",
    "#     # output path\n",
    "#     output_path = os.path.join(BASE_DIR, 'output', output_name)\n",
    "#     if not os.path.isdir(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "        \n",
    "#     # log parameters\n",
    "#     params = {\n",
    "#         'mode': mode,\n",
    "#         'stock_name': stock_name,\n",
    "#         'rl_method': rl_method,\n",
    "#         'net': net,\n",
    "#         'start_date': start_date,\n",
    "#         'end_date': end_date,\n",
    "#         'lr': lr,\n",
    "#         'discount_factor': discount_factor,\n",
    "#         'initial_balance': initial_balance,\n",
    "#         'stock_code': stock_code,\n",
    "#     }\n",
    "#     # params = json.dumps(vars(args))\n",
    "#     with open(os.path.join(output_path, 'params.json'), 'w') as f:\n",
    "#         f.write(str(params))\n",
    "    \n",
    "#     # model path\n",
    "#     value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "#     policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name)\n",
    "    \n",
    "#     # setting for logging\n",
    "#     # log level DEBUG < INFO < WARNING < CRITICAL. more than DEBUG\n",
    "#     log_path = os.path.join(output_path, f'{output_name}.log')\n",
    "#     if os.path.exists(log_path):\n",
    "#         os.remove(log_path)\n",
    "#     logging.basicConfig(format='%(message)s')\n",
    "#     logger = logging.getLogger(LOGGER_NAME)\n",
    "#     logger.setLevel(logging.DEBUG)\n",
    "#     logger.propagate = False\n",
    "#     stream_handler = logging.StreamHandler(sys.stdout)\n",
    "#     stream_handler.setLevel(logging.INFO)\n",
    "#     file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "#     file_handler.setLevel(logging.DEBUG)\n",
    "#     logger.addHandler(stream_handler)\n",
    "#     logger.addHandler(file_handler)\n",
    "    \n",
    "#     common_params = {}\n",
    "#     # list_stock_code = []\n",
    "#     # list_training_data = []\n",
    "#     # list_min_trading_price = []\n",
    "#     # list_max_trading_price = []\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     _, stock_data, training_data = load_data(\n",
    "#         stock_code, start_date, end_date\n",
    "#     )\n",
    "        \n",
    "#     assert len(stock_data) >= num_steps\n",
    "        \n",
    "#     # minimum and maximum trading price policy\n",
    "#     min_trading_price = min_trading_price\n",
    "#     max_trading_price = max_trading_price\n",
    "        \n",
    "#     # common parameters\n",
    "#     common_params = {\n",
    "#         'rl_method': rl_method,\n",
    "#         'net': net, 'num_steps': num_steps,  'lr': lr,\n",
    "#         'balance': initial_balance, 'num_epochs': num_epochs,\n",
    "#         'discount_factor': discount_factor, 'start_epsilon': start_epsilon,\n",
    "#         'output_path': output_path, 'reuse_models': reuse_models\n",
    "#     }\n",
    "        \n",
    "#     # start reinforcement learning\n",
    "#     learner = None\n",
    "#     common_params.update({\n",
    "#         'stock_code': stock_code,\n",
    "#         'stock_data': stock_data,\n",
    "#         'training_data': training_data,\n",
    "#         'min_trading_price': min_trading_price,\n",
    "#         'max_trading_price': max_trading_price\n",
    "#     })\n",
    "    \n",
    "#     if rl_method == 'dqn':    \n",
    "#         learner = DQNLearner(**{**common_params, 'value_network_path': value_network_path})\n",
    "    \n",
    "#     elif rl_method == 'pg':\n",
    "#         learner = PolicyGradientLearner(**{**common_params, 'policy_network_path': policy_network_path})\n",
    "    \n",
    "#     elif rl_method == 'ac':\n",
    "#         learner = ActorCriticLearner(**{**common_params, 'value_network_path': value_network_path, 'policy_network_path': policy_network_path})\n",
    "    \n",
    "#     elif rl_method == 'a2c':\n",
    "#         learner = A2CLearner(**{**common_params,\n",
    "#                                 'value_network_path': value_network_path,\n",
    "#                                 'policy_network_path': policy_network_path})\n",
    "#     elif rl_method == 'ppo':\n",
    "#         learner = PPOLearner(**{**common_params,\n",
    "#                                 'value_network_path': value_network_path,\n",
    "#                                 'policy_network_path': policy_network_path})\n",
    "        \n",
    "#     # check learner is not None\n",
    "#     assert learner is not None\n",
    "        \n",
    "#     if mode in ['train', 'test', 'update']:\n",
    "#         learner.run(learning=learning)\n",
    "            \n",
    "#         if mode in ['train', 'update']:\n",
    "#             learner.save_models()\n",
    "        \n",
    "#     elif mode == 'predict':\n",
    "#         learner.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_trader(stock_name='TESLA', stock_code='TSLA', net='cnn', rl_method='ppo', num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import logging\n",
    "# import argparse\n",
    "# import json\n",
    "\n",
    "# def run_trader_a3c(mode='train', stock_names=['Apple', 'Tesla', 'Microsoft'], rl_method='dqn', net='cnn',\n",
    "#                start_date='2020-01-01', end_date='2023-12-31',\n",
    "#                lr=0.01, discount_factor=0.9,\n",
    "#                initial_balance=1000000, stock_codes=['AAPL', 'TSLA', 'MSFT'],\n",
    "#                min_trading_price=1, max_trading_price=10000, num_epochs=1000):\n",
    "    \n",
    "#     os.environ['RLTRADER_BASE'] = 'C:\\project\\github\\projects\\trader'\n",
    "#     ''' \n",
    "#     Arguments\n",
    "#     --------\n",
    "#     mode : 'train', 'test', 'update', 'predict'\n",
    "#     rl_method : 'dqn', 'a2c', 'a3c', 'pg', 'ppo'\n",
    "#     '''\n",
    "\n",
    "#     # learner's parameter\n",
    "#     output_name = f'{mode}_{stock_names}_{rl_method}_{net}'\n",
    "#     # reinforcement learning True or False\n",
    "#     learning = mode in ['train', 'update']\n",
    "#     # use model flag\n",
    "#     reuse_models = mode in ['test', 'update', 'predict']\n",
    "#     value_network_name = f'{stock_names}_{rl_method}_{net}_value.mdl'\n",
    "#     policy_network_name = f'{stock_names}_{rl_method}_{net}_policy.mdl'\n",
    "#     start_epsilon = 1 if mode in ['train', 'update'] else 0\n",
    "#     num_epochs = num_epochs if mode in ['train', 'update'] else 0\n",
    "#     num_steps = 5 if net in ['lstm', 'cnn', 'alex'] else 1\n",
    "    \n",
    "#     # output path\n",
    "#     output_path = os.path.join(BASE_DIR, 'output', output_name)\n",
    "#     if not os.path.isdir(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "        \n",
    "#     # log parameters\n",
    "#     params = {\n",
    "#         'mode': mode,\n",
    "#         'stock_names': stock_names,\n",
    "#         'rl_method': rl_method,\n",
    "#         'net': net,\n",
    "#         'start_date': start_date,\n",
    "#         'end_date': end_date,\n",
    "#         'lr': lr,\n",
    "#         'discount_factor': discount_factor,\n",
    "#         'initial_balance': initial_balance,\n",
    "#         'stock_codes': stock_codes,\n",
    "#     }\n",
    "#     # params = json.dumps(vars(args))\n",
    "#     with open(os.path.join(output_path, 'params.json'), 'w') as f:\n",
    "#         f.write(str(params))\n",
    "    \n",
    "#     # model path\n",
    "#     value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "#     policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name)\n",
    "    \n",
    "#     # setting for logging\n",
    "#     # log level DEBUG < INFO < WARNING < CRITICAL. more than DEBUG\n",
    "#     log_path = os.path.join(output_path, f'{output_name}.log')\n",
    "#     if os.path.exists(log_path):\n",
    "#         os.remove(log_path)\n",
    "#     logging.basicConfig(format='%(message)s')\n",
    "#     logger = logging.getLogger(LOGGER_NAME)\n",
    "#     logger.setLevel(logging.DEBUG)\n",
    "#     logger.propagate = False\n",
    "#     stream_handler = logging.StreamHandler(sys.stdout)\n",
    "#     stream_handler.setLevel(logging.INFO)\n",
    "#     file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "#     file_handler.setLevel(logging.DEBUG)\n",
    "#     logger.addHandler(stream_handler)\n",
    "#     logger.addHandler(file_handler)\n",
    "    \n",
    "#     common_params = {}\n",
    "#     list_stock_code = []\n",
    "#     list_stock_data = []\n",
    "#     list_training_data = []\n",
    "#     list_min_trading_price = []\n",
    "#     list_max_trading_price = []\n",
    "    \n",
    "#     for stock_code in stock_codes:\n",
    "#         _, stock_data, training_data = load_data(\n",
    "#             stock_code, start_date, end_date\n",
    "#         )\n",
    "        \n",
    "#         assert len(stock_data) >= num_steps\n",
    "        \n",
    "#         # minimum and maximum trading price policy\n",
    "#         min_trading_price = min_trading_price\n",
    "#         max_trading_price = max_trading_price\n",
    "        \n",
    "#         # common parameters\n",
    "#         common_params = {\n",
    "#             'rl_method': rl_method,\n",
    "#             'net': net, 'num_steps': num_steps,  'lr': lr,\n",
    "#             'balance': initial_balance, 'num_epochs': num_epochs,\n",
    "#             'discount_factor': discount_factor, 'start_epsilon': start_epsilon,\n",
    "#             'output_path': output_path, 'reuse_models': reuse_models\n",
    "#         }\n",
    "                \n",
    "#         # start reinforcement learning\n",
    "#         learner = None\n",
    "#         if rl_method != 'a3c':\n",
    "#             common_params.update({\n",
    "#                 'stock_code': stock_code,\n",
    "#                 'stock_data': stock_data,\n",
    "#                 'training_data': training_data,\n",
    "#                 'min_trading_price': min_trading_price,\n",
    "#                 'max_trading_price': max_trading_price\n",
    "#             })\n",
    "            \n",
    "#             if rl_method == 'dqn':    \n",
    "#                 learner = DQNLearner(**{**common_params, 'value_network_path': value_network_path})\n",
    "            \n",
    "#             elif rl_method == 'pg':\n",
    "#                 learner = PolicyGradientLearner(**{**common_params, 'policy_network_path': policy_network_path})\n",
    "            \n",
    "#             elif rl_method == 'ac':\n",
    "#                 learner = ActorCriticLearner(**{**common_params, 'value_network_path': value_network_path, 'policy_network_path': policy_network_path})\n",
    "            \n",
    "#             elif rl_method == 'a2c':\n",
    "#                 learner = A2CLearner(**{**common_params,\n",
    "#                                         'value_network_path': value_network_path,\n",
    "#                                         'policy_network_path': policy_network_path})\n",
    "#             elif rl_method == 'ppo':\n",
    "#                 learner = PPOLearner(**{**common_params,\n",
    "#                                         'value_network_path': value_network_path,\n",
    "#                                         'policy_network_path': policy_network_path})\n",
    "#         else:\n",
    "#             list_stock_code.append(stock_code)\n",
    "#             list_stock_data.append(stock_data)\n",
    "#             list_training_data.append(training_data)\n",
    "#             list_min_trading_price.append(min_trading_price)\n",
    "#             list_max_trading_price.append(max_trading_price)\n",
    "    \n",
    "#     # in case of A3CLearner, create A2C instances as many as list size\n",
    "#     if rl_method == 'a3c':\n",
    "#         learner = A3CLearner(**{\n",
    "#             **common_params,\n",
    "#             'list_stock_code': list_stock_code,\n",
    "#             'list_stock_data': list_stock_data,\n",
    "#             'list_training_data': list_training_data,\n",
    "#             'list_min_trading_price': list_min_trading_price,\n",
    "#             'list_max_trading_price': list_max_trading_price,\n",
    "#             'value_network_path': value_network_path,\n",
    "#             'policy_network_path': value_network_path\n",
    "#         })\n",
    "        \n",
    "#     # check learner is not None\n",
    "#     assert learner is not None\n",
    "        \n",
    "#     if mode in ['train', 'test', 'update']:\n",
    "#         learner.run(learning=learning)\n",
    "            \n",
    "#         if mode in ['train', 'update']:\n",
    "#             learner.save_models()\n",
    "        \n",
    "#     elif mode == 'predict':  \n",
    "#         learner.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "def run_trader(mode='train', stock_names=['Apple', 'Tesla', 'Microsoft'], rl_method='dqn', net='cnn',\n",
    "               start_date='2020-01-01', end_date='2023-12-31',\n",
    "               lr=0.01, discount_factor=0.9,\n",
    "               initial_balance=1000000, stock_codes=['AAPL', 'TSLA', 'MSFT'],\n",
    "               min_trading_price=1, max_trading_price=10000, num_epochs=1000, num_steps=5):\n",
    "    \n",
    "    os.environ['RLTRADER_BASE'] = 'C:\\project\\github\\projects\\trader'\n",
    "    ''' \n",
    "    Arguments\n",
    "    --------\n",
    "    mode : 'train', 'test', 'update', 'predict'\n",
    "    rl_method : 'dqn', 'a2c', 'a3c', 'pg', 'ppo'\n",
    "    '''\n",
    "\n",
    "    # learner's parameter\n",
    "    output_name = f'{mode}_{stock_names}_{rl_method}_{net}'\n",
    "    # reinforcement learning True or False\n",
    "    learning = mode in ['train', 'update']\n",
    "    # use model flag\n",
    "    reuse_models = mode in ['test', 'update', 'predict']\n",
    "    value_network_name = f'{stock_names}_{rl_method}_{net}_value.mdl'\n",
    "    policy_network_name = f'{stock_names}_{rl_method}_{net}_policy.mdl'\n",
    "    start_epsilon = 1 if mode in ['train', 'update'] else 0\n",
    "    num_epochs = num_epochs if mode in ['train', 'update'] else 0\n",
    "    num_steps = num_steps if net in ['lstm', 'cnn', 'alex'] else 1\n",
    "    \n",
    "    # output path\n",
    "    output_path = os.path.join(BASE_DIR, 'output', output_name)\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    # log parameters\n",
    "    params = {\n",
    "        'mode': mode,\n",
    "        'stock_names': stock_names,\n",
    "        'rl_method': rl_method,\n",
    "        'net': net,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'lr': lr,\n",
    "        'discount_factor': discount_factor,\n",
    "        'initial_balance': initial_balance,\n",
    "        'stock_codes': stock_codes,\n",
    "    }\n",
    "    # params = json.dumps(vars(args))\n",
    "    with open(os.path.join(output_path, 'params.json'), 'w') as f:\n",
    "        f.write(str(params))\n",
    "    \n",
    "    # model path\n",
    "    value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "    policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name)\n",
    "    \n",
    "    # setting for logging\n",
    "    # log level DEBUG < INFO < WARNING < CRITICAL. more than DEBUG\n",
    "    log_path = os.path.join(output_path, f'{output_name}.log')\n",
    "    if os.path.exists(log_path):\n",
    "        os.remove(log_path)\n",
    "    logging.basicConfig(format='%(message)s')\n",
    "    logger = logging.getLogger(LOGGER_NAME)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.propagate = False\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    common_params = {}\n",
    "    list_stock_code = []\n",
    "    list_stock_data = []\n",
    "    list_training_data = []\n",
    "    list_min_trading_price = []\n",
    "    list_max_trading_price = []\n",
    "    \n",
    "    for stock_code in stock_codes:\n",
    "        _, stock_data, training_data = load_data(\n",
    "            stock_code, start_date, end_date\n",
    "        )\n",
    "        \n",
    "        assert len(stock_data) >= num_steps\n",
    "        \n",
    "        # minimum and maximum trading price policy\n",
    "        min_trading_price = min_trading_price\n",
    "        max_trading_price = max_trading_price\n",
    "        \n",
    "        # common parameters\n",
    "        common_params = {\n",
    "            'rl_method': rl_method,\n",
    "            'net': net, 'num_steps': num_steps,  'lr': lr,\n",
    "            'balance': initial_balance, 'num_epochs': num_epochs,\n",
    "            'discount_factor': discount_factor, 'start_epsilon': start_epsilon,\n",
    "            'output_path': output_path, 'reuse_models': reuse_models\n",
    "        }\n",
    "                \n",
    "        # start reinforcement learning\n",
    "        learner = None\n",
    "        if rl_method != 'a3c':\n",
    "            common_params.update({\n",
    "                'stock_code': stock_code,\n",
    "                'stock_data': stock_data,\n",
    "                'training_data': training_data,\n",
    "                'min_trading_price': min_trading_price,\n",
    "                'max_trading_price': max_trading_price\n",
    "            })\n",
    "            \n",
    "            if rl_method == 'dqn':    \n",
    "                learner = DQNLearner(**{**common_params, 'value_network_path': value_network_path})\n",
    "            \n",
    "            elif rl_method == 'pg':\n",
    "                learner = PolicyGradientLearner(**{**common_params, 'policy_network_path': policy_network_path})\n",
    "            \n",
    "            elif rl_method == 'ac':\n",
    "                learner = ActorCriticLearner(**{**common_params, 'value_network_path': value_network_path, 'policy_network_path': policy_network_path})\n",
    "            \n",
    "            elif rl_method == 'a2c':\n",
    "                learner = A2CLearner(**{**common_params,\n",
    "                                        'value_network_path': value_network_path,\n",
    "                                        'policy_network_path': policy_network_path})\n",
    "            elif rl_method == 'ppo':\n",
    "                learner = PPOLearner(**{**common_params,\n",
    "                                        'value_network_path': value_network_path,\n",
    "                                        'policy_network_path': policy_network_path})\n",
    "        else:\n",
    "            list_stock_code.append(stock_code)\n",
    "            list_stock_data.append(stock_data)\n",
    "            list_training_data.append(training_data)\n",
    "            list_min_trading_price.append(min_trading_price)\n",
    "            list_max_trading_price.append(max_trading_price)\n",
    "    \n",
    "    # in case of A3CLearner, create A2C instances as many as list size\n",
    "    if rl_method == 'a3c':\n",
    "        learner = A3CLearner(**{\n",
    "            **common_params,\n",
    "            'list_stock_code': list_stock_code,\n",
    "            'list_stock_data': list_stock_data,\n",
    "            'list_training_data': list_training_data,\n",
    "            'list_min_trading_price': list_min_trading_price,\n",
    "            'list_max_trading_price': list_max_trading_price,\n",
    "            'value_network_path': value_network_path,\n",
    "            'policy_network_path': value_network_path\n",
    "        })\n",
    "        \n",
    "    # check learner is not None\n",
    "    assert learner is not None\n",
    "        \n",
    "    if mode in ['train', 'test', 'update']:\n",
    "        learner.run(learning=learning)\n",
    "            \n",
    "        if mode in ['train', 'update']:\n",
    "            learner.save_models()\n",
    "        \n",
    "    elif mode == 'predict':  \n",
    "        learner.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2020-01-01' AND '2023-12-31' \n",
      "                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'train_on_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_trader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApple\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAAPL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma3c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrl_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdqn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 152\u001b[0m, in \u001b[0;36mrun_trader\u001b[1;34m(mode, stock_names, rl_method, net, start_date, end_date, lr, discount_factor, initial_balance, stock_codes, min_trading_price, max_trading_price, num_epochs, num_steps)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m learner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    155\u001b[0m         learner\u001b[38;5;241m.\u001b[39msave_models()\n",
      "Cell \u001b[1;32mIn[23], line 405\u001b[0m, in \u001b[0;36mReinforcementLearner.run\u001b[1;34m(self, learning)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# training network after completing an epoch\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m learning:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# log about an epoch info\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# check the length of epoch number string\u001b[39;00m\n\u001b[0;32m    409\u001b[0m num_epochs_digit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs))\n",
      "Cell \u001b[1;32mIn[23], line 271\u001b[0m, in \u001b[0;36mReinforcementLearner.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# update value network\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m(x, y_value)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# update policy network\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network\u001b[38;5;241m.\u001b[39mtrain_on_batch(x, y_policy)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'train_on_batch'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_trader(stock_names=['Apple'], stock_codes=['AAPL'], net='a3c', rl_method='dqn', num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_trader(stock_names=['Tesla', 'Apple', 'Miscrosoft'], stock_codes=['TSLA', 'AAPL', 'MSFT'], net='alex', rl_method='a3c', num_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
