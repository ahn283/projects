{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db connection\n",
    "\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import keyring\n",
    "import platform\n",
    "import numpy as np\n",
    "\n",
    "user = 'root'\n",
    "pw = keyring.get_password('macmini_db', user)\n",
    "host = '192.168.219.106' if platform.system() == 'Windows' else '127.0.0.1'\n",
    "port = 3306\n",
    "db = 'stock'\n",
    "\n",
    "\n",
    "# # connect DB\n",
    "# engine = create_engine(f'mysql+pymysql://{self.user}:{self.pw}@{self.host}:{self.port}/{self.db}')\n",
    "\n",
    "# con = pymysql.connect(\n",
    "#     user=user,\n",
    "#     passwd=pw,\n",
    "#     host=host,\n",
    "#     db=db,\n",
    "#     charset='utf8'\n",
    "# )\n",
    "        \n",
    "# mycursor = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data\n",
    "COLUMNS_STOCK_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "COLUMNS_TRAINING_DATA = ['open', 'high', 'low', 'close', 'volume', 'close_ma5', 'volume_ma5', 'close_ma5_ratio', 'volume_ma5_ratio',\n",
    "       'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio',\n",
    "       'low_close_ratio', 'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "       'close_ma10', 'volume_ma10', 'close_ma10_ratio', 'volume_ma10_ratio',\n",
    "       'close_ma20', 'volume_ma20', 'close_ma20_ratio', 'volume_ma20_ratio',\n",
    "       'close_ma60', 'volume_ma60', 'close_ma60_ratio', 'volume_ma60_ratio',\n",
    "       'close_ma120', 'volume_ma120', 'close_ma120_ratio',\n",
    "       'volume_ma120_ratio', 'close_ma240', 'volume_ma240',\n",
    "       'close_ma240_ratio', 'volume_ma240_ratio', 'upper_bb',\n",
    "       'lower_bb', 'bb_pb', 'bb_width', 'macd',\n",
    "       'macd_signal', 'macd_oscillator', 'rs', 'rsi']\n",
    "# COLUMNS_TRAINING_DATA = ['open', 'high', 'low', 'close', 'volume', 'close_ma5', 'volume_ma5', 'close_ma5_ratio', 'volume_ma5_ratio',\n",
    "#        'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio',\n",
    "#        'low_close_ratio', 'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "#        'close_ma10', 'volume_ma10', 'close_ma10_ratio', 'volume_ma10_ratio',\n",
    "#        'close_ma20', 'volume_ma20', 'close_ma20_ratio', 'volume_ma20_ratio',\n",
    "#        'close_ma60', 'volume_ma60', 'close_ma60_ratio', 'volume_ma60_ratio',\n",
    "#        'close_ma120', 'volume_ma120', 'close_ma120_ratio',\n",
    "#        'volume_ma120_ratio', 'close_ma240', 'volume_ma240',\n",
    "#        'close_ma240_ratio', 'volume_ma240_ratio', 'middle_bb', 'upper_bb',\n",
    "#        'lower_bb', 'bb_pb', 'bb_width', 'ema_short', 'ema_long', 'macd',\n",
    "#        'macd_signal', 'macd_oscillator', 'close_change', 'close_up',\n",
    "#        'close_down', 'rs', 'rsi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# get us stock price of a specific ticker\n",
    "def get_prices_from_ticker(ticker, fro=None, to=None):\n",
    "\n",
    "    # connect DB\n",
    "    engine = create_engine(f'mysql+pymysql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    con = pymysql.connect(\n",
    "        user=user,\n",
    "        passwd=pw,\n",
    "        host=host,\n",
    "        db=db,\n",
    "        charset='utf8'\n",
    "    )\n",
    "            \n",
    "    mycursor = con.cursor()\n",
    "    \n",
    "    if fro is not None:\n",
    "        if to is not None:               \n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date BETWEEN '{fro}' AND '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date >= '{fro}'\n",
    "                    \"\"\"\n",
    "    \n",
    "    else:\n",
    "        if to is not None:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date <= '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    \"\"\"\n",
    "            \n",
    "    print(query)\n",
    "    prices = pd.read_sql(query, con=engine)\n",
    "    con.close()\n",
    "    engine.dispose()\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# str format on date, time\n",
    "FORMAT_DATE = '%Y%m%d'\n",
    "FORMAT_DATETIME = '%Y%m%d%H%M%S'\n",
    "\n",
    "def get_today_str():\n",
    "    today = datetime.datetime.combine(\n",
    "        datetime.date.today(), datetime.datetime.min.time()\n",
    "    )\n",
    "    today_str = today.strftime(FORMAT_DATE)\n",
    "    return today_str\n",
    "\n",
    "def get_time_str():\n",
    "    return datetime.datetime.fromtimestamp(\n",
    "        int(time.time())\n",
    "    ).strftime(FORMAT_DATETIME)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    x = max(min(x, 10), -10)\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2010-01-01' AND '2020-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "stock_code = 'AAPL'\n",
    "fro = '2010-01-01'\n",
    "to = '2020-12-31'\n",
    "df = get_prices_from_ticker(stock_code, fro=fro, to=to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_STOCK_RATIO_DATA = [\n",
    "    'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio', 'low_close_ratio',\n",
    "    'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "]\n",
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    # moving average\n",
    "    windows = [5, 10, 20, 60, 120, 240]\n",
    "    for window in windows:\n",
    "        data[f'close_ma{window}'] = data['close'].rolling(window).mean()\n",
    "        data[f'volume_ma{window}'] = data['volume'].rolling(window).mean()\n",
    "        data[f'close_ma{window}_ratio'] = (data['close'] - data[f'close_ma{window}']) / data[f'close_ma{window}']\n",
    "        data[f'volume_ma{window}_ratio'] = (data['volume'] - data[f'volume_ma{window}']) / data[f'volume_ma{window}']\n",
    "        data['open_close_ratio'] = (data['open'].values - data['close'].values) / data['close'].values\n",
    "        data['open_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'open_prev_close_ratio'] = (data['open'][1:].values - data['close'][:-1].values) / data['close'][:-1].values\n",
    "        data['high_close_ratio'] = (data['high'].values - data['close'].values) / data['close'].values\n",
    "        data['low_close_ratio'] = (data['low'].values - data['close'].values) / data['close'].values\n",
    "        data['close_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'close_prev_close_ratio'] = (data['close'][1:].values - data['close'][:-1].values) / data['close'][:-1].values \n",
    "        data['volume_prev_volume_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'volume_prev_volume_ratio'] = (\n",
    "            # if volume is 0, change it into non zero value exploring previous volume continuously\n",
    "            (data['volume'][1:].values - data['volume'][:-1].values) / data['volume'][:-1].replace(to_replace=0, method='ffill').replace(to_replace=0, method='bfill').values\n",
    "        )\n",
    "    \n",
    "    # Bollinger band\n",
    "    data['middle_bb'] = data['close'].rolling(20).mean()\n",
    "    data['upper_bb'] = data['middle_bb'] + 2 * data['close'].rolling(20).std()\n",
    "    data['lower_bb'] = data['middle_bb'] - 2 * data['close'].rolling(20).std()\n",
    "    data['bb_pb'] = (data['close'] - data['lower_bb']) / (data['upper_bb'] - data['lower_bb'])\n",
    "    data['bb_width'] = (data['upper_bb'] - data['lower_bb']) / data['middle_bb']\n",
    "    \n",
    "    # MACD\n",
    "    macd_short, macd_long, macd_signal = 12, 26, 9\n",
    "    data['ema_short'] = data['close'].ewm(macd_short).mean()\n",
    "    data['ema_long'] = data['close'].ewm(macd_long).mean()\n",
    "    data['macd'] = data['ema_short'] - data['ema_long']\n",
    "    data['macd_signal'] = data['macd'].ewm(macd_signal).mean()\n",
    "    data['macd_oscillator'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # RSI\n",
    "    data['close_change'] = data['close'].diff()\n",
    "    # data['close_up'] = np.where(data['close_change'] >=0, df['close_change'], 0)\n",
    "    data['close_up'] = data['close_change'].apply(lambda x: x if x >= 0 else 0)\n",
    "    # data['close_down'] = np.where(data['close_change'] < 0, df['close_change'].abs(), 0)\n",
    "    data['close_down'] = data['close_change'].apply(lambda x: -x if x < 0 else 0)\n",
    "    data['rs'] = data['close_up'].ewm(alpha=1/14, min_periods=14).mean() / data['close_down'].ewm(alpha=1/14, min_periods=14).mean()\n",
    "    data['rsi'] = 100 - (100 / (1 + data['rs']))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock_code, fro, to):\n",
    "    df = get_prices_from_ticker(stock_code, fro, to)\n",
    "    df_adj = preprocess(df).dropna().reset_index(drop=True)\n",
    "    # df_adj.dropna(inplace=True).reset_index(drop=True)\n",
    "    \n",
    "    stock_data = df_adj[COLUMNS_STOCK_DATA]\n",
    "    training_data = df_adj[COLUMNS_TRAINING_DATA]\n",
    "    \n",
    "    return df_adj, stock_data, training_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2010-01-01' AND '2020-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "df_adj, stock_data, training_data = load_data(stock_code, fro, to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# environment\n",
    "\n",
    "class Environment:\n",
    "    ''' \n",
    "    Attribute\n",
    "    ---------\n",
    "    - stock_data : stock price data such as 'open', 'close', 'volume', 'bb', 'rsi', etc.\n",
    "    - state : current state\n",
    "    - idx : current postion of stock data\n",
    "    \n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize idx and state\n",
    "    - step() : move idx into next postion and get a new state\n",
    "    - get_price() : get close price of current state\n",
    "    - get_state() : get current state\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, stock_data=None):\n",
    "        self.PRICE_IDX = 4  # index postion of close price\n",
    "        self.stock_data = stock_data\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        \n",
    "    def step(self):\n",
    "        # if there is no more idx, return None\n",
    "        if len(self.stock_data) > self.idx + 1:\n",
    "            self.idx += 1\n",
    "            self.state = self.stock_data.iloc[self.idx]\n",
    "            return self.state\n",
    "        return None\n",
    "    \n",
    "    def get_price(self):\n",
    "        # return close price\n",
    "        if self.state is not None:\n",
    "            return self.state[self.PRICE_IDX]\n",
    "        return None\n",
    "    \n",
    "    def get_state(self):\n",
    "        # return current state\n",
    "        if self.state is not None:\n",
    "            return self.state\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.441429138183594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Environment(df_adj)\n",
    "a.reset()\n",
    "a.step()\n",
    "a.step()\n",
    "a.get_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                         2010-12-16\n",
       "high                            11.4675\n",
       "low                           11.521786\n",
       "open                          11.432143\n",
       "close                         11.473214\n",
       "volume                         9.713216\n",
       "adj_close                   322030800.0\n",
       "ticker                             AAPL\n",
       "close_ma5                     11.458071\n",
       "volume_ma5                     9.700397\n",
       "close_ma5_ratio                0.001322\n",
       "volume_ma5_ratio               0.001321\n",
       "open_close_ratio               -0.00358\n",
       "open_prev_close_ratio         -0.000812\n",
       "high_close_ratio              -0.000498\n",
       "low_close_ratio                0.004233\n",
       "close_prev_close_ratio         0.002778\n",
       "volume_prev_volume_ratio       0.002778\n",
       "close_ma10                    11.431071\n",
       "volume_ma10                    9.677539\n",
       "close_ma10_ratio               0.003687\n",
       "volume_ma10_ratio              0.003687\n",
       "close_ma20                    11.304143\n",
       "volume_ma20                    9.570082\n",
       "close_ma20_ratio               0.014957\n",
       "volume_ma20_ratio              0.014956\n",
       "close_ma60                    10.969405\n",
       "volume_ma60                    9.286692\n",
       "close_ma60_ratio               0.045929\n",
       "volume_ma60_ratio              0.045928\n",
       "close_ma120                   10.071384\n",
       "volume_ma120                   8.526428\n",
       "close_ma120_ratio              0.139189\n",
       "volume_ma120_ratio             0.139189\n",
       "close_ma240                    9.198582\n",
       "volume_ma240                   7.787514\n",
       "close_ma240_ratio              0.247281\n",
       "volume_ma240_ratio             0.247281\n",
       "middle_bb                     11.304143\n",
       "upper_bb                      11.636183\n",
       "lower_bb                      10.972103\n",
       "bb_pb                          0.754594\n",
       "bb_width                       0.058747\n",
       "ema_short                     11.278047\n",
       "ema_long                      10.968925\n",
       "macd                           0.309122\n",
       "macd_signal                    0.335923\n",
       "macd_oscillator               -0.026801\n",
       "close_change                   0.031785\n",
       "close_up                       0.031785\n",
       "close_down                          0.0\n",
       "rs                             1.487603\n",
       "rsi                           59.800667\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - enviroment : instance of environment\n",
    "    - initial_balance : initial capital balance\n",
    "    - min_trading_price : minimum trading price\n",
    "    - max_trading_price : maximum trading price\n",
    "    - balance : cash balance\n",
    "    - num_stocks : obtained stocks\n",
    "    - portfolio_value : value of portfolios (balance + price * num_stocks)\n",
    "    - num_buy : number of buying\n",
    "    - num_sell : number of selling\n",
    "    - num_hold : number of holding\n",
    "    - ratio_hold : ratio of holding stocks\n",
    "    - profitloss : current profit or loss\n",
    "    - avg_buy_price_ratio : the ratio average price of a stock bought to the current price\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize an agent\n",
    "    - set_balance() : initialize balance\n",
    "    - get_states() : get the state of an agent\n",
    "    - decide_action() : exploration or exploitation behavior according to the policy net\n",
    "    - validate_action() : validate actions\n",
    "    - decide_trading_unit() : decide how many stocks are sold or bought\n",
    "    - act() : act the actions\n",
    "    '''\n",
    "    \n",
    "    # agent state dimensions\n",
    "    ## (ratio_hold, profit-loss ratio, current price to avg_buy_price ratio)\n",
    "    STATE_DIM = 3\n",
    "    \n",
    "    # trading charge and tax\n",
    "    TRADING_CHARGE = 0.00015    # trading charge 0.015%\n",
    "    TRADING_TAX = 0.02          # trading tax = 0.2%\n",
    "    \n",
    "    # action space\n",
    "    ACTION_BUY = 0      # buy\n",
    "    ACTION_SELL = 1     # sell\n",
    "    ACTION_HOLD = 2     # hold\n",
    "    \n",
    "    # get probabilities from neural nets\n",
    "    ACTIONS = [ACTION_BUY, ACTION_SELL, ACTION_HOLD]\n",
    "    NUM_ACTIONS = len(ACTIONS)      # output number from nueral nets\n",
    "    \n",
    "    def __init__(self, environment, initial_balance, min_trading_price, max_trading_price):\n",
    "        # get current price from the environment\n",
    "        self.environment = environment\n",
    "        self.initial_balance = initial_balance\n",
    "        \n",
    "        # minumum and maximum trainding price\n",
    "        self.min_trading_price = min_trading_price\n",
    "        self.max_trading_price = max_trading_price\n",
    "        \n",
    "        # attributes for an agent class\n",
    "        self.balance = initial_balance\n",
    "        self.num_stocks = 0\n",
    "        \n",
    "        # value of portfolio : balance + num_stocks * {current stock price}\n",
    "        self.portfolio_value = 0\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        \n",
    "        # the state of Agent class\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        \n",
    "    def set_initial_balance(self, balance):\n",
    "        self.initial_balance = balance\n",
    "        \n",
    "    def get_states(self):\n",
    "        # return ratio hold, profit/loss ratio and \n",
    "        # ratio_hold = num_stokcs / (portfoilo_value / price) = (num_stocks * price) / portfolio_value\n",
    "        self.ratio_hold = self.num_stocks * self.environment.get_price() / self.portfolio_value\n",
    "        \n",
    "        return (\n",
    "            self.ratio_hold,\n",
    "            self.profitloss,        # profitloss = (portfolio_value / initial_balance) - 1\n",
    "            (self.environment.get_price() / self.avg_buy_price) - 1 if self.avg_buy_price > 0 else 0\n",
    "        )\n",
    "        \n",
    "    def decide_action(self, pred_value, pred_policy, epsilon):\n",
    "        # act randomly with epsilon probability, act according to neural network  with (1 - epsilon) probability\n",
    "        confidence = 0\n",
    "        \n",
    "        # if theres is a pred_policy, follow it, otherwise follow a pred_value\n",
    "        pred = pred_policy\n",
    "        if pred is None:\n",
    "            pred = pred_value\n",
    "            \n",
    "        # there is no prediction from both pred_policy and pred_value, explore!\n",
    "        if pred is None:\n",
    "            epsilon = 1\n",
    "        else:\n",
    "            maxpred = np.max(pred)\n",
    "            # if values for actions are euqal, explore!\n",
    "            if (pred == maxpred).all():\n",
    "                epsilon = 1\n",
    "        \n",
    "            # if the diffrence between buying and selling prediction policy value is less than 0.05, explore!\n",
    "            if pred_policy is not None:\n",
    "                if np.max(pred_policy) - np.min(pred_policy) < 0.05:\n",
    "                    epsilon = 1\n",
    "            # if pred is not None:\n",
    "            #     if np.max(pred) - np.min(pred) < 0.05:\n",
    "            #         epsilon = 1     \n",
    "                    \n",
    "        # decide whether exploration will be done or not\n",
    "        if np.random.rand() < epsilon:\n",
    "            exploration = True\n",
    "            action = np.random.randint(self.NUM_ACTIONS) \n",
    "        else: \n",
    "            exploration = False\n",
    "            action = np.argmax(pred)\n",
    "            \n",
    "        confidence = .5\n",
    "        if pred_policy is not None:\n",
    "            confidence = pred[action]  \n",
    "        elif pred_value is not None:\n",
    "            confidence = sigmoid(pred[action])\n",
    "            \n",
    "        return action, confidence, exploration\n",
    "    \n",
    "    def validate_action(self, action):\n",
    "        # validate if the action is available\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # check if al least one stock can be bought.\n",
    "            if self.balance < self.environment.get_price() * (1 + self.TRADING_CHARGE):\n",
    "                return False\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # check if there is any sotck that can be sold\n",
    "            if self.num_stocks <= 0:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def decide_trading_unit(self, confidence):\n",
    "        # adjust number of stocks for buying and selling according to confidence level\n",
    "        if np.isnan(confidence):\n",
    "            return self.min_trading_price\n",
    "        \n",
    "        # set buying price range between self.min_trading_price + added_trading_price [min_trading_price, max_trading_price]\n",
    "        # in case that confidence > 1 causes the price over max_trading_price, we set min() so that the value cannot have larger value than self.max_trading_price - self.min_trading_price\n",
    "        # in case that confidence < 0, we set max() so that added_trading_price cannot have negative value.\n",
    "        added_trading_price = max(min(\n",
    "            int(confidence * (self.max_trading_price - self.min_trading_price)),\n",
    "            self.max_trading_price - self.min_trading_price\n",
    "        ), 0)\n",
    "        \n",
    "        trading_price = self.min_trading_price + added_trading_price\n",
    "        \n",
    "        return max(int(trading_price / self.environment.get_price()), 1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def act(self, action, confidence):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - action : decided action from decide_action() method based on exploration or exploitation (0 or 1)\n",
    "        - confidence : probabilitu from decide_action() method, the probability from policy network or the softmax probability from value network\n",
    "        '''\n",
    "        \n",
    "        if not self.validate_action(action):\n",
    "            action = Agent.ACTION_HOLD\n",
    "        \n",
    "        # get the price from the environment\n",
    "        curr_price = self.environment.get_price()\n",
    "        \n",
    "        # buy\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # decide how many stocks will be bought\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            balance = (\n",
    "                self.balance - curr_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            )\n",
    "            \n",
    "            # if lacks of balance, buy maximum units within the amount of money available\n",
    "            if balance < 0:\n",
    "                trading_unit = min(\n",
    "                    int(self.balance / (curr_price * (1 + self.TRADING_CHARGE))),\n",
    "                    int(self.max_trading_price / curr_price)\n",
    "                )\n",
    "                \n",
    "            # total amount of money with trading charge\n",
    "            invest_amount = curr_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            if invest_amount > 0:\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks + curr_price * trading_unit) / (self.num_stocks + trading_unit)\n",
    "                self.balance -= invest_amount\n",
    "                self.num_stocks += trading_unit\n",
    "                self.num_buy += 1\n",
    "                \n",
    "        # sell\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # decide how many stocks will be sold\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            \n",
    "            # if lacks of stocks, sell maximum units available\n",
    "            trading_unit = min(trading_unit, self.num_stocks)\n",
    "            \n",
    "            # selling amount\n",
    "            invest_amount = curr_price * (\n",
    "                1 - (self.TRADING_TAX + self.TRADING_CHARGE)\n",
    "            ) * trading_unit\n",
    "            \n",
    "            if invest_amount > 0:\n",
    "                # update average buy price\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks - curr_price * trading_unit) / (self.num_stocks - trading_unit) if self.num_stocks > trading_unit else 0\n",
    "                self.num_stocks -= trading_unit\n",
    "                self.balance += invest_amount\n",
    "                self.num_sell += 1\n",
    "                \n",
    "        # hold\n",
    "        elif action == Agent.ACTION_HOLD:\n",
    "            self.num_hold += 1\n",
    "            \n",
    "        # update portfolio value\n",
    "        self.portfolio_value = self.balance + curr_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        \n",
    "        return self.profitloss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    ''' \n",
    "    Common attributes and methods for neural networks\n",
    "    \n",
    "    Attributes\n",
    "    ---------\n",
    "    - input_dim\n",
    "    - output_dim\n",
    "    - lr : learning rate\n",
    "    - shared_network : head of neural network which is shared with various networks (e.g., A2C)\n",
    "    - activation : activation layer function ('linear', 'sigmoid', 'tanh', 'softmax')\n",
    "    - loss : loss function for networks\n",
    "    - model : final neural network model\n",
    "    \n",
    "    Functions\n",
    "    ---------\n",
    "    - predict() : calculate value or probability of actions\n",
    "    - train_on_batch() : generate batch data for training\n",
    "    - save_model()\n",
    "    - load_model()\n",
    "    - get_share_network() : generate network head according to the networks\n",
    "    '''\n",
    "    \n",
    "    # thread lock for A3C\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    def __init__(self, input_dim=0, output_dim=0, num_steps=1, lr=0.001,\n",
    "                 shared_network=None, activation='sigmoid', loss='mse'):\n",
    "        self.input_dim = input_dim\n",
    "        self.outpu_dim = output_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.shared_network = shared_network\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        \n",
    "        # data shape for various networks\n",
    "        # CNN, LSTMNetwork has 3 dimensional shape, so we set input shape as (num_stpes, input_dim). In DNN, we set input shape as (input_dim, )\n",
    "        inp = None\n",
    "        if self.num_steps > 1:\n",
    "            inp = (self.num_steps, input_dim)\n",
    "        else:\n",
    "            inp = (self.input_dim,)\n",
    "        \n",
    "        # in case that shared_network is used\n",
    "        self.head = None\n",
    "        if self.shared_network is None:\n",
    "            self.head = self.get_network_head(inp, self.outpu_dim)\n",
    "        else:\n",
    "            self.head = self.shared_network\n",
    "            \n",
    "        # neural network model\n",
    "        ## generate network model for head\n",
    "        self.model = torch.nn.Sequential(self.head)\n",
    "        if self.activation == 'linear':\n",
    "            pass\n",
    "        elif self.activation == 'relu':        \n",
    "            self.model.add_module('activation', torch.nn.ReLU())   \n",
    "        elif self.activation == 'leaky_relu':\n",
    "            self.model.add_module('activation', torch.nn.LeakyReLU()) \n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.model.add_module('activation', torch.nn.Sigmoid())\n",
    "        elif self.activation == 'tanh':\n",
    "            self.model.add_module('activation', torch.nn.Tanh())\n",
    "        elif self.activation == 'softmax':\n",
    "            self.model.add_module('activation', torch.nn.Softmax(dim=1))\n",
    "        self.model.apply(Network.init_weights)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.NAdam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = None\n",
    "        if loss == 'mse':\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "        elif loss == 'binary_crossentropy':\n",
    "            self.criterion = torch.nn.BCELoss()\n",
    "            \n",
    "    def predict(self, sample):\n",
    "        # return prediction of buy, sell and hold on given sample\n",
    "        # value network returns each actions' values on sample and policy network returns each actions' probabilities on sample\n",
    "        with self.lock:\n",
    "            # transform evaluation mode : deavtivate module used only on traininig such as Drop out\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(sample).float().to(device)\n",
    "                pred = self.model(x).detach().cpu().numpy()\n",
    "                pred = pred.flatten()\n",
    "            return pred\n",
    "        \n",
    "    def train_on_batch(self, x, y):\n",
    "        if self.num_steps > 1:\n",
    "            x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        else:\n",
    "            x = np.array(x).reshape((-1, self.input_dim))\n",
    "        loss = 0\n",
    "        with self.lock:\n",
    "            # transform training mode\n",
    "            self.model.train()\n",
    "            _x = torch.from_numpy(x).float().to(device)\n",
    "            _y = torch.from_numpy(y).float().to(device)\n",
    "            y_pred = self.model(_x)\n",
    "            _loss = self.criterion(y_pred, _y)\n",
    "            self.optimizer.zero_grad()\n",
    "            _loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += _loss.item()\n",
    "        return loss\n",
    "    \n",
    "    def train_on_batch_for_ppo(self, x, y, a, eps, K):\n",
    "        if self.num_steps > 1:\n",
    "            x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        else:\n",
    "            x = np.array(x).reshape((-1, self.input_dim))\n",
    "            \n",
    "        loss = 0.\n",
    "        with self.lock:\n",
    "            self.model.train()\n",
    "            _x = torch.from_numpy(x).float().to(device)\n",
    "            _y = torch.from_numpy(y).float().to(device)\n",
    "            probs = F.softmax(_y, dim=1)\n",
    "            for _ in range(K):\n",
    "                y_pred = self.model(_x)\n",
    "                probs_pred = F.softmax(y_pred, dim=1)\n",
    "                rto = torch.exp(torch.log(probs[:, a]) - torch.log(probs_pred[:, a]))\n",
    "                rto_adv = rto * _y[:, a]\n",
    "                clp_adv = torch.clamp(rto, 1 - eps, 1 + eps) * _y[:, a]\n",
    "                _loss = -torch.min(rto_adv, clp_adv).mean()\n",
    "                _loss.backward()\n",
    "                self.optimizer.step()\n",
    "                loss += _loss.item()\n",
    "        return loss\n",
    "    \n",
    "    @classmethod\n",
    "    def get_shared_network(cls, net='dnn', num_steps=1, input_dim=0, output_dim=0):\n",
    "        if net == 'dnn':\n",
    "            return DNN.get_network_head((input_dim, ), output_dim)\n",
    "        elif net == 'lstm':\n",
    "            return LSTMNetwork.get_network_head((num_steps, input_dim), output_dim)\n",
    "        elif net == 'cnn':\n",
    "            return CNN.get_network_head((num_steps, input_dim), output_dim)\n",
    "        elif net == 'alex':\n",
    "            return AlexNet.get_network_head((num_steps, input_dim), output_dim)\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        # initialize weights as weighted normal distribution\n",
    "        if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=0.01)\n",
    "        elif isinstance(m, torch.nn.LSTM):\n",
    "            for weights in m.all_weights:\n",
    "                for weight in weights:\n",
    "                    torch.nn.init.normal_(weight, std=0.01)\n",
    "                    \n",
    "    def save_model(self, model_path):\n",
    "        if model_path is not None and self.model is not None:\n",
    "            torch.save(self.model, model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        if model_path is not None:\n",
    "            self.model = torch.load(model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Network):\n",
    "    # @staticmethod\n",
    "    # def get_network_head(inp, outpu_dim):\n",
    "    #     return torch.nn.Sequential(\n",
    "    #         torch.nn.BatchNorm1d(inp[0]),   # input shape = (input_dim, )\n",
    "    #         torch.nn.Linear(inp[0], 256),\n",
    "    #         torch.nn.BatchNorm1d(256),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(256, 128),\n",
    "    #         torch.nn.BatchNorm1d(128),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(128, 64),\n",
    "    #         torch.nn.BatchNorm1d(64),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(64, 32),\n",
    "    #         torch.nn.BatchNorm1d(32),\n",
    "    #         torch.nn.Dropout(p=0.1),\n",
    "    #         torch.nn.Linear(32, outpu_dim),\n",
    "    #     )\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, outpu_dim):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),   # input shape = (input_dim, )\n",
    "            torch.nn.Linear(inp[0], 1024),\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, outpu_dim),\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),\n",
    "            LSTMModule(inp[1], 128, batch_first=True, use_last_only=True),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, output_dim),\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((-1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "    \n",
    "class LSTMModule(torch.nn.LSTM):\n",
    "    def __init__(self, *args, use_last_only=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_last_only = use_last_only\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, (h_n, _) = super().forward(x)\n",
    "        if self.use_last_only:\n",
    "            return h_n[-1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),\n",
    "            torch.nn.Conv1d(inp[0], 1, kernel_size),\n",
    "            torch.nn.BatchNorm1d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(inp[1] - (kernel_size - 1), 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Do print / debug stuff\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2,\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        print(inp)\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(inp, 96, kernel_size=kernel_size, stride=stride),        # (5, 96)\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.Conv2d(96, 256, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.Conv2d(256, 384, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 384, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 256, kernel_size=kernel_size, padding=padding),\n",
    "            PrintLayer(),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            PrintLayer(),\n",
    "            \n",
    "            # classifier\n",
    "            torch.nn.Linear(6, 32),\n",
    "            PrintLayer(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(32, output_dim),\n",
    "            PrintLayer()\n",
    "        )\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "class Visualizer:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - fig : matplotlib Figure instance plays like a canvas\n",
    "    - plot() : print charts except daily price chart\n",
    "    - save() : save Figure as an image file\n",
    "    - clear() : initialze all chart but daily price chart\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    - Figure title : parameter, epsilon\n",
    "    - Axes 1 : daily price chart\n",
    "    - Axes 2 : number of stocks and agent action chart\n",
    "    - Axes 3 : value network chart\n",
    "    - Axes 4 : policy network and epsilon chart\n",
    "    - Axes 5 : Portfolio value and learning point chart\n",
    "    '''\n",
    "    \n",
    "    COLORS = ['r', 'b', 'g']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.canvas = None \n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.title = ''\n",
    "        self.x = []\n",
    "        self.xticks = []\n",
    "        self.xlabels = []\n",
    "        \n",
    "    def prepare(self, stock_data, title):\n",
    "        self.title = title\n",
    "        # shared x-axis among all charts\n",
    "        # self.x = np.arange(stock_data['date'])\n",
    "        # self.x_label = [datetime.strptime(date, '%Y%m%d').date() for date in stock_data['date']]\n",
    "        with lock:\n",
    "            # preare for printing five charts\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                nrows=5, ncols=1, facecolor='w', sharex=True\n",
    "            )\n",
    "            for ax in self.axes:\n",
    "                # deactivate scientific marks\n",
    "                ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # change y-axis to the right\n",
    "                ax.yaxis.tick_right()\n",
    "            \n",
    "            # chart 1. daily price data\n",
    "            self.axes[0].set_ylabel('Env.')\n",
    "            x = np.arange(len(stock_data))\n",
    "            # make two dimensional array with open, high, low and close order\n",
    "            ohlc = np.hstack((\n",
    "                x.reshape(-1, 1), np.array(stock_data)[:, 1:5]\n",
    "            ))\n",
    "            # red for positive, blue for negative\n",
    "            candlestick_ohlc(self.axes[0], ohlc, colorup='r', colordown='b')\n",
    "            # visualize volume\n",
    "            ax = self.axes[0].twinx()\n",
    "            volume = np.array(stock_data)[:, 5].tolist()\n",
    "            ax.bar(x, volume, color='b', alpha=0.3)\n",
    "            # set x-axis\n",
    "            self.x = np.arange(len(stock_data['date']))\n",
    "            self.xticks = stock_data.index[[0, -1]]\n",
    "            self.xlabels = stock_data.iloc[[0, -1]]['date']\n",
    "            \n",
    "            \n",
    "    def plot(self, epoch_str=None, num_epochs=None, epsilon=None,\n",
    "             action_list=None, actions=None, num_stocks=None,\n",
    "             outvals_value=[], outvals_policy=[], exps=None,\n",
    "             initial_balance=None, pvs=None):\n",
    "        ''' \n",
    "        Attributes\n",
    "        ---------\n",
    "        - epoch_str : epoch for Figure title\n",
    "        - num_epochs : number of total epochs\n",
    "        - epsilon : exploration rate\n",
    "        - action_list : total action list of an agent\n",
    "        - num_stocks : number of stocks\n",
    "        - outvals_value : output array of value network\n",
    "        - outvals_policy : ouput array of policy network\n",
    "        - exps : array whether exploration is true or not\n",
    "        - initial_balance \n",
    "        - pvs : array of portfolio value\n",
    "        '''\n",
    "        \n",
    "        with lock:\n",
    "            # action, num_stocks, outvals_value, outvals_policy, pvs has same size\n",
    "            # create an array with same size as actions and use as x-axis\n",
    "            actions = np.array(actions)     # action array of an agent\n",
    "            # turn value network output as an array\n",
    "            outvals_value = np.array(outvals_value)\n",
    "            # turn policy network output as an array\n",
    "            outvals_policy = np.array(outvals_policy)\n",
    "            # turn initial balance as an array\n",
    "            pvs_base = np.zeros(len(actions)) + initial_balance     # array([initial_balance, initial_balance, initial_balance, ...])\n",
    "            \n",
    "            # chart 2. agent states (action, num_stocks)\n",
    "            for action, color in zip(action_list, self.COLORS):\n",
    "                for i in self.x[actions == action]:\n",
    "                    # express actions as background color : red for buying, blue for selling\n",
    "                    self.axes[1].axvline(i, color=color, alpha=0.1)\n",
    "            self.axes[1].plot(self.x, num_stocks, '-k')     # plot number of stocks\n",
    "            \n",
    "            # chart 3. value network (prediction value for action)\n",
    "            if (len(outvals_value)) > 0:\n",
    "                max_actions = np.argmax(outvals_value, axis=1)\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    # plot background\n",
    "                    for idx in self.x:\n",
    "                        if max_actions[idx] == action:\n",
    "                            self.axes[2].axvline(idx, color=color, alpha=0.1)\n",
    "                    # plot value network\n",
    "                    ## red for buying, blue for selling, green for holding\n",
    "                    ## if no prediction for action, no plot green chart\n",
    "                    self.axes[2].plot(self.x, outvals_value[:, action], color=color, linestyle='-')\n",
    "            \n",
    "            # chart 4. policy network\n",
    "            # plot exploration as yellow background\n",
    "            for exp_idx in exps:\n",
    "                self.axes[3].axvline(exp_idx, color='y')\n",
    "            # plot action as background\n",
    "            _outvals = outvals_policy if len(outvals_policy) > 0 else outvals_value\n",
    "            for idx, outval in zip(self.x, _outvals):\n",
    "                color = 'white'\n",
    "                if np.isnan(outval.max()):\n",
    "                    continue\n",
    "                # with no exploration area, red for buying, blue for selling\n",
    "                if outval.argmax() == Agent.ACTION_BUY:\n",
    "                    color = self.COLORS[0]      # red for buying\n",
    "                elif outval.argmax() == Agent.ACTION_SELL:\n",
    "                    color = self.COLORS[1]      # blue for selling\n",
    "                elif outval.argmax() == Agent.ACTION_HOLD:\n",
    "                    color = self.COLORS[2]      # green for holding\n",
    "                self.axes[3].axvline(idx, color=color, alpha=0.1)\n",
    "                \n",
    "            # plot policy network\n",
    "            # red for buying policy network output, blue for selling policy network\n",
    "            # when red line is above blue line, buy stocks, otherwise sell stocks\n",
    "            if len(outvals_policy) > 0:\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    self.axes[3].plot(\n",
    "                        self.x, outvals_policy[:, action],\n",
    "                        color=color, linestyle='-'\n",
    "                    )\n",
    "                    \n",
    "            # chart 5. portfolio value\n",
    "            # horzontal line for initial balance\n",
    "            self.axes[4].axhline(\n",
    "                initial_balance, linestyle='-', color='gray'\n",
    "            )\n",
    "            \n",
    "            self.axes[4].fill_between(\n",
    "                self.x, pvs, pvs_base,\n",
    "                where=pvs > pvs_base, facecolor='r', alpha=0.1\n",
    "            )\n",
    "            self.axes[4].plot(self.x, pvs, '-k')\n",
    "            self.axes[4].xaxis.set_ticks(self.xticks)\n",
    "            self.axes[4].xaxis.set_ticklabels(self.xlabels)\n",
    "            \n",
    "            # epoch and exploration rate\n",
    "            self.fig.suptitle(f'{self.title}\\nEPOCH:{epoch_str}/{num_epochs} EPSILON:{epsilon:.2f}')\n",
    "            # adjust canvas layout\n",
    "            self.fig.tight_layout()\n",
    "            self.fig.subplots_adjust(top=0.85)\n",
    "            \n",
    "    def clear(self, xlim):\n",
    "        with lock:\n",
    "            _axes = self.axes.tolist()\n",
    "            # intial chart except non changeable value\n",
    "            for ax in _axes[1:]:\n",
    "                ax.cla()        # initialize chart\n",
    "                ax.relim()      # initialize limit\n",
    "                ax.autoscale()  # reset scale\n",
    "            \n",
    "            # reset y-axis label\n",
    "            self.axes[1].set_ylabel('Agent')\n",
    "            self.axes[2].set_ylabel('V')\n",
    "            self.axes[3].set_ylabel('P')\n",
    "            self.axes[4].set_ylabel('PV')\n",
    "            for ax in _axes:\n",
    "                ax.set_xlim(xlim)       # reset limit in x-axis\n",
    "                ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # set equal width horizontally\n",
    "                ax.ticklabel_format(useOffset=False)\n",
    "                \n",
    "    def save(self, path):\n",
    "        with lock:\n",
    "            self.fig.savefig(path)\n",
    "            \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Policy $\\pi$ is a function that connectes states to action probabilities. Action probabilities are for getting $a\\sim\\pi(s)$. In the REINFORCE algorithm, agent trains policies and acts as trained policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "- Reward $R_t(\\tau)$\n",
    "$$R_t(\\tau)=\\sum_{t^{\\\\prime}=t}^T\\gamma^{t^{\\prime}-t}r_{t^\\prime}$$\n",
    "\n",
    "- If $t=0$, the equation above is the reward of a full episode, and an object can be defined as an expectated reward of total episodes.\n",
    "\n",
    "$$J(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^T \\gamma^t r_t\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient\n",
    "\n",
    "- Policy gradient algorithm solves the problem below.\n",
    "$$\\max_{\\theta}J(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "- We perform policy gradient for maximizing the object.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha\\triangledown_\\theta j(\\pi_\\theta)$$\n",
    "\n",
    "- And policy gradient can be defined as follows.\n",
    "$$\\triangledown_{\\theta}J(\\pi_\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t=0}^T R_r(\\tau)\\triangledown_\\theta\\log\\pi_\\theta(a_t|s_t)\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm\n",
    "\n",
    "#### pseudo code\n",
    "\n",
    "- Initialize learning rate $\\alpha$\n",
    "\n",
    "- Initialize the weights $\\theta$ of policy network $\\pi_\\theta$\n",
    "\n",
    "- For episode = 0, ..., MAX_EPSIODE do:\n",
    "\n",
    "    - Get samples $\\tau=(s_0,a_0,r_0), ..., (s_T, a_T, r_T)$\n",
    "\n",
    "    - Set $\\triangledown_\\theta J(\\pi_\\theta)=0$\n",
    "\n",
    "    - For $t=0, ..., T$ do:\n",
    "\n",
    "        - $R_t(\\tau)=\\sum_{t^\\prime=t}^T\\gamma^{t^\\prime-t}r^\\prime_t$\n",
    "\n",
    "        - $\\triangledown_\\theta J(\\pi_\\theta)=\\triangle_\\theta J(\\pi_\\theta)+R_t(\\tau)\\triangledown_\\theta\\log\\pi_\\theta(a_t|s_t)$\n",
    "    \n",
    "    - End for\n",
    "\n",
    "    - $\\theta=\\theta+\\alpha\\triangledown_\\theta J(\\pi_\\theta)$\n",
    "\n",
    "- End for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import abc \n",
    "import collections\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "LOGGER_NAME = 'rltrader'\n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.path.pardir))\n",
    "\n",
    "class ReinforcementLearner:\n",
    "    ''' \n",
    "    Attributes\n",
    "    ----------\n",
    "    - stock_code\n",
    "    - stock_data : stock data for plotting chart\n",
    "    - environment\n",
    "    - agent\n",
    "    - training_data : data for training a model\n",
    "    - value_network : value network for a model if needed\n",
    "    - policy_network : policy network for a model if neede\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - init_value_network() : function for creating value network\n",
    "    - init_policy_network() : function for creating policy network\n",
    "    - build_sample() : get samples from environment instances\n",
    "    - get_batch() : create batch training data\n",
    "    - update_network() : training value network and policy network\n",
    "    - fit() : request train value and policy network\n",
    "    - run() : perform reinforcement learning\n",
    "    - save_models() : save value and policy network\n",
    "    '''\n",
    "    \n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    def __init__(self, rl_method='dqn', stock_code=None,\n",
    "                 stock_data=None, training_data=None, \n",
    "                 min_trading_price=100, max_trading_price=10000,\n",
    "                 net='cnn', num_steps=1, lr=0.0005,\n",
    "                 discount_factor=0.9, num_epochs=1000,\n",
    "                 balance=100000, start_epsilon=1,\n",
    "                 value_network=None, policy_network=None,\n",
    "                 output_path='', reuse_models=True, gen_output=True):\n",
    "        ''' \n",
    "        Attributes\n",
    "        ---------\n",
    "        - rl_method : reinforcement learning method 'dqn', 'pg', 'ac', 'a2c', 'a3c', 'ppo', ...\n",
    "        - stock_code\n",
    "        - stock_data\n",
    "        - training_data : preprocessed data\n",
    "        - min_trading_price, max_trading_price\n",
    "        - net : network. 'dnn', 'lstm', 'cnn', 'alex'\n",
    "        - n_steps : LTSM, CNN sequence length\n",
    "        - lr : learning rate\n",
    "        - discount_rate\n",
    "        - num_epochs : number of training epochs\n",
    "        - balance : initial balance\n",
    "        - start_epsilon\n",
    "        - value_network, policy_network\n",
    "        - output_path \n",
    "        - reuse_models\n",
    "        '''\n",
    "        \n",
    "        # check arguments\n",
    "        assert min_trading_price > 0\n",
    "        assert max_trading_price > 0\n",
    "        assert max_trading_price >= min_trading_price\n",
    "        assert num_epochs > 0\n",
    "        assert lr > 0\n",
    "        \n",
    "        # set reinforcement learning\n",
    "        self.rl_method = rl_method\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_epochs = num_epochs\n",
    "        self.start_epsilon = start_epsilon\n",
    "        \n",
    "        # set environment\n",
    "        self.stock_code = stock_code\n",
    "        self.stock_data = stock_data\n",
    "        self.environment = Environment(stock_data)\n",
    "        \n",
    "        # set agent\n",
    "        self.agent = Agent(self.environment, balance, min_trading_price, max_trading_price)\n",
    "        \n",
    "        # training data\n",
    "        self.training_data = training_data\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # vector size = training vector size + agent state size\n",
    "        self.num_features = self.agent.STATE_DIM\n",
    "        if self.training_data is not None:\n",
    "            self.num_features += self.training_data.shape[1]\n",
    "        \n",
    "        # set network\n",
    "        self.net = net\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "        self.reuse_models = reuse_models\n",
    "        \n",
    "        # visualization module\n",
    "        self.visualizer = Visualizer()\n",
    "        \n",
    "        # memeory\n",
    "        self.memory_sample = []     # training data sample\n",
    "        self.memory_action = []     # actions taken\n",
    "        self.memory_reward = []     # reward obtained\n",
    "        self.memory_value = []      # prediction value for action\n",
    "        self.memory_policy = []     # prediction probability for action\n",
    "        self.memory_pv = []         # portfolio value\n",
    "        self.memory_num_stocks = [] # number of stocks\n",
    "        self.memory_exp_idx = []    # exploration index\n",
    "        \n",
    "        # exploration epoch info\n",
    "        self.loss = 0               # loss during epoch\n",
    "        self.itr_cnt = 0            # number of iterations with profit\n",
    "        self.exploration_cnt = 0    # count of exploration\n",
    "        self.batch_size = 0         # number of training\n",
    "        \n",
    "        # log output\n",
    "        self.output_path = output_path\n",
    "        self.gen_output = gen_output\n",
    "        \n",
    "    def init_value_network(self, shared_network=None, activation='linear', loss='mse'):\n",
    "        if self.net == 'dnn':\n",
    "            self.value_network = DNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, \n",
    "                # num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'cnn':\n",
    "            self.value_network = CNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'lstm':\n",
    "            self.value_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'alex':\n",
    "            self.value_network = AlexNet(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        if self.reuse_models and os.path.exists(self.value_network_path):\n",
    "            self.value_network.load_model(model_path=self.value_network_path)\n",
    "            \n",
    "    def init_policy_network(self, shared_network=None, activation='sigmoid', loss='binary_crossentropy'):\n",
    "        if self.net == 'dnn':\n",
    "            self.policy_network = DNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, \n",
    "                # num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "        \n",
    "        elif self.net == 'lstm':\n",
    "            self.policy_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        elif self.net == 'cnn':\n",
    "            self.policy_network = CNN(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "        elif self.net == 'alex':\n",
    "            self.policy_network = AlexNet(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=activation, loss=loss\n",
    "            )\n",
    "            \n",
    "        if self.reuse_models and os.path.exists(self.policy_network_path):\n",
    "            self.policy_network.load_model(model_path=self.policy_network_path)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # reset environment\n",
    "        self.environment.reset()\n",
    "        \n",
    "        # reset agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # reset visualizer\n",
    "        self.visualizer.clear([0, len(self.stock_data)])\n",
    "        \n",
    "        # reset memories\n",
    "        self.memory_sample = []\n",
    "        self.memory_action = []\n",
    "        self.memory_reward = []\n",
    "        self.memory_value = []\n",
    "        self.memory_policy = []\n",
    "        self.memory_pv = []\n",
    "        self.memory_num_stocks = []\n",
    "        self.memory_exp_idx = []\n",
    "        \n",
    "        # reset epoch info\n",
    "        self.loss = 0.\n",
    "        self.itr_cnt = 0\n",
    "        self.exploration_cnt = 0\n",
    "        self.batch_size = 0\n",
    "        \n",
    "    def build_sample(self):\n",
    "        # get next index data\n",
    "        self.environment.step()\n",
    "        # 47 samples + 3 agnet states = 50 features\n",
    "        if len(self.training_data) > self.training_data_idx + 1:\n",
    "            self.training_data_idx += 1\n",
    "            self.sample = self.training_data[self.training_data_idx].tolist()\n",
    "            self.sample.extend(self.agent.get_states())\n",
    "            return self.sample\n",
    "        print(self.sample)\n",
    "        return None\n",
    "    \n",
    "    # abstract method\n",
    "    @abc.abstractmethod\n",
    "    def get_batch(self):\n",
    "        pass\n",
    "    \n",
    "    # after generate batch data, call train_on_batch() medho to train value network and policy network\n",
    "    # value network : DQNLearner, ActorCriticLearner, A2CLearner\n",
    "    # policy network : PolicyGradientLearner, ActorCriticLearner, A2CLearner\n",
    "    # loss value after training is saves as instance. in case of training value and policy network return sum of both loss\n",
    "    def fit(self):\n",
    "        # generate batch data\n",
    "        x, y_value, y_policy = self.get_batch()\n",
    "        # initialize loss\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # update value network\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            if y_policy is not None:\n",
    "                # update policy network\n",
    "                loss += self.policy_network.train_on_batch(x, y_policy)\n",
    "            self.loss = loss\n",
    "            \n",
    "    # visualize one complete epoch\n",
    "    # in case of LSTM, CNN agent, the number of agent's actions, num_stocks, output of value network, output of policy network and portfolio value is less than daily price data by (num_steps -1). So we fill (num_steps -1) meaningless data \n",
    "    def visualize(self, epoch_str, num_epochs, epsilon):\n",
    "        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action\n",
    "        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks\n",
    "        if self.value_network is not None:\n",
    "            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] * (self.num_steps - 1) + self.memory_value\n",
    "        if self.policy_network is not None:\n",
    "            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] * (self.num_steps - 1) + self.memory_policy\n",
    "        \n",
    "        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv\n",
    "        self.visualizer.plot(\n",
    "            epoch_str=epoch_str, num_epochs=num_epochs,\n",
    "            epsilon=epsilon, action_list=Agent.ACTIONS,\n",
    "            actions=self.memory_action,\n",
    "            num_stocks=self.memory_num_stocks,\n",
    "            outvals_value=self.memory_value,\n",
    "            outvals_policy=self.memory_policy,\n",
    "            exps=self.memory_exp_idx,\n",
    "            initial_balance=self.agent.initial_balance,\n",
    "            pvs=self.memory_pv,\n",
    "        )\n",
    "        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))\n",
    "                             \n",
    "    def run(self, learning=True):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - learning : boolean if learning will be done or not\n",
    "            - True : after training, build value and policy network\n",
    "            - False: simulation with pretrined model\n",
    "        '''\n",
    "        info = (\n",
    "            f'[{self.stock_code}] RL:{self.rl_method} NET:{self.net}'\n",
    "            f' LR:{self.lr} DF:{self.discount_factor}'\n",
    "        )\n",
    "        with self.lock:\n",
    "            logger.debug(info)\n",
    "        \n",
    "        # start time\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # prepare visualization\n",
    "        self.visualizer.prepare(self.environment.stock_data, info)\n",
    "        \n",
    "        # prepare folders foe saving results\n",
    "        if self.gen_output:\n",
    "            self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}')\n",
    "            if not os.path.isdir(self.epoch_summary_dir):\n",
    "                os.makedirs(self.epoch_summary_dir)\n",
    "            else:\n",
    "                for f in os.listdir(self.epoch_summary_dir):\n",
    "                    os.remove(os.path.join(self.epoch_summary_dir, f))\n",
    "                    \n",
    "        # reset info about training\n",
    "        # save the most highest portfolio value at max_portfolio_value variable\n",
    "        max_portfolio_value = 0\n",
    "        # save the count of epochs with profit\n",
    "        epoch_win_cnt = 0\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # start time of an epoch\n",
    "            time_start_epoch = time.time()\n",
    "            \n",
    "            # queue for making step samples\n",
    "            q_sample = collections.deque(maxlen=self.num_steps)\n",
    "            \n",
    "            # reset environment, networks, visualizer and memories\n",
    "            self.reset()\n",
    "            \n",
    "            # decaying exploration rate\n",
    "            if learning:\n",
    "                epsilon = self.start_epsilon * (1 - (epoch / (self.num_epochs - 1)))\n",
    "            else:\n",
    "                epsilon = self.start_epsilon\n",
    "                \n",
    "            for i in tqdm(range(len(self.training_data)), leave=False):\n",
    "                # create samples\n",
    "                next_sample = self.build_sample()\n",
    "                if next_sample is None:\n",
    "                    break\n",
    "                \n",
    "                # save samples until its size becomes as num_steps\n",
    "                q_sample.append(next_sample)\n",
    "                if len(q_sample) < self.num_steps:\n",
    "                    continue\n",
    "                \n",
    "                # prediction of value and policyn entwork\n",
    "                pred_value = None\n",
    "                pred_policy = None\n",
    "                # get predicted value of actions\n",
    "                if self.value_network is not None:\n",
    "                    pred_value = self.value_network.predict(list(q_sample))\n",
    "                # get predicted probabilities of actions\n",
    "                if self.policy_network is not None:\n",
    "                    pred_policy = self.policy_network.predict(list(q_sample))\n",
    "                    \n",
    "                # make decisions based on predicted value and probabilities\n",
    "                # decide actions based on based on networks or exploration\n",
    "                # decide actions randomly with epsilon probability or according to network output with (1 - epsilon)\n",
    "                # policy network output is the probabilities that selling or buying increase portfolio value. if output for buying is larger than that for selling, then buy the stock. Otherwise, sell it.\n",
    "                # if there is no output of policy network, select the action with the hightes output of value network.\n",
    "                action, confidence, exploration = self.agent.decide_action(pred_value, pred_policy, epsilon)\n",
    "                \n",
    "                # get rewards from action\n",
    "                reward = self.agent.act(action, confidence)\n",
    "                \n",
    "                # save action and the results in the memory\n",
    "                self.memory_sample.append(list(q_sample))\n",
    "                self.memory_action.append(action)\n",
    "                self.memory_reward.append(reward)\n",
    "                if self.value_network is not None:\n",
    "                    self.memory_value.append(pred_value)\n",
    "                if self.policy_network is not None:\n",
    "                    self.memory_policy.append(pred_policy)\n",
    "                self.memory_pv.append(self.agent.portfolio_value)\n",
    "                self.memory_num_stocks.append(self.agent.num_stocks)\n",
    "                if exploration:\n",
    "                    self.memory_exp_idx.append(self.training_data_idx)\n",
    "                    \n",
    "                # update iteration info\n",
    "                self.batch_size += 1\n",
    "                self.itr_cnt += 1\n",
    "                self.exploration_cnt +=1 if exploration else 0\n",
    "                \n",
    "            # training network after completing an epoch\n",
    "            if learning:\n",
    "                self.fit()\n",
    "            \n",
    "            # log about an epoch info\n",
    "            # check the length of epoch number string\n",
    "            num_epochs_digit = len(str(self.num_epochs))\n",
    "            # fill '0' as same size as the length of number of epochs\n",
    "            epoch_str = str(epoch + 1).rjust(num_epochs_digit, '0')\n",
    "            time_end_epoch = time.time()\n",
    "            # save time of an epoch\n",
    "            elapsed_time_epoch = time_end_epoch - time_start_epoch\n",
    "            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}]'\n",
    "                         f'Epsilon:{epsilon:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '\n",
    "                         f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '\n",
    "                         f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '\n",
    "                         f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')\n",
    "            \n",
    "            # visualize epoch information\n",
    "            if self.gen_output:\n",
    "                if self.num_epochs == 1 or (epoch + 1) % max(int(self.num_epochs / 100), 1) == 0:\n",
    "                    self.visualize(epoch_str, self.num_epochs, epsilon)\n",
    "            \n",
    "            # update training info\n",
    "            max_portfolio_value = max(\n",
    "                max_portfolio_value, self.agent.portfolio_value\n",
    "            )\n",
    "            if self.agent.portfolio_value > self.agent.initial_balance:\n",
    "                epoch_win_cnt += 1\n",
    "                \n",
    "        # end time\n",
    "        time_end = time.time()\n",
    "        elapsed_time = time_end - time_start\n",
    "        \n",
    "        # log about training\n",
    "        with self.lock:\n",
    "            logger.debug(f'[{self.stock_code} Elapsed Time:{elapsed_time:.4f}]'\n",
    "                         f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')\n",
    "        \n",
    "    def save_models(self):\n",
    "        if self.value_network is not None and self.value_network_path is not None:\n",
    "            self.value_network.save_model(self.value_network_path)\n",
    "        if self.policy_network is not None and self.policy_network_path is not None:\n",
    "            self.policy_network.save_model(self.policy_network_path)\n",
    "            \n",
    "    # wihtou training, just predict actions based on samples\n",
    "    def predict(self):\n",
    "        # initiate an agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # queue for step samples\n",
    "        q_sample = collections.deque(maxlen=self.num_steps)\n",
    "        \n",
    "        result = []\n",
    "        while True:\n",
    "            # create samples\n",
    "            next_sample = self.build_sample()\n",
    "            if next_sample is None:\n",
    "                break\n",
    "            \n",
    "            # save samples as many as num_steps\n",
    "            q_sample.append(next_sample)\n",
    "            if len(q_sample) < self.num_steps:\n",
    "                continue\n",
    "            \n",
    "            # prediction based on value and policy network\n",
    "            pred_value = None\n",
    "            pred_policy = None\n",
    "            if self.value_network is not None:\n",
    "                pred_value = self.value_network.predict(list(q_sample)).tolist()\n",
    "            if self.policy_network is not None:\n",
    "                pred_policy = self.policy_network.predict(list(q_sample)).tolist()\n",
    "                \n",
    "            # decide action based on the network\n",
    "            result.append((self.environment.step[0]. pred_value, pred_policy))\n",
    "            \n",
    "        if self.gen_output:\n",
    "            with open(os.path.join(self.output_path, f'pred_{self.stock_code}.json'), 'w') as f:\n",
    "                print(json.dumps(result), file=f)\n",
    "                \n",
    "        return result\n",
    "             \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ReinforcementLearner(stock_code=stock_code, stock_data=stock_data, training_data=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_1508\\1959431608.py:92: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  self.ratio_hold = self.num_stocks * self.environment.get_price() / self.portfolio_value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.39285659790039,\n",
       " 11.490357398986816,\n",
       " 11.519286155700684,\n",
       " 11.438928604125977,\n",
       " 9.684192657470703,\n",
       " 11.452071189880371,\n",
       " 9.695317649841309,\n",
       " -0.0011476164910682693,\n",
       " -0.0011474603280056447,\n",
       " -0.004027650474972625,\n",
       " -0.008300415107919342,\n",
       " 0.004495945087225187,\n",
       " 0.007024919409473574,\n",
       " -0.004290043428398761,\n",
       " -0.0042899045944829175,\n",
       " 11.405857181549072,\n",
       " 9.656193733215332,\n",
       " 0.0028995122462521575,\n",
       " 0.0028995818672382804,\n",
       " 11.233571481704711,\n",
       " 9.510336017608642,\n",
       " 0.01828066192089625,\n",
       " 0.018280809378360626,\n",
       " 10.927684545516968,\n",
       " 9.251372035344442,\n",
       " 0.04678429876700131,\n",
       " 0.04678447915322072,\n",
       " 10.039654779434205,\n",
       " 8.499566344420115,\n",
       " 0.13937469518953213,\n",
       " 0.13937491220693674,\n",
       " 9.16685270468394,\n",
       " 7.760652500391006,\n",
       " 0.24785779510573847,\n",
       " 0.24785804505262704,\n",
       " 11.689641764102015,\n",
       " 10.777501199307407,\n",
       " 0.725137583336738,\n",
       " 0.08119773540232902,\n",
       " 0.3162068794955424,\n",
       " 0.3418614977965761,\n",
       " -0.025654618301033716,\n",
       " 1.4156986399453448,\n",
       " 58.60410800154174,\n",
       " nan,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.build_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, value_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.value_network_path = value_network_path\n",
    "        # create value network\n",
    "        self.init_value_network()\n",
    "    \n",
    "    # abstract method\n",
    "    def get_batch(self):\n",
    "        # reverse memory array\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        \n",
    "        # prepare sample array 'x' and label array 'y_value' with 0 value\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        \n",
    "        # we can handle from the last data becase of reversed memory\n",
    "        for i, (sample, action, value, reward) in enumerate(memory):\n",
    "            # sample\n",
    "            x[i] = sample\n",
    "            # # reward for training\n",
    "            ## memory_reward[-1] : last profit/loss in the batch data\n",
    "            ## reward : profit/loss at the time of action\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # value network output\n",
    "            y_value[i] = value\n",
    "            # state-action value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next\n",
    "            # save the maximum next state value\n",
    "            value_max_next = value.max()\n",
    "            \n",
    "        # return sample array, value network label, policy network label\n",
    "        # DQN has no policy network, return None\n",
    "        return x, y_value, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, policy_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.policy_network_path = policy_network_path\n",
    "        self.init_policy_network()\n",
    "        \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        # sample array composed of training data and agent states\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        # label array for training policy network\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        for i, (sample, action, policy, reward) in enumerate(memory):\n",
    "            # feature vector\n",
    "            x[i] = sample\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # policy network output\n",
    "            y_policy[i, :] = policy\n",
    "            # make it label with sigmoid to the reward\n",
    "            y_policy[i, action] = sigmoid(r)\n",
    "        \n",
    "        # there is no value network in PG\n",
    "        return x, None, y_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, shared_network=None,\n",
    "                 value_network_path=None, policy_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # shared layer of policy and value network\n",
    "        if shared_network is None:\n",
    "            self.shared_network = Network.get_shared_network(\n",
    "                net=self.net, num_steps=self.num_steps,\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS\n",
    "            )\n",
    "        else:\n",
    "            self.shared_network = shared_network\n",
    "        self.value_network_path = value_network_path\n",
    "        self.policy_network_path = policy_network_path\n",
    "        if self.value_network is None:\n",
    "            self.init_value_network(shared_network=self.shared_network)\n",
    "        if self.policy_network is None:\n",
    "            self.init_policy_network(shared_network=self.shared_network)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # value network label\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next\n",
    "            y_policy[i, :] = policy\n",
    "            # policy network label\n",
    "            y_policy[i, action] = sigmoid(r)\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CLearner(ActorCriticLearner):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward)\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        reward_next = self.memory_reward[-1]\n",
    "        \n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = reward_next + self.memory_reward[-1] - reward * 2\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = np.tanh(r + self.discount_factor * value_max_next)\n",
    "            advantage = y_value[i, action] - y_value[i].mean()\n",
    "            y_policy[i, :] = policy\n",
    "            y_policy[i, action] = sigmoid(advantage)\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, list_stock_code=None,\n",
    "                 list_stock_data=None, list_training_data=None,\n",
    "                 list_min_trading_price=None, list_max_trading_price=None,\n",
    "                 value_network_path=None, policy_network_path=None,\n",
    "                 **kwargs):\n",
    "        assert len(list_training_data) > 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_features += list_training_data[0].shape[1]\n",
    "        \n",
    "        ''' \n",
    "        create A2C learner instances as many as list size\n",
    "        each A2C learner shares value and policy network\n",
    "        '''\n",
    "        \n",
    "        # create shared network\n",
    "        self.shared_network = Network.get_shared_network(\n",
    "            net=self.net, num_steps=self.num_steps,\n",
    "            input_dim=self.num_features,\n",
    "            output_dim=self.agent.NUM_ACTIONS\n",
    "        )\n",
    "        self.value_network_path = value_network_path\n",
    "        self.policy_network_path = policy_network_path\n",
    "        if self.value_network is None:\n",
    "            self.init_value_network(shared_network=self.shared_network)\n",
    "        if self.policy_network is None:\n",
    "            self.init_policy_network(shared_network=self.shared_network)\n",
    "            \n",
    "        # create A2C instances\n",
    "        self.learners = []\n",
    "        for (stock_code, stock_data, training_data,\n",
    "             min_trading_price, max_trading_price) in zip(\n",
    "                 list_stock_code, list_stock_data, list_training_data,\n",
    "                 list_min_trading_price, list_max_trading_price\n",
    "             ):\n",
    "                 learner = A2CLearner(*args, stock_code=stock_code,\n",
    "                                      stock_data=stock_data,\n",
    "                                      training_data=training_data,\n",
    "                                      min_trading_price=min_trading_price,\n",
    "                                      max_trading_price=max_trading_price,\n",
    "                                      shared_network=self.shared_network,\n",
    "                                      value_network=self.value_network,\n",
    "                                      policy_network=self.policy_network, **kwargs)\n",
    "                 self.learners.append(learner)\n",
    "    \n",
    "    def run(self, learning=True):\n",
    "        threads = []\n",
    "        # execute run() method of each A2CLearne instances simultaneiously\n",
    "        for learner in self.learners:\n",
    "            threads.append(threading.Thread(\n",
    "                target=learner.run, daemon=True, kwargs={'learning': learning}\n",
    "            ))\n",
    "        \n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "            \n",
    "    def predict(self):\n",
    "        threads = []\n",
    "        for learner in self.learners:\n",
    "            threads.append(threading.Thread(\n",
    "                target=learner.predict, daemon=True\n",
    "            ))\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOLearner(A2CLearner):\n",
    "    def __init__(self, *args, lmb=0.95, eps=0.1, K=3, **kwargs):\n",
    "        # kwargs['value_network_activation'] = 'tanh'\n",
    "        # kwargs['policy_network_activation'] = 'tanh'\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lmb = lmb\n",
    "        self.eps = eps\n",
    "        self.K = K\n",
    "        \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        reward_next = self.memory_reward[-1]\n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = np.tanh(reward + self.discount_factor * value_max_next)\n",
    "            advantage = y_value[i, action] - y_value[i].mean()\n",
    "            y_policy[i, :] = policy\n",
    "            y_policy[i, action] = advantage\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy\n",
    "    \n",
    "    def fit(self):\n",
    "        x, y_value, y_policy = self.get_batch()\n",
    "        # initialize loss\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # update value network\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            if y_policy is not None:\n",
    "                # update policy network\n",
    "                loss += self.policy_network.train_on_batch_for_ppo(x, y_policy, list(reversed(self.memory_action)), self.eps, self.K)\n",
    "            self.loss = loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "def run_trader(mode='train', stock_names=['Apple', 'Tesla', 'Microsoft'], rl_method='dqn', net='cnn',\n",
    "               start_date='2020-01-01', end_date='2023-12-31',\n",
    "               lr=0.01, discount_factor=0.9,\n",
    "               initial_balance=1000000, stock_codes=['AAPL', 'TSLA', 'MSFT'],\n",
    "               min_trading_price=1, max_trading_price=10000, num_epochs=1000, num_steps=5):\n",
    "    \n",
    "    os.environ['RLTRADER_BASE'] = 'C:\\project\\github\\projects\\trader'\n",
    "    ''' \n",
    "    Arguments\n",
    "    --------\n",
    "    mode : 'train', 'test', 'update', 'predict'\n",
    "    rl_method : 'dqn', 'a2c', 'a3c', 'pg', 'ppo'\n",
    "    '''\n",
    "\n",
    "    # learner's parameter\n",
    "    output_name = f'{mode}_{stock_names}_{rl_method}_{net}'\n",
    "    # reinforcement learning True or False\n",
    "    learning = mode in ['train', 'update']\n",
    "    # use model flag\n",
    "    reuse_models = mode in ['test', 'update', 'predict']\n",
    "    value_network_name = f'{stock_names}_{rl_method}_{net}_value.mdl'\n",
    "    policy_network_name = f'{stock_names}_{rl_method}_{net}_policy.mdl'\n",
    "    start_epsilon = 1 if mode in ['train', 'update'] else 0\n",
    "    num_epochs = num_epochs if mode in ['train', 'update'] else 0\n",
    "    num_steps = num_steps if net in ['lstm', 'cnn', 'alex'] else 1\n",
    "    \n",
    "    # output path\n",
    "    output_path = os.path.join(BASE_DIR, 'output', output_name)\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    # log parameters\n",
    "    params = {\n",
    "        'mode': mode,\n",
    "        'stock_names': stock_names,\n",
    "        'rl_method': rl_method,\n",
    "        'net': net,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'lr': lr,\n",
    "        'discount_factor': discount_factor,\n",
    "        'initial_balance': initial_balance,\n",
    "        'stock_codes': stock_codes,\n",
    "    }\n",
    "    # params = json.dumps(vars(args))\n",
    "    with open(os.path.join(output_path, 'params.json'), 'w') as f:\n",
    "        f.write(str(params))\n",
    "    \n",
    "    # model path\n",
    "    value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "    policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name)\n",
    "    \n",
    "    # setting for logging\n",
    "    # log level DEBUG < INFO < WARNING < CRITICAL. more than DEBUG\n",
    "    log_path = os.path.join(output_path, f'{output_name}.log')\n",
    "    if os.path.exists(log_path):\n",
    "        os.remove(log_path)\n",
    "    logging.basicConfig(format='%(message)s')\n",
    "    logger = logging.getLogger(LOGGER_NAME)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.propagate = False\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    common_params = {}\n",
    "    list_stock_code = []\n",
    "    list_stock_data = []\n",
    "    list_training_data = []\n",
    "    list_min_trading_price = []\n",
    "    list_max_trading_price = []\n",
    "    \n",
    "    for stock_code in stock_codes:\n",
    "        _, stock_data, training_data = load_data(\n",
    "            stock_code, start_date, end_date\n",
    "        )\n",
    "        \n",
    "        assert len(stock_data) >= num_steps\n",
    "        \n",
    "        # minimum and maximum trading price policy\n",
    "        min_trading_price = min_trading_price\n",
    "        max_trading_price = max_trading_price\n",
    "        \n",
    "        # common parameters\n",
    "        common_params = {\n",
    "            'rl_method': rl_method,\n",
    "            'net': net, 'num_steps': num_steps,  'lr': lr,\n",
    "            'balance': initial_balance, 'num_epochs': num_epochs,\n",
    "            'discount_factor': discount_factor, 'start_epsilon': start_epsilon,\n",
    "            'output_path': output_path, 'reuse_models': reuse_models\n",
    "        }\n",
    "                \n",
    "        # start reinforcement learning\n",
    "        learner = None\n",
    "        if rl_method != 'a3c':\n",
    "            common_params.update({\n",
    "                'stock_code': stock_code,\n",
    "                'stock_data': stock_data,\n",
    "                'training_data': training_data,\n",
    "                'min_trading_price': min_trading_price,\n",
    "                'max_trading_price': max_trading_price\n",
    "            })\n",
    "            \n",
    "            if rl_method == 'dqn':    \n",
    "                learner = DQNLearner(**{**common_params, 'value_network_path': value_network_path})\n",
    "            \n",
    "            elif rl_method == 'pg':\n",
    "                learner = PolicyGradientLearner(**{**common_params, 'policy_network_path': policy_network_path})\n",
    "            \n",
    "            elif rl_method == 'ac':\n",
    "                learner = ActorCriticLearner(**{**common_params, 'value_network_path': value_network_path, 'policy_network_path': policy_network_path})\n",
    "            \n",
    "            elif rl_method == 'a2c':\n",
    "                learner = A2CLearner(**{**common_params,\n",
    "                                        'value_network_path': value_network_path,\n",
    "                                        'policy_network_path': policy_network_path})\n",
    "            elif rl_method == 'ppo':\n",
    "                learner = PPOLearner(**{**common_params,\n",
    "                                        'value_network_path': value_network_path,\n",
    "                                        'policy_network_path': policy_network_path})\n",
    "        else:\n",
    "            list_stock_code.append(stock_code)\n",
    "            list_stock_data.append(stock_data)\n",
    "            list_training_data.append(training_data)\n",
    "            list_min_trading_price.append(min_trading_price)\n",
    "            list_max_trading_price.append(max_trading_price)\n",
    "    \n",
    "    # in case of A3CLearner, create A2C instances as many as list size\n",
    "    if rl_method == 'a3c':\n",
    "        learner = A3CLearner(**{\n",
    "            **common_params,\n",
    "            'list_stock_code': list_stock_code,\n",
    "            'list_stock_data': list_stock_data,\n",
    "            'list_training_data': list_training_data,\n",
    "            'list_min_trading_price': list_min_trading_price,\n",
    "            'list_max_trading_price': list_max_trading_price,\n",
    "            'value_network_path': value_network_path,\n",
    "            'policy_network_path': value_network_path\n",
    "        })\n",
    "        \n",
    "    # check learner is not None\n",
    "    assert learner is not None\n",
    "        \n",
    "    if mode in ['train', 'test', 'update']:\n",
    "        learner.run(learning=learning)\n",
    "            \n",
    "        if mode in ['train', 'update']:\n",
    "            learner.save_models()\n",
    "        \n",
    "    elif mode == 'predict':  \n",
    "        learner.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_trader(stock_names=['Apple'], stock_codes=['AAPL'], net='alex', rl_method='dqn', num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_trader(stock_names=['Tesla', 'Apple', 'Miscrosoft'], stock_codes=['TSLA', 'AAPL', 'MSFT'], net='alex', rl_method='a3c', num_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
