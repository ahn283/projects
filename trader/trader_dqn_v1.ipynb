{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db connection\n",
    "\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import keyring\n",
    "import platform\n",
    "import numpy as np\n",
    "\n",
    "user = 'root'\n",
    "pw = keyring.get_password('macmini_db', user)\n",
    "host = '192.168.219.106' if platform.system() == 'Windows' else '127.0.0.1'\n",
    "port = 3306\n",
    "db = 'stock'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data\n",
    "COLUMNS_STOCK_DATA = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "COLUMNS_TRAINING_DATA = ['open', 'high', 'low', 'close', 'volume', 'close_ma5', 'volume_ma5', 'close_ma5_ratio', 'volume_ma5_ratio',\n",
    "       'open_close_ratio', 'open_prev_close_ratio', 'high_close_ratio',\n",
    "       'low_close_ratio', 'close_prev_close_ratio', 'volume_prev_volume_ratio',\n",
    "       'close_ma10', 'volume_ma10', 'close_ma10_ratio', 'volume_ma10_ratio',\n",
    "       'close_ma20', 'volume_ma20', 'close_ma20_ratio', 'volume_ma20_ratio',\n",
    "       'close_ma60', 'volume_ma60', 'close_ma60_ratio', 'volume_ma60_ratio',\n",
    "       'close_ma120', 'volume_ma120', 'close_ma120_ratio',\n",
    "       'volume_ma120_ratio', 'close_ma240', 'volume_ma240',\n",
    "       'close_ma240_ratio', 'volume_ma240_ratio', 'upper_bb',\n",
    "       'lower_bb', 'bb_pb', 'bb_width', 'macd',\n",
    "       'macd_signal', 'macd_oscillator', 'rs', 'rsi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# get us stock price of a specific ticker\n",
    "def get_stock_data(ticker, fro=None, to=None):\n",
    "\n",
    "    # connect DB\n",
    "    engine = create_engine(f'mysql+pymysql://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "    con = pymysql.connect(\n",
    "        user=user,\n",
    "        passwd=pw,\n",
    "        host=host,\n",
    "        db=db,\n",
    "        charset='utf8'\n",
    "    )\n",
    "            \n",
    "    mycursor = con.cursor()\n",
    "    \n",
    "    if fro is not None:\n",
    "        if to is not None:               \n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date BETWEEN '{fro}' AND '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date >= '{fro}'\n",
    "                    \"\"\"\n",
    "    \n",
    "    else:\n",
    "        if to is not None:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    AND date <= '{to}' \n",
    "                    \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" \n",
    "                    SELECT * FROM price_global\n",
    "                    WHERE ticker = '{ticker}'\n",
    "                    \"\"\"\n",
    "            \n",
    "    print(query)\n",
    "    prices = pd.read_sql(query, con=engine)\n",
    "    con.close()\n",
    "    engine.dispose()\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2018-01-01' AND '2022-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "stock_code = 'AAPL'\n",
    "fro = '2018-01-01'\n",
    "to = '2022-12-31'\n",
    "df = get_stock_data(stock_code, fro=fro, to=to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = max(min(x, 10), -10)\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \n",
    "    # moving average\n",
    "    windows = [5, 10, 20, 60, 120, 240]\n",
    "    for window in windows:\n",
    "        data[f'close_ma{window}'] = data['close'].rolling(window).mean()\n",
    "        data[f'volume_ma{window}'] = data['volume'].rolling(window).mean()\n",
    "        data[f'close_ma{window}_ratio'] = (data['close'] - data[f'close_ma{window}']) / data[f'close_ma{window}']\n",
    "        data[f'volume_ma{window}_ratio'] = (data['volume'] - data[f'volume_ma{window}']) / data[f'volume_ma{window}']\n",
    "        data['open_close_ratio'] = (data['open'].values - data['close'].values) / data['close'].values\n",
    "        data['open_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'open_prev_close_ratio'] = (data['open'][1:].values - data['close'][:-1].values) / data['close'][:-1].values\n",
    "        data['high_close_ratio'] = (data['high'].values - data['close'].values) / data['close'].values\n",
    "        data['low_close_ratio'] = (data['low'].values - data['close'].values) / data['close'].values\n",
    "        data['close_prev_close_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'close_prev_close_ratio'] = (data['close'][1:].values - data['close'][:-1].values) / data['close'][:-1].values \n",
    "        data['volume_prev_volume_ratio'] = np.zeros(len(data))\n",
    "        data.loc[1:, 'volume_prev_volume_ratio'] = (\n",
    "            # if volume is 0, change it into non zero value exploring previous volume continuously\n",
    "            (data['volume'][1:].values - data['volume'][:-1].values) / data['volume'][:-1].replace(to_replace=0, method='ffill').replace(to_replace=0, method='bfill').values\n",
    "        )\n",
    "    \n",
    "    # Bollinger band\n",
    "    data['middle_bb'] = data['close'].rolling(20).mean()\n",
    "    data['upper_bb'] = data['middle_bb'] + 2 * data['close'].rolling(20).std()\n",
    "    data['lower_bb'] = data['middle_bb'] - 2 * data['close'].rolling(20).std()\n",
    "    data['bb_pb'] = (data['close'] - data['lower_bb']) / (data['upper_bb'] - data['lower_bb'])\n",
    "    data['bb_width'] = (data['upper_bb'] - data['lower_bb']) / data['middle_bb']\n",
    "    \n",
    "    # MACD\n",
    "    macd_short, macd_long, macd_signal = 12, 26, 9\n",
    "    data['ema_short'] = data['close'].ewm(macd_short).mean()\n",
    "    data['ema_long'] = data['close'].ewm(macd_long).mean()\n",
    "    data['macd'] = data['ema_short'] - data['ema_long']\n",
    "    data['macd_signal'] = data['macd'].ewm(macd_signal).mean()\n",
    "    data['macd_oscillator'] = data['macd'] - data['macd_signal']\n",
    "    \n",
    "    # RSI\n",
    "    data['close_change'] = data['close'].diff()\n",
    "    # data['close_up'] = np.where(data['close_change'] >=0, df['close_change'], 0)\n",
    "    data['close_up'] = data['close_change'].apply(lambda x: x if x >= 0 else 0)\n",
    "    # data['close_down'] = np.where(data['close_change'] < 0, df['close_change'].abs(), 0)\n",
    "    data['close_down'] = data['close_change'].apply(lambda x: -x if x < 0 else 0)\n",
    "    data['rs'] = data['close_up'].ewm(alpha=1/14, min_periods=14).mean() / data['close_down'].ewm(alpha=1/14, min_periods=14).mean()\n",
    "    data['rsi'] = 100 - (100 / (1 + data['rs']))\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adj = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "load_data() function is a combined function for getting data from databases and preprocessing it into training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock_code, fro, to):\n",
    "    ''' \n",
    "    Arguments\n",
    "    ----------\n",
    "    - stock_code : unique stock code\n",
    "    - fro : start date\n",
    "    - to : end data\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    df_adj : entire prerprocessed data\n",
    "    stock_data : data for plotting chart\n",
    "    training_data : data for training a model\n",
    "    '''\n",
    "    \n",
    "    df = get_stock_data(stock_code, fro, to)\n",
    "    df_adj = preprocess(df).dropna().reset_index(drop=True)\n",
    "    # df_adj.dropna(inplace=True).reset_index(drop=True)\n",
    "    \n",
    "    stock_data = df_adj[COLUMNS_STOCK_DATA]\n",
    "    training_data = df_adj[COLUMNS_TRAINING_DATA]\n",
    "    \n",
    "    return df_adj, stock_data, training_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2018-01-01' AND '2022-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "df_adj, df_stock_data, df_training_data = load_data(stock_code, fro, to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps(step, eps_init=1.0, eps_final=0.05, eps_decrease_step=1000):\n",
    "    # eps_init = self.eps_init\n",
    "    # eps_final = self.eps_final\n",
    "    if step >= eps_decrease_step:\n",
    "        eps = eps_final\n",
    "    else:\n",
    "        m = (eps_final - eps_init) / eps_decrease_step\n",
    "        eps = eps_init + m * step\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# environment\n",
    "\n",
    "class Environment:\n",
    "    ''' \n",
    "    Attribute\n",
    "    ---------\n",
    "    - stock_data : stock price data such as 'open', 'close', 'high', 'low', 'volume'\n",
    "    - state : current state\n",
    "    - idx : current postion of stock data\n",
    "    \n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize idx and state\n",
    "    - observe() : move idx into next postion and get a new state\n",
    "    - get_close_price() : get close price of current state\n",
    "    - get_open_price() : get open price of current state\n",
    "    - get_state() : get current state\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, stock_data=None):\n",
    "        self.close_price_idx = 4    # index postion of close price\n",
    "        self.open_price_idx = 1     # index position of open price\n",
    "        self.stock_data = stock_data\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        self.max_idx = len(stock_data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = None\n",
    "        self.idx = -1\n",
    "        # self.idx = 0\n",
    "        # self.state = self.stock_data.iloc[self.idx]\n",
    "        \n",
    "    def observe(self):\n",
    "        # move to next day and get price data\n",
    "        # if there is no more idx, return None\n",
    "        if len(self.stock_data) > self.idx + 1:\n",
    "            self.idx += 1\n",
    "            self.state = self.stock_data.iloc[self.idx]\n",
    "            return self.state\n",
    "        return None\n",
    "    \n",
    "    def get_close_price(self):\n",
    "        # return close price\n",
    "        if self.state is not None:\n",
    "            return self.state[self.close_price_idx]\n",
    "        return None\n",
    "    \n",
    "    def get_next_close_price(self):\n",
    "        # return tomorrow close price\n",
    "        if self.idx < self.max_idx - 1:\n",
    "            return self.stock_data.iloc[self.idx + 1, self.close_price_idx]\n",
    "        else:\n",
    "            return self.stock_data.iloc[self.idx, self.close_price_idx]\n",
    "    \n",
    "    def get_open_price(self):\n",
    "        # return open price\n",
    "        if self.state is not None:\n",
    "            return self.state[self.open_price_idx]\n",
    "        \n",
    "    def get_next_open_price(self):\n",
    "        # return tomorrow open price\n",
    "        if self.idx < self.max_idx - 1:\n",
    "            return self.stock_data.iloc[self.idx + 1, self.open_price_idx]\n",
    "        else:\n",
    "            return self.stock_data.iloc[self.idx, self.open_price_idx]\n",
    "        \n",
    "    \n",
    "    def get_state(self):\n",
    "        # return current state\n",
    "        if self.state is not None:\n",
    "            return self.state\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(df_stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       2018-12-13\n",
       "open        42.387501\n",
       "high        42.622501\n",
       "low         43.142502\n",
       "close       42.737499\n",
       "volume    127594400.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       2018-12-13\n",
       "open        42.387501\n",
       "high        42.622501\n",
       "low         43.142502\n",
       "close       42.737499\n",
       "volume    127594400.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.369998931884766"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_next_close_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       2018-12-14\n",
       "open            41.32\n",
       "high            42.25\n",
       "low             42.27\n",
       "close       41.369999\n",
       "volume    162814800.0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.369998931884766"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.369998931884766"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_close_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.682498931884766"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_next_open_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       2018-12-17\n",
       "open        40.682499\n",
       "high        41.362499\n",
       "low         42.087502\n",
       "close       40.985001\n",
       "volume    177151600.0\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.682498931884766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()['open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.682498931884766"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_open_price()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - enviroment : instance of environment\n",
    "    - initial_balance : initial capital balance\n",
    "    - min_trading_price : minimum trading price\n",
    "    - max_trading_price : maximum trading price\n",
    "    - balance : cash balance\n",
    "    - num_stocks : obtained stocks\n",
    "    - portfolio_value : value of portfolios (balance + price * num_stocks)\n",
    "    - num_buy : number of buying\n",
    "    - num_sell : number of selling\n",
    "    - num_hold : number of holding\n",
    "    - ratio_hold : ratio of holding stocks\n",
    "    - profitloss : current profit or loss\n",
    "    - avg_buy_price_ratio : the ratio average price of a stock bought to the current price\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - reset() : initialize an agent\n",
    "    - set_balance() : initialize balance\n",
    "    - get_states() : get the state of an agent\n",
    "    - decide_action() : exploration or exploitation behavior according to the policy net\n",
    "    - validate_action() : validate actions\n",
    "    - decide_trading_unit() : decide how many stocks are sold or bought\n",
    "    - act() : act the actions\n",
    "    '''\n",
    "    \n",
    "    # action space\n",
    "    ACTION_BUY = 0      # buy\n",
    "    ACTION_SELL = 1     # sell\n",
    "    ACTION_HOLD = 2     # hold\n",
    "    \n",
    "    def __init__(self, env,\n",
    "                 initial_balance=None, min_trading_price=None, max_trading_price=None):      \n",
    "        \n",
    "        # agent state dimensions\n",
    "        # (ratio_hold, profit-loss ratio, current price to avg_buy_price ratio)\n",
    "        self.STATE_DIM = 3\n",
    "        \n",
    "        # trading charge and tax\n",
    "        self.TRADING_CHARGE = 0.00015    # trading charge 0.015%\n",
    "        self.TRADING_TAX = 0.002          # trading tax = 0.2%\n",
    "        \n",
    "        # action space\n",
    "        self.ACTION_BUY = 0      # buy\n",
    "        self.ACTION_SELL = 1     # sell\n",
    "        self.ACTION_HOLD = 2     # hold\n",
    "        \n",
    "        # get probabilities from neural nets\n",
    "        self.ACTIONS = [self.ACTION_BUY, self.ACTION_SELL, self.ACTION_HOLD]\n",
    "        self.NUM_ACTIONS = len(self.ACTIONS)      # output number from nueral nets\n",
    "        \n",
    "        \n",
    "        # get current price from the environment\n",
    "        self.env = env\n",
    "        self.initial_balance = initial_balance\n",
    "        self.done = False\n",
    "        \n",
    "        # minumum and maximum trainding price\n",
    "        self.min_trading_price = min_trading_price\n",
    "        self.max_trading_price = max_trading_price\n",
    "        \n",
    "        # attributes for an agent class\n",
    "        self.balance = initial_balance\n",
    "        self.num_stocks = 0\n",
    "        \n",
    "        # value of portfolio : balance + num_stocks * {current stock price}\n",
    "        self.portfolio_value = self.balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        \n",
    "        # three states of Agent class\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "        self.done = False\n",
    "        # self.env.reset()\n",
    "        \n",
    "    def set_initial_balance(self, balance):\n",
    "        self.initial_balance = balance\n",
    "        \n",
    "    def get_states(self):\n",
    "        # return current profitloss based on close price\n",
    "        close_price = self.env.get_close_price()\n",
    "        self.ratio_hold = self.num_stocks * close_price / self.portfolio_value\n",
    "        self.portfolio_value = self.balance + close_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        return (\n",
    "            self.ratio_hold,\n",
    "            self.profitloss,        # profitloss = (portfolio_value / initial_balance) - 1\n",
    "            (self.env.get_close_price() / self.avg_buy_price) if self.avg_buy_price > 0 else 0\n",
    "        )\n",
    "        \n",
    "    def decide_action(self, pred_value, eps):\n",
    "        # act randomly with epsilon probability, act according to neural network  with (1 - epsilon) probability\n",
    "        confidence = 0\n",
    "        \n",
    "        # if theres is a pred_policy, follow it, otherwise follow a pred_value\n",
    "        pred = pred_value\n",
    "            \n",
    "        # there is no prediction from both pred_policy and pred_value, explore!\n",
    "        if pred is None:\n",
    "            eps = 1\n",
    "        else:\n",
    "            maxpred = np.max(pred)\n",
    "            # if values for actions are euqal, explore!\n",
    "            if (pred == maxpred).all():\n",
    "                eps = 1\n",
    "                    \n",
    "        # decide whether exploration will be done or not\n",
    "        if np.random.rand() < eps:\n",
    "            exploration = True\n",
    "            action = np.random.randint(self.NUM_ACTIONS) \n",
    "        else: \n",
    "            exploration = False\n",
    "            action = np.argmax(pred)\n",
    "            \n",
    "        confidence = .5\n",
    "        if pred_value is not None:\n",
    "            confidence = sigmoid(pred[action])\n",
    "            \n",
    "        return action, confidence, exploration\n",
    "    \n",
    "    def validate_action(self, action):\n",
    "        # validate if the action is available\n",
    "        if action == self.ACTION_BUY:\n",
    "            # check if al least one stock can be bought.\n",
    "            if self.balance < self.env.get_open_price() * (1 + self.TRADING_CHARGE):\n",
    "                return False\n",
    "        elif action == self.ACTION_SELL:\n",
    "            # check if there is any sotck that can be sold\n",
    "            if self.num_stocks <= 0:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def decide_trading_unit(self, confidence):\n",
    "        # adjust number of stocks for buying and selling according to confidence level\n",
    "        if np.isnan(confidence):\n",
    "            return self.min_trading_price\n",
    "        \n",
    "        # set buying price range between self.min_trading_price + added_trading_price [min_trading_price, max_trading_price]\n",
    "        # in case that confidence > 1 causes the price over max_trading_price, we set min() so that the value cannot have larger value than self.max_trading_price - self.min_trading_price\n",
    "        # in case that confidence < 0, we set max() so that added_trading_price cannot have negative value.\n",
    "        added_trading_price = max(min(\n",
    "            int(confidence * (self.max_trading_price - self.min_trading_price)),\n",
    "            self.max_trading_price - self.min_trading_price\n",
    "        ), 0)\n",
    "        \n",
    "        trading_price = self.min_trading_price + added_trading_price\n",
    "        \n",
    "        return max(int(trading_price / self.env.get_open_price()), 1)\n",
    "    \n",
    "    def step(self, action, confidence):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - action : decided action from decide_action() method based on exploration or exploitation (0 or 1)\n",
    "        - confidence : probability from decide_action() method, the probability from policy network or the softmax probability from value network\n",
    "        '''\n",
    "        \n",
    "        # get the next open price from the environment\n",
    "        \n",
    "        open_price = self.env.get_next_open_price()\n",
    "        \n",
    "        if not self.validate_action(action):\n",
    "            action = self.ACTION_HOLD\n",
    "        \n",
    "        # buy\n",
    "        if action == self.ACTION_BUY:\n",
    "            # decide how many stocks will be bought\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            balance = (\n",
    "                self.balance - open_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            )\n",
    "            \n",
    "            # if lacks of balance, buy maximum units within the amount of money available\n",
    "            if balance < 0:\n",
    "                trading_unit = min(\n",
    "                    int(self.balance / (open_price * (1 + self.TRADING_CHARGE))),\n",
    "                    int(self.max_trading_price / open_price)\n",
    "                )\n",
    "                \n",
    "            # total amount of money with trading charge\n",
    "            invest_amount = open_price * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            if invest_amount > 0:\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks + open_price * trading_unit) / (self.num_stocks + trading_unit)\n",
    "                self.balance -= invest_amount\n",
    "                self.num_stocks += trading_unit\n",
    "                self.num_buy += 1\n",
    "                \n",
    "        # sell\n",
    "        elif action == self.ACTION_SELL:\n",
    "            # decide how many stocks will be sold\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            \n",
    "            # if lacks of stocks, sell maximum units available\n",
    "            trading_unit = min(trading_unit, self.num_stocks)\n",
    "            \n",
    "            # selling amount\n",
    "            invest_amount = open_price * (\n",
    "                1 - (self.TRADING_TAX + self.TRADING_CHARGE)\n",
    "            ) * trading_unit\n",
    "            \n",
    "            if invest_amount > 0:\n",
    "                # update average buy price\n",
    "                self.avg_buy_price = (self.avg_buy_price * self.num_stocks - open_price * trading_unit) / (self.num_stocks - trading_unit) if self.num_stocks > trading_unit else 0\n",
    "                self.num_stocks -= trading_unit\n",
    "                self.balance += invest_amount\n",
    "                self.num_sell += 1\n",
    "                \n",
    "        # hold\n",
    "        elif action == self.ACTION_HOLD:\n",
    "            self.num_hold += 1\n",
    "            \n",
    "        # update portfolio value with close price\n",
    "        close_price = self.env.get_next_close_price()\n",
    "        \n",
    "        self.portfolio_value = self.balance + close_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        \n",
    "        # info = {\n",
    "        #     'num_stocks': self.num_stocks,\n",
    "        #     'num_hold': self.num_hold,\n",
    "        #     'num_buy': self.num_buy,\n",
    "        #     'num_sell': self.num_sell\n",
    "        # }\n",
    "        \n",
    "        # return next_state, self.profitloss, self.done, info             # (next_states, profitloss, done, info)\n",
    "        return self.profitloss\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime\n",
    "import threading\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "class Visualizer:\n",
    "    ''' \n",
    "    Attributes\n",
    "    ---------\n",
    "    - fig : matplotlib Fugure instance plays like a canvas\n",
    "    - plot() : plot charts except daily price chart stock data chart)\n",
    "    - save() : save Figure as an image file\n",
    "    - clear() : initialize all charts but daily price chart (stock data chart)\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    - Figure title : parameters, epsilon\n",
    "    - Axes 1 : daily price chart (stock data chart)\n",
    "    - Axes 2 : number of stocks and agent action chart\n",
    "    - Axes 3 : value network chart\n",
    "    - Axes 4 : policy network and epsilon chart\n",
    "    - Axes 5 : portfolio value and learning point chart\n",
    "    '''\n",
    "    \n",
    "    COLORS = ['r', 'b', 'g']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.canvas = None\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.title = ''\n",
    "        self.x = []\n",
    "        self.xticks = []\n",
    "        self.xlabels = []\n",
    "        \n",
    "    def prepare(self, stock_data, title):\n",
    "        self.title = title\n",
    "        # shred x-axis among all charts\n",
    "        with lock:\n",
    "            # prepare for plotting fice charts\n",
    "            self.fig, self.axes = plt.subplots(   \n",
    "                nrows=5, ncols=1, facecolor='w', sharex=True\n",
    "            )\n",
    "            for ax in self.axes:\n",
    "                # deactivate scientific marks\n",
    "                ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # move y-axis to the right\n",
    "                ax.yaxis.tick_right()\n",
    "                \n",
    "            # chart 1. daily price data (stock data)\n",
    "            self.axes[0].set_ylabel('Env.')\n",
    "            x = np.arange(len(stock_data))\n",
    "            # make two dimentional array with open, high, low and close order\n",
    "            ohlc = np.hstack((\n",
    "                x.reshape(-1, 1), np.array(stock_data)[:, 1:5]\n",
    "            ))\n",
    "            # red for positive, blue for negative\n",
    "            candlestick_ohlc(self.axes[0], ohlc, colorup='r', colordown='b')\n",
    "            # visualize volume\n",
    "            ax = self.axes[0].twinx()\n",
    "            volume = np.array(stock_data)[:, 5].tolist()\n",
    "            ax.bar(x, volume, color='b', alpha=0.3)\n",
    "            # set x-axis\n",
    "            self.x = np.arange(len(stock_data['date']))\n",
    "            self.xticks = stock_data.index[[0, -1]]\n",
    "            self.xlabels = stock_data.iloc[[0, -1]]['date']\n",
    "            \n",
    "    def plot(self, epoch_str=None, num_epochs=None, eps=None,\n",
    "             action_list=None, actions=None, num_stocks=None,\n",
    "             outvals_value=[], outvals_policy=[], exps=None,\n",
    "             initial_balance=None, pvs=None):\n",
    "        ''' \n",
    "        Attributes\n",
    "        --------\n",
    "        - epoch_str : epoch for Figure title\n",
    "        - num_epochs : number of total epochs\n",
    "        - eps : exploration rate\n",
    "        - action_list : total action list of an agent\n",
    "        - num_stocks : number of stocks\n",
    "        - outvals_value : output array of value network\n",
    "        - outvals_policy : output array of policy network\n",
    "        - exps : array of values if exploration is true or not\n",
    "        - initial_balance\n",
    "        - pvs : array of portfolio values\n",
    "        '''\n",
    "        \n",
    "        with lock:\n",
    "            # action, num_stocks, outvals_value, outvals_policy, pvs has same size\n",
    "            # create an array with same size as actions and use it as an x-axis\n",
    "            actions = np.array(actions)     # action array of an agent\n",
    "            # turn value network output as an array\n",
    "            outvals_value = np.array(outvals_value)\n",
    "            # turn policy network ouput as an array\n",
    "            outvals_policy = np.array(outvals_policy)\n",
    "            # turn initial balance as an array\n",
    "            pvs_base = np.zeros(len(actions)) + initial_balance     # array([initial_balance, initial_balance, initial_balance, ...])\n",
    "            # chart 2. agent states (action, num_stocks)\n",
    "            for action, color in zip(action_list, self.COLORS):\n",
    "                for i in self.x[actions == action]:\n",
    "                    # express actions as background color : red for buying, blue for selling\n",
    "                    self.axes[1].axvline(i, color=color, alpha=0.1)\n",
    "            self.axes[1].plot(self.x, num_stocks, '-k')     # plot number of stocks\n",
    "            \n",
    "            # chart 3. value network (prediction value for action)\n",
    "            if (len(outvals_value)) > 0:\n",
    "                max_actions = np.argmax(outvals_value, axis=1)\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    # plot background\n",
    "                    for idx in self.x:\n",
    "                        if max_actions[idx] == action:\n",
    "                            self.axes[2].axvline(idx, color=color, alpha=0.1)\n",
    "                    # plot value network\n",
    "                    ## red for buying, blue for selling, green for holding\n",
    "                    ## if no predicted action, no plot green chart\n",
    "                    self.axes[2].plot(self.x, outvals_value[:, action], color=color, linestyle='-')\n",
    "                    \n",
    "            # chart 4. policy network\n",
    "            # plot exploration with yellow background\n",
    "            for exp_idx in exps:\n",
    "                self.axes[3].axvline(exp_idx, color='y')\n",
    "            # plot action as background\n",
    "            _outvals = outvals_policy if len(outvals_policy) > 0 else outvals_value\n",
    "            for idx, outval in zip(self.x, _outvals):\n",
    "                color = 'white'\n",
    "                if np.isnan(outval.max()):\n",
    "                    continue\n",
    "                # with no exploration area, red for buying, blue for selling\n",
    "                if outval.argmax() == Agent.ACTION_BUY:\n",
    "                    color = self.COLORS[0]      # red for buying\n",
    "                elif outval.argmax() == Agent.ACTION_SELL:\n",
    "                    color = self.COLORS[1]      # blue for selling\n",
    "                elif outval.argmax() == Agent.ACTION_HOLD:\n",
    "                    color = self.COLORS[2]      # green for holding\n",
    "                self.axes[3].axvline(idx, color=color, alpha=0.1)\n",
    "            \n",
    "            # plot policy network\n",
    "            # red for buying policy network output, blue for selling policy entwork\n",
    "            # when red line is above blue line, buy stocks, otherwise sell stocks\n",
    "            if len(outvals_policy) > 0:\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    self.axes[3].plot(\n",
    "                        self.x, outvals_policy[:, action],\n",
    "                        color=color, linestyle='-'\n",
    "                    )\n",
    "            \n",
    "            # chart 5. portfolio value\n",
    "            # horizontal line for initial balance\n",
    "            self.axes[4].axhline(\n",
    "                initial_balance, linestyle='-', color='gray'\n",
    "            )\n",
    "            self.axes[4].fill_between(\n",
    "                self.x, pvs, pvs_base,\n",
    "                where=pvs > pvs_base, facecolor='r', alpha=0.1\n",
    "            )\n",
    "            self.axes[4].plot(self.x, pvs, '-k')\n",
    "            self.axes[4].xaxis.set_ticks(self.xticks)\n",
    "            self.axes[4].xaxis.set_ticklabels(self.xlabels)\n",
    "            \n",
    "            # epoch and exploration rate\n",
    "            self.fig.suptitle(f'{self.title}\\nEPOCH:{epoch_str}/{num_epochs} EPSILON:{eps:.2f}')\n",
    "            # adjust canvas layout\n",
    "            self.fig.tight_layout()\n",
    "            self.fig.subplots_adjust(top=0.85)\n",
    "            \n",
    "    def clear(self, xlim):\n",
    "        with lock:\n",
    "            _axes = self.axes.tolist()\n",
    "            # initial chart except non changeable value\n",
    "            for ax in _axes[1:]:\n",
    "                ax.cla()        # initialize chart\n",
    "                ax.relim()      # initialize limit\n",
    "                ax.autoscale()  # reset scale\n",
    "                \n",
    "            # reset y-axis label\n",
    "            self.axes[1].set_ylabel('Agent')\n",
    "            self.axes[2].set_ylabel('V')\n",
    "            self.axes[3].set_ylabel('P')\n",
    "            self.axes[4].set_ylabel('PV')\n",
    "            for ax in _axes:\n",
    "                ax.set_xlim(xlim)       # reset limit in x-axis\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "                # set equal width horizontally\n",
    "                ax.ticklabel_format(useOffset=False)\n",
    "                \n",
    "    def save(self, path):\n",
    "        with lock:\n",
    "            self.fig.savefig(path)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNNNetwork:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - input_dim\n",
    "    - output_dim\n",
    "    - lr : learning rate\n",
    "    - activation : activation layer function ('linear', 'sigmoid', 'tanh', 'softmax')\n",
    "    - loss : loss function for networks\n",
    "    - model : final neural network model\n",
    "    \n",
    "    Functions\n",
    "    --------\n",
    "    - predict() : calculate value of actions\n",
    "    - train_on_batch() : generate batch data for training\n",
    "    - save_model()\n",
    "    - load_model()\n",
    "    - get_share_network() : generate network head according to the networks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim=0, output_dim=0, num_steps=5, lr=0.01,\n",
    "                 activation='sigmoid', loss='mse'):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.loss = loss \n",
    "        \n",
    "        # data shape for CNN network\n",
    "        # CNN has 3 dimensional shape, so we set input shape as (num_steps, input_dim)\n",
    "        inp = (self.num_steps, input_dim)\n",
    "        \n",
    "        self.head = self.get_network_head(inp, self.output_dim)\n",
    "        \n",
    "        # generate network model for head with activation function\n",
    "        self.model = nn.Sequential(self.head)\n",
    "        if self.activation == 'linear':\n",
    "            pass\n",
    "        elif self.activation == 'relu':\n",
    "            self.model.add_module('activation', nn.ReLU())\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            self.model.add_module('activation', nn.LeakyReLU())\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.model.add_module('activation', nn.Sigmoid())\n",
    "        elif self.activation == 'tanh':\n",
    "            self.model.add_module('activation', nn.Tanh())\n",
    "        elif self.activation == 'softmax':\n",
    "            self.model.add_module('activation', nn.Softmax(dim=1))\n",
    "        self.model.apply(CNNNetwork.init_weights)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.NAdam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = None\n",
    "        if loss == 'mse':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif loss == 'binary_crossentropy':\n",
    "            self.criterion = nn.BCELoss()\n",
    "            \n",
    "    def predict(self, sample):\n",
    "        # return prediction of buying, selling and holding on given samples\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.from_numpy(sample).float().to(device)\n",
    "            pred = self.model(x).detach().cpu().numpy()\n",
    "            pred = pred.flatten()\n",
    "        return pred\n",
    "    \n",
    "    def train_on_batch(self, x, y):\n",
    "        x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        loss = 0\n",
    "        self.model.train()\n",
    "        _x = torch.from_numpy(x).float().to(device)\n",
    "        _y = torch.from_numpy(y).float().to(device)\n",
    "        y_pred = self.model(_x)\n",
    "        _loss = self.criterion(y_pred, _y)\n",
    "        self.optimizer.zero_grad()\n",
    "        _loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss += _loss.item()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod \n",
    "    def get_network_head(inp, output_dim):\n",
    "        kernel_size = 2\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(inp[0]),\n",
    "            torch.nn.Conv1d(inp[0], 1, kernel_size),\n",
    "            torch.nn.BatchNorm1d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(inp[1] - (kernel_size - 1), 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(32, output_dim)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        # initialize weights as weighted normal distribution\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "        \n",
    "    def save_model(self, model_path):\n",
    "        if model_path is not None and self.model is not None:\n",
    "            torch.save(self.model, model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        if model_path is not None:\n",
    "            self.model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "##### Experience buffer\n",
    "\n",
    "- We get transition data $(s,a,r,s^\\prime)$, save into buffer, and train with uniform random sampling data from buffer.\n",
    "\n",
    "- Q-learning is off-poilcy learning.\n",
    "\n",
    "#### Target netwrok\n",
    "\n",
    "- We set separate target network and update it periodically.\n",
    "\n",
    "    - $\\theta_i$ : training parameter for $i$th iteration\n",
    "\n",
    "    - $\\theta^-_i$ : target calculation network for $i$th iteration\n",
    "\n",
    "    - $U(D)$ : replay memory of transitions $\\rightarrow \\pi_0, \\cdots, \\pi_i$ dataset.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "$$L_i(\\theta_i)=E_{s,a,r,s^\\prime}\\left[(r+\\gamma\\max_{a^\\prime}Q(s^\\prime, a^\\prime, Q^-_i)-Q(s,a;\\theta_i))^2\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "\n",
    "- To reudce compuration complexity, neural network get states and return all action-values.\n",
    "\n",
    "<img src='./image/3-s2.0-B9780323857871000117-f06-02-9780323857871.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMemory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, replay_capacity=480, replay_init_ratio=0.3):\n",
    "        self.replay_capacity = replay_capacity\n",
    "        self.replay_init_ratio = replay_init_ratio\n",
    "        self.buffer =deque([], maxlen=self.replay_capacity)\n",
    "        \n",
    "    def getsize(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        if buffer_size >= size:\n",
    "            samples = random.sample(self.buffer, size)\n",
    "        else:\n",
    "            assert False, f\"Buffer size ({buffer_size}) is smaller than the sample size ({size})\"\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ReplayMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([], maxlen=480)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_learner.py\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "import collections\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "LOGGER_NAME = 'rltrader'\n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "class DQNLearner:\n",
    "    ''' \n",
    "    Attributes\n",
    "    --------\n",
    "    - stock_code\n",
    "    - env : environment\n",
    "    - agent : agent\n",
    "    - value_network : value network for a model if needed\n",
    "    Functions\n",
    "    --------\n",
    "    - init_value_network() : function for creatinf value network\n",
    "    - build_sample() : get samples from env instances\n",
    "    - get_batch() : create batch training data\n",
    "    - update_network() : train value network\n",
    "    - fit() : reques train value network\n",
    "    - run() : perform reinforcement learning\n",
    "    - save_models() : save value network\n",
    "    '''\n",
    "    def __init__(self, stock_code=None, fro=None, to=None,\n",
    "                 min_trading_price=100, max_trading_price=10000,\n",
    "                 num_steps=5, lr=0.0005,\n",
    "                 discount_factor=0.9, num_epochs=1000,\n",
    "                 balance=100000, eps_init=1,\n",
    "                 value_network=None, value_network_path=None,\n",
    "                 output_path='', reuse_models=False, gen_output=True):\n",
    "        self.stock_code = stock_code\n",
    "        _, self.stock_data, self.training_data = load_data(stock_code=self.stock_code, fro=fro, to=to)\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_epochs = num_epochs\n",
    "        self.eps_init = eps_init\n",
    "        \n",
    "        self.env = Environment(self.stock_data) \n",
    "        self.agent = Agent(self.env, balance, min_trading_price, max_trading_price)\n",
    "        \n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # vector size = training vector size + agent state size\n",
    "        self.num_features = self.agent.STATE_DIM\n",
    "        if self.training_data is not None:\n",
    "            self.num_features += self.training_data.shape[1]\n",
    "            \n",
    "            \n",
    "        # set network\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.value_network = value_network\n",
    "        self.reuse_models = reuse_models\n",
    "\n",
    "        # visualization module\n",
    "        self.visualizer = Visualizer()\n",
    "        \n",
    "        # memeory\n",
    "        self.memory_sample = []     # training data sample\n",
    "        self.memory_action = []     # actions taken\n",
    "        self.memory_reward = []     # reward obtained\n",
    "        self.memory_value = []      # prediction value for action\n",
    "        # self.memory_policy = []     # prediction probability for action\n",
    "        self.memory_pv = []         # portfolio value\n",
    "        self.memory_num_stocks = [] # number of stocks\n",
    "        self.memory_exp_idx = []    # exploration index\n",
    "        \n",
    "        # exploration epoch info\n",
    "        self.loss = 0               # loss during epoch\n",
    "        self.itr_cnt = 0            # number of iterations with profit\n",
    "        self.exploration_cnt = 0    # count of exploration\n",
    "        self.batch_size = 0         # number of training\n",
    "        \n",
    "        # log output\n",
    "        self.output_path = output_path\n",
    "        self.gen_output = gen_output    \n",
    "        \n",
    "        self.value_network_path = value_network_path\n",
    "        # create value network\n",
    "        self.init_value_network()\n",
    "        \n",
    "    def init_value_network(self, activation='linear', loss='mse', kernel_size=2):\n",
    "        self.value_network = CNNNetwork(\n",
    "            input_dim=self.num_features,\n",
    "            output_dim=self.agent.NUM_ACTIONS,\n",
    "            lr=self.lr, num_steps=self.num_steps,\n",
    "            # shared_network=shared_network,\n",
    "            activation=activation, loss=loss\n",
    "        )\n",
    "        \n",
    "        if self.reuse_models and os.path.exists(self.value_network_path):\n",
    "            self.value_network.load_model(model_path=self.value_network_path)\n",
    "            \n",
    "        \n",
    "    def reset(self):\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        \n",
    "        # reset environment\n",
    "        self.env.reset()\n",
    "        \n",
    "        # reset agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # reset visualizer\n",
    "        self.visualizer.clear([0, len(self.stock_data)])\n",
    "        \n",
    "        # reset memories\n",
    "        self.memory_sample = []\n",
    "        self.memory_action = []\n",
    "        self.memory_reward = []\n",
    "        self.memory_value = []\n",
    "        self.memory_policy = []\n",
    "        self.memory_pv = []\n",
    "        self.memory_num_stocks = []\n",
    "        self.memory_exp_idx = []\n",
    "        \n",
    "        # reset epoch info\n",
    "        self.loss = 0.\n",
    "        self.itr_cnt = 0\n",
    "        self.exploration_cnt = 0\n",
    "        self.batch_size = 0\n",
    "        \n",
    "    def build_sample(self):\n",
    "        # get next index data\n",
    "        self.env.observe()\n",
    "        # 47 samples + 3 agent states = 50 features\n",
    "        if len(self.training_data) > self.training_data_idx + 1:\n",
    "            self.training_data_idx += 1\n",
    "            self.sample = self.training_data[self.training_data_idx].tolist()\n",
    "            self.sample.extend(self.agent.get_states())\n",
    "            return self.sample\n",
    "        print(self.sample)\n",
    "        return None    \n",
    "    \n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        \n",
    "        # prepare sample array 'x' and label array 'y_value' with 0 values\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        \n",
    "        # we can handle from the last data bacause of reversed memory\n",
    "        for i, (sample, action, value, reward) in enumerate(memory):\n",
    "            # sample\n",
    "            x[i] = sample\n",
    "            ## reward for training\n",
    "            ## memory_reward[-1] : last profit/loss in the batch data\n",
    "            ## reward : profit/loss at the time of action\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            # value network output\n",
    "            y_value[i] = value\n",
    "            # state-action value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next\n",
    "            # save the maximum next state value\n",
    "            value_max_next = value.max()\n",
    "        \n",
    "        # return sample array, value network label\n",
    "        return x, y_value\n",
    "    \n",
    "    \n",
    "    # after generate batch data, call train_on_batch() medho to train value network and policy network\n",
    "    # value network : DQNLearner, ActorCriticLearner, A2CLearner\n",
    "    # policy network : PolicyGradientLearner, ActorCriticLearner, A2CLearner\n",
    "    # loss value after training is saves as instance. in case of training value and policy network return sum of both loss\n",
    "    def fit(self):\n",
    "        # generate batch data\n",
    "        x, y_value = self.get_batch()\n",
    "        # initialize loss\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # update value network\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            self.loss = loss\n",
    "            \n",
    "    # visualize one complete epoch\n",
    "    # in case of LSTM, CNN agent, the number of agent's actions, num_stocks, output of value network, output of policy network and portfolio value is less than daily price data by (num_steps -1). So we fill (num_steps -1) meaningless data \n",
    "    def visualize(self, epoch_str, num_epochs, eps):\n",
    "        self.memory_action = [self.agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action\n",
    "        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks\n",
    "        if self.value_network is not None:\n",
    "            self.memory_value = [np.array([np.nan] * len(self.agent.ACTIONS))] * (self.num_steps - 1) + self.memory_value\n",
    "        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv\n",
    "        self.visualizer.plot(\n",
    "            epoch_str=epoch_str, num_epochs=num_epochs,\n",
    "            eps=eps, action_list=self.agent.ACTIONS,\n",
    "            actions=self.memory_action,\n",
    "            num_stocks=self.memory_num_stocks,\n",
    "            outvals_value=self.memory_value,\n",
    "            outvals_policy=self.memory_policy,\n",
    "            exps=self.memory_exp_idx,\n",
    "            initial_balance=self.agent.initial_balance,\n",
    "            pvs=self.memory_pv,\n",
    "        )\n",
    "        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))\n",
    "        \n",
    "        \n",
    "    def run(self, learning=True):\n",
    "        '''\n",
    "        Arguments\n",
    "        ---------\n",
    "        - learning : boolean if learning will be done or not\n",
    "            - True : after training, build value and policy network\n",
    "            - False: simulation with pretrined model\n",
    "        '''\n",
    "        info = (\n",
    "            f'[{self.stock_code}] RL:dqn NET:cnn'\n",
    "            f' LR:{self.lr} DF:{self.discount_factor}'\n",
    "        )\n",
    "        \n",
    "        logger.debug(info)\n",
    "        \n",
    "        # start time\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # prepare visualization\n",
    "        self.visualizer.prepare(self.env.stock_data, info)\n",
    "        \n",
    "        # prepare folders foe saving results\n",
    "        if self.gen_output:\n",
    "            self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}')\n",
    "            if not os.path.isdir(self.epoch_summary_dir):\n",
    "                os.makedirs(self.epoch_summary_dir)\n",
    "            else:\n",
    "                for f in os.listdir(self.epoch_summary_dir):\n",
    "                    os.remove(os.path.join(self.epoch_summary_dir, f))\n",
    "                    \n",
    "        # reset info about training\n",
    "        # save the most highest portfolio value at max_portfolio_value variable\n",
    "        max_portfolio_value = 0\n",
    "        # save the count of epochs with profit\n",
    "        epoch_win_cnt = 0\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # start time of an epoch\n",
    "            time_start_epoch = time.time()\n",
    "            \n",
    "            # queue for making step samples\n",
    "            q_sample = collections.deque(maxlen=self.num_steps)\n",
    "            \n",
    "            # reset environment, networks, visualizer and memories\n",
    "            self.reset()\n",
    "            \n",
    "            # decaying exploration rate\n",
    "            if learning:\n",
    "                eps = self.eps_init * (1 - (epoch / (self.num_epochs - 1)))\n",
    "            else:\n",
    "                eps = self.eps_init\n",
    "                \n",
    "            for i in tqdm(range(len(self.training_data)), leave=False):\n",
    "                # create samples\n",
    "                next_sample = self.build_sample()\n",
    "                if next_sample is None:\n",
    "                    break\n",
    "                \n",
    "                # save samples until its size becomes as num_steps\n",
    "                q_sample.append(next_sample)\n",
    "                if len(q_sample) < self.num_steps:\n",
    "                    continue\n",
    "                \n",
    "                # prediction of value and policyn entwork\n",
    "                pred_value = None\n",
    "                # get predicted value of actions\n",
    "                if self.value_network is not None:\n",
    "                    pred_value = self.value_network.predict(list(q_sample))\n",
    "                    \n",
    "                # make decisions based on predicted value and probabilities\n",
    "                # decide actions based on based on networks or exploration\n",
    "                # decide actions randomly with epsilon probability or according to network output with (1 - epsilon)\n",
    "                # policy network output is the probabilities that selling or buying increase portfolio value. if output for buying is larger than that for selling, then buy the stock. Otherwise, sell it.\n",
    "                # if there is no output of policy network, select the action with the hightes output of value network.\n",
    "                action, confidence, exploration = self.agent.decide_action(pred_value=pred_value, eps=eps)\n",
    "                \n",
    "                # get rewards from action\n",
    "                reward = self.agent.step(action, confidence)\n",
    "                \n",
    "                # save action and the results in the memory\n",
    "                self.memory_sample.append(list(q_sample))\n",
    "                self.memory_action.append(action)\n",
    "                self.memory_reward.append(reward)\n",
    "                if self.value_network is not None:\n",
    "                    self.memory_value.append(pred_value)\n",
    "                self.memory_pv.append(self.agent.portfolio_value)\n",
    "                self.memory_num_stocks.append(self.agent.num_stocks)\n",
    "                if exploration:\n",
    "                    self.memory_exp_idx.append(self.training_data_idx)\n",
    "                    \n",
    "                # update iteration info\n",
    "                self.batch_size += 1\n",
    "                self.itr_cnt += 1\n",
    "                self.exploration_cnt +=1 if exploration else 0\n",
    "                \n",
    "            # training network after completing an epoch\n",
    "            if learning:\n",
    "                self.fit()\n",
    "            \n",
    "            # log about an epoch info\n",
    "            # check the length of epoch number string\n",
    "            num_epochs_digit = len(str(self.num_epochs))\n",
    "            # fill '0' as same size as the length of number of epochs\n",
    "            epoch_str = str(epoch + 1).rjust(num_epochs_digit, '0')\n",
    "            time_end_epoch = time.time()\n",
    "            # save time of an epoch\n",
    "            elapsed_time_epoch = time_end_epoch - time_start_epoch\n",
    "            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}]'\n",
    "                         f'Epsilon:{eps:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '\n",
    "                         f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '\n",
    "                         f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '\n",
    "                         f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')\n",
    "            \n",
    "            # visualize epoch information\n",
    "            if self.gen_output:\n",
    "                if self.num_epochs == 1 or (epoch + 1) % max(int(self.num_epochs / 100), 1) == 0:\n",
    "                    self.visualize(epoch_str, self.num_epochs, eps)\n",
    "            \n",
    "            # update training info\n",
    "            max_portfolio_value = max(\n",
    "                max_portfolio_value, self.agent.portfolio_value\n",
    "            )\n",
    "            if self.agent.portfolio_value > self.agent.initial_balance:\n",
    "                epoch_win_cnt += 1\n",
    "             \n",
    "        # end time\n",
    "        time_end = time.time()\n",
    "        elapsed_time = time_end - time_start\n",
    "        \n",
    "        # log about training\n",
    "        logger.debug(f'[{self.stock_code} Elapsed Time:{elapsed_time:.4f}]'\n",
    "                    f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')\n",
    "        \n",
    "    def save_models(self):\n",
    "        if self.value_network is not None and self.value_network_path is not None:\n",
    "            self.value_network.save_model(self.value_network_path)\n",
    "            \n",
    "    # wihtou training, just predict actions based on samples\n",
    "    def predict(self):\n",
    "        # initiate an agent\n",
    "        self.agent.reset()\n",
    "        \n",
    "        # queue for step samples\n",
    "        q_sample = collections.deque(maxlen=self.num_steps)\n",
    "        \n",
    "        result = []\n",
    "        while True:\n",
    "            # create samples\n",
    "            next_sample = self.build_sample()\n",
    "            if next_sample is None:\n",
    "                break\n",
    "            \n",
    "            # save samples as many as num_steps\n",
    "            q_sample.append(next_sample)\n",
    "            if len(q_sample) < self.num_steps:\n",
    "                continue\n",
    "            \n",
    "            # prediction based on value and policy network\n",
    "            pred_value = None\n",
    "            pred_policy = None\n",
    "            if self.value_network is not None:\n",
    "                pred_value = self.value_network.predict(list(q_sample)).tolist()\n",
    "                \n",
    "            # decide action based on the network\n",
    "            result.append((self.environment.step[0]. pred_value, pred_policy))\n",
    "            \n",
    "        if self.gen_output:\n",
    "            with open(os.path.join(self.output_path, f'pred_{self.stock_code}.json'), 'w') as f:\n",
    "                print(json.dumps(result), file=f)\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dqn_learner.py\n",
    "\n",
    "# import torch\n",
    "# from torch import nn \n",
    "\n",
    "# class DQNLearner: \n",
    "    \n",
    "#     def __init__(self, \n",
    "#                  env, agent,\n",
    "#                  stock_code=None, fro='2020-01-01', to='2022-12-31',\n",
    "#                  stock_data=None, training_data=None,\n",
    "#                  discount_factor=0.8, num_epochs=1000, batch_size=60, replay_init_ratio=0.3, replacy_capacity=480,\n",
    "#                  num_steps=10, lr=0.005, value_network=None, reuse_models=True, \n",
    "#                  output_path='', gen_output=True):\n",
    "        \n",
    "#         # data, environemnt and agent\n",
    "#         self.stock_code = stock_code\n",
    "#         self.stock_data = stock_data\n",
    "#         self.training_data = training_data\n",
    "#         # _, self.stock_data, self.training_data = load_data(stock_code=self.stock_code, fro=fro, to=to)\n",
    "        \n",
    "#         self.env = env\n",
    "#         self.agent = agent\n",
    "        \n",
    "        \n",
    "#         # reinforcement learning parameters\n",
    "#         self.discount_factor = discount_factor\n",
    "#         self.num_epochs = num_epochs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.replay_capacity = replacy_capacity\n",
    "#         self.replay_init_ratio = replay_init_ratio \n",
    "        \n",
    "#         #  eps_init=1, eps_final=0.05, eps_decrease_step=1000\n",
    "#         # self.eps_init = eps_init \n",
    "#         # self.eps_final = eps_final \n",
    "#         # self.eps_decrease_step = eps_decrease_step\n",
    "        \n",
    "#         # network\n",
    "#         self.lr = lr\n",
    "#         self.num_steps = num_steps\n",
    "#         self.value_network = value_network\n",
    "#         self.reuse_models = reuse_models \n",
    "#         self.input_dim = int(self.num_steps) * int(self.training_data.shape[1])\n",
    "#         kernel_size = 2\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.BatchNorm2d(self.input_dim),\n",
    "#             nn.Conv2d(self.input_dim, 1, kernel_size),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(self.input_dim - (kernel_size - 1), 128),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(32, self.agent.NUM_ACTIONS)\n",
    "#         )\n",
    "#         self.target_network = nn.Sequential(\n",
    "#             nn.BatchNorm2d(self.input_dim),\n",
    "#             nn.Conv2d(self.input_dim, 1, kernel_size),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(self.input_dim - (kernel_size - 1), 128),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Dropout(p=0.1),\n",
    "#             nn.Linear(32, self.agent.NUM_ACTIONS)\n",
    "#         )   \n",
    "        \n",
    "#         # We do not train target_network\n",
    "#         for param in self.target_network.parameters():\n",
    "#             param.requires_grad = False    \n",
    "        \n",
    "#         # memory\n",
    "#         self.replay_memory = ReplayMemory()\n",
    "#         self.memory_sample = []     # training data sample\n",
    "#         self.memory_action = []     # actions taken\n",
    "#         self.memory_reward = []     # reward obtained\n",
    "#         self.memory_value = []      # prediction value for action\n",
    "#         self.memory_pv = []         # portfolio value\n",
    "#         self.memory_num_stocks = [] # number of stocks\n",
    "#         self.memory_exp_idx = []    # exploration index\n",
    "        \n",
    "#         # exploration epoch info\n",
    "#         self.loss = 0               # loss during epoch\n",
    "#         self.itr_cnt = 0            # number of iterations with profit\n",
    "#         self.exploration_cnt = 0    # count of exploration\n",
    "#         self.batch_size = 0         # number of training\n",
    "        \n",
    "#         # log path\n",
    "#         self.output_path = output_path\n",
    "#         self.gen_output = gen_output\n",
    "        \n",
    "#     def update_target_network(self):\n",
    "#         self.target_network.load_state_dict(self.network.state_dict())\n",
    "        \n",
    "#     def set_optimizer(self):\n",
    "#         self.optimizer = torch.optim.NAdam(\n",
    "#             params=self.network.parameters(),\n",
    "#             lr=self.lr,\n",
    "#             weight_decay=1e-3\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         Qs = self.network(x)\n",
    "#         return Qs\n",
    "    \n",
    "#     def forward_target_network(self, x):\n",
    "#         Qs = self.target_network(x)\n",
    "#         return Qs\n",
    "    \n",
    "#     def get_argmax_action(self, x):\n",
    "#         # transform torch tensor with two dimentional matrix\n",
    "#         s = torch.from_numpy(x).reshape(1, -1).float()\n",
    "#         Qs = self.forward(s)\n",
    "#         # get item(integer) in argmax tensor\n",
    "#         argmax_action = Qs.argmax(dim=-1).item()\n",
    "#         return argmax_action\n",
    "    \n",
    "#     def train(self):\n",
    "#         print(self.replay_memory.getsize())\n",
    "#         transitions = self.replay_memory.sample(self.batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        \n",
    "#         states_array = np.stack(states, axis=0)                     # (n_batch, states)\n",
    "#         actions_array = np.stack(actions, axis=0, dtype=np.int64)   # (n_batch)\n",
    "#         rewards_array = np.stack(rewards, axis=0)                   # (n_batch)\n",
    "#         next_states_array = np.stack(next_states, axis=0)           # (n_batch, states)\n",
    "#         dones_array = np.stack(dones, axis=0)                       # (n_batch)\n",
    "        \n",
    "#         states_tensor = torch.from_numpy(states_array).float()\n",
    "#         actions_tensor = torch.from_numpy(actions_array)\n",
    "#         rewards_tensor = torch.from_numpy(rewards_array).float()\n",
    "#         next_states_tensor = torch.from_numpy(next_states_array).float()\n",
    "#         dones_tensor = torch.from_numpy(dones_array).float()\n",
    "        \n",
    "#         Qs = self.forward(states_tensor)\n",
    "#         next_Qs = self.forward_target_network(next_states_tensor)\n",
    "        \n",
    "#         chosen_Q = Qs.gather(dim=-1, index=actions_tensor.reshape(-1, 1)).reshape(-1)   # (n_batch, 1) -> (n_batch)\n",
    "#         target_Q = rewards_tensor + (1 - dones_tensor) * self.discount_factor * next_Qs.max(dim=-1).values\n",
    "        \n",
    "#         # loss function\n",
    "#         criterion = nn.SmoothL1Loss()\n",
    "#         loss = criterion(chosen_Q, target_Q)\n",
    "        \n",
    "#         # update by gradient descent\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "        \n",
    "#         return loss.item()    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'AAPL'\n",
      "                    AND date BETWEEN '2018-01-01' AND '2023-12-31' \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "dqn = DQNLearner(stock_code=stock_code, fro='2018-01-01', to='2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.38750076293945,\n",
       " 42.622501373291016,\n",
       " 43.14250183105469,\n",
       " 42.73749923706055,\n",
       " 127594400.0,\n",
       " 42.3385009765625,\n",
       " 175292480.0,\n",
       " 0.009424005368516047,\n",
       " -0.2721056830275891,\n",
       " -0.008189493544760023,\n",
       " 0.0026611291070369274,\n",
       " -0.002690795339513195,\n",
       " 0.00947651596897686,\n",
       " 0.010940217492328647,\n",
       " -0.10466855845311374,\n",
       " 43.52825088500977,\n",
       " 170264920.0,\n",
       " -0.018166400713830202,\n",
       " -0.25061251607201296,\n",
       " 44.43225040435791,\n",
       " 173363500.0,\n",
       " -0.03814236622890343,\n",
       " -0.26400655270573103,\n",
       " 51.03562507629395,\n",
       " 158317206.66666666,\n",
       " -0.16259477231499375,\n",
       " -0.1940585443207878,\n",
       " 51.0888960202535,\n",
       " 133001283.33333333,\n",
       " -0.16346794379510873,\n",
       " -0.04065286588086803,\n",
       " 47.61911471684774,\n",
       " 133074575.0,\n",
       " -0.10251378062809866,\n",
       " -0.04118123240295902,\n",
       " 48.18011567132509,\n",
       " 40.68438513739073,\n",
       " 0.27390447006800017,\n",
       " 0.16870022260225612,\n",
       " -2.821814528255821,\n",
       " -2.0390696683620253,\n",
       " -0.7827448598937958,\n",
       " 0.5614399715311799,\n",
       " 35.956551757838014,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.build_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "def run_trader(mode='train', \n",
    "               stock_name=None, stock_code=None, fro='2018-01-01', to='2023-12-31',\n",
    "               lr=0.01, discount_factor=0.9,\n",
    "               initial_balance=1000000, min_trading_price=1, max_trading_price=10000,\n",
    "               num_epochs=1000, num_steps=20):\n",
    "    \n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.path.pardir))\n",
    "    # os.environ['RLTRADER_BASE'] = 'C:\\project\\github\\projects\\trader'\n",
    "    \n",
    "    ''' \n",
    "    Arguments\n",
    "    ----------\n",
    "    mode : 'train', 'test', 'update', 'predict\n",
    "    '''\n",
    "    \n",
    "    # learner's parameter\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    output_name = f'{mode}_{stock_name}_dqn_cnn_{now}'\n",
    "    learning = mode in ['train', 'update']\n",
    "    \n",
    "    # use model flag\n",
    "    reuse_models = mode in ['test', 'update', 'predict']\n",
    "    \n",
    "    # value network file\n",
    "    value_network_name = f'{stock_name}_dqn_cnn_value.mdl'\n",
    "    eps_init = 1 if mode in ['train', 'update'] else 0\n",
    "    num_epochs = num_epochs if mode in ['train', 'update'] else 0\n",
    "    \n",
    "    # output path\n",
    "    output_path = os.path.join(BASE_DIR, 'trader', 'output', output_name)\n",
    "    if not os.path.isdir(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    # log parameters\n",
    "    params = {\n",
    "        'mode': mode,\n",
    "        'stock_names': stock_name,\n",
    "        'rl_method': 'dqn',\n",
    "        'net': 'cnn',\n",
    "        'start_date': fro,\n",
    "        'end_date': to,\n",
    "        'lr': lr,\n",
    "        'discount_factor': discount_factor,\n",
    "        'initial_balance': initial_balance,\n",
    "        'stock_codes': stock_code,\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_path, 'params.json'), 'w') as f:\n",
    "        f.write(str(params))\n",
    "        \n",
    "    # model_path\n",
    "    value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "    \n",
    "    # setting for logging\n",
    "    # log level DEBUG < INFO < WARNING < CRITICAL. more than DEBUG\n",
    "    log_path = os.path.join(output_path, f'{output_path}.log')\n",
    "    if os.path.exists(log_path):\n",
    "        os.remove(log_path)\n",
    "    logging.basicConfig(format='%(message)s')\n",
    "    logger = logging.getLogger(LOGGER_NAME)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.propagate = False\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "        \n",
    "    # minimum and maximum trading price policy\n",
    "    min_trading_price = min_trading_price\n",
    "    max_trading_price = max_trading_price\n",
    "        \n",
    "        \n",
    "    # start reinforcement learning\n",
    "    learner = DQNLearner(\n",
    "        stock_code=stock_code, fro=fro, to= to,\n",
    "        min_trading_price=min_trading_price, max_trading_price=max_trading_price,\n",
    "        num_steps=num_steps, lr=lr, discount_factor=discount_factor, num_epochs=num_epochs,\n",
    "        balance=initial_balance, eps_init=eps_init, \n",
    "        value_network_path=value_network_path,\n",
    "        output_path=output_path, reuse_models=reuse_models\n",
    "    )\n",
    "        \n",
    "    if mode in ['train', 'test', 'update']:\n",
    "        learner.run(learning=learning)\n",
    "        \n",
    "        if mode in ['train', 'update']:\n",
    "            learner.save_models()\n",
    "    \n",
    "    elif mode == 'predict':\n",
    "        learner.predict()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                    SELECT * FROM price_global\n",
      "                    WHERE ticker = 'MSFT'\n",
      "                    AND date BETWEEN '2018-01-01' AND '2023-12-31' \n",
      "                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 199/10000 [03:27<2:42:23,  1.01it/s]C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_3900\\415254748.py:167: UserWarning: The figure layout has changed to tight\n",
      "  self.fig.tight_layout()\n",
      " 53%|    | 5267/10000 [1:28:48<1:16:01,  1.04it/s]"
     ]
    }
   ],
   "source": [
    "run_trader(stock_name='Apple', stock_code='MSFT', num_epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset replay buffer\n",
    "# init_replay_buffer_size = int(learner.replay_memory.replay_capacity * learner.replay_memory.replay_init_ratio)\n",
    "# s = agent.reset()\n",
    "# step_count = 0\n",
    "\n",
    "# # create initial replay buffer\n",
    "# for _ in range(learner.replay_memory.replay_capacity):\n",
    "#     a = np.random.choice(agent.NUM_ACTIONS)     # uniform random action\n",
    "#     confidence = np.random.random()\n",
    "#     s_next, r, done, info = agent.step(a, confidence=confidence)\n",
    "#     step_count += 1\n",
    "#     s = s_next\n",
    "#     if done:\n",
    "#         env.reset()\n",
    "#         agent.reset()\n",
    "#         step_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train agent\n",
    "# train_env_steps = 20 * 12 * 3\n",
    "# target_update_period = 20 * 3\n",
    "# env.reset()\n",
    "# s = agent.reset()\n",
    "# step_count = 0\n",
    "# for step_train in range(train_env_steps):\n",
    "#     eps = get_eps(step=step_train)\n",
    "#     is_random_action = np.random.choice(2, p=[1 - eps, eps])\n",
    "#     if is_random_action:\n",
    "#         a = np.random.choice(agent.NUM_ACTIONS)     # uniform random action\n",
    "#     else:\n",
    "#         a = learner.get_argmax_action(s)\n",
    "        \n",
    "#     confidence = np.random.random()\n",
    "#     s_next, r, done, info = agent.step(a, confidence)\n",
    "#     step_count += 1\n",
    "    \n",
    "#     transition = (s, a, r, s_next, done)\n",
    "#     learner.replay_memory.append(transition)\n",
    "    \n",
    "#     s = s_next\n",
    "#     if done:\n",
    "#         env.reset()\n",
    "#         s = agent.reset()\n",
    "#         step_count = 0\n",
    "        \n",
    "#     # target update period\n",
    "#     if step_train % target_update_period == 0:\n",
    "#         learner.update_target_network()\n",
    "        \n",
    "#     # train every 5 steps\n",
    "#     if step_train % 5 == 0:\n",
    "#         loss = learner.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
