{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 Y와 예측치 시각화\n",
    "def plot_prediction(Y_true_pred):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(Y_true_pred, linewidth=5, label=Y_true_pred.columns)\n",
    "    plt.xticks(fontsize=25, rotation=0)\n",
    "    plt.yticks(fontsize=25)\n",
    "    plt.xlabel('Index', fontname='serif', fontsize=28)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(X_Data, Y_Pred, Residual, graph_on=False):\n",
    "    if graph_on == True:\n",
    "        ##### 시각화\n",
    "        # 잔차의 정규본포성 확인\n",
    "        # sns.displot(Residual, norm_hist='True', fit=stats.norm)\n",
    "        sns.displot(Residual, kind='hist')\n",
    "        plt.show()\n",
    "\n",
    "        # 잔차의 등분산성 확인\n",
    "        temp = pd.concat([Y_Pred, Residual.reset_index().iloc[:,[1]]], axis=1)\n",
    "        sns.scatterplot(x='Pred', y='Error', data=temp)\n",
    "        plt.show()\n",
    "\n",
    "        # 잔차의 자기상관성 확인\n",
    "        sm.graphics.tsa.plot_acf(Residual, lags=50, use_vlines=True)\n",
    "        plt.show()\n",
    "\n",
    "    ##### 통계량\n",
    "    # 정규분포\n",
    "    # Null Hypothesis: The residuals are normally distributed\n",
    "    Normality = pd.DataFrame([stats.shapiro(Residual)],\n",
    "                             index=['Normality'], columns=['Test Statistics', 'p-value']).T\n",
    "\n",
    "    # 등분산성\n",
    "    # Null Hypothesis: Error terms are homoscedastic\n",
    "    Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Residual, X_Data.values, alternative='two-sided')],\n",
    "                                      index=['Heteroscedasticity'],\n",
    "                                      columns=['Test Statistics', 'p-value', 'Alternative']).T\n",
    "\n",
    "    # 자기상관\n",
    "    # Null Hypothesis: Autocorrelation is absent\n",
    "    Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Residual, lags=[10,50]).iloc[:,0]),\n",
    "                             pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Residual, lags=[10,50]).iloc[:,1])], axis=1).T\n",
    "    Autocorrelation.index = ['Test Statistics', 'p-value']\n",
    "    Autocorrelation.columns = ['Autocorr(lag10)', 'Autocorr(lag50)']\n",
    "\n",
    "    Error_Analysis = pd.concat([Normality, Heteroscedasticity, Autocorrelation], join='outer', axis=1)\n",
    "\n",
    "    return Error_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수화\n",
    "def evaluation_reg(Y_real, Y_pred):\n",
    "    MAE = mean_absolute_error(Y_real, Y_pred)\n",
    "    MSE = mean_squared_error(Y_real, Y_pred)\n",
    "    MAPE = mean_absolute_percentage_error(Y_real, Y_pred)\n",
    "    Score = pd.DataFrame([MAE, MSE, MAPE], index=['MAE', 'MSE', 'MAPE'], columns=['Score']).T\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test 모두의 검증 함수화\n",
    "def evaluation_reg_trte(Y_real_tr, Y_pred_tr, Y_real_te, Y_pred_te):\n",
    "    Score_tr = evaluation_reg(Y_real_tr, Y_pred_tr)\n",
    "    Score_te = evaluation_reg(Y_real_te, Y_pred_te)\n",
    "    Score_trte = pd.concat([Score_tr, Score_te], axis=0)\n",
    "    Score_trte.index = ['Train', 'Test']\n",
    "\n",
    "    return Score_trte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prices(code, name):\n",
    "    ticker = yf.Ticker(code)\n",
    "    data = ticker.history(period=\"max\")[[\"Close\"]]\n",
    "    data.rename(columns={\"Close\":name}, inplace=True)\n",
    "    data.index = pd.to_datetime(data.index).strftime('%Y-%m-%d')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and names\n",
    "# Code and names\n",
    "apple = [\"AAPL\", \"Apple\"]\n",
    "tesla = [\"TSLA\", \"Tesla\"]\n",
    "bond = [\"^FVX\", \"bond_rate\"]\n",
    "dow = [\"^DJI\", \"Dow\"]\n",
    "tech_index = [\"XLK\", \"tech_index\"]\n",
    "health_index = [\"XLV\", \"health_index\"]\n",
    "consumer_dis_index = [\"XLY\", \"consumer_dis_index\"]\n",
    "consumer_staples_index = [\"XLP\", \"consumer_staples_index\"]\n",
    "energy_index = [\"XLE\", \"energy_index\"]\n",
    "financial_index = [\"XLF\", 'financial_index']\n",
    "industrial_index = [\"XLI\", \"industrial_index\"]\n",
    "material_index = [\"XLB\", \"material_index\"]\n",
    "real_estate_index = [\"XLRE\", \"real_estate_index\"]\n",
    "utilities_index = [\"XLU\", \"utilities_index\"]\n",
    "jpyusd = [\"JPYUSD=X\", \"JPYUSD\"]\n",
    "cnyusd = [\"CNYUSD=X\", \"CNYUSD\"]\n",
    "eurusd = [\"EURUSD=X\", \"EURUSD\"]\n",
    "\n",
    "data_lists = [apple, tesla, bond, dow, tech_index, health_index, consumer_dis_index, consumer_staples_index,\n",
    "              energy_index, financial_index, industrial_index, material_index, real_estate_index,\n",
    "              utilities_index, jpyusd, cnyusd, eurusd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>bond_rate</th>\n",
       "      <th>Dow</th>\n",
       "      <th>tech_index</th>\n",
       "      <th>health_index</th>\n",
       "      <th>consumer_dis_index</th>\n",
       "      <th>consumer_staples_index</th>\n",
       "      <th>energy_index</th>\n",
       "      <th>financial_index</th>\n",
       "      <th>industrial_index</th>\n",
       "      <th>material_index</th>\n",
       "      <th>real_estate_index</th>\n",
       "      <th>utilities_index</th>\n",
       "      <th>JPYUSD</th>\n",
       "      <th>CNYUSD</th>\n",
       "      <th>EURUSD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>0.099450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.094261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.087343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.089504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.092099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>175.740005</td>\n",
       "      <td>276.040009</td>\n",
       "      <td>4.419</td>\n",
       "      <td>34907.109375</td>\n",
       "      <td>171.842941</td>\n",
       "      <td>132.695724</td>\n",
       "      <td>174.469681</td>\n",
       "      <td>72.160965</td>\n",
       "      <td>92.676109</td>\n",
       "      <td>34.854198</td>\n",
       "      <td>105.181526</td>\n",
       "      <td>82.233582</td>\n",
       "      <td>36.637894</td>\n",
       "      <td>64.695755</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.137574</td>\n",
       "      <td>1.073422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>175.009995</td>\n",
       "      <td>274.390015</td>\n",
       "      <td>4.453</td>\n",
       "      <td>34618.238281</td>\n",
       "      <td>168.539993</td>\n",
       "      <td>131.650009</td>\n",
       "      <td>171.484985</td>\n",
       "      <td>71.565002</td>\n",
       "      <td>91.335999</td>\n",
       "      <td>34.675003</td>\n",
       "      <td>104.573997</td>\n",
       "      <td>81.318001</td>\n",
       "      <td>36.499001</td>\n",
       "      <td>64.418007</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.137440</td>\n",
       "      <td>1.063717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>177.970001</td>\n",
       "      <td>265.279999</td>\n",
       "      <td>4.462</td>\n",
       "      <td>34624.300781</td>\n",
       "      <td>169.350006</td>\n",
       "      <td>131.449997</td>\n",
       "      <td>169.619995</td>\n",
       "      <td>71.540001</td>\n",
       "      <td>92.110001</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>104.690002</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>36.180000</td>\n",
       "      <td>64.370003</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.137478</td>\n",
       "      <td>1.066826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>179.070007</td>\n",
       "      <td>266.500000</td>\n",
       "      <td>4.521</td>\n",
       "      <td>34517.730469</td>\n",
       "      <td>169.259995</td>\n",
       "      <td>131.559998</td>\n",
       "      <td>168.770004</td>\n",
       "      <td>71.370003</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>104.250000</td>\n",
       "      <td>80.910004</td>\n",
       "      <td>35.990002</td>\n",
       "      <td>64.019997</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.137163</td>\n",
       "      <td>1.069267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>175.490005</td>\n",
       "      <td>262.589996</td>\n",
       "      <td>4.515</td>\n",
       "      <td>34440.878906</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>131.570007</td>\n",
       "      <td>167.089996</td>\n",
       "      <td>71.440002</td>\n",
       "      <td>90.400002</td>\n",
       "      <td>34.490002</td>\n",
       "      <td>103.839996</td>\n",
       "      <td>80.059998</td>\n",
       "      <td>36.060001</td>\n",
       "      <td>64.080002</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.137069</td>\n",
       "      <td>1.068205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10783 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Apple       Tesla  bond_rate           Dow  tech_index  \\\n",
       "Date                                                                      \n",
       "1980-12-12    0.099450         NaN     13.630           NaN         NaN   \n",
       "1980-12-15    0.094261         NaN     13.750           NaN         NaN   \n",
       "1980-12-16    0.087343         NaN     14.040           NaN         NaN   \n",
       "1980-12-17    0.089504         NaN     13.870           NaN         NaN   \n",
       "1980-12-18    0.092099         NaN     13.790           NaN         NaN   \n",
       "...                ...         ...        ...           ...         ...   \n",
       "2023-09-14  175.740005  276.040009      4.419  34907.109375  171.842941   \n",
       "2023-09-15  175.009995  274.390015      4.453  34618.238281  168.539993   \n",
       "2023-09-18  177.970001  265.279999      4.462  34624.300781  169.350006   \n",
       "2023-09-19  179.070007  266.500000      4.521  34517.730469  169.259995   \n",
       "2023-09-20  175.490005  262.589996      4.515  34440.878906  166.600006   \n",
       "\n",
       "            health_index  consumer_dis_index  consumer_staples_index  \\\n",
       "Date                                                                   \n",
       "1980-12-12           NaN                 NaN                     NaN   \n",
       "1980-12-15           NaN                 NaN                     NaN   \n",
       "1980-12-16           NaN                 NaN                     NaN   \n",
       "1980-12-17           NaN                 NaN                     NaN   \n",
       "1980-12-18           NaN                 NaN                     NaN   \n",
       "...                  ...                 ...                     ...   \n",
       "2023-09-14    132.695724          174.469681               72.160965   \n",
       "2023-09-15    131.650009          171.484985               71.565002   \n",
       "2023-09-18    131.449997          169.619995               71.540001   \n",
       "2023-09-19    131.559998          168.770004               71.370003   \n",
       "2023-09-20    131.570007          167.089996               71.440002   \n",
       "\n",
       "            energy_index  financial_index  industrial_index  material_index  \\\n",
       "Date                                                                          \n",
       "1980-12-12           NaN              NaN               NaN             NaN   \n",
       "1980-12-15           NaN              NaN               NaN             NaN   \n",
       "1980-12-16           NaN              NaN               NaN             NaN   \n",
       "1980-12-17           NaN              NaN               NaN             NaN   \n",
       "1980-12-18           NaN              NaN               NaN             NaN   \n",
       "...                  ...              ...               ...             ...   \n",
       "2023-09-14     92.676109        34.854198        105.181526       82.233582   \n",
       "2023-09-15     91.335999        34.675003        104.573997       81.318001   \n",
       "2023-09-18     92.110001        34.770000        104.690002       80.980003   \n",
       "2023-09-19     91.250000        34.730000        104.250000       80.910004   \n",
       "2023-09-20     90.400002        34.490002        103.839996       80.059998   \n",
       "\n",
       "            real_estate_index  utilities_index    JPYUSD    CNYUSD    EURUSD  \n",
       "Date                                                                          \n",
       "1980-12-12                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-15                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-16                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-17                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-18                NaN              NaN       NaN       NaN       NaN  \n",
       "...                       ...              ...       ...       ...       ...  \n",
       "2023-09-14          36.637894        64.695755  0.006787  0.137574  1.073422  \n",
       "2023-09-15          36.499001        64.418007  0.006780  0.137440  1.063717  \n",
       "2023-09-18          36.180000        64.370003  0.006763  0.137478  1.066826  \n",
       "2023-09-19          35.990002        64.019997  0.006775  0.137163  1.069267  \n",
       "2023-09-20          36.060001        64.080002  0.006769  0.137069  1.068205  \n",
       "\n",
       "[10783 rows x 17 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for list in data_lists:\n",
    "    code = list[0]\n",
    "    name = list[1]\n",
    "    data = get_prices(code, name)\n",
    "    df[name] = data\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>bond_rate</th>\n",
       "      <th>Dow</th>\n",
       "      <th>tech_index</th>\n",
       "      <th>health_index</th>\n",
       "      <th>consumer_dis_index</th>\n",
       "      <th>consumer_staples_index</th>\n",
       "      <th>energy_index</th>\n",
       "      <th>financial_index</th>\n",
       "      <th>industrial_index</th>\n",
       "      <th>material_index</th>\n",
       "      <th>real_estate_index</th>\n",
       "      <th>utilities_index</th>\n",
       "      <th>JPYUSD</th>\n",
       "      <th>CNYUSD</th>\n",
       "      <th>EURUSD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>0.099450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.094261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.087343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.089504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.092099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-14</th>\n",
       "      <td>175.740005</td>\n",
       "      <td>276.040009</td>\n",
       "      <td>4.419</td>\n",
       "      <td>34907.109375</td>\n",
       "      <td>171.842941</td>\n",
       "      <td>132.695724</td>\n",
       "      <td>174.469681</td>\n",
       "      <td>72.160965</td>\n",
       "      <td>92.676109</td>\n",
       "      <td>34.854198</td>\n",
       "      <td>105.181526</td>\n",
       "      <td>82.233582</td>\n",
       "      <td>36.637894</td>\n",
       "      <td>64.695755</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.137574</td>\n",
       "      <td>1.073422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-15</th>\n",
       "      <td>175.009995</td>\n",
       "      <td>274.390015</td>\n",
       "      <td>4.453</td>\n",
       "      <td>34618.238281</td>\n",
       "      <td>168.539993</td>\n",
       "      <td>131.650009</td>\n",
       "      <td>171.484985</td>\n",
       "      <td>71.565002</td>\n",
       "      <td>91.335999</td>\n",
       "      <td>34.675003</td>\n",
       "      <td>104.573997</td>\n",
       "      <td>81.318001</td>\n",
       "      <td>36.499001</td>\n",
       "      <td>64.418007</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.137440</td>\n",
       "      <td>1.063717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-18</th>\n",
       "      <td>177.970001</td>\n",
       "      <td>265.279999</td>\n",
       "      <td>4.462</td>\n",
       "      <td>34624.300781</td>\n",
       "      <td>169.350006</td>\n",
       "      <td>131.449997</td>\n",
       "      <td>169.619995</td>\n",
       "      <td>71.540001</td>\n",
       "      <td>92.110001</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>104.690002</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>36.180000</td>\n",
       "      <td>64.370003</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.137478</td>\n",
       "      <td>1.066826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-19</th>\n",
       "      <td>179.070007</td>\n",
       "      <td>266.500000</td>\n",
       "      <td>4.521</td>\n",
       "      <td>34517.730469</td>\n",
       "      <td>169.259995</td>\n",
       "      <td>131.559998</td>\n",
       "      <td>168.770004</td>\n",
       "      <td>71.370003</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>104.250000</td>\n",
       "      <td>80.910004</td>\n",
       "      <td>35.990002</td>\n",
       "      <td>64.019997</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.137163</td>\n",
       "      <td>1.069267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-20</th>\n",
       "      <td>175.490005</td>\n",
       "      <td>262.589996</td>\n",
       "      <td>4.515</td>\n",
       "      <td>34440.878906</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>131.570007</td>\n",
       "      <td>167.089996</td>\n",
       "      <td>71.440002</td>\n",
       "      <td>90.400002</td>\n",
       "      <td>34.490002</td>\n",
       "      <td>103.839996</td>\n",
       "      <td>80.059998</td>\n",
       "      <td>36.060001</td>\n",
       "      <td>64.080002</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.137069</td>\n",
       "      <td>1.068205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10783 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Apple       Tesla  bond_rate           Dow  tech_index  \\\n",
       "Date                                                                      \n",
       "1980-12-12    0.099450         NaN     13.630           NaN         NaN   \n",
       "1980-12-15    0.094261         NaN     13.750           NaN         NaN   \n",
       "1980-12-16    0.087343         NaN     14.040           NaN         NaN   \n",
       "1980-12-17    0.089504         NaN     13.870           NaN         NaN   \n",
       "1980-12-18    0.092099         NaN     13.790           NaN         NaN   \n",
       "...                ...         ...        ...           ...         ...   \n",
       "2023-09-14  175.740005  276.040009      4.419  34907.109375  171.842941   \n",
       "2023-09-15  175.009995  274.390015      4.453  34618.238281  168.539993   \n",
       "2023-09-18  177.970001  265.279999      4.462  34624.300781  169.350006   \n",
       "2023-09-19  179.070007  266.500000      4.521  34517.730469  169.259995   \n",
       "2023-09-20  175.490005  262.589996      4.515  34440.878906  166.600006   \n",
       "\n",
       "            health_index  consumer_dis_index  consumer_staples_index  \\\n",
       "Date                                                                   \n",
       "1980-12-12           NaN                 NaN                     NaN   \n",
       "1980-12-15           NaN                 NaN                     NaN   \n",
       "1980-12-16           NaN                 NaN                     NaN   \n",
       "1980-12-17           NaN                 NaN                     NaN   \n",
       "1980-12-18           NaN                 NaN                     NaN   \n",
       "...                  ...                 ...                     ...   \n",
       "2023-09-14    132.695724          174.469681               72.160965   \n",
       "2023-09-15    131.650009          171.484985               71.565002   \n",
       "2023-09-18    131.449997          169.619995               71.540001   \n",
       "2023-09-19    131.559998          168.770004               71.370003   \n",
       "2023-09-20    131.570007          167.089996               71.440002   \n",
       "\n",
       "            energy_index  financial_index  industrial_index  material_index  \\\n",
       "Date                                                                          \n",
       "1980-12-12           NaN              NaN               NaN             NaN   \n",
       "1980-12-15           NaN              NaN               NaN             NaN   \n",
       "1980-12-16           NaN              NaN               NaN             NaN   \n",
       "1980-12-17           NaN              NaN               NaN             NaN   \n",
       "1980-12-18           NaN              NaN               NaN             NaN   \n",
       "...                  ...              ...               ...             ...   \n",
       "2023-09-14     92.676109        34.854198        105.181526       82.233582   \n",
       "2023-09-15     91.335999        34.675003        104.573997       81.318001   \n",
       "2023-09-18     92.110001        34.770000        104.690002       80.980003   \n",
       "2023-09-19     91.250000        34.730000        104.250000       80.910004   \n",
       "2023-09-20     90.400002        34.490002        103.839996       80.059998   \n",
       "\n",
       "            real_estate_index  utilities_index    JPYUSD    CNYUSD    EURUSD  \n",
       "Date                                                                          \n",
       "1980-12-12                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-15                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-16                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-17                NaN              NaN       NaN       NaN       NaN  \n",
       "1980-12-18                NaN              NaN       NaN       NaN       NaN  \n",
       "...                       ...              ...       ...       ...       ...  \n",
       "2023-09-14          36.637894        64.695755  0.006787  0.137574  1.073422  \n",
       "2023-09-15          36.499001        64.418007  0.006780  0.137440  1.063717  \n",
       "2023-09-18          36.180000        64.370003  0.006763  0.137478  1.066826  \n",
       "2023-09-19          35.990002        64.019997  0.006775  0.137163  1.069267  \n",
       "2023-09-20          36.060001        64.080002  0.006769  0.137069  1.068205  \n",
       "\n",
       "[10783 rows x 17 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>bond_rate</th>\n",
       "      <th>Dow</th>\n",
       "      <th>tech_index</th>\n",
       "      <th>health_index</th>\n",
       "      <th>consumer_dis_index</th>\n",
       "      <th>consumer_staples_index</th>\n",
       "      <th>energy_index</th>\n",
       "      <th>financial_index</th>\n",
       "      <th>industrial_index</th>\n",
       "      <th>material_index</th>\n",
       "      <th>real_estate_index</th>\n",
       "      <th>utilities_index</th>\n",
       "      <th>JPYUSD</th>\n",
       "      <th>CNYUSD</th>\n",
       "      <th>EURUSD</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-12-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.092099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-12-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10778</th>\n",
       "      <td>175.740005</td>\n",
       "      <td>276.040009</td>\n",
       "      <td>4.419</td>\n",
       "      <td>34907.109375</td>\n",
       "      <td>171.842941</td>\n",
       "      <td>132.695724</td>\n",
       "      <td>174.469681</td>\n",
       "      <td>72.160965</td>\n",
       "      <td>92.676109</td>\n",
       "      <td>34.854198</td>\n",
       "      <td>105.181526</td>\n",
       "      <td>82.233582</td>\n",
       "      <td>36.637894</td>\n",
       "      <td>64.695755</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.137574</td>\n",
       "      <td>1.073422</td>\n",
       "      <td>2023-09-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779</th>\n",
       "      <td>175.009995</td>\n",
       "      <td>274.390015</td>\n",
       "      <td>4.453</td>\n",
       "      <td>34618.238281</td>\n",
       "      <td>168.539993</td>\n",
       "      <td>131.650009</td>\n",
       "      <td>171.484985</td>\n",
       "      <td>71.565002</td>\n",
       "      <td>91.335999</td>\n",
       "      <td>34.675003</td>\n",
       "      <td>104.573997</td>\n",
       "      <td>81.318001</td>\n",
       "      <td>36.499001</td>\n",
       "      <td>64.418007</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.137440</td>\n",
       "      <td>1.063717</td>\n",
       "      <td>2023-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10780</th>\n",
       "      <td>177.970001</td>\n",
       "      <td>265.279999</td>\n",
       "      <td>4.462</td>\n",
       "      <td>34624.300781</td>\n",
       "      <td>169.350006</td>\n",
       "      <td>131.449997</td>\n",
       "      <td>169.619995</td>\n",
       "      <td>71.540001</td>\n",
       "      <td>92.110001</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>104.690002</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>36.180000</td>\n",
       "      <td>64.370003</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.137478</td>\n",
       "      <td>1.066826</td>\n",
       "      <td>2023-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10781</th>\n",
       "      <td>179.070007</td>\n",
       "      <td>266.500000</td>\n",
       "      <td>4.521</td>\n",
       "      <td>34517.730469</td>\n",
       "      <td>169.259995</td>\n",
       "      <td>131.559998</td>\n",
       "      <td>168.770004</td>\n",
       "      <td>71.370003</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>104.250000</td>\n",
       "      <td>80.910004</td>\n",
       "      <td>35.990002</td>\n",
       "      <td>64.019997</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.137163</td>\n",
       "      <td>1.069267</td>\n",
       "      <td>2023-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10782</th>\n",
       "      <td>175.490005</td>\n",
       "      <td>262.589996</td>\n",
       "      <td>4.515</td>\n",
       "      <td>34440.878906</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>131.570007</td>\n",
       "      <td>167.089996</td>\n",
       "      <td>71.440002</td>\n",
       "      <td>90.400002</td>\n",
       "      <td>34.490002</td>\n",
       "      <td>103.839996</td>\n",
       "      <td>80.059998</td>\n",
       "      <td>36.060001</td>\n",
       "      <td>64.080002</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.137069</td>\n",
       "      <td>1.068205</td>\n",
       "      <td>2023-09-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10783 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Apple       Tesla  bond_rate           Dow  tech_index  \\\n",
       "0        0.099450         NaN     13.630           NaN         NaN   \n",
       "1        0.094261         NaN     13.750           NaN         NaN   \n",
       "2        0.087343         NaN     14.040           NaN         NaN   \n",
       "3        0.089504         NaN     13.870           NaN         NaN   \n",
       "4        0.092099         NaN     13.790           NaN         NaN   \n",
       "...           ...         ...        ...           ...         ...   \n",
       "10778  175.740005  276.040009      4.419  34907.109375  171.842941   \n",
       "10779  175.009995  274.390015      4.453  34618.238281  168.539993   \n",
       "10780  177.970001  265.279999      4.462  34624.300781  169.350006   \n",
       "10781  179.070007  266.500000      4.521  34517.730469  169.259995   \n",
       "10782  175.490005  262.589996      4.515  34440.878906  166.600006   \n",
       "\n",
       "       health_index  consumer_dis_index  consumer_staples_index  energy_index  \\\n",
       "0               NaN                 NaN                     NaN           NaN   \n",
       "1               NaN                 NaN                     NaN           NaN   \n",
       "2               NaN                 NaN                     NaN           NaN   \n",
       "3               NaN                 NaN                     NaN           NaN   \n",
       "4               NaN                 NaN                     NaN           NaN   \n",
       "...             ...                 ...                     ...           ...   \n",
       "10778    132.695724          174.469681               72.160965     92.676109   \n",
       "10779    131.650009          171.484985               71.565002     91.335999   \n",
       "10780    131.449997          169.619995               71.540001     92.110001   \n",
       "10781    131.559998          168.770004               71.370003     91.250000   \n",
       "10782    131.570007          167.089996               71.440002     90.400002   \n",
       "\n",
       "       financial_index  industrial_index  material_index  real_estate_index  \\\n",
       "0                  NaN               NaN             NaN                NaN   \n",
       "1                  NaN               NaN             NaN                NaN   \n",
       "2                  NaN               NaN             NaN                NaN   \n",
       "3                  NaN               NaN             NaN                NaN   \n",
       "4                  NaN               NaN             NaN                NaN   \n",
       "...                ...               ...             ...                ...   \n",
       "10778        34.854198        105.181526       82.233582          36.637894   \n",
       "10779        34.675003        104.573997       81.318001          36.499001   \n",
       "10780        34.770000        104.690002       80.980003          36.180000   \n",
       "10781        34.730000        104.250000       80.910004          35.990002   \n",
       "10782        34.490002        103.839996       80.059998          36.060001   \n",
       "\n",
       "       utilities_index    JPYUSD    CNYUSD    EURUSD       date  \n",
       "0                  NaN       NaN       NaN       NaN 1980-12-12  \n",
       "1                  NaN       NaN       NaN       NaN 1980-12-15  \n",
       "2                  NaN       NaN       NaN       NaN 1980-12-16  \n",
       "3                  NaN       NaN       NaN       NaN 1980-12-17  \n",
       "4                  NaN       NaN       NaN       NaN 1980-12-18  \n",
       "...                ...       ...       ...       ...        ...  \n",
       "10778        64.695755  0.006787  0.137574  1.073422 2023-09-14  \n",
       "10779        64.418007  0.006780  0.137440  1.063717 2023-09-15  \n",
       "10780        64.370003  0.006763  0.137478  1.066826 2023-09-18  \n",
       "10781        64.019997  0.006775  0.137163  1.069267 2023-09-19  \n",
       "10782        64.080002  0.006769  0.137069  1.068205 2023-09-20  \n",
       "\n",
       "[10783 rows x 18 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.copy()\n",
    "\n",
    "data['date'] = pd.to_datetime(data.index)\n",
    "data = data.reset_index().drop([\"Date\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time index\n",
    "# data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "# data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# data[\"time_idx\"] = data.index\n",
    "data[\"country\"] = \"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_apple\"] = np.log(data.Apple)\n",
    "data[\"log_tesla\"] = np.log(data.Tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1995 entries, 8782 to 10782\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   Apple                   1995 non-null   float64       \n",
      " 1   Tesla                   1995 non-null   float64       \n",
      " 2   bond_rate               1995 non-null   float64       \n",
      " 3   Dow                     1995 non-null   float64       \n",
      " 4   tech_index              1995 non-null   float64       \n",
      " 5   health_index            1995 non-null   float64       \n",
      " 6   consumer_dis_index      1995 non-null   float64       \n",
      " 7   consumer_staples_index  1995 non-null   float64       \n",
      " 8   energy_index            1995 non-null   float64       \n",
      " 9   financial_index         1995 non-null   float64       \n",
      " 10  industrial_index        1995 non-null   float64       \n",
      " 11  material_index          1995 non-null   float64       \n",
      " 12  real_estate_index       1995 non-null   float64       \n",
      " 13  utilities_index         1995 non-null   float64       \n",
      " 14  JPYUSD                  1995 non-null   float64       \n",
      " 15  CNYUSD                  1995 non-null   float64       \n",
      " 16  EURUSD                  1995 non-null   float64       \n",
      " 17  date                    1995 non-null   datetime64[ns]\n",
      " 18  country                 1995 non-null   object        \n",
      " 19  month                   1995 non-null   category      \n",
      " 20  log_apple               1995 non-null   float64       \n",
      " 21  log_tesla               1995 non-null   float64       \n",
      "dtypes: category(1), datetime64[ns](1), float64(19), object(1)\n",
      "memory usage: 345.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apple</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>bond_rate</th>\n",
       "      <th>Dow</th>\n",
       "      <th>tech_index</th>\n",
       "      <th>health_index</th>\n",
       "      <th>consumer_dis_index</th>\n",
       "      <th>consumer_staples_index</th>\n",
       "      <th>energy_index</th>\n",
       "      <th>financial_index</th>\n",
       "      <th>...</th>\n",
       "      <th>utilities_index</th>\n",
       "      <th>JPYUSD</th>\n",
       "      <th>CNYUSD</th>\n",
       "      <th>EURUSD</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>month</th>\n",
       "      <th>log_apple</th>\n",
       "      <th>log_tesla</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.848478</td>\n",
       "      <td>15.114667</td>\n",
       "      <td>1.398</td>\n",
       "      <td>17050.750000</td>\n",
       "      <td>37.280872</td>\n",
       "      <td>59.642277</td>\n",
       "      <td>70.730675</td>\n",
       "      <td>39.916466</td>\n",
       "      <td>49.383331</td>\n",
       "      <td>16.221682</td>\n",
       "      <td>...</td>\n",
       "      <td>33.919159</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.157577</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>2015-10-08</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.212797</td>\n",
       "      <td>2.715666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.443027</td>\n",
       "      <td>14.712667</td>\n",
       "      <td>1.406</td>\n",
       "      <td>17084.490234</td>\n",
       "      <td>37.443344</td>\n",
       "      <td>59.913647</td>\n",
       "      <td>70.812660</td>\n",
       "      <td>40.005424</td>\n",
       "      <td>49.063110</td>\n",
       "      <td>16.117653</td>\n",
       "      <td>...</td>\n",
       "      <td>33.757267</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>1.128694</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.236442</td>\n",
       "      <td>2.688709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.325027</td>\n",
       "      <td>14.372000</td>\n",
       "      <td>1.400</td>\n",
       "      <td>17131.859375</td>\n",
       "      <td>37.488480</td>\n",
       "      <td>60.071239</td>\n",
       "      <td>71.149796</td>\n",
       "      <td>40.110584</td>\n",
       "      <td>48.429707</td>\n",
       "      <td>16.131527</td>\n",
       "      <td>...</td>\n",
       "      <td>34.057907</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.157846</td>\n",
       "      <td>1.136997</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.231793</td>\n",
       "      <td>2.665282</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.368141</td>\n",
       "      <td>14.616667</td>\n",
       "      <td>1.362</td>\n",
       "      <td>17081.890625</td>\n",
       "      <td>37.407246</td>\n",
       "      <td>59.318390</td>\n",
       "      <td>70.767113</td>\n",
       "      <td>39.867935</td>\n",
       "      <td>47.924400</td>\n",
       "      <td>16.006693</td>\n",
       "      <td>...</td>\n",
       "      <td>33.980816</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.158403</td>\n",
       "      <td>1.135551</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.233494</td>\n",
       "      <td>2.682162</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.009596</td>\n",
       "      <td>14.458667</td>\n",
       "      <td>1.280</td>\n",
       "      <td>16924.750000</td>\n",
       "      <td>37.335041</td>\n",
       "      <td>59.204559</td>\n",
       "      <td>70.047333</td>\n",
       "      <td>39.406887</td>\n",
       "      <td>48.337189</td>\n",
       "      <td>15.874920</td>\n",
       "      <td>...</td>\n",
       "      <td>33.973110</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.157920</td>\n",
       "      <td>1.138498</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>3.219260</td>\n",
       "      <td>2.671294</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>175.740005</td>\n",
       "      <td>276.040009</td>\n",
       "      <td>4.419</td>\n",
       "      <td>34907.109375</td>\n",
       "      <td>171.842941</td>\n",
       "      <td>132.695724</td>\n",
       "      <td>174.469681</td>\n",
       "      <td>72.160965</td>\n",
       "      <td>92.676109</td>\n",
       "      <td>34.854198</td>\n",
       "      <td>...</td>\n",
       "      <td>64.695755</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.137574</td>\n",
       "      <td>1.073422</td>\n",
       "      <td>2023-09-14</td>\n",
       "      <td>US</td>\n",
       "      <td>9</td>\n",
       "      <td>5.169006</td>\n",
       "      <td>5.620546</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>175.009995</td>\n",
       "      <td>274.390015</td>\n",
       "      <td>4.453</td>\n",
       "      <td>34618.238281</td>\n",
       "      <td>168.539993</td>\n",
       "      <td>131.650009</td>\n",
       "      <td>171.484985</td>\n",
       "      <td>71.565002</td>\n",
       "      <td>91.335999</td>\n",
       "      <td>34.675003</td>\n",
       "      <td>...</td>\n",
       "      <td>64.418007</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.137440</td>\n",
       "      <td>1.063717</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>US</td>\n",
       "      <td>9</td>\n",
       "      <td>5.164843</td>\n",
       "      <td>5.614551</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>177.970001</td>\n",
       "      <td>265.279999</td>\n",
       "      <td>4.462</td>\n",
       "      <td>34624.300781</td>\n",
       "      <td>169.350006</td>\n",
       "      <td>131.449997</td>\n",
       "      <td>169.619995</td>\n",
       "      <td>71.540001</td>\n",
       "      <td>92.110001</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>...</td>\n",
       "      <td>64.370003</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.137478</td>\n",
       "      <td>1.066826</td>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>US</td>\n",
       "      <td>9</td>\n",
       "      <td>5.181615</td>\n",
       "      <td>5.580786</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>179.070007</td>\n",
       "      <td>266.500000</td>\n",
       "      <td>4.521</td>\n",
       "      <td>34517.730469</td>\n",
       "      <td>169.259995</td>\n",
       "      <td>131.559998</td>\n",
       "      <td>168.770004</td>\n",
       "      <td>71.370003</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>64.019997</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.137163</td>\n",
       "      <td>1.069267</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>US</td>\n",
       "      <td>9</td>\n",
       "      <td>5.187777</td>\n",
       "      <td>5.585374</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>175.490005</td>\n",
       "      <td>262.589996</td>\n",
       "      <td>4.515</td>\n",
       "      <td>34440.878906</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>131.570007</td>\n",
       "      <td>167.089996</td>\n",
       "      <td>71.440002</td>\n",
       "      <td>90.400002</td>\n",
       "      <td>34.490002</td>\n",
       "      <td>...</td>\n",
       "      <td>64.080002</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.137069</td>\n",
       "      <td>1.068205</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>US</td>\n",
       "      <td>9</td>\n",
       "      <td>5.167582</td>\n",
       "      <td>5.570594</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1995 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Apple       Tesla  bond_rate           Dow  tech_index  \\\n",
       "0      24.848478   15.114667      1.398  17050.750000   37.280872   \n",
       "1      25.443027   14.712667      1.406  17084.490234   37.443344   \n",
       "2      25.325027   14.372000      1.400  17131.859375   37.488480   \n",
       "3      25.368141   14.616667      1.362  17081.890625   37.407246   \n",
       "4      25.009596   14.458667      1.280  16924.750000   37.335041   \n",
       "...          ...         ...        ...           ...         ...   \n",
       "1990  175.740005  276.040009      4.419  34907.109375  171.842941   \n",
       "1991  175.009995  274.390015      4.453  34618.238281  168.539993   \n",
       "1992  177.970001  265.279999      4.462  34624.300781  169.350006   \n",
       "1993  179.070007  266.500000      4.521  34517.730469  169.259995   \n",
       "1994  175.490005  262.589996      4.515  34440.878906  166.600006   \n",
       "\n",
       "      health_index  consumer_dis_index  consumer_staples_index  energy_index  \\\n",
       "0        59.642277           70.730675               39.916466     49.383331   \n",
       "1        59.913647           70.812660               40.005424     49.063110   \n",
       "2        60.071239           71.149796               40.110584     48.429707   \n",
       "3        59.318390           70.767113               39.867935     47.924400   \n",
       "4        59.204559           70.047333               39.406887     48.337189   \n",
       "...            ...                 ...                     ...           ...   \n",
       "1990    132.695724          174.469681               72.160965     92.676109   \n",
       "1991    131.650009          171.484985               71.565002     91.335999   \n",
       "1992    131.449997          169.619995               71.540001     92.110001   \n",
       "1993    131.559998          168.770004               71.370003     91.250000   \n",
       "1994    131.570007          167.089996               71.440002     90.400002   \n",
       "\n",
       "      financial_index  ...  utilities_index    JPYUSD    CNYUSD    EURUSD  \\\n",
       "0           16.221682  ...        33.919159  0.008339  0.157577  1.124000   \n",
       "1           16.117653  ...        33.757267  0.008338  0.157642  1.128694   \n",
       "2           16.131527  ...        34.057907  0.008321  0.157846  1.136997   \n",
       "3           16.006693  ...        33.980816  0.008333  0.158403  1.135551   \n",
       "4           15.874920  ...        33.973110  0.008353  0.157920  1.138498   \n",
       "...               ...  ...              ...       ...       ...       ...   \n",
       "1990        34.854198  ...        64.695755  0.006787  0.137574  1.073422   \n",
       "1991        34.675003  ...        64.418007  0.006780  0.137440  1.063717   \n",
       "1992        34.770000  ...        64.370003  0.006763  0.137478  1.066826   \n",
       "1993        34.730000  ...        64.019997  0.006775  0.137163  1.069267   \n",
       "1994        34.490002  ...        64.080002  0.006769  0.137069  1.068205   \n",
       "\n",
       "           date  country  month log_apple log_tesla time_idx  \n",
       "0    2015-10-08       US     10  3.212797  2.715666        0  \n",
       "1    2015-10-09       US     10  3.236442  2.688709        1  \n",
       "2    2015-10-12       US     10  3.231793  2.665282        2  \n",
       "3    2015-10-13       US     10  3.233494  2.682162        3  \n",
       "4    2015-10-14       US     10  3.219260  2.671294        4  \n",
       "...         ...      ...    ...       ...       ...      ...  \n",
       "1990 2023-09-14       US      9  5.169006  5.620546     1990  \n",
       "1991 2023-09-15       US      9  5.164843  5.614551     1991  \n",
       "1992 2023-09-18       US      9  5.181615  5.580786     1992  \n",
       "1993 2023-09-19       US      9  5.187777  5.585374     1993  \n",
       "1994 2023-09-20       US      9  5.167582  5.570594     1994  \n",
       "\n",
       "[1995 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.drop([\"index\"], axis=1, inplace=True)\n",
    "data['time_idx'] = data.index\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'Tesla',\n",
       " 'bond_rate',\n",
       " 'Dow',\n",
       " 'tech_index',\n",
       " 'health_index',\n",
       " 'consumer_dis_index',\n",
       " 'consumer_staples_index',\n",
       " 'energy_index',\n",
       " 'financial_index',\n",
       " 'industrial_index',\n",
       " 'material_index',\n",
       " 'real_estate_index',\n",
       " 'utilities_index',\n",
       " 'JPYUSD',\n",
       " 'CNYUSD',\n",
       " 'EURUSD']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = []\n",
    "\n",
    "for list in data_lists:\n",
    "    var.append(list[1])\n",
    "\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 5\n",
    "max_encoder_length = 60\n",
    "\n",
    "training_cut_off = data['time_idx'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cut_off],\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"Apple\",\n",
    "    group_ids = [\"country\"],\n",
    "    min_encoder_length = max_encoder_length // 2,\n",
    "    max_encoder_length = max_encoder_length,\n",
    "    min_prediction_length = 1,\n",
    "    max_prediction_length = max_prediction_length,\n",
    "    static_categoricals = [],\n",
    "    static_reals = [],\n",
    "    time_varying_known_categoricals = [],\n",
    "    time_varying_known_reals = [\"date\", \"month\"],\n",
    "    time_varying_unknown_reals = [\n",
    "        'bond_rate',\n",
    "        'Dow',\n",
    "        'tech_index',\n",
    "        'health_index',\n",
    "        'consumer_dis_index',\n",
    "        'consumer_staples_index',\n",
    "        'energy_index',\n",
    "        'financial_index',\n",
    "        'industrial_index',\n",
    "        'material_index',\n",
    "        'real_estate_index',\n",
    "        'utilities_index',\n",
    "        'JPYUSD',\n",
    "        'CNYUSD',\n",
    "        'EURUSD',\n",
    "        'log_apple',\n",
    "    ],\n",
    "    # add_relative_time_idx = True,\n",
    "    add_target_scales = True,\n",
    "    add_encoder_length = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 12:04:47 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '29ef170f72de48bfb72eb1ab88d5119b', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "2023/09/21 12:04:47 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '969b2f3d1cc54087ada8120c62cd8dbd', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "2023/09/21 12:04:47 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'c34ee3fd0b0c4d8fbfb2249c2f79ba07', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "2023/09/21 12:04:47 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '044fcea0260a40d8877f8b2390a4998d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(output=tensor([[174.2100, 174.2100, 174.2100, 174.2100, 174.2100]], device='mps:0'), x=None, index=None, decoder_lengths=None, y=(tensor([[175.7400, 175.0100, 177.9700, 179.0700, 175.4900]], device='mps:0'), None))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"stock price prediction\")\n",
    "\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# calculate baseline mean absolute error, i.e., predict the last available value from the history\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "baseline_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4460, device='mps:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174.210007</td>\n",
       "      <td>175.740005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174.210007</td>\n",
       "      <td>175.009995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174.210007</td>\n",
       "      <td>177.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>174.210007</td>\n",
       "      <td>179.070007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174.210007</td>\n",
       "      <td>175.490005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred        real\n",
       "0  174.210007  175.740005\n",
       "1  174.210007  175.009995\n",
       "2  174.210007  177.970001\n",
       "3  174.210007  179.070007\n",
       "4  174.210007  175.490005"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "baseline_pred_data = baseline_predictions.output.cpu().numpy()[0]\n",
    "baseline_real_data = baseline_predictions.y[0].cpu().numpy()[0]\n",
    "\n",
    "baseline_df = pd.DataFrame()\n",
    "baseline_df[\"pred\"] = baseline_pred_data\n",
    "baseline_df[\"real\"] = baseline_real_data\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJ0UlEQVR4nO3dd3wUdf7H8demU5LQSyR0BAUJIUhHuoiKIuh5iIhKVVCRO0/5nYLtDk8sp4iCSoDTO1H0gFMU6UVCCxCkGYqRIk1aQgKElPn9MRAMhLJJNt/Zzfv5eOyD2d3J5j2uIW9mZz7jsizLQkRERMRB/EwHEBEREbmYCoqIiIg4jgqKiIiIOI4KioiIiDiOCoqIiIg4jgqKiIiIOI4KioiIiDiOCoqIiIg4ToDpAPmRnZ3N/v37CQ0NxeVymY4jIiIi18CyLE6ePElERAR+flfeR+KVBWX//v1ERkaajiEiIiL5sHfvXqpVq3bFdbyyoISGhgL2BoaFhRlOIyIiItciJSWFyMjInN/jV+KVBeX8xzphYWEqKCIiIl7mWg7P0EGyIiIi4jgqKCIiIuI4KigiIiLiOF55DIqIiIgnZWVlkZGRYTqG1/H39ycgIKBQRoCooIiIiPxOamoq+/btw7Is01G8UsmSJalatSpBQUEFeh0VFBERkXOysrLYt28fJUuWpGLFihoG6gbLsjh79iy//fYbSUlJ1KtX76rD2K5EBUVEROScjIwMLMuiYsWKlChRwnQcr1OiRAkCAwPZvXs3Z8+eJSQkJN+vpYNkRURELqI9J/lXkL0muV6nUF5FREREpBCpoIiIiMg1q1mzJv/85z89/n1UUERERMRxVFBERESKmbNnz5qOcFUqKCIiUniyMmHDv2HnAtNJipUOHTowfPhwhg8fTnh4OBUqVOCFF17ImeVSs2ZNXnnlFR566CHCwsIYPHgwAD/88APt2rWjRIkSREZG8uSTT5KWlpbzuocPH6ZHjx6UKFGCWrVq8e9//7vItkmnGYuISOE4/BPMGgr7N4DLH57cAGVrmE5VIJZlcTojy8j3LhHo79bZRNOmTWPAgAGsWbOG+Ph4Bg8eTPXq1Rk0aBAAb7zxBqNHj2bMmDEA7Nq1i9tuu41XX32V2NhYfvvtt5ySM2XKFAAefvhh9u/fz+LFiwkMDOTJJ5/k8OHDhb+xeXC7oCxbtoxx48axbt06Dhw4wMyZM+nZs2fO85f7j/n666/zzDPPALB+/XqeffZZ1q5di7+/P7179+att96idOnS+dsKERExJzsLVr4Hi/4GWen2Y1YWrJ4It401m62ATmdkcePo7418760vd6Nk0LX/mo6MjOTtt9/G5XJRv359Nm3axNtvv51TUDp16sSf/vSnnPUHDhxI3759GTFiBAD16tXj3XffpX379nzwwQfs2bOH7777jjVr1nDzzTcDMHnyZG644YbC28grcPsjnrS0NKKiopgwYUKezx84cCDXLTY2FpfLRe/evQHYv38/Xbp0oW7duqxevZq5c+eyZcsWHn744QJtiIiIGHBkJ8TeBvNH2+Wkble46z37uXXT4PRxs/mKkZYtW+baSdCqVSt27NhBVpa9B6hZs2a51t+4cSNTp06ldOnSObdu3bqRnZ1NUlIS27ZtIyAggJiYmJyvadCgAWXKlCmS7XF7D0r37t3p3r37ZZ+vUqVKrvuzZ8+mY8eO1K5dG4BvvvmGwMBAJkyYkDPMZeLEiTRu3JidO3dSt25ddyOJiEhRy86GNZNgwUuQeRqCQuG2v0N0P/v5VR/A4S0QPwXajTSbtQBKBPqz9eVuxr53YSpVqlSu+6mpqQwZMoQnn3zyknWrV6/O9u3bC/X7u8ujx6AcOnSIOXPmMG3atJzH0tPTCQoKyjVp7vw44R9++CHPgpKenk56enrO/ZSUFA+mFhGRKzqWBLOHwe4V9v3aHey9JmUiL6zT+gn7eJTVE6HVMAgINhK1oFwul1sfs5i0evXqXPdXrVpFvXr18PfPu+g0bdqUrVu3XnbHQIMGDcjMzGTdunU5H/EkJiZy4sSJQs19OR49i2fatGmEhobSq1evnMc6derEwYMHGTduHGfPnuX48eM899xzgP3xUF7Gjh1LeHh4zi0yMjLP9URExIMsC9Z+DB+0sctJYCm4403oNyt3OQFo1BtCIyD1EPz4hZG4xc2ePXsYOXIkiYmJfPbZZ4wfP56nnnrqsus/++yzxMXFMXz4cBISEtixYwezZ89m+PDhANSvX5/bbruNIUOGsHr1atatW8fAgQOL7BpFHi0osbGx9O3bN9fFgho2bMi0adN48803KVmyJFWqVKFWrVpUrlz5svP7R40aRXJycs5t7969nowtIiIXO7EXPukJc/4EGWlQow08tgJuHgh5nRwREAQth9rLcePtj4TEox566CFOnz5N8+bNGTZsGE899VTO6cR5ady4MUuXLmX79u20a9eO6OhoRo8eTURERM46U6ZMISIigvbt29OrVy8GDx5MpUqVimJzcFnnT5LOzxe7XJecxXPe8uXLueWWW0hISCAqKirPrz906BClSpXC5XIRFhbG9OnTue+++676fVNSUggPDyc5OZmwsLD8xhcRkauxLNjwKXz/f5CeAgEloMsYaD4ErnZRuDPJ8FZDOHsSHvgCrjdzLIc7zpw5Q1JSErVq1SrQlXiLWocOHWjSpEmRjKC/miv9N3Tn97fH9qBMnjyZmJiYy5YTgMqVK1O6dGk+//xzQkJC6Nq1q6fiiIiIu1IOwH/+AP8bbpeTas1h6A/Q8rGrlxOAkHCI6W8vx433bFbxOW4f+ZOamsrOnTtz7iclJZGQkEC5cuWoXr06YDekGTNm8Oabb+b5Gu+99x6tW7emdOnSzJ8/n2eeeYbXXnutyE5dEhGRK7As+7iR756x94L4B0Gn56HVcPBz88ySlo/ZB8r+shx+XQ/XNfVMZvE5bheU+Ph4OnbsmHN/5Ej79LH+/fszdepUAKZPn45lWfTp0yfP11izZg1jxowhNTWVBg0aMGnSJPr165eP+CIiUqhSD8PXIyBxjn0/Ihp6ToRKDfL3euHVoNG98ON0iHsX7ptaWEnld5YsWWI6QqEr0DEopugYFBERD9j8X/sg2NPHwC8Q2j8LbZ8G/wKeZntwE0xsCy6/c+PvaxZKXE/w1mNQnMTxx6CIiIiXSDsKX/SHLx+xy0nlm2DwYmj/TMHLCUCVm6BOJ7CyYeX7BX89KRZUUEREirNt38D7LWDrLPsCf7f8BQYtsktFYWr9hP3nhk/g1LHCfW3xSSooIiLF0enj8NUg+LwvpP0GFW+AQQuh01/tGSaFrXZHu/RknIL4yYX/+uJzVFBERIqb7d/DhJaw6Qv7uJA2I2DIUvuAWE9xuaD1uWu+rP4QMs547nuJT1BBEREpLs4kw6xh9myT1INQvh48Og+6vlQ018ppeA+EVYO0w/Dj557/fuLVVFBERIqDXYvg/daQ8CnggpbDYOhyiLy56DL4B9pzUUDj773AL7/8gsvlIiEhwcj3V0EREfFl6anwzdPwyT2Qsg/K1oJHvoXb/g6BRXPRt1yaPgTBYXB0B+z4vui/v3gNFRQREV+VtBw+aAXxsfb9mwfZF/ir0dpcppAwaPaIvbziXXM5fNzZs2dNRygwFRQREV9z9hR89yxMuxNO7IHw6vDQ/+CONyColOl00GKoPQhuTxzsizedxid06NCB4cOHM2LECCpUqEC3bt3YvHkz3bt3p3Tp0lSuXJl+/fpx5MiRnK+ZO3cubdu2pUyZMpQvX54777yTXbt2GdyK3FRQRER8yZ7VMLGNff0bgKb97b0mtdubzfV7YRFw07kr18c5fC+KZcHZNDM3Nwe9T5s2jaCgIFasWMFrr71Gp06diI6OJj4+nrlz53Lo0CH+8Ic/5KyflpbGyJEjiY+PZ+HChfj5+XHPPfeQ7ZBjgwphRKCIiBiXcQYWvwpx7wEWhEbA3eOhbhfTyfLW+gnY+B/Y9jUc+xnK1TadKG8Zp+DvEWa+9//td2uPV7169Xj99dcBePXVV4mOjubvf/97zvOxsbFERkayfft2rr/+enr37p3r62NjY6lYsSJbt26lUaNGhbMNBaA9KCIi3m7fOpjUzj4zBguiHoDHVzq3nABUvtHOp/H3hSYmJiZneePGjSxevJjSpUvn3Bo0sC/4eP5jnB07dtCnTx9q165NWFgYNWvWBGDPnj1Fnj0v2oMiIuKtMtNh6T/gh3+ClQWlK8Od/4QGt5tOdm1aPwk7F8CGT6HDKChV3nSiSwWWtPdkmPrebihV6sLeltTUVHr06ME//vGPS9arWrUqAD169KBGjRp89NFHREREkJ2dTaNGjRxzgK0KioiINzqwEWY+Boe32Pcb3Qu3j4OS5czmcketW6BqlL0t8ZOh/V9MJ7qUy+WMA4vd1LRpU7766itq1qxJQMClv+qPHj1KYmIiH330Ee3atQPghx9+KOqYV6SPeEREvElWBix5DT7qZJeTkuXhD/+Ceyd7VzmBi8bfT4KM02bz+JBhw4Zx7Ngx+vTpw9q1a9m1axfff/89jzzyCFlZWZQtW5by5cvz4YcfsnPnThYtWsTIkSNNx85FBUVExFsc2gofd4YlYyE7E264Cx5fDTfebTpZ/t3YE8Ij4dQR2PiZ6TQ+IyIighUrVpCVlcWtt97KTTfdxIgRIyhTpgx+fn74+fkxffp01q1bR6NGjXj66acZN26c6di5uCzLzfOYHCAlJYXw8HCSk5MJCwszHUdExLOyMiHuHXvPSdZZCCkDd7wJjXrbeyG83cr34ftRUK4ODI8HP3P/dj5z5gxJSUnUqlWLkJAQYzm82ZX+G7rz+1t7UEREnOy37RDbDRa+bJeT67vDsNVw072+UU7AHn8fEg7HdkHit6bTiEOooIiIOFF2lj3TZFI7+DUegsOh5wfQ5zMIrWI6XeEKLg3NBtjLcePNZhHHUEEREXGao7tg6h0w76+QeQbqdLbnmjR5wHf2mlysxRDwD4K9q2DvGtNpxAFUUEREnCI7G1Z/CBPbwp6VEFQaerwDD34F4deZTudZoVWg8bkx7CveMZtFHEEFRUTECY7vhn/dBd89Y49Xr9kOHouDmId9d6/JxVo9Yf/50xx7L5IUayooIiImWRbET4EPWsMvy+3pobe/YV99uGwN0+mKVqUGUK8bYMHK94xG8cITXB2jsP7bqaCIiJiSvA8+7QXfjICzqVC9FQz9AZoPMnqqrVFtzg1uS/gPpB0p8m/v7+8P4Jhx797o1KlTAAQGBhbodTTqXkSkqFmW/Qt47nOQngIBIdB5NLQYCn7+ptOZVaMNRETD/g2w5iPoOKpIv31AQAAlS5bkt99+IzAwEL/iWhTzwbIsTp06xeHDhylTpkxO2csvFRQRkaJ08iB8/RRsn2vfv66ZffpwxevN5nKK8+Pvv3wE1nwIbZ6CIPcumlewb++iatWqJCUlsXv37iL7vr6kTJkyVKlS8FPhVVBERIqCZcGmL+HbP8OZE/YptR1G2b+M/fVXcS433AVlqsOJPbDxP3DzwCL99kFBQdSrV08f8+RDYGBggfecnKefChERT0v9DeY8Ddu+tu9XjYKeE6HyjWZzOZV/ALQaDt/9xR5WF/NIkX/05efnp1H3hunDNRERT9oyC95vYZcTvwDo8H8wcKHKydVEP2hfc+h4kn3asRQ7KigiIp5w6hh8+SjM6A+njkLlRjBoMXR4FvwLdnZDsRBU6sJHO3Hv2h+RSbGigiIiUth++hYmtIDNX4HLH9r92S4nVRubTuZdzo+/37cW9qwynUaKmAqKiEhhOX0CZg6F6X0g7TBUqA8D50PnFyAgyHQ671O6EkT90V7WRQSLHRUUEZHCsGMBvN8KNn4GnDtVdsgyuC7GdDLvdn78feK3cGSH2SxSpFRQREQK4kwK/O8J+HdvOLkfytWBR7+HW1+BQJ0FUmAVr4fru+OE8fdStFRQRETy6+cl9jV01v/Lvt/iMXtUffUWRmP5nJzx959B6mGzWaTIqKCIiLgrPRXm/An+dTck74UyNeDhOdD9tSKdelpsVG9lT9zNSreny0qxoIIiIuKO3XEwsQ2s/di+32wAPBYHNduazeXLXC5ofe5YlLUfw9k0s3mkSKigiIhci4zTMHcUTLkdjv8C4ZHQbxbc+RYElzadzvfd0APK1oTTx2HDv02nkSKggiIicjV718LEtrDqfcCC6H72XpM6HU0nKz78/O3x92AfLJudZTaPeJwKiojI5WScgfmjIfZWOLoTQqtC3y/h7vcgJMx0uuKnSV8oUQ5O7IZt/zOdRjxMBUVEJC/7N8CH7WHFO2BlQ+M/wuMroV5X08mKr6CS0HyQvbxC4+99nQqKiMjvZZ6FRX+DjzrDbz9BqYrwx/9Ar0lQoqzpdHLzIAgIgf3r7QOWxWepoIiInHdwE3zUCZa9DlYWNOwFj6+GBneYTibnla4IUX3s5bh3zWYRj1JBERHJyoCl4+DDjnBok32cw31T4b4pUKq86XRysVbDARdsnwu/JZpOIx6igiIixdvhbfBxF1j8KmRnQIM7YdhqaHiP6WRyORXqXtirpYsI+iwVFBEpnrKz4Id/wqRb4EAChJSBXh/B/Z/aV9EVZ2t9bvz9j5/DyUNms4hHqKCISPFzZCfEdoMFYyDrLNTrBo+vgsZ/sKeWivNVbwHVmtvv35pJptOIB6igiEjxkZ0NK9+3R9XvWwvBYXD3BHjgcwirajqduOv8RQTXTravjyQ+RQVFRIqHYz/D1Dvg+1GQeQZqd7SnwUY/qL0m3qr+7VCuNpw5ARs+NZ1GCpkKioj4tuxsWPMRfNAW9sRBYCm4823oNxPKRJpOJwXx+/H3qyZAVqbZPFKoVFBExHed2AOf9IRv/wwZaVCzHTweB80e1V4TX9HkAShZ3n6vt802nUYKkQqKiPgey4J10+D91pC0FAJKQPfX4aH/2VfEFd8RWAKaD7aXNf7ep6igiIhvSdkP/74Pvn4Szp6EyBbw2ApoMQT89FeeTzo//v5AAvyy3HQaKST6aRUR32BZsHE6vN8Sds4H/2C49VV45DsoX8d0OvGkUuXtKx2DBrf5EBUUEfF+Jw/B9L4wcwicSYaIpjB0ObR+wj6QUnxfq2GAC3bMs6cDi9dTQRER77b5K3i/BSTOAb9A6PQCDJgPFeubTiZFqXwduKGHvRz3ntksUihUUETEO6UdgS/6w5ePwunjUKUxDFkKt/wZ/ANMpxMTfj/+PuWA2SxSYCooIuJ9tn0NE1rA1lngFwDtn4NBi6ByQ9PJxKTIm6F6K/uij6snmk4jBeR2QVm2bBk9evQgIiICl8vFrFmzcj3vcrnyvI0bNy5nne3bt3P33XdToUIFwsLCaNu2LYsXLy7wxoiIjzt1DL4aBJ8/CKeOQKUbYeBC6DgK/ANNpxMnaP2E/Wf8FEg/aTaLFIjbBSUtLY2oqCgmTJiQ5/MHDhzIdYuNjcXlctG7d++cde68804yMzNZtGgR69atIyoqijvvvJODBw/mf0tExLdt/x7ebwWbvgCXH7QdCYOXQEQT08nESa7vDuXrQXoyrP+X6TRSAC7Lyv9UG5fLxcyZM+nZs+dl1+nZsycnT55k4cKFABw5coSKFSuybNky2rVrB8DJkycJCwtj/vz5dOnS5arfNyUlhfDwcJKTkwkLC8tvfBHxBmeSYe7/QcK5a61UuB56ToRqMWZziXOtmwpfPwXhkfDkBu1dcxB3fn979BiUQ4cOMWfOHAYMGJDzWPny5alfvz7/+te/SEtLIzMzk0mTJlGpUiViYvL+Cyc9PZ2UlJRcNxEpBnYutPeaJHwKuOzrrgxZpnIiV9b4j1CqIiTvhS2zTKeRfPJoQZk2bRqhoaH06tUr5zGXy8WCBQvYsGEDoaGhhISE8NZbbzF37lzKli2b5+uMHTuW8PDwnFtkpC7wJeLT0k/C1yPg016Q8qt9xdpH50K3v9mjzUWuJDAEmg+xl+Pe0fh7L+XRghIbG0vfvn0JCQnJecyyLIYNG0alSpVYvnw5a9asoWfPnvTo0YMDB/I+LWzUqFEkJyfn3Pbu3evJ2CJi0t618EFrWDfFvt98CAz9Aaq3NJtLvMvNAyCwJBzcZF+PSbyOx4YFLF++nMTERD7//PNcjy9atIhvvvmG48eP53z+9P777zN//nymTZvGc889d8lrBQcHExwc7KmoIuIU6Sdheh9I+w3KVIe734da7UynEm9UshxEPwhrPrQvIli7g+lE4iaP7UGZPHkyMTExREVF5Xr81KlT9je+6KJdfn5+ZGdneyqOiHiDFe/a5aRcbXgsTuVECqbl4/YZX7sWwsHNptOIm9wuKKmpqSQkJJCQkABAUlISCQkJ7NmzJ2edlJQUZsyYwcCBAy/5+latWlG2bFn69+/Pxo0b2b59O8888wxJSUnccccd+d8SEfFuKQdg5bkR5V1ehOBQo3HEB5SrBTfcZS+v1Ph7b+N2QYmPjyc6Opro6GgARo4cSXR0NKNHj85ZZ/r06ViWRZ8+fS75+goVKjB37lxSU1Pp1KkTzZo144cffmD27NmX7G0RkWJk8d8g4xREtrjwS0WkoNqcG3+/aQYk/2o2i7ilQHNQTNEcFBEfc2grTGwDVjY8Og+qtzCdSHzJlNth9wr7Wj23vmI6TbHmmDkoIiLXZP5ou5zccJfKiRS+8xcRXDcVzmiOlrdQQRERs35eAjvn2xf96/Ki6TTii+rdChXqQ3oKrJ9mOo1cIxUUETEnOxvmvWAvNxsA5euYzSO+yc8PWg+3l1d9AFkZZvPINVFBERFzNn0BB3+E4DBo/6zpNOLLGt8PpSrZk4k3/9d0GrkGKigiYkbGaVh47oDFtk9DqfJm84hvCwiGFufH37+r8fdeQAVFRMxYPRFS9kFYNWj5mOk0UhzcPAACS8GhzbBrkek0chUqKCJS9NKOwvK37OVOz+sCgFI0SpSFpg/Zy3HjzWaRq1JBEZGit+x1+4yKKjfZxwaIFJWWj4HLH35eDAd+NJ1GrkAFRUSK1tFdsPZje7nrK/YZFiJFpWwNaNjTXtb4e0fT3wwiUrQWvgTZmVC3C9TpaDqNFEetn7D/3PwVJO8zm0UuSwVFRIrO3jWwdbZ9hdmuGjkuhkREQ812dlFe9YHpNHIZKigiUjQsC+Y9by836QuVbzSbR4q3Nk/Zf66bCqdPmEwil6GCIiJFY9vXsHc1BJaEjn81nUaKu7pdoOINcDbVLiniOCooIuJ5WRmw4EV7udVwCKtqNI4ILteFY1FWT4TMs2bzyCVUUETE8+KnwLFdUKoitHnSdBoR2033QukqcPIAbP7SdBq5iAqKiHjWmWRY+pq93GEUBIeazSNyXkAwtBxqL8eN1/h7h1FBERHP+uGfcOooVLgemvY3nUYkt5hHIKg0HN4KOxeaTiO/o4IiIp6TvA9WvW8vd3kJ/APM5hG5WIkyF4pz3DtGo0huKigi4jmL/gaZZ6BGG6jf3XQakbydH3+ftAz2J5hOI+eooIiIZxz4ETZ+Zi/f+op91oSIE5WJhEa97GVdRNAxVFBEpPBZFsx/AbCgUW+4LsZ0IpErO3/K8ZaZcGKP2SwCqKCIiCfsXAg/LwH/IOg82nQakaurGgW12oOVpfH3DqGCIiKFKzvr3N4ToPlgKFvTaByRa3Z+Rs+6aXD6uNksooIiIoUs4T/2KZsh4dDuT6bTiFy7Op2hUkPISLOHC4pRKigiUnjOpsHiv9nLtzwDJcuZzSPijkvG36ebzVPMqaCISOFZ+b49NrxMdfvjHRFv06g3hEZA6iH48QvTaYo1FRQRKRyph2HFP+3lzmPsMeIi3iYgKPf4++xss3mKMRUUESkcS16zL10fEQ0Ne5lOI5J/MQ9DUCgcSYSd802nKbZUUESk4H7bDuum2su3vgp++qtFvFhIODR72F7W4DZj9LeIiBTcghft+RHXd4eabU2nESm4FkPBLwB+WQ6/rjedplhSQRGRgtkdB4lz7GuZdH3JdBqRwhFeDRrday9rL4oRKigikn+WBfOet5ebPgQV65vNI1KYWg+3/9w6C47/YjJJsaSCIiL5t+W/8Os6CCwFHUaZTiNSuKrcBHU6gZVtn0IvRUoFRUTyJzMdFpz7SKfNUxBa2WweEU9ofW78/YZP4NQxs1mKGRUUEcmftR/Did1QusqFXeEivqZ2B3tPSsYpiJ9sOk2xooIiIu47fRyWvm4vd/w/CCplNo+Ip7hcF/airP4QMs6YzVOMqKCIiPuWvwlnTkDFGyD6QdNpRDyr4T0QVg3SDsOPn5tOU2yooIiIe47vhtWT7OWuL4Ofv9k8Ip7mHwgtH7OXNf6+yKigiIh7Fr0CWWeh1i1Qr6vpNCJFI6Y/BIfD0R2w43vTaYoFFRQRuXa/rodNM+zlrq/Yn8+LFAfBoRfG369412iU4kIFRUSujWXB/NH2cuP7IaKJ0TgiRa7FUPALhD1xsC/edBqfp4IiItdm+/f2dUn8g6HT86bTiBS9sAi46T57OU57UTxNBUVEri4r88Lek5ZDoUx1s3lETGn9hP3ntq/h2M9ms/g4FRQRuboNn8CRRChRDtqONJ1GxJzKN0Ldrhp/XwRUUETkytJTYfHf7eX2f4ESZYzGETHu/F6UDZ9C2lGzWXyYCoqIXFnceHtAVdla0GyA6TQi5tW6BapGQeZpjb/3IBUUEbm8kwcvHAzYZQwEBJnNI+IEucbfT4KM02bz+CgVFBG5vMV/ty+SVu1muLGn6TQiznFjTwivDqeOwMbPTKfxSSooIpK3w9vsg2MBbn1VQ9lEfs8/AFo9bi/Hvafx9x6ggiIieZs/xj5TocGdUL2l6TQizhPdD0LC4dguSPzWdBqfo4IiIpdKWmZfb8QvALq8ZDqNiDMFl75w4HjceLNZfJAKiojklp0N885Nio15BCrUNZtHxMlaDAH/INi7CvauMZ3Gp6igiEhum7+EAxshKBQ6PGc6jYizhVaBxn+wl1e8YzaLj1FBEZELMs7Awpft5bYjoFQFo3FEvEKrc4PbfpoDR3eZzeJDVFBE5II1kyB5L4RGQMvHTacR8Q6VGkC9boAFK98zncZnqKCIiO3UMVj2pr3c6XkIKmk2j4g3aXNucFvCfyDtiNksPkIFRURsy8ZBejJUbgRRfzSdRsS71GgDEdGQeQbWfGQ6jU9QQRER+7Lx5/9S7foy+PmbzSPibX4//n7Nh3D2lNk8PsDtgrJs2TJ69OhBREQELpeLWbNm5Xre5XLleRs3bhwAS5Ysuew6a9euLZSNEhE3LXwZsjOgTieo29l0GhHvdMNdUKYGnD4GG/9jOo3Xc7ugpKWlERUVxYQJE/J8/sCBA7lusbGxuFwuevfuDUDr1q0vWWfgwIHUqlWLZs2aFWxrRMR9++Jhy0zABV1fMZ1GxHv5B0CrYfZy3HuQnWU2j5cLcPcLunfvTvfu3S/7fJUqVXLdnz17Nh07dqR27doABAUF5VonIyOD2bNn88QTT+DStT5EipZlXRjK1uQBqNLIbB4Rbxf9oH2RzeNJ9mnHN95lOpHX8ugxKIcOHWLOnDkMGDDgsuv873//4+jRozzyyCOejCIieflpDuxZCQEloONfTacR8X5BpeDmgfZy3Lv2PwIkXzxaUKZNm0ZoaCi9evW67DqTJ0+mW7duVKtW7bLrpKenk5KSkusmIgWUlQELxtjLrR6H8OvM5hHxFS2GgH8w7FsLe1ebTuO1PFpQYmNj6du3LyEhIXk+v2/fPr7//vsr7mEBGDt2LOHh4Tm3yMhIT8QVKV7WTYWjO6FkBWgzwnQaEd9RutKFU/VXvGs2ixfzWEFZvnw5iYmJDBw48LLrTJkyhfLly3PXXVf+jG7UqFEkJyfn3Pbu3VvYcUWKlzMpsOQ1e7nDcxASZjaPiK9pNdz+M/FbOLLDbBYv5bGCMnnyZGJiYoiKisrzecuymDJlCg899BCBgYFXfK3g4GDCwsJy3USkAFa8A6eOQPm6EPOw6TQivqfi9VD/djT+Pv/cLiipqakkJCSQkJAAQFJSEgkJCezZsydnnZSUFGbMmHHFvSeLFi0iKSnpiuuIiAek7IeV58YEdHkR/K/8DwQRyafW5y4imPAZpB42m8ULuV1Q4uPjiY6OJjo6GoCRI0cSHR3N6NGjc9aZPn06lmXRp0+fy77O5MmTad26NQ0aNMhHbBHJt0V/g8zTENkSGtxpOo2I76reCq5rBlnpGn+fDy7L8r5zoFJSUggPDyc5OVkf94i44+BmmNgWsGDgQqim4YgiHrV1NnzxEJQoC09vsU9DLsbc+f2ta/GIFCfzRwMWNLxH5USkKDS4E8rWgtPHYcO/TafxKiooIsXFrkWwayH4BULnMabTiBQPfv4Xxt+v1Ph7d6igiBQH2Vkw79xxYs0HQblaZvOIFCdN+kKJcnBiN2z7n+k0XkMFRaQ4+PFzOLQJgsPhlmdMpxEpXoJK2v8wAHtwm/cd+mmECoqIr8s4DYtetZdv+ROULGc2j0hxdPMgCAiB/ethd5zpNF5BBUXE1616H1J+hfDq0HyI6TQixVPpihB1bvRGnMbfXwsVFBFflnYElr9tL3d+AQLzvi6WiBSBVsMBF2yfC78lmk7jeCooIr5s6T/g7EmoGgWN7jWdRqR4q1AXGtxhL8eNN5vFC6igiPiqIzshPtZevvVV8NOPu4hxrZ+0//zxczh5yGwWh9PfWCK+auGLkJ0J9bpBrVtMpxERgOotoFpzyDoLayaZTuNoKigivmjPKtj2Nbj8oOvLptOIyO+1ObcXZe1kSE81m8XBVFBEfI1lwbwX7OXoflBJF+QUcZT6t0O5OnDmBGz41HQax1JBEfE1W2fDvjUQWAo6/p/pNCJysd+Pv181AbIyzeZxKBUUEV+SeRYWvGgvt34CQqsYjSMil9HkAShZHk7sgW2zTadxJBUUEV8SHwvHk6B0ZbugiIgzBZaA5oPtZY2/z5MKioivOH3CnnsC0GEUBJc2GkdEruLmQRBQAg4kwC/LTadxHBUUEV/xw9tw+hhUqG8fHCsizlaqPET3tZc1uO0SKigivuDEXlj1gb3c9WXwDzCbR0SuTcvHARfsmAeHt5lO4ygqKCK+YNGrkJUONdvB9d1MpxGRa1W+DtzQw16Oe89sFodRQRHxdgc22mOzAW59BVwus3lExD2/H3+fcsBsFgdRQRHxZjlD2Sy46T6IiDadSETcFXkzVG8F2RmweqLpNI6hgiLizXYugKSl4B8EnV4wnUZE8uv8XpT4KZB+0mwWh1BBEfFW2Vkwf7S93GIIlK1hNo+I5N/1t0H5epCeDOv/ZTqNI6igiHirhH/D4a0QUgba/cl0GhEpCD8/aD3cXl71AWRlmM3jACooIt7obBos+pu93P4vUKKs2TwiUnCN/wilKkLyXtgyy3Qa41RQRLzRygmQehDK1ICbB5pOIyKFITAEmg+xl+M0/l4FRcTbpB6GFe/Yy13GQECw2TwiUnhuHgCBJeHgj/YB8MWYCoqIt1kyFs6mwnUx0LCX6TQiUphKloPoB+3lFe+azWKYCoqIN/ltO6ybZi/f+qqGson4opaPg8sPdi2Eg5tNpzFGBUXEmywYA1YW1L8DarQ2nUZEPKFcLbjhLnt5ZfEdf6+CIuItfvkBEr8Flz90fcl0GhHxpDbnBrdtmgHJv5rNYogKiog3yM6Gec/byzEPQ4V6RuOIiIddFwM12kJ2ZrEdf6+CIuINtvwX9m+AoNLQ4TnTaUSkKLR+wv5z3VQ4k2I0igkqKCJOl5kOC899pNNmBJSuZDSOiBSRerdChfqQngLrp5lOU+RUUEScbs2HcGIPhFaFVsNMpxGRolLMx9+roIg42aljsGycvdzxrxBU0mweESlaje+H0pUh5VfY/F/TaYqUCoqIky1/E84kQ6WG0OQB02lEpKgFBNtXK4diN/5eBUXEqY7/Yn+8A9D1ZfDzNxpHRAxp9igEloJDm2HXItNpiowKiohTLXwZss5C7Q5Qt7PpNCJiSomy0PQhezluvNksRUgFRcSJfl0Hm78CXND1FY20FynuWj5mD2n8eTEc+NF0miKhgiLiNJYF816wl6P+CFUbm80jIuaVrQENe9rLxWT8vQqKiNMkfge7V0BACHR63nQaEXGK84PbNn8FyfvMZikCKigiTpKVaV8QEOxduuHVzOYREeeIiIaa7ezx96s+MJ3G41RQRJxk/TQ4sh1Kloe2T5tOIyJO0+Yp+891U+H0CZNJPE4FRcQp0k/CkrH2cvtnISTcbB4RcZ66XaDiDXA21S4pPkwFRcQpVrwLab9BudoQ84jpNCLiRC7XhWNRVk+EzLNm83iQCoqIE6QcuHBkfpcXISDIaBwRcbCb7rOvzXXyAGz+0nQaj1FBEXGCxX+DjFMQ2QJuuMt0GhFxsoCg342/H++z4+9VUERMO7QVEv5tL2som4hci5hHIKg0HN4KOxeaTuMRKigips0fDVa2veekegvTaUTEG5QoA03728tx7xiN4ikqKCIm/bwEds4HvwD72BMRkWt1fvx90jLYn2A6TaFTQRExJTv7wkj7ZgOgfB2zeUTEu5SJhEa97WUfvIigCoqIKZu+gIM/QnCYPfdERMRd50853jITTuwxm6WQqaCImJBxGha+Yi+3fRpKlTebR0S8U9XGULsDWFk+N/5eBUXEhNUTIWUfhFWzP0cWEcmv83tR1k2D08fNZilEKigiRS3tKCx/y17u9DwEljCbR0S8W53OULkRZKRB/BTTaQqNCopIUVv2OqSnQJWboPH9ptOIiLe7ZPx9utk8hUQFRaQoHd0Faz+2l7u+An76ERSRQtCwF4RGQOoh2DTDdJpCob8dRYrSwpcgOxPqdoU6HU2nERFfERB04Xi2uPH2GAMv53ZBWbZsGT169CAiIgKXy8WsWbNyPe9yufK8jRs3Ltd6c+bMoUWLFpQoUYKyZcvSs2fPgmyHiPPtXQNbZ4PLD7q+bDqNiPiamP4QFAq//WQPgPRybheUtLQ0oqKimDBhQp7PHzhwINctNjYWl8tF7969c9b56quv6NevH4888ggbN25kxYoVPPDAA/nfChGnsyyY97y93KQvVL7RbB4R8T0h4dDsYXvZBwa3uSwr/5dBdLlczJw584p7P3r27MnJkydZuNC+mFFmZiY1a9bkpZdeYsCAAfn6vikpKYSHh5OcnExYWFi+XkOkSG39H3zRDwJLwhPrIayq6UQi4ouS98E7UfZHyYMWw3VNTSfKxZ3f3x49BuXQoUPMmTMnVxFZv349v/76K35+fkRHR1O1alW6d+/O5s2bL/s66enppKSk5LqJeI2sDFjwor3carjKiYh4Tng1aHSvvezle1E8WlCmTZtGaGgovXr1ynns559/BuDFF1/k+eef55tvvqFs2bJ06NCBY8eO5fk6Y8eOJTw8POcWGRnpydgihSt+ChzbBaUqQpsnTacREV93/pTjrbPg+C8mkxSIRwtKbGwsffv2JSQkJOex7HNHFv/1r3+ld+/exMTEMGXKFFwuFzNm5H1q1KhRo0hOTs657d2715OxRQrPmWRY+pq93GEUBIeazSMivq9KI6jTCaxsWPm+6TT55rGCsnz5chITExk4cGCux6tWtXdv33jjhYMEg4ODqV27Nnv25H2ho+DgYMLCwnLdRLzCD/+EU0ehwvXQtL/pNCJSXLQ+t7d2wydwKu9PJ5zOYwVl8uTJxMTEEBUVlevxmJgYgoODSUxMzHksIyODX375hRo1angqjkjRS94Hq87966XLS+AfYDaPiBQftTvY06ozTkH8ZNNp8sXtgpKamkpCQgIJCQkAJCUlkZCQkGvvR0pKCjNmzLhk7wlAWFgYQ4cOZcyYMcybN4/ExEQee8weLnPfffflczNEHGjR3yDzDNRoA/W7m04jIsWJy3VhL8rqDyHjjNk8+eD2P+ni4+Pp2PHCBMyRI0cC0L9/f6ZOnQrA9OnTsSyLPn365Pka48aNIyAggH79+nH69GlatGjBokWLKFu2bD42QcSBDm6CjZ/Zy7e+Yv9lISJSlBreAwtesq+c/uPn9iA3L1KgOSimaA6KON6/esLPi6FRb7g31nQaESmu4t6DeX+F8vVg2Brj1/9yzBwUkWJp5wK7nPgHQefRptOISHEW0x+Cw+HoDtjxvek0blFBESlM2Vkw71wpaT4YytY0GkdEirng0Avj71e8azSKu1RQRArTxs/g8Bb7mhjt/mQ6jYgItHgM/AJhTxzsized5pqpoIgUlrOnYNGr9vItz0DJcmbziIiAfXmNxn+wl+O8Zy+KCopIYVk1AU4egDLV7Y93REScotVw+89tX8Oxn81muUYqKCKFIfU3+OEde7nzGAgINptHROT3Kt8Idbt61fh7FRSRwrD0NTh7EiKioWGvq68vIlLUzl9EcMOnkHbUbJZroIIiUlBHdthXLAa49VXjcwZERPJU6xaoGgWZp71i/L3+JhUpqAUvgpUF13eHmm1NpxERyVuu8feTIOO02TxXoYLye9nZsHQcHP/FdBLxFrvj4KdvwOUPXV8ynUZE5Mpu7Anh1eHUkQuX43AoFZTf27kAFr8K70bDFw/B3rWmE4mTWRbMe8FebvoQVKxvNo+IyNX4B0Crx+3luPfsf5g7lArK75UsD3U62Uc5b50Nk7vAx13t5ews0+nEabbMhF/jIbAUdBhlOo2IyLWJ7mcPkzy2CxK/NZ3mslRQfq9aDPSbCY/FQZMH7Wup7Ftj7015NxpWTYT0VNMpxQky02HhuY902jwFoZXN5hERuVbBpaHZAHs5brzZLFeggpKXyg2h5wQYsRna/RlKlIUTu2Hus/D2jTB/DCT/ajqlmLR2sn2sUukq0Hq46TQiIu5pMcT+R/jeVbB3jek0eVJBuZLQytD5BXh6K9zxJpSrA2eSYcU/4Z3G8N/BcGCj6ZRS1E6fgGWv28sd/w+CShmNIyLittAqjh9/r4JyLYJKws0DYXg8/PEzqNEGsjPhx89h0i0w9U5InOvog42kEC1/E04fh4o3QPSDptOIiORPq3OD27Z9A0d3mc2SBxUUd/j5QYPb4ZFvYdBiaHSvfXrpL8vhs/vh/Rb2wC6Hn1suBXBijz0/AKDry+DnbzaPiEh+VWoA9boBFqx8z3SaS6ig5Nd1TeHeyTDiR3t8cHAYHNkO34yAtxvC4r/b12cR37LwFchKtycy1utqOo2ISMG0OTe4LeE/kHbEbJaLqKAUVHg1e7z501ug29hzA3COwtJ/2EVl9nA4/JPplFIY9m+ATV/Yy11fsacyioh4sxptIKIpZJ6BNR+ZTpOLCkphCQmzh988uQHunQLXxdj/0t7wif3Rz6f3wq7F9nAv8T6/H8rW+H6IaGI0johIoXC5LlxEcO1HcPaU2Ty/o4JS2PwDoFEvGLgQHv0ebugBuGDnfPikJ0xsa+9KyzxrOqm4Y8c8+1gj/2Do9LzpNCIiheeGu6BMDXvv/8b/mE6TQwXFU1wuqN4S7v8UnlwPzQdDYEk4tBlmPQb/vMk+G+TUMdNJ5WqyMmH+aHu55VAoU91sHhGRwuQfAK2G2ctx7zlmcroKSlEoVxtuHwcjt0LnMRBaFVIPwsKX7eNU5vzZkad4yTkJn8JvP0GJctB2pOk0IiKFL/pBeyjp8ST4aY7pNIAKStEqURbajYSnfoR7JkHlmyDjlP253/gYmN4Xdq/UcSpOkp5qn5EF0P4vUKKM0TgiIh4RVMqe9wX24DYH/B5SQTEhIAii/ghDl8ND/4N6twIW/PQNTLkNPuoEm7+yP1oQs1a+B6mHoGytC9euEBHxRc0H28fZ7VsLe1ebTqOCYpTLBbXbQ98Z8PhqaNrf/p9j/3r48lF4t4n9eeCZFNNJi6eTh2DFuRHQXcbYxVJExFeVrmT/4xku/N1nkAqKU1RqAHe9a89T6TAKSlaA5L0w76/w1o3w/V/hxF7TKYuXJX+HjDSodjPc2NN0GhERz2t17uKnid/CkR1Go6igOE3pitDhOXh6M/R4FyrUh7Mn7Y8a3omy96z8us50St93+CdY/y97+dZXNZRNRIqHitdD/dtxwvh7FRSnCiwBMf3h8VXwwAyo1R6sLPvYlI86QWx3+wJPDjkdzOcsGANWNjS40z5dXESkuGh9bvz9ib1GL4LrsiwHHKrrppSUFMLDw0lOTiYsLMx0nKJz4EdYOQE2f2lfTRnsU5hbPg5NHrCPwpaCS1oO0+4EvwD72KAKdU0nEhEpOpYFh7dC5YaF/tLu/P5WQfFGKfthzYcQHwtnku3HSpSFZo/aR2GHVjGbz5tlZ8NHHeFAAtw8CO54w3QiERGfoYJSXKSn2mPzV02A47/Yj/kFwk332VMBqzQyGs8r/TgD/jsQgkLhqQQoVcF0IhERn+HO728dg+LNgktDi8HwxHp7pH5kS8jOsK+lMLEN/Otu2LHAEQN3vELGGXu6L0DbESonIiIGBZgOIIXAz9++KOENPWBfvH3k9dbZ8PMS+1axgb1H5aY/QGCI6bTOteZDSN4DoRH2cT0iImKMPuLxVcd3w+pJsH4anE21HytV0T6u4uYB2jtwsVPH7MF4Z5Lh7vchuq/pRCIiPkcf8QiUrQG3/d2+QGHXVyDsOkj7zR4+9nZD+Pop+G276ZTOsewNu5xUbnRhkqKIiBijPSjFRVaG/bFP3Hj7DJXzrr/N/vinZrviO4zsWBK8d7N9/M6D/4W6nU0nEhHxSdqDIpfyD4Sb7oXBS+Dhb89NCnTB9rkwrQd82B5+/MIuMsXNwpftclKnk8qJiIhDaA9KcXZkJ6x63z5VOfO0/VhoBLQYAjEPQ4kyJtMVjX3r4ONOgAuG/qBTs0VEPEh7UOTaVKgLd75lH6fS6XkoVQlO7rfHvL91I3z37IX5Kr7IsmDe8/ZykwdUTkREHER7UOSCzHTY9KV9mvLhrfZjLj/7ejStn4DI5mbzFbaf5sD0ByCgBDyxDsKvM51IRMSnaQ+K5E9AsH167WNx9sGidTrbF8zb9j+Y3BU+7gJbZvnGBQqzMmD+GHu51eMqJyIiDqOCIpdyueyDRfv9Fx5bCdEPgn8Q7FsLM/rDu9GwaiKknzSdNP/WTYWjO6BkBWgzwnQaERG5iD7ikWtz8hCs/di+nT5mPxYcDs0ehuZDvGsPxJkUu2SdOgK3vwHNB5lOJCJSLOgjHil8oZWh01/h6S1wx1tQvi6kJ8OKd+CdxvDVINifYDrltVnxjl1Oyte1z1YSERHHUUER9wSVtEflD1sLfaZDjbaQnQmbvrBnqUy9ExLnQna26aR5S9kPKyfYy11etOfDiIiI4+higZI/fn5Qv7t927/B/qW/+b/wy3L7Vr6effBpVB8ILGE67QWL/mbPfKneyj47SUREHEnHoEjhSd5nX6Bw3TT74x+AkuWh2QD7OI/SlczmO7gZJrYFLBi4EKo1M5tHRKSY0TEoYkZ4Nbj1FRi5BW57DcpUh1NHYdnr9gUKZw+Dw9vM5Zs/GrCg4T0qJyIiDqc9KOI5WZnw0zf24Ld9ay88XreLfYHC2h2L7gKFuxbBJ/eAXyAMXwvlahXN9xURkRzagyLO4B8ADXvCwAXw6Dy44S57Mu3OBXZZ+KANbPi3PcHWk7KzYN5oe7n5IJUTEREvoIIiRaN6C7j/E3ukfPMhEFgKDm+B2Y/DP2+CZW/AqWOe+d4/fg6HNtlzW255xjPfQ0RECpUKihStcrXh9tft41S6vAihVSH1ECx6xT5OZc6f4Oiuwvt+Gadh0av28i1/gpLlCu+1RUTEY1RQxIwSZaHt0/DUj3DPh1DlJsg4ZU+qHR8Dnz0Au+PsKw4XxKr3IeVXCK9u77kRERGvoIIiZgUEQdT9MGQ59P8a6nUDLEicA1O6w0cd7SssZ2W6/9ppR2D52/Zy5xcgMKRQo4uIiOeooIgzuFxQ6xbo+4U9pTbmYQgIsYfAfTUA3m0CcePhTPK1v+bSf8DZk1A1Chrd66nkIiLiATrNWJwr7QisnQxrPrSvnQMQFApNH4KWQ+05K5dzZCe838Iew9//a7v8iIiIUTrNWHxDqQrQ4Vn7AoU93oUK9e09IqsmwDtNYMYjsG9d3l+78EW7nNTrpnIiIuKF3C4oy5Yto0ePHkREROByuZg1a1au510uV563cePG5axTs2bNS55/7bXXCrwx4qMCQyCmPzy+Cvp+CbU7gJUFW/4LH3eC2Ntg2zf2vBOAPatg29f2zJWuLxuNLiIi+eP2xQLT0tKIiori0UcfpVevXpc8f+DAgVz3v/vuOwYMGEDv3r1zPf7yyy8zaNCgnPuhoaHuRpHixs8P6nW1bwc3wcr3YdMM2LPSvpWrDS0ft+eeAET3g0oNzGYWEZF8cbugdO/ene7du1/2+SpVquS6P3v2bDp27Ejt2rVzPR4aGnrJuiLXrMpNcM8H0Hm0fYxKfCwc+xm+/bP9fGAp6Ph/ZjOKiEi+efQYlEOHDjFnzhwGDBhwyXOvvfYa5cuXJzo6mnHjxpGZefnTSNPT00lJScl1EwEgrCp0GQMjt8Ltb0DZc2Psb/kzhKoAi4h4K7f3oLhj2rRphIaGXvJR0JNPPknTpk0pV64ccXFxjBo1igMHDvDWW2/l+Tpjx47lpZde8mRU8XZBpezr7DR7FJL3QpkaphOJiEgBFOg0Y5fLxcyZM+nZs2eezzdo0ICuXbsyfvz4K75ObGwsQ4YMITU1leDg4EueT09PJz39wgXlUlJSiIyM1GnGIiIiXsSd04w9tgdl+fLlJCYm8vnnn1913RYtWpCZmckvv/xC/fr1L3k+ODg4z+IiIiIivsljx6BMnjyZmJgYoqKirrpuQkICfn5+VKpUyVNxRERExIu4vQclNTWVnTt35txPSkoiISGBcuXKUb26PdkzJSWFGTNm8Oabb17y9StXrmT16tV07NiR0NBQVq5cydNPP82DDz5I2bJlC7ApIiIi4ivcLijx8fF07Ngx5/7IkSMB6N+/P1OnTgVg+vTpWJZFnz59Lvn64OBgpk+fzosvvkh6ejq1atXi6aefznkdEREREV2LR0RERIqErsUjIiIiXk0FRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcJ8B0ACexLIvTGVmmY4iIiDhCiUB/XC6Xke+tgvI7pzOyuHH096ZjiIiIOMLWl7tRMshMVdBHPCIiIuI42oPyOyUC/dn6cjfTMURERByhRKC/se+tgvI7LpfL2K4sERERuUAf8YiIiIjjqKCIiIiI46igiIiIiOOooIiIiIjjqKCIiIiI46igiIiIiOOooIiIiIjjqKCIiIiI47hdUJYtW0aPHj2IiIjA5XIxa9asXM+7XK48b+PGjbvktdLT02nSpAkul4uEhIT8boOIiIj4GLcLSlpaGlFRUUyYMCHP5w8cOJDrFhsbi8vlonfv3pes+5e//IWIiAj3U4uIiIhPc3uue/fu3enevftln69SpUqu+7Nnz6Zjx47Url071+Pfffcd8+bN46uvvuK7775zN4aIiIj4MI9eeObQoUPMmTOHadOmXfL4oEGDmDVrFiVLlrzq66Snp5Oenp5zPyUlpdCzioiIiHN49CDZadOmERoaSq9evXIesyyLhx9+mKFDh9KsWbNrep2xY8cSHh6ec4uMjPRUZBEREXEAj+5BiY2NpW/fvoSEhOQ8Nn78eE6ePMmoUaOu+XVGjRrFyJEjc+4nJydTvXp17UkRERHxIud/b1uWddV1PVZQli9fTmJiIp9//nmuxxctWsTKlSsJDg7O9XizZs3o27fvJR8HAQQHB+da//wGak+KiIiI9zl58iTh4eFXXMdlXUuNudwXu1zMnDmTnj17XvLcww8/zObNm4mPj8/1+J49e3Lt+di/fz/dunXjyy+/pEWLFlSrVu2q3zc7O5v9+/cTGhqKy+XKb/w8paSkEBkZyd69ewkLCyvU13YCbZ/38/Vt9PXtA9/fRm2f9/PUNlqWxcmTJ4mIiMDP78pHmbi9ByU1NZWdO3fm3E9KSiIhIYFy5cpRvXp1wN6wGTNm8Oabb17y9efXOa906dIA1KlT55rKCYCfn981r5tfYWFhPvs/Hmj7fIGvb6Ovbx/4/jZq+7yfJ7bxantOznO7oMTHx9OxY8ec++ePDenfvz9Tp04FYPr06ViWRZ8+fdx9eRERERH3C0qHDh2uenDL4MGDGTx48DW9Xs2aNa/pYBkREREpPnQtnosEBwczZsyYSw7i9RXaPu/n69vo69sHvr+N2j7v54RtLNBBsiIiIiKeoD0oIiIi4jgqKCIiIuI4KigiIiLiOCooIiIi4jjFsqBMmDCBmjVrEhISQosWLVizZs0V158xYwYNGjQgJCSEm266iW+//baIkuaPO9s3depUXC5Xrtvvr53kNMuWLaNHjx5ERETgcrmYNWvWVb9myZIlNG3alODgYOrWrZszr8eJ3N2+JUuWXPL+uVwuDh48WDSB3TR27FhuvvlmQkNDqVSpEj179iQxMfGqX+dNP4P52UZv+jn84IMPaNy4cc4Ar1atWvHdd99d8Wu86f1zd/u86b3Ly2uvvYbL5WLEiBFXXM/Ee1jsCsrnn3/OyJEjGTNmDOvXrycqKopu3bpx+PDhPNePi4ujT58+DBgwgA0bNtCzZ0969uzJ5s2bizj5tXF3+8CeFHjgwIGc2+7du4swsXvS0tKIiopiwoQJ17R+UlISd9xxBx07diQhIYERI0YwcOBAvv/+ew8nzR93t++8xMTEXO9hpUqVPJSwYJYuXcqwYcNYtWoV8+fPJyMjg1tvvZW0tLTLfo23/QzmZxvBe34Oq1Wrxmuvvca6deuIj4+nU6dO3H333WzZsiXP9b3t/XN3+8B73ruLrV27lkmTJtG4ceMrrmfsPbSKmebNm1vDhg3LuZ+VlWVFRERYY8eOzXP9P/zhD9Ydd9yR67EWLVpYQ4YM8WjO/HJ3+6ZMmWKFh4cXUbrCBVgzZ8684jp/+ctfrIYNG+Z67P7777e6devmwWSF41q2b/HixRZgHT9+vEgyFbbDhw9bgLV06dLLruNtP4MXu5Zt9OafQ8uyrLJly1off/xxns95+/tnWVfePm99706ePGnVq1fPmj9/vtW+fXvrqaeeuuy6pt7DYrUH5ezZs6xbt44uXbrkPObn50eXLl1YuXJlnl+zcuXKXOsDdOvW7bLrm5Sf7QP7+ko1atQgMjLyqv9S8Dbe9P4VRJMmTahatSpdu3ZlxYoVpuNcs+TkZADKlSt32XW8/T28lm0E7/w5zMrKYvr06aSlpdGqVas81/Hm9+9atg+8870bNmwYd9xxxyXvTV5MvYfFqqAcOXKErKwsKleunOvxypUrX/Yz+4MHD7q1vkn52b769esTGxvL7Nmz+fTTT8nOzqZ169bs27evKCJ73OXev5SUFE6fPm0oVeGpWrUqEydO5KuvvuKrr74iMjKSDh06sH79etPRrio7O5sRI0bQpk0bGjVqdNn1vOln8GLXuo3e9nO4adMmSpcuTXBwMEOHDmXmzJnceOONea7rje+fO9vnbe8d2NfLW79+PWPHjr2m9U29h25fi0d8S6tWrXL9y6B169bccMMNTJo0iVdeecVgMrkW9evXp379+jn3W7duza5du3j77bf55JNPDCa7umHDhrF582Z++OEH01E85lq30dt+DuvXr09CQgLJycl8+eWX9O/fn6VLl172l7i3cWf7vO2927t3L0899RTz5893/MG8xaqgVKhQAX9/fw4dOpTr8UOHDlGlSpU8v6ZKlSpurW9SfrbvYoGBgURHR7Nz505PRCxyl3v/wsLCKFGihKFUntW8eXPH/9IfPnw433zzDcuWLaNatWpXXNebfgZ/z51tvJjTfw6DgoKoW7cuADExMaxdu5Z33nmHSZMmXbKuN75/7mzfxZz+3q1bt47Dhw/TtGnTnMeysrJYtmwZ7733Hunp6fj7++f6GlPvYbH6iCcoKIiYmBgWLlyY81h2djYLFy687OeLrVq1yrU+wPz586/4eaQp+dm+i2VlZbFp0yaqVq3qqZhFypvev8KSkJDg2PfPsiyGDx/OzJkzWbRoEbVq1brq13jbe5ifbbyYt/0cZmdnk56enudz3vb+5eVK23cxp793nTt3ZtOmTSQkJOTcmjVrRt++fUlISLiknIDB99Cjh+A60PTp063g4GBr6tSp1tatW63BgwdbZcqUsQ4ePGhZlmX169fPeu6553LWX7FihRUQEGC98cYb1rZt26wxY8ZYgYGB1qZNm0xtwhW5u30vvfSS9f3331u7du2y1q1bZ/3xj3+0QkJCrC1btpjahCs6efKktWHDBmvDhg0WYL311lvWhg0brN27d1uWZVnPPfec1a9fv5z1f/75Z6tkyZLWM888Y23bts2aMGGC5e/vb82dO9fUJlyRu9v39ttvW7NmzbJ27Nhhbdq0yXrqqacsPz8/a8GCBaY24Yoee+wxKzw83FqyZIl14MCBnNupU6dy1vH2n8H8bKM3/Rw+99xz1tKlS62kpCTrxx9/tJ577jnL5XJZ8+bNsyzL+98/d7fPm967y7n4LB6nvIfFrqBYlmWNHz/eql69uhUUFGQ1b97cWrVqVc5z7du3t/r3759r/S+++MK6/vrrraCgIKthw4bWnDlzijixe9zZvhEjRuSsW7lyZev222+31q9fbyD1tTl/Wu3Ft/Pb1L9/f6t9+/aXfE2TJk2soKAgq3bt2taUKVOKPPe1cnf7/vGPf1h16tSxQkJCrHLlylkdOnSwFi1aZCb8Nchr24Bc74m3/wzmZxu96efw0UcftWrUqGEFBQVZFStWtDp37pzzy9uyvP/9c3f7vOm9u5yLC4pT3kOXZVmWZ/fRiIiIiLinWB2DIiIiIt5BBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHEcFRURERBxHBUVEREQcRwVFREREHOf/AdfjV6IC3sJyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 30.1k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator = 'cpu',\n",
    "    gradient_clip_val = 0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate = 0.03,\n",
    "    hidden_size = 16,\n",
    "    attention_head_size = 1,\n",
    "    dropout = 0.1,\n",
    "    hidden_continuous_size = 8,\n",
    "    loss = QuantileLoss(),\n",
    "    optimizer = \"Ranger\"\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 12:05:10 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '929256065adb4ba793908cc9f3f6a9a8', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:28<00:00,  3.54it/s]\n",
      "Learning rate set to 0.07943282347242808\n",
      "Restoring states from the checkpoint path at /Users/woojin/Documents/github/projects/stock/.lr_find_8e3fa060-563b-4aa2-8f6b-fe1f36950366.ckpt\n",
      "Restored all states from the checkpoint at /Users/woojin/Documents/github/projects/stock/.lr_find_8e3fa060-563b-4aa2-8f6b-fe1f36950366.ckpt\n",
      "Finding best initial lr:   6%|▌         | 6/100 [03:35<56:16, 35.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate : 0.07943282347242808\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders = train_dataloader,\n",
    "    val_dataloaders = val_dataloader,\n",
    "    max_lr = 10.0,\n",
    "    min_lr = 1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate : {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network : 46.3k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor = \"val_loss\", min_delta = 1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = 30,\n",
    "    accelerator = 'cpu',\n",
    "    enable_model_summary = True,\n",
    "    gradient_clip_val = 0.23,\n",
    "    limit_train_batches = 50,\n",
    "    callbacks = [lr_logger, early_stop_callback],\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate = 0.03,\n",
    "    hidden_size = 19,\n",
    "    attention_head_size = 2,\n",
    "    dropout = 0.22,\n",
    "    hidden_continuous_size = 12,\n",
    "    loss = QuantileLoss(),\n",
    "    log_interval = 10,\n",
    "    optimizer = \"Ranger\",\n",
    "    reduce_on_plateau_patience = 4\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network : {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 12:05:45 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '27c24e9580f74e13b5569f0334d35fbb', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "Missing logger folder: lightning_logs/lightning_logs\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 504   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.9 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 22.0 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.0 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.6 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.6 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.6 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.6 K \n",
      "11 | lstm_encoder                       | LSTM                            | 3.0 K \n",
      "12 | lstm_decoder                       | LSTM                            | 3.0 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 760   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 38    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.9 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 798   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.6 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 798   \n",
      "20 | output_layer                       | Linear                          | 140   \n",
      "----------------------------------------------------------------------------------------\n",
      "46.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.3 K    Total params\n",
      "0.185     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 31/31 [00:10<00:00,  3.04it/s, v_num=0, train_loss_step=0.877, val_loss=1.110, train_loss_epoch=0.965]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 31/31 [00:10<00:00,  2.92it/s, v_num=0, train_loss_step=0.877, val_loss=1.110, train_loss_epoch=0.965]\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders = train_dataloader,\n",
    "    val_dataloaders = val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperprameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that MPS is available\n",
    "# if not torch.backends.mps.is_available():\n",
    "#     if not torch.backends.mps.is_built():\n",
    "#         print(\"MPS not available because the current PyTorch install was not \"\n",
    "#               \"built with MPS enabled.\")\n",
    "#     else:\n",
    "#         print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "#               \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "# else:\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "\n",
    "#     # Create a Tensor directly on the mps device\n",
    "#     x = torch.ones(5, device=mps_device)\n",
    "#     # Or\n",
    "#     x = torch.ones(5, device=\"mps\")\n",
    "\n",
    "#     # Any operation happens on the GPU\n",
    "#     y = x * 2\n",
    "\n",
    "#     # Move your model to mps just like any other device\n",
    "#     model = YourFavoriteNet()\n",
    "#     model.to(mps_device)\n",
    "\n",
    "#     # Now every call runs on the GPU\n",
    "#     pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-21 12:11:38,688] A new study created in memory with name: no-name-0bc25c62-3b45-4175-9c8f-0f3f207bac9f\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023/09/21 12:11:38 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8788135d24084287b4299bcd8e0ba9cf', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "[W 2023-09-21 12:11:39,112] Trial 0 failed with parameters: {'gradient_clip_val': 0.06330455600009514, 'hidden_size': 9, 'dropout': 0.2182811914154805, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.09113306340496463} because of the following error: NotImplementedError(\"The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py\", line 199, in objective\n",
      "    trainer.fit(model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py\", line 571, in safe_patch_function\n",
      "    patch_function(call_original, *args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py\", line 250, in patch_with_managed_run\n",
      "    result = patch_function(original, *args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/pytorch/_lightning_autolog.py\", line 386, in patched_fit\n",
      "    result = original(self, *args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py\", line 552, in call_original\n",
      "    return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py\", line 487, in call_original_fn_with_event_logging\n",
      "    original_fn_result = original_fn(*og_args, **og_kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py\", line 549, in _original_fn\n",
      "    original_result = original(*_og_args, **_og_kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 532, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 571, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 980, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1021, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1050, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py\", line 181, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 376, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 294, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 393, in validation_step\n",
      "    return self.model.validation_step(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py\", line 630, in validation_step\n",
      "    log, out = self.step(x, y, batch_idx)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py\", line 777, in step\n",
      "    out = self(x, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\", line 421, in forward\n",
      "    static_embedding, static_variable_selection = self.static_variable_selection(static_embedding)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\", line 328, in forward\n",
      "    var_outputs.append(self.single_variable_grns[name](variable_embedding))\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\", line 235, in forward\n",
      "    residual = self.resample_norm(residual)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\", line 117, in forward\n",
      "    x = self.resample(x)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\", line 53, in forward\n",
      "    return self.interpolate(x)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\", line 46, in interpolate\n",
      "    upsampled = F.interpolate(x.unsqueeze(1), self.output_size, mode=\"linear\", align_corners=True).squeeze(1)\n",
      "  File \"/Users/woojin/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py\", line 3954, in interpolate\n",
      "    return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors)\n",
      "NotImplementedError: The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "[W 2023-09-21 12:11:39,119] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb Cell 39\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_forecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtemporal_fusion_transformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtuning\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize_hyperparameters\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# create study\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m study \u001b[39m=\u001b[39m optimize_hyperparameters(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     val_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model_path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39moptuna_test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     n_trials \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     max_epochs \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     gradient_clip_val_range \u001b[39m=\u001b[39;49m (\u001b[39m0.01\u001b[39;49m, \u001b[39m1.0\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     hidden_size_range \u001b[39m=\u001b[39;49m (\u001b[39m8\u001b[39;49m, \u001b[39m128\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     hidden_continuous_size_range \u001b[39m=\u001b[39;49m (\u001b[39m8\u001b[39;49m, \u001b[39m128\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     attention_head_size_range \u001b[39m=\u001b[39;49m (\u001b[39m1\u001b[39;49m, \u001b[39m4\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     learning_rate_range \u001b[39m=\u001b[39;49m (\u001b[39m0.001\u001b[39;49m, \u001b[39m0.1\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     dropout_range \u001b[39m=\u001b[39;49m (\u001b[39m0.1\u001b[39;49m, \u001b[39m0.3\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     trainer_kwargs \u001b[39m=\u001b[39;49m \u001b[39mdict\u001b[39;49m(limit_train_batches \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     reduce_on_plateau_patience \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     use_learning_rate_finder \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# from google.colab import drive\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# drive.mount('./content/drive')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# import os\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# with open(\"tft_tesla.plk\", \"wb\") as fout:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#     pickle.dump(study. fout)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:207\u001b[0m, in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(train_dataloaders, val_dataloaders, model_path, max_epochs, n_trials, timeout, gradient_clip_val_range, hidden_size_range, hidden_continuous_size_range, attention_head_size_range, dropout_range, learning_rate_range, use_learning_rate_finder, trainer_kwargs, log_dir, study, verbose, pruner, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m study \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m, pruner\u001b[39m=\u001b[39mpruner)\n\u001b[0;32m--> 207\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m study\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/optuna/study/study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     _optimize(\n\u001b[1;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    452\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:199\u001b[0m, in \u001b[0;36moptimize_hyperparameters.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    196\u001b[0m     model\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_loguniform(\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mlearning_rate_range)\n\u001b[1;32m    198\u001b[0m \u001b[39m# fit\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39;49mval_dataloaders)\n\u001b[1;32m    201\u001b[0m \u001b[39m# report result\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mcallback_metrics[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:571\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     patch_function\u001b[39m.\u001b[39mcall(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    570\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    573\u001b[0m session\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m try_log_autologging_event(\n\u001b[1;32m    576\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    577\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     kwargs,\n\u001b[1;32m    582\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:250\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     managed_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[1;32m    249\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     result \u001b[39m=\u001b[39m patch_function(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    252\u001b[0m     \u001b[39m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/pytorch/_lightning_autolog.py:386\u001b[0m, in \u001b[0;36mpatched_fit\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[1;32m    379\u001b[0m         __MLflowPLCallback(\n\u001b[1;32m    380\u001b[0m             client, metrics_logger, run_id, log_models, log_every_n_epoch, log_every_n_step\n\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    382\u001b[0m     ]\n\u001b[1;32m    384\u001b[0m client\u001b[39m.\u001b[39mflush(synchronous\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 386\u001b[0m result \u001b[39m=\u001b[39m original(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m early_stop_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     _log_early_stop_metrics(early_stop_callback, client, run_id)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    550\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    480\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    481\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         og_kwargs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    489\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    490\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    491\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m         og_kwargs,\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    546\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ):\n\u001b[0;32m--> 549\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1021\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1023\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1052\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 294\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    297\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:393\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    392\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:630\u001b[0m, in \u001b[0;36mBaseModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    629\u001b[0m     x, y \u001b[39m=\u001b[39m batch\n\u001b[0;32m--> 630\u001b[0m     log, out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(x, y, batch_idx)\n\u001b[1;32m    631\u001b[0m     log\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_log(x, y, out, batch_idx))\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step_outputs\u001b[39m.\u001b[39mappend(log)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:777\u001b[0m, in \u001b[0;36mBaseModel.step\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 777\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    779\u001b[0m     \u001b[39m# calculate loss\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     prediction \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:421\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_variables) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    419\u001b[0m     \u001b[39m# static embeddings will be constant over entire batch\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     static_embedding \u001b[39m=\u001b[39m {name: input_vectors[name][:, \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_variables}\n\u001b[0;32m--> 421\u001b[0m     static_embedding, static_variable_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatic_variable_selection(static_embedding)\n\u001b[1;32m    422\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     static_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m    424\u001b[0m         (x_cont\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mhidden_size), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    425\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:328\u001b[0m, in \u001b[0;36mVariableSelectionNetwork.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    326\u001b[0m         variable_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprescalers[name](variable_embedding)\n\u001b[1;32m    327\u001b[0m     weight_inputs\u001b[39m.\u001b[39mappend(variable_embedding)\n\u001b[0;32m--> 328\u001b[0m     var_outputs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msingle_variable_grns[name](variable_embedding))\n\u001b[1;32m    329\u001b[0m var_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(var_outputs, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[39m# calculate variable weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:235\u001b[0m, in \u001b[0;36mGatedResidualNetwork.forward\u001b[0;34m(self, x, context, residual)\u001b[0m\n\u001b[1;32m    232\u001b[0m     residual \u001b[39m=\u001b[39m x\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual:\n\u001b[0;32m--> 235\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresample_norm(residual)\n\u001b[1;32m    237\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:117\u001b[0m, in \u001b[0;36mResampleNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size:\n\u001b[0;32m--> 117\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresample(x)\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_add:\n\u001b[1;32m    120\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask) \u001b[39m*\u001b[39m \u001b[39m2.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:53\u001b[0m, in \u001b[0;36mTimeDistributedInterpolation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msize()) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolate(x)\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Squash samples and timesteps into a single axis\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     x_reshape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))  \u001b[39m# (samples * timesteps, input_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:46\u001b[0m, in \u001b[0;36mTimeDistributedInterpolation.interpolate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minterpolate\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m     upsampled \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable:\n\u001b[1;32m     48\u001b[0m         upsampled \u001b[39m=\u001b[39m upsampled \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39m2.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:3954\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3952\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3953\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3954\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_linear1d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3956\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path = \"optuna_test\",\n",
    "    n_trials = 10,\n",
    "    max_epochs = 50,\n",
    "    gradient_clip_val_range = (0.01, 1.0),\n",
    "    hidden_size_range = (8, 128),\n",
    "    hidden_continuous_size_range = (8, 128),\n",
    "    attention_head_size_range = (1, 4),\n",
    "    learning_rate_range = (0.001, 0.1),\n",
    "    dropout_range = (0.1, 0.3),\n",
    "    trainer_kwargs = dict(limit_train_batches = 30),\n",
    "    reduce_on_plateau_patience = 4,\n",
    "    use_learning_rate_finder = False\n",
    ")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('./content/drive')\n",
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/Colab Notebook')\n",
    "\n",
    "# with open(\"tft_tesla.plk\", \"wb\") as fout:\n",
    "#     pickle.dump(study. fout)\n",
    "\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4125)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean absolute error on validation set\n",
    "# predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "predictions = tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb Cell 43\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# raw predictions are dictionary from which all kind including quantiles can be extracted\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m raw_predictions \u001b[39m=\u001b[39m tft\u001b[39m.\u001b[39;49mpredict(val_dataloader, return_x\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:1423\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, data, mode, return_index, return_decoder_lengths, batch_size, num_workers, fast_dev_run, return_x, return_y, mode_kwargs, trainer_kwargs, write_interval, output_dir, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mpytorch_lightning\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msetLevel(logging\u001b[39m.\u001b[39mWARNING)\n\u001b[1;32m   1422\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(fast_dev_run\u001b[39m=\u001b[39mfast_dev_run, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_kwargs)\n\u001b[0;32m-> 1423\u001b[0m trainer\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m, dataloader)\n\u001b[1;32m   1424\u001b[0m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mlightning\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msetLevel(log_level_lighting)\n\u001b[1;32m   1425\u001b[0m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m\"\u001b[39m\u001b[39mpytorch_lightning\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msetLevel(log_level_pytorch_lightning)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:852\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    851\u001b[0m _verify_strategy_supports_compile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 852\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    853\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    854\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:894\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, predict_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    891\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    892\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    893\u001b[0m )\n\u001b[0;32m--> 894\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    896\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/prediction_loop.py:112\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    113\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/loops/prediction_loop.py:229\u001b[0m, in \u001b[0;36m_PredictionLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    228\u001b[0m \u001b[39m# configure step_kwargs\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m predictions \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mpredict_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 294\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    297\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:413\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpredict_step_context():\n\u001b[1;32m    412\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, PredictStep)\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:625\u001b[0m, in \u001b[0;36mBaseModel.predict_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    623\u001b[0m predict_callback \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(c, PredictCallback)][\u001b[39m0\u001b[39m]\n\u001b[1;32m    624\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[0;32m--> 625\u001b[0m _, out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(x, y, batch_idx, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpredict_callback\u001b[39m.\u001b[39;49mpredict_kwargs)\n\u001b[1;32m    626\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:777\u001b[0m, in \u001b[0;36mBaseModel.step\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 777\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    779\u001b[0m     \u001b[39m# calculate loss\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     prediction \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:421\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_variables) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    419\u001b[0m     \u001b[39m# static embeddings will be constant over entire batch\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     static_embedding \u001b[39m=\u001b[39m {name: input_vectors[name][:, \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_variables}\n\u001b[0;32m--> 421\u001b[0m     static_embedding, static_variable_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatic_variable_selection(static_embedding)\n\u001b[1;32m    422\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     static_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m    424\u001b[0m         (x_cont\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mhidden_size), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    425\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:328\u001b[0m, in \u001b[0;36mVariableSelectionNetwork.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    326\u001b[0m         variable_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprescalers[name](variable_embedding)\n\u001b[1;32m    327\u001b[0m     weight_inputs\u001b[39m.\u001b[39mappend(variable_embedding)\n\u001b[0;32m--> 328\u001b[0m     var_outputs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msingle_variable_grns[name](variable_embedding))\n\u001b[1;32m    329\u001b[0m var_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(var_outputs, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[39m# calculate variable weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:235\u001b[0m, in \u001b[0;36mGatedResidualNetwork.forward\u001b[0;34m(self, x, context, residual)\u001b[0m\n\u001b[1;32m    232\u001b[0m     residual \u001b[39m=\u001b[39m x\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual:\n\u001b[0;32m--> 235\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresample_norm(residual)\n\u001b[1;32m    237\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:117\u001b[0m, in \u001b[0;36mResampleNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size:\n\u001b[0;32m--> 117\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresample(x)\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_add:\n\u001b[1;32m    120\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask) \u001b[39m*\u001b[39m \u001b[39m2.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:53\u001b[0m, in \u001b[0;36mTimeDistributedInterpolation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msize()) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolate(x)\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Squash samples and timesteps into a single axis\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     x_reshape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))  \u001b[39m# (samples * timesteps, input_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py:46\u001b[0m, in \u001b[0;36mTimeDistributedInterpolation.interpolate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minterpolate\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m     upsampled \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable:\n\u001b[1;32m     48\u001b[0m         upsampled \u001b[39m=\u001b[39m upsampled \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39m2.0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:3954\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3952\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3953\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3954\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_linear1d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3956\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# raw predictions are dictionary from which all kind including quantiles can be extracted\n",
    "# raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "raw_predictions = tft.predict(val_dataloader, return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb Cell 44\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# for idx in range(10):\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/woojin/Documents/github/projects/stock/tft_tesla.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tft\u001b[39m.\u001b[39;49mplot_prediction(predictions\u001b[39m.\u001b[39;49mx, predictions\u001b[39m.\u001b[39;49moutput, idx\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, add_loss_to_title\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:711\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.plot_prediction\u001b[0;34m(self, x, out, idx, plot_attention, add_loss_to_title, show_future_observed, ax, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39mPlot actuals vs prediction and attention\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39m    plt.Figure: matplotlib figure\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# plot prediction as normal\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m fig \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mplot_prediction(\n\u001b[1;32m    712\u001b[0m     x,\n\u001b[1;32m    713\u001b[0m     out,\n\u001b[1;32m    714\u001b[0m     idx\u001b[39m=\u001b[39;49midx,\n\u001b[1;32m    715\u001b[0m     add_loss_to_title\u001b[39m=\u001b[39;49madd_loss_to_title,\n\u001b[1;32m    716\u001b[0m     show_future_observed\u001b[39m=\u001b[39;49mshow_future_observed,\n\u001b[1;32m    717\u001b[0m     ax\u001b[39m=\u001b[39;49max,\n\u001b[1;32m    718\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    719\u001b[0m )\n\u001b[1;32m    721\u001b[0m \u001b[39m# add attention on secondary axis\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m plot_attention:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:987\u001b[0m, in \u001b[0;36mBaseModel.plot_prediction\u001b[0;34m(self, x, out, idx, add_loss_to_title, show_future_observed, ax, quantiles_kwargs, prediction_kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[39mPlot prediction of prediction vs actuals\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39m    matplotlib figure\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39m# all true values for y of the first sample in batch\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m encoder_targets \u001b[39m=\u001b[39m to_list(x[\u001b[39m\"\u001b[39;49m\u001b[39mencoder_target\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    988\u001b[0m decoder_targets \u001b[39m=\u001b[39m to_list(x[\u001b[39m\"\u001b[39m\u001b[39mdecoder_target\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    990\u001b[0m y_raws \u001b[39m=\u001b[39m to_list(out[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# raw predictions - used for calculating loss\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# for idx in range(10):\n",
    "# best_tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=0, add_loss_to_title=True)\n",
    "tft.plot_prediction(raw_predictions.y, raw_predictions.output,idx=0, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worst performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
